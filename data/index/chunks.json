[
  {
    "id": 0,
    "text": "Sector Rotation by Factor Model and Fundamental Analysis\nRunjia Yang1 and Beining Shi2\n1University of California, Davis\n2University of California, Davis\nSept 2023\nAbstract\nThisstudypresentsananalyticalapproachtosectorrotation,leveragingbothfactormodelsandfundamental\nmetrics. We initiate with a systematic classification of sectors, followed by an empirical investigation into\ntheir returns. Through factor analysis, the paper underscores the significance of momentum and short-term\nreversion in dictating sectoral shifts. A subsequent in-depth fundamental analysis evaluates metrics such\nas PE, PB, EV-to-EBITDA, Dividend Yield, among others. Our primary contribution lies in developing a\npredictiveframeworkbasedonthesefundamentalindicators. Theconstructedmodels,postrigoroustraining,\nexhibit noteworthy predictive capabilities. The findings furnish a nuanced understanding of sector rotation\nstrategies, with implications for asset management and portfolio construction in the financial domain."
  },
  {
    "id": 1,
    "text": "strategies, with implications for asset management and portfolio construction in the financial domain.\nKeywords: US Industrial Sectors, Factor Analysis, Fundamental Analysis, Trading Strategy.\n1 Introduction\nSector is composed by a basket of stocks that representing companies in certain business class, which has\nunique features according to the business. Under certain conditions, such as economic cycles, sectors may\nbehave accordingly due to the different characteristics of businesses. In this report, we are exploring how to\ncapture returns by finding the hidden features behind different sectors and determining the leading sectors\ninsomeparticularmarketconditionsorsocialenvironments. Generally,thisreportcoversabriefexploration\nofmarketandfundamentalfactors, explainingthemeaningofeachfactorsandhowtheyarerelatedtosome\nsectors.Then we applied a neural network model to do a classification and prediction using the fundamental"
  },
  {
    "id": 2,
    "text": "ofmarketandfundamentalfactors, explainingthemeaningofeachfactorsandhowtheyarerelatedtosome\nsectors.Then we applied a neural network model to do a classification and prediction using the fundamental\nfactors as inputs. At the end of the report, we also covers how sectors behaved under global events.\n2 Sector Classification and Return Analysis\nThere are many different ways to divide sectors. For the purpose of common acceptance and convenience\nfor future data acquirement, we used the MSCI Global Industry Classification Standard, which includes 11\nlevel one sectors, 24 level two industry groups, 69 level three industries, and 158 sub-industries. We use the\n11 level one sectors as our main target. They are Energy, Materials, Industrials, Consumer Discretionary,\nConsumerStaples,HealthCare,Financials,InformationTechnology,CommunicationServices,Utilities,and\nReal Estate. In order to track the performance of each sector, we use the S&P500 GICS Indices which"
  },
  {
    "id": 3,
    "text": "ConsumerStaples,HealthCare,Financials,InformationTechnology,CommunicationServices,Utilities,and\nReal Estate. In order to track the performance of each sector, we use the S&P500 GICS Indices which\nare constructed exactly as the MSCI classification.Before working on any strategies further, we need to\ndetermineifthereareactuallypossibleprofits. Inourcase,weneedtocheckhowbigthedifferencesbetween\n(cid:80)\nsectors’ returns are. For each observation time period, Define: Return Difference =( Sector Return\ntop3\n(cid:80)\n- Sector Return )/3 Based on a monthly frequency, we calculate the return difference and get the\nbottom3\nfollowing plot.\n1\n3202\nvoN\n81\n]MP.nif-q[\n1v10000.1042:viXra"
  },
  {
    "id": 4,
    "text": "Also, by calculation, the quarterly return difference has a mean of 0.1306, median of 0.1185, standard\ndeviation of 0.0523. We can reach to a result that half of the quarterly return difference is more than\n11.85%. It is easy to see that there does exist potential investment opportunity by capturing the return\ndifferences between sectors.\n3 Factor Analysis\n3.1 Momentum Factor Exploration\nMomentumpremiumwasfirstrecognizedbyUCLAscholarsNarasimhanJegadeeshandSheridanTitmanin\n1993. Themomentumpremiumisestablishedontheobservationthatassetsthathaveperformedwellinthe\npast have the trend to persist good performance in the future. Though the momentum effect is considered\nto be a market anomaly, it has been recognized widely among many asset classes. We will explore the\nmomentum effect based on the sector indices introduced above.\n3.1.1 Factor Construction\nFirst of all, we need to construct the momentum factor. Typically, the momentum factor is constructed by"
  },
  {
    "id": 5,
    "text": "momentum effect based on the sector indices introduced above.\n3.1.1 Factor Construction\nFirst of all, we need to construct the momentum factor. Typically, the momentum factor is constructed by\nthe past 6 or 12 months cumulative return and excludes the most recent month’s return, considering that\nthere are also short-term reversion effects based on the mean-reversion effects. However, without a clear\nidea of how the sector indices carry the momentum effect, we need to explore through time intervals to find\nthe best possible momentum factor. Then we constructed 12 different momentum factors using the past 1\nto 12 month’s return and excluding the most recent 0.1 portion trading days of each time period to avoid\nshort-term reversion. For each of the factors with the period of n months\n(cid:88) (cid:88)\nMOM nM = R − R\nd d\nPastn∗21days Past0.1∗n∗21days\nwhere R is the daily return.\nd\n3.1.2 Calculate Factor Returns"
  },
  {
    "id": 6,
    "text": "(cid:88) (cid:88)\nMOM nM = R − R\nd d\nPastn∗21days Past0.1∗n∗21days\nwhere R is the daily return.\nd\n3.1.2 Calculate Factor Returns\nFor the 12 factors we got, we normalized them cross sections. Then we rank the factor exposures for each\nsector and take long positions of sectors with the highest two factor exposures, take short position of sectors\nwith the lowest two factor exposures. Then we trade our portfolio under a monthly frequency. Here are the\nresults from 2002 to 2022 February:\n2"
  },
  {
    "id": 7,
    "text": "Factor Factor Return Sharpe Ratio\nMOM 1M −0.0297 −0.09\nMOM 2M −0.0583 −0.19\nMOM 3M 0.0082 0.02\nMOM 4M −0.0245 −0.08\nMOM 5M −0.0136 −0.04\nMOM 6M 0.0195 0.06\nMOM 7M 0.0535 0.15\nMOM 8M 0.0721 0.21\nMOM 9M 0.0413 0.11\nMOM 10M −0.0470 −0.13\nMOM 11M 0.0371 0.10\nMOM 12M 0.0327 0.01\nSince there are several market crashes where the momentum factor led to negative returns, we also take a\nlook at the most recent five years from 2017 to 2022 February:\nFactor Factor Return Sharpe Ratio\nMOM 1M 0.0213 0.06\nMOM 2M −0.0494 −0.14\nMOM 3M 0.1154 0.29\nMOM 4M −0.1578 0.41\nMOM 5M −0.1788 0.47\nMOM 6M 0.1163 0.35\nMOM 7M 0.2119 0.62\nMOM 8M 0.1869 0.56\nMOM 9M 0.1583 0.43\nMOM 10M 0.0504 0.12\nMOM 11M 0.1547 0.40\nMOM 12M 0.1289 0.33\nFromthistable, wecantellthatbyusingtheMOM 7Mfactor, wecanreachamaximumannualreturnrate\nof21.19%andamaximumSharperatioof0.62. Itisalsointerestingthatwefindthemomentumfactorwith\na short time period, for example, MOM 1M and MOM 2M, have a very small even negative return rate."
  },
  {
    "id": 8,
    "text": "of21.19%andamaximumSharperatioof0.62. Itisalsointerestingthatwefindthemomentumfactorwith\na short time period, for example, MOM 1M and MOM 2M, have a very small even negative return rate.\nHowever, it exactly conforms to the short term reversion effect that the typical momentum factor would\nexclude.\n3.2 Short Term Reversion Factor Exploration\nShort term reversion factor follows the simple principle that asset’s price will have the trend to stay on an\naveragelevel. Sincewecanseefromthepreviousresultsofthemomentumfactorthattheredoesexistshort\nterm reversion effect, we can try different reversion factors and find out what would be the best short term\nreversion observation period.\n3.2.1 Factor Construction\nSimilarly, we can define several reversion factors with different time periods. And we take the negative\nnumber of the past n days cumulative return as the factor exposures.\n(cid:88)\nREV nD =− R\nd\nPastndays"
  },
  {
    "id": 9,
    "text": "number of the past n days cumulative return as the factor exposures.\n(cid:88)\nREV nD =− R\nd\nPastndays\nFor the purpose of exploring the optimal time period, we take 5-day time interval and create 12 reversion\nfactors from 5 trade days to 55 trade days.\n3"
  },
  {
    "id": 10,
    "text": "3.2.2 Calculate Factor Returns\nBy using the same method, we compute the rank of each sector’s factor exposure, and long the top two\nsectors, short the last two sectors on a monthly observation frequency. Between 2002 and 2022 February,\nthe results are:\nFactor Factor Return Sharpe Ratio\nRev 5D −0.0059 −0.0597\nRev 10D 0.0294 0.3320\nRev 15D 0.0127 0.1338\nRev 20D 0.0072 0.0715\nRev 25D 0.0690 0.7768\nRev 30D 0.0877 0.8735\nRev 35D 0.0135 0.1288\nRev 40D −0.0314 −0.3422\nRev 45D −0.0694 −0.8417\nRev 50D −0.0183 −0.5919\nRev 55D −0.0517 −0.1863\nFromthistable,wecantellthatforthetimebetween2002torecenttime,theshorttermreversioneffect\nis optimal for taking the past 30 days cumulative return. It has an optimal annual return rate of 8.77% on\naverage and leads to a sharp ratio of 0.8735.\n4 Fundamental Analysis\nFundamental Analysis are always a good aspect to look at for investing. We collected quarterly data for"
  },
  {
    "id": 11,
    "text": "average and leads to a sharp ratio of 0.8735.\n4 Fundamental Analysis\nFundamental Analysis are always a good aspect to look at for investing. We collected quarterly data for\nall 11 indices from Bloomberg, including their P/E ratio, EV/EBIT, Profit Margin, etc. Our fundamental\nanalysiswouldstartfromdiscoveringfeaturesforeachofthefundamentalratio,thenwearetryingtopredict\nthe sector performance by constructing using some of the features we found.\n4.1 PE Ratio\nThe P/E is one of the most widely used tools to determine a stock’s relative valuation. The purpose of\nanalyzing the ratio is to show whether certain sector is worth to be invested because P/E ratio can reflect\nthe investment risk in this sector. The figure below shows distribution of P/E ratios in different sectors.\n4"
  },
  {
    "id": 12,
    "text": "Bycomparingcross-sectionaldata,itisobviousthatP/EofRealEstateSectorandConsumerDiscretionary\nSector are higher than others. The reason is that earning growth in the future is expected to grow fast in\nthetwosectorsorthesesectorshavesomespecialadvantagesthatguaranteelong-termprofitabilitywithlow\nrisk. On the other hand, Financials Sector’s ratio is relatively low compared with other sectors, which may\nresult from its high volatility so investors are reluctant to pay for it.\nWealsonoticethattheratioinEnergySectorsurgedin2015,whichisrelatedtosomechangesinthesector.\nThe end of the oil age and emergence of alternative energy have reduced the earnings of the original sector.\nAsaresult,itsrelativepricebecomeshigherthanbefore. ThechangeinEV/EBITandEV/EBITDAisalso\ndue to this reason.\n4.2 PB Ratio\nThe P/B ratio provides a valuable reality check for investors who are seeking growth at a reasonable price."
  },
  {
    "id": 13,
    "text": "due to this reason.\n4.2 PB Ratio\nThe P/B ratio provides a valuable reality check for investors who are seeking growth at a reasonable price.\nFor those sectors with more assets, their book value and market value are close, so P/B ratio is more useful\nwhen we analyze Real Estate sector and Financials sector. The figure below shows distribution of P/B ratio\nin different sectors.\nAs the picture shows, Consumer Discretionary sector and IT sector have higher P/B ratio while Financials\nsector and Energy sector have relatively low ratios. What’s more, Real Estate sector with high P/E ratio\nhas relatively lower P/B ratio.\n4.3 EV/Sales\nEV/Sales can help investors better understand cost relative to unit sales and whether the company is\novervaluedorundervalued. IfEV/Salesisrelativelyhigh,thecompanyorsectorislessattractivetoinvestors.\nThe figure below shows distribution of EV/Sales in different sectors.\n5"
  },
  {
    "id": 14,
    "text": "TheresultshowsthatRealEstatesector’sratioishigherthanothers’,whichmeansthatindexinthissector\nis overvalued. On the other hand, ratio in Energy sector is low, which can attract more investors.\n4.4 EV/EBIT & EV/EBITDA\nEV/EBIT and EV/EBITDA are independent of the capital structure of the company, whereas multiples\nlike P/E ratio are impacted by financing decisions. Because of this reason, the two are the most commonly\nrelied-upon multiples in relative valuation. However, one obvious distinction is that EV/EBIT considers\ndepreciation and amortization. In some capital-intensive industries which have significant differences in\nD&A, EV/EBIT may make it a more accurate measure of value. But in our analysis, there is no such\ndifference in the comparison of these two ratios under different sectors.\n6"
  },
  {
    "id": 15,
    "text": "The results can show that Real Estate sector has a higher ratio. The three ratio EV/Sales, EV/EBIT and\nEV/EBITDA can give a consensus conclusion that Real Estate sector is overvalued in the market.\n4.5 Dividend Yield\nDividend Yield is used to measure the amount of cash flow investors are getting back for each dollar. It is\nessentiallythereturnoninvestmentforastockwithoutanycapitalgains. Thefigurebelowshowsdistribution\nof Dividend yield in different sectors.\nThe ratio in Communication Services sector is higher before 2018 while Energy sector’s ratio is higher after\nthat time. This is because communication services sector took place a reorganization of S&P500 index in\n2018. It now includes at least eighteen companies from IT and Consumer Discretionary sectors. Due to this\nreshuffling, dividend yield of this sector is impacted.\n7"
  },
  {
    "id": 16,
    "text": "4.6 Gross Margin\nGross margin equals net sales less the cost of goods sold (COGS). Net sales are equivalent to the total\nrevenue from sales, and COGS is the direct cost associated with producing goods. By calculating gross\nmargin, we could measure one company’s retain revenue after subtracting the production cost. The higher\nthegrossmargin,themorecapitalacompanyretains,whichitcanthenusetopayothercostsorsatisfydebt\nobligations. Generally, companies with good gross margins would have a relatively sustainable competitive\nadvantage. By analyzing gross margin data across sectors, we may observe some sectors that have more\nstable development in the long run.\nFor our 11 sectors’ gross margin data, the line chart above shows significant differences between the sectors.\nOverall, each industry index is relatively flat on its own, and have gaps between each others. Utilities,\nCommunication Services and Information Technology(IT) have been among the top spears for last 10 years,"
  },
  {
    "id": 17,
    "text": "Communication Services and Information Technology(IT) have been among the top spears for last 10 years,\noccupyingthefirst,secondandthirdpositionsrespectively,allabove40%. Onthecontrary,theenergysector\nhas been an under-performer for the past decade, ranking at the bottom, with gross margins consistently\nbelow 20%. Gross margins in the rest industries are concentrated in the 25%-35% range and have not\nfluctuate much.\nAtthesametime,byobservingthecomparisonoffluctuationsbetweenindustries,itisnotdifficulttoseethat\nthe gross margin fluctuations of the energy industry and the utilities industry maybe relatively high in the\npastdecade,andtheirpeakscorrespondtoeachother. During2016,theutilitiesindustrygrewsignificantly,\nwhile energy declined comparatively. The trend was even more pronounced in 2020, with utilities reaching\nits highest level and the energy industry fell to the bottom.\nGenerally, the gross margin feature maybe a significant indicator for Utilities, Communication Services and"
  },
  {
    "id": 18,
    "text": "its highest level and the energy industry fell to the bottom.\nGenerally, the gross margin feature maybe a significant indicator for Utilities, Communication Services and\nIT sectors. And our conjecture about the correlation between utilities and energy sectors will need further\nobservation and verification.\n4.7 Operating Margin & Profit Margin:\nOperating margin equals operating income divided by revenue, it is a profitability ratio measuring revenue\nafter covering operating and non-operating expenses of a business. And profit margin measures the profit\nratio after paying for variable costs of production. It is calculated by the formula:\n(Total Revenue - Total Expenses )\nProfit Margin=\nTotal Revenue\n8"
  },
  {
    "id": 19,
    "text": "Both operating margin and profit margin are used to gauge the degree of the company’s activity makes\nmoney. Higher ratios are generally better, illustrating the company is efficient in its operations and is good\natturningsalesintoprofits. Inouranalysis, thereisnotaverybigdifferenceinthecomparisonofthesetwo\nratios under different sectors, which is determined by their definition.\nFor these two ratios, Real Estate sector, IT sector and Financial sector have the top three high ratios.And\nEnergy sector has the relatively lowest ratio. Also, both operating margin and profit margin for almost all\nsectors have similar trends in the last decade curves. This is attributed to the definition difference between\nthe two features, and that’s why the operating margin was slightly higher than the profit margin.\nAnother thing that is worth to mentioning is that for Energy Sector, not just operating margin and profit"
  },
  {
    "id": 20,
    "text": "Another thing that is worth to mentioning is that for Energy Sector, not just operating margin and profit\nmargin, but also the gross margin, it always has the relatively lowest ratios and similar curve fluctuation,\nwith sharp declines in 2016 and 2020. The two time nodes may consistent with some big revolution in the\nenergy industry, which we will analyze later.\n9"
  },
  {
    "id": 21,
    "text": "4.8 Return on Asset & Return on Equity\nReturnonequity(ROE)andreturnonassets(ROA)aretwoofthemostimportantmeasuresforevaluating\nhoweffectivelyacompany’smanagementteamisdoingitsjobofmanagingthecapitalentrustedtoit. ROE\nequals to generally net income divided by equity, while Return on Assets (ROA) is net income divided by\naverage assets. So the primary differentiator between ROE and ROA is financial leverage or debt. ROE\nmeasures profitability and ROA is an efficiency measure of how well a company is using its assets. Investors\nmay prefer to observe ROE, since equity represents the owner’s interest in the business. Compared to other\nsources of fund, equity capital tends to be the most expensive source of funding and carries the largest risk\npremium of all financing options. Therefore, in our analysis, ROE may be a better feature that it could\nreflect the trend of market investment.\nAs shown in the picture, IT sector has the highest ROA, the Consumer Staples sector and Consumer"
  },
  {
    "id": 22,
    "text": "reflect the trend of market investment.\nAs shown in the picture, IT sector has the highest ROA, the Consumer Staples sector and Consumer\nDiscretionary sector also have a relatively higher ratio. In contrast, Financial sector has a lower ROA. The\npast ten years, or even twenty years, has been an era of rapid development of information technology. And\ncompared with traditional industry and commerce, information technology is more flexible in the time and\nform of investment assets, that’s the reason why IT will have the highest ratio. Also for the the Consumer\nStaples sector and Consumer Discretionary sector,they are all industries with fast innovation and short\nproduction cycle. Generally, these three will have constantly higher ratio for the long run. Therefore, for\nthesethreeindustries,iftheROAindicatorfluctuatessignificantly,itmayhaveanimpactontheinvestment\ntrend.\n10"
  },
  {
    "id": 23,
    "text": "For ROE ratio, similarly, IT, Consumer Staples stay high, and Consumer Discretionary sectors is also at\na slightly higher level, except that the IT sector lost its prominence in ROA ratio. By comparing cross-\nsectional data, the Consumer Discretionary Sector and Industrials Sector have similar patterns in the last\ndecade for both ROA and ROE ratios. They both have a low peak in 2020. It is conceivable that this is\naffectedbythegeneralenvironmentoftheepidemic. Andaswementionedbefore,theROEandROAcurves\nof the energy sector still have a similar pattern, falling sharply in 2016 and 2020. In 2016, it was affected\nby changes in energy policy since 2015, reducing oil production while encouraging the development of clean\nand new energy. For 2020, we attribute this decline to the outbreak of the COVID-19 pandemic.\n5 Prediction by Fundamental Factors\n5.1 Factor and Future Return\nHaving these fundamental data, next step is to find out what quantitative relationships they have to futures"
  },
  {
    "id": 24,
    "text": "5 Prediction by Fundamental Factors\n5.1 Factor and Future Return\nHaving these fundamental data, next step is to find out what quantitative relationships they have to futures\nsector returns. For fundamental factors, they are usually exposed in the company report with annual, semi-\nannual, or quarterly frequency. Our fundamental factors for each sector are reported quarterly, leading to\na problem that the sample size for each individual sector is very small. To have a better performance of\nthe prediction model, we need to combine all the sectors together and make a uniformed and comparable\nlarge sample. We neutralized each factor cross-sectional for the factor to have a mean of 0 and standard\ndeviation of 1. If X denotes one specific factor exposure for i-th sector at time t, in this case would be at\ni,t\nt-th quarter, then for each individual t we have the neutralized exposure to be:\nX −Mean(X ,i from 1 to 11)\nX -Neutral= i,t i,t\ni,t Standard Deviation(X ,i from 1 to 11)\ni,t"
  },
  {
    "id": 25,
    "text": "i,t\nt-th quarter, then for each individual t we have the neutralized exposure to be:\nX −Mean(X ,i from 1 to 11)\nX -Neutral= i,t i,t\ni,t Standard Deviation(X ,i from 1 to 11)\ni,t\nThen we used the next quarter’s cross-sectional normalized return as the corresponding return. First, we\nwanttohaveageneralviewoftherelations. Thescatterplotsbetweenneutralizedfactorsandfuturereturns\nare as following:\n11"
  },
  {
    "id": 26,
    "text": "12"
  },
  {
    "id": 27,
    "text": "From the scatter plots, the relations between all factors and their future returns cannot be well interpreted\nby simple linear models. However, it is very common in the financial field that the sample will have a very\nlow signal-noise ratio.\n5.2 Model Construction and Training\nAsweobservedbefore,therelationsbetweeneachfactoranditsfuturereturncannotbeinterpretedverywell\nby linear models. Also, we have no idea what model would exactly best fit the data. Therefore, converting\npredictionoffuturereturnstoaclassificationproblemandfittingthetrainingsamplewithaneuralnetwork\nmodel which has comparably good performance with non-linear relations would be a great start point.\nInput Hidden Hidden Output\nlayer layer 1 layer 2 layer\nx\n1\nx\n2\nx\n3\nh(1) h(2)\n1 1\nx\n4\nh(1) h(2)\n2 2\nx 5 yˆ 1\nh(1) h(2)\n3 3\nx 6 yˆ 2\nh(1) h(2)\n4 4\nx\n7\nh(1) h(2)\n5 5\nx\n8\nx\n9\nx 0\n1\nNeural network takes a vector as the input, and goes to each of the neuron in the first hidden layer and"
  },
  {
    "id": 28,
    "text": "4\nh(1) h(2)\n2 2\nx 5 yˆ 1\nh(1) h(2)\n3 3\nx 6 yˆ 2\nh(1) h(2)\n4 4\nx\n7\nh(1) h(2)\n5 5\nx\n8\nx\n9\nx 0\n1\nNeural network takes a vector as the input, and goes to each of the neuron in the first hidden layer and\ngains new activation vectors which act as the input for next hidden layer. After the last hidden layer,\nneural network model would pass out the probability for each of the prediction class and we choose the one\nwith the highest probability as the prediction. This process is called front propagation. After comparing\nthe prediction to the actual results, we adjust the weights of the nodes by using back propagation for each\n13"
  },
  {
    "id": 29,
    "text": "trainingpairinthetrainingsamples. Also,weusetherectifiedlinearunitfunctionastheactivationfunction\nforhiddenlayersandsigmoidfunctionastheactivationfunctionforfinaloutput. Sinceweonlyhaveasample\nof size 200, choosing quasi-Newton methods as the solver has better performance for small sample training.\nThen we need to construct the training, validation, and test sets. Since the fundamental factors are already\nneutralized (normalized) within each sector, we divide the sample data to 60%, 20%, 20% by convention.\nWithout shuffling, we will have the historical data divided where test set contains the most recent data.\nFor the corresponding output value, we assign 1 to samples with positive future return and 0 with negative\nreturns. Thecomplexityofneuralnetworkdirectlyrelatedtothenumberandsizesofhiddenlayers. Forthe\npurpose of avoiding overfitting or under-fitting, we need to find proper hyper parameters for neural network"
  },
  {
    "id": 30,
    "text": "purpose of avoiding overfitting or under-fitting, we need to find proper hyper parameters for neural network\nmodel. We start from a simple model with two layers. Let N denote the number of nodes in each hidden\nlayer,alphaisthehyperparameterforL2regularizationpenaltyfunction. WithlargerN,themodelismore\ncomplex. If alpha increases, the penalty for large weights increases, which makes the model tend to be more\nsimple. Considering our sample size is small, intuitively we need to focus more on the overfitting problem.\nFor a range of alpha and N, we train the model using the training set data, and get the score for prediction\non validation set. The score represents the probability of making a right prediction. Here are the results:\nN\\alpha 3 2.5 2 1.5 1 0.5 0.25 0.1 0.01 0.001\n5 0.49 0.45 0.49 0.56 0.62 0.58 0.62 0.58 0.56 0.56\n6 0.51 0.55 0.45 0.49 0.51 0.49 0.47 0.56 0.56 0.56\n7 0.55 0.53 0.56 0.51 0.47 0.51 0.58 0.49 0.53 0.57\n8 0.51 0.47 0.55 0.51 0.51 0.64 0.6 0.6 0.59 0.6"
  },
  {
    "id": 31,
    "text": "6 0.51 0.55 0.45 0.49 0.51 0.49 0.47 0.56 0.56 0.56\n7 0.55 0.53 0.56 0.51 0.47 0.51 0.58 0.49 0.53 0.57\n8 0.51 0.47 0.55 0.51 0.51 0.64 0.6 0.6 0.59 0.6\n9 0.53 0.49 0.47 0.53 0.56 0.56 0.55 0.6 0.53 0.6\n10 0.55 0.53 0.53 0.53 0.51 0.51 0.53 0.6 0.6 0.56\n11 0.53 0.58 0.51 0.51 0.47 0.65 0.58 0.55 0.56 0.51\n12 0.55 0.55 0.56 0.51 0.49 0.51 0.54 0.53 0.53 0.53\n13 0.53 0.47 0.55 0.49 0.56 0.6 0.55 0.58 0.6 0.56\n14 0.49 0.51 0.56 0.53 0.55 0.53 0.49 0.51 0.6 0.53\n15 0.47 0.51 0.51 0.51 0.45 0.49 0.56 0.6 0.56 0.55\nTobetterunderstandhowthehyperparametersinfluencemodelperformance, wevisualizethedatabyusing\nN and alpha as the bottom coordinates, and use the corresponding probability as the height.\nFrom the figure, we can tell that the model have several local optimal pairs. And the scores at the optimal\npointswithrelativelargeNvaluesarealsocombinedwithsmallalphavalues. Forexample, thecombination\nof14nodesandalphaequals0.01hasalocaloptimalscoreof0.6. Sincewearetrainingwithasmallsample,"
  },
  {
    "id": 32,
    "text": "pointswithrelativelargeNvaluesarealsocombinedwithsmallalphavalues. Forexample, thecombination\nof14nodesandalphaequals0.01hasalocaloptimalscoreof0.6. Sincewearetrainingwithasmallsample,\nusingsuchacomplexmodelwithahighscoreishighlylikelyoverfitting. Therefore,westartfromthesimple\nmodel by looking at models with 5 nodes model and check how the score varies with alpha. For model with\n5 nodes, we see there are two local peaks with alpha equal to 1 and 0.25, then we pick the middle value 0.5\nas the value of alpha considering the trade-off between variance and bias.\n14"
  },
  {
    "id": 33,
    "text": "5.3 Model Performance\nConstructed and trained the model, next we would test the model by feeding a new data set to the model.\nOn the test set, the score of the model is 0.64, which means the model predicts 64% of the results correctly.\nMore detailed results are showed in the following table:\nPositive Return Negative Return\nActual Sample 27 28\nPrediction 37 18\nCorrect Prediction 22 13\nWinning Rate 0.59 0.72\nOn the test set, we have a 0.59 winning rates on the positive predictions and 0.72 on the negative\npredictions, which gives an overall winning rate of 0.64.\n5.4 Trade with Model Prediction\nBy using the predictions from the trained model, we used the data from validation set to get trade signals.\nInstead of having signals of 1 or 0 as the model’s output, we choose the probability of the prediction output\nbeing1,whichisgivenbytheactivationsigmoidfunction. Thenwewillhaveatimeseriesoftheprobability"
  },
  {
    "id": 34,
    "text": "being1,whichisgivenbytheactivationsigmoidfunction. Thenwewillhaveatimeseriesoftheprobability\nfor each sector, and rank the probability from highest to lowest where the highest probability will have a\nrank1. Foreachcross-sectionalranking, weequally-weightedlongsectorswithrank1to3andshortsectors\nwith rank 9 to 11 to construct a dollar-neutral portfolio. On the test set, which is from September in 2020\nto September in 2021, we have a Sharpe ratio of 2.21. The cumulative return plot is following:\n5.5 Other Considerations\nThere are still issues that need to be considered carefully in the future. First is the factor neutralization. In\nprevious model, we neutralized the factor exposure cross-sectionally, where the exposures reflect the relative\nlevel of factor exposure for one sector compared to other sectors at a given time. However, different sectors\nmay have inner trends of higher exposures than others for some factors, especially for fundamental factor."
  },
  {
    "id": 35,
    "text": "may have inner trends of higher exposures than others for some factors, especially for fundamental factor.\nWhat’s more, we only have quarterly fundamental data available from 2017 and it is hard to implement\ntime series normalization for each sector. Therefore, how to modify the factor exposures to make them\ncomparable is a difficult problem. Secondly, as the sample size is small, the model might not be applicable\non a wider range of time since we only trained and tested on the most recent five years. One possible way\nto improve this model is to use daily factors such as volume, close price as input, and convert fundamental\nfactorstodailyfrequencybythecorrespondingquarter. Thenwewouldhaveasamplesizeofapproximately\n1250 for each sector and over 13000 samples for training. However, the model might depends more on the\ndailyfactorsratherthanfundamentalfactorssincetheirexposureswouldbethesamevalueforeachquarter.\n15"
  },
  {
    "id": 36,
    "text": "6 References\n1. Returns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency Narasimhan\nJegadeesh; Sheridan Titman The Journal of Finance, Vol. 48, No. 1. (Mar., 1993), pp. 65-91.\n2.The Global Industry Classification Standard, MSCI (1999)\n16"
  },
  {
    "id": 37,
    "text": "Draft version January 2, 2024\nTypesetusingLATEXtwocolumnstyleinAASTeX63\nPrompt emission of relativistic protons up to GeV energies from M6.4-class solar flare on July 17,\n2023\nC.E. Navia,1 M.N. de Oliveira,1 and A.A. Nepomuceno2\n1Instituto de F´ısica, Universidade Federal Fluminense, 24210-346, Niter´oi, RJ, Brazil\n2Departamento de Ciˆencias da Natureza, Universidade Federal Fluminense, 28890-000, Rio das Ostras, RJ, Brazil\nABSTRACT\nWeshowevidenceofparticleaccelerationatGEVenergiesassociateddirectlywithprotonsfromthe\nprompt emission of a long-duration M6-class solar flare on July 17, 2023, rather than from protons\nacceleration by shocks from its associated Coronal Mass Ejection (CME), which erupted with a speed\nof 1342 km/s. Solar Energetic Particles (SEP) accelerated by the blast have reached Earth, up to\nan almost S3 (strong) category of a radiation storm on the NOAA scale. Also, we show a temporal"
  },
  {
    "id": 38,
    "text": "of 1342 km/s. Solar Energetic Particles (SEP) accelerated by the blast have reached Earth, up to\nan almost S3 (strong) category of a radiation storm on the NOAA scale. Also, we show a temporal\ncorrelation between the fast rising of GOES-16 proton and muon excess at ground level in the count\nrateoftheNew-TupimuondetectoratthecentralSAAregion. AMonteCarlospectralanalysisbased\non muon excess at New-Tupi is consistent with the acceleration of electrons and protons (ions) up to\nrelativistic energies (GeV energy range) in the impulsive phase of the flare. In addition, we present\nanother two marginal particle excesses (with low confidence) at ground-level detectors in correlation\nwith the solar flare prompt emission.\nKeywords: sun:activity, high-speed stream, cosmic rays modulation\n1. INTRODUCTION High-Energy Proton and Alpha Detector (HEPAD)\non the Geostationary Operations Environmental Satel-\nSince 1950 the observation of solar energetic particles"
  },
  {
    "id": 39,
    "text": "1. INTRODUCTION High-Energy Proton and Alpha Detector (HEPAD)\non the Geostationary Operations Environmental Satel-\nSince 1950 the observation of solar energetic particles\nlite (GOES) and the Advanced Composition Explorer\nfromthesolarflaresandcoronalmassejections(CMEs)\n(ACE) spacecraft at Lagrange L1 point, through the\nhave been done with ground-level experiments, such as\nElectron Proton Alpha Monitor (EPAM) and the Solar\nthe neutron monitors (NMs) (Meyer et al. 1956; Simp-\nIsotope Spectrometer (SIS), among others.\nson2000;Moraaletal.2000)aswellasthesolarneutron\nNotallofthesolarenergeticparticlescanbemeasured\ntelescope network (Hu & Semones 2022; Vald´es-Galicia\natgroundlevel. EventhoseSEPsfromsolareventswith\net al. 2009), all around the world. These observations\na good geoeffectiveness can be dissipated by the IMF,\nhave yielded a lot of new information. For instance,\nor deflected or captured by the Earth’s magnetic\nthe existence of a prompt and gradual emission of solar"
  },
  {
    "id": 40,
    "text": "have yielded a lot of new information. For instance,\nor deflected or captured by the Earth’s magnetic\nthe existence of a prompt and gradual emission of solar\nfield or until absorbed by atmosphere.\nenergetic particles (SEP) in flares and CMEs, respec-\nOn the other hand, ground-level enhancements\ntively, the correlations of the cosmic ray intensity with\n(GLEs), typically in the MeV-GeV energy range, are\nCMEs and other solar disturbances crossing the Earth,\nsudden increases in cosmic ray intensities registered in\netc. (Chupp et al. 1987; Moraal et al. 2000)\nmost cases by NMs. GLEs are quite rare events, and\nAlso, the solar modulation of galactic cosmic rays is\nfewerthan100GLEshavebeenobservedbyNMsinthe\ninverselycorrelatedwithsolaractivity, inferredthrough\nlast 70 years. In most cases, the NMs that observed\nthe number of sunspots, which can be the key to under-\nGLEs are located at regions with small geomagnetic\nstanding more about space weather (Cade III & Chan-"
  },
  {
    "id": 41,
    "text": "the number of sunspots, which can be the key to under-\nGLEs are located at regions with small geomagnetic\nstanding more about space weather (Cade III & Chan-\nrigidity cutoff, that is, at high latitudes (Shea & Smart\nPark 2015).\n2012).\nNowadays, particles accelerated to near the Sun can\nThe GLEs follow the solar radiation storms, solar en-\nbe detected by space-borne instruments such as the\nergetic particles (mostly protons) observed by GOES.\nThey occur when a large-scale magnetic eruption, a\nCorrespondingauthor: C.E.Navia coronal mass ejection and associated solar flare, accel-\ncarlos navia@id.uff.br erates charged particles in the solar atmosphere to high\nenergies.\n3202\nvoN\n82\n]RS.hp-ortsa[\n1v20000.1042:viXra"
  },
  {
    "id": 42,
    "text": "2\nFigure1. Toppanel: strengthofEarth’smagneticfieldin2000,accordingtomeasurementsbytheEuropeanSpacingAgency’s\nSWARMsatellites. Bottomleftpanel: Corsika-Flukasimulationresultsofthelateralparticledistributioninprotonair-showers\nof cosmic rays, as detected from several ground level detectors. Bottom right panel: correlation between R (from Corsika-\nmax\nFluka) versus the geomagnetic Stormer rigidity cutoff of six different places (black circles) including the SAA-CR region (blue\nsquare). The red solid line is a linear fit, and the two dotted red lines delimit the region with a confidence of ±1σ.\nHowever,inthepresentcase,despitearadiationstorm TheNew-Tupimuondetectoriscompletelyunmoder-\nreaching above the S2-class on the NOAA scale on July ated (without no surrounding lead or other material).\n18,2023,itdidnotgenerateaGLE,onlyapromptemis- Themuondetectionenergythresholdisabout200MeV\nsion of relativistic protons (ions) above GeV energies, (see Appendix A). That contrasts with other muon de-"
  },
  {
    "id": 43,
    "text": "18,2023,itdidnotgenerateaGLE,onlyapromptemis- Themuondetectionenergythresholdisabout200MeV\nsion of relativistic protons (ions) above GeV energies, (see Appendix A). That contrasts with other muon de-\nduringthephaseeruptive,andobservedbyground-level tectorsthathave,inmostcases,asurroundingleadma-\ndetectors strategically located, within the SAA central terial with a thickness of up to 5 cm.\nregion (New-Tupi muon detector) and by the (Yangba- The shielding effect of the Earth’s magnetic field on\nginmuontelescope)attheYangbajingCosmicRayOb- cosmic ray particles is quantified by the magnetic rigid-\nservatory (Tibet 4440 m a.s.l) (Zhang et al. 2010). ity cutoff from a specific location (Smart & Shea 2009).\nAlso, we looked for any signal in the counting rate The smaller the rigidity cutoff, the lower the energy\nat the Neutron Monitor’s (NM) network around the cosmic ray particles penetrate the magnetosphere. On"
  },
  {
    "id": 44,
    "text": "at the Neutron Monitor’s (NM) network around the cosmic ray particles penetrate the magnetosphere. On\nworld from Neutron Monitor Data Base (NMDB) https: the other hand, a restricted area between latitudes 20\n//www.nmdb.eu/nest/, with negative results. However, and 40 of the southern hemisphere, over South Amer-\nwefoundalowconfidencesignalonlyatKerguelenNM, ica and the Atlantic Ocean poses a geomagnetic field\natgeographicalcoordinates(49.3S,70.3E),altitudeof33 withananomalouslylowerintensity(around22,000nT).\nm a.s.l, and an effective vertical cutoff rigidity of 1.14 The region is known as the South Atlantic Anomaly\nGV. We present details of these observations. (SAA) (Pav´on-Carrasco & De Santis 2016). According\nto Swarm’s satellite observations (Finlay et al. 2020),\n2. NEW-TUPI TELESCOPE WITHIN THE SOUTH the SAA appears splitting into two, a smaller area over\nATLANTIC ANOMALY theAtlanticOceaninsouthwestAfricaandalargerarea"
  },
  {
    "id": 45,
    "text": "3\nover eastern South America. Fig. 1 (top panel) summa-\nrizes the situation. We would like to point out that the\nlocation of the New-Tupi telescope coincides with the\ncentral part of the SAA indicated by the arrow on the\nleft of Fig. 1 (top panel).\nThe main effect of the SAA is on the satellites since\nthe ’70s. We know the frequent failures when they pass\nthrough the SAA region. A large amount of charged\nparticles precipitation in this region damages and per-\nturbates the satellites’ electronics. Also, according to\nthe results from the PAMELA detector at satellite’s al-\ntitudes (Casolino et al. 2009), the effect of geomagnetic Figure 2. Leftpanel: NASA’sSolarDynamicsObservatory\ncutoffonlow-energyparticlesispresentinhighlatitudes image of a blast near the sun’s southwestern limb, erupted\nfrom a big sunspot AR3363, with onset at the last hours of\nclosetothepolesandalsointheSAAregion,composed\nJuly 17, 2023. Right panel: LASCO-C2 coronograph image"
  },
  {
    "id": 46,
    "text": "from a big sunspot AR3363, with onset at the last hours of\nclosetothepolesandalsointheSAAregion,composed\nJuly 17, 2023. Right panel: LASCO-C2 coronograph image\nmostly of low energy cosmic protons (E < 200 MeV ).\non July 18, 2023, at 00:42 UT showing the CME eruption\nIn other words, the Pamella satellite has shown that\nassociated to M6-class flare with a speed of 1342 km/s.\nthe SAA introduces a sub-cutoff in the magnetic rigid-\nity, below the Stormer’s magnetic rigidity cutoff. We\ncorrelation. From an interpolation, it is possible to see\nshow that the SAA also affects secondary cosmic rays\nthat the small value of R at SAA-CR correspond to\ndetected at ground level. As the horizontal magnetic max\nthe rigidity of only 3.1+3.0 GV, within a confidence of\ncomponent on Earth’s surface is smaller on the SAA, −1.7\n±1.0σ. This behavior of having a location close to the\nthe magnetic lateral dispersion of the secondary parti-\nEquator, with a nominal lower magnetic rigidity cutoff,"
  },
  {
    "id": 47,
    "text": "±1.0σ. This behavior of having a location close to the\nthe magnetic lateral dispersion of the secondary parti-\nEquator, with a nominal lower magnetic rigidity cutoff,\ncles forming an air shower is smaller too. The effect in-\nfavors the observation of phenomena such as the SEPs.\ncreases the number of particles reaching a detector. In\nother words, this behavior mimics a magnetic rigidity\n3. ANALYSIS\nsub-cutoff below the Stormer’s rigidity cutoff. We show\nOn July 17, 2023, at ∼18h UT, the active region AR\nthat effect through a Monte Carlo simulation based on\n13363hadanexplosion,reachinganM6-classsolarflare\nCORSIKA-Flukacode(Hecketal.2012;Battistonietal.\nfollowed by a resplendent coronal mass ejection. Fig. 2\n2008),where1.0×106 protonair-showersaresimulated,\nleft panel shows the image from the Solar Dynamo ob-\ntaking into account the magnetic coordinates (latitude,\nservatory of the blaze of fire responsible for the X-ray\nlongitude) and height of several places where detectors"
  },
  {
    "id": 48,
    "text": "taking into account the magnetic coordinates (latitude,\nservatory of the blaze of fire responsible for the X-ray\nlongitude) and height of several places where detectors\nflux reaching M6-class flare. Already the right panel\nare installed (mostly neutron monitors).\nshows the LASCO-C2 coronograph image of its associ-\nFig.1bottomleftpanelshowsthelateralparticledis-\nated CME on July 18, 2023, at 00:42 UT.\ntributioninair-showersofcosmicrays,asdetectedfrom\nNOAApredictionmodelsconfirmedthataCMEorig-\nseveralground-leveldetectors. Inallcases,thereisafast\ninated in the powerful M6-class flare from sunspot\nriseofparticleswiththeshowerlateraldevelopmentun-\nAR3363wouldpassthroughthemagnetosphereonJuly\ntil reach um maximum value that happens for different\n20, triggering at least a G1-class (minor) geomagnetic\nvaluesofR,calledhereafterasR Wecanseethatthe\nmax\nstorm. However, no magnetic storms were observed.\nnumber of shower particles at R in the SAA central\nmax"
  },
  {
    "id": 49,
    "text": "valuesofR,calledhereafterasR Wecanseethatthe\nmax\nstorm. However, no magnetic storms were observed.\nnumber of shower particles at R in the SAA central\nmax\nFig. 3 shows the GOES-18 X-ray flux (upper panel)\nregion(SAA-CR)rigidity9.6GVishigherthanatRome\nand the GOES-16 proton flux (bottom panel). The X-\nand Athens, both with the rigidity of 6.3 GV, and 8.5\nrayfluxpeaksat18:00UT,whiletheprotonfluxhastwo\nGV,respectively,i.e.,minorsthantheSAA-CR.Already\npeaks. The first (in orange) is due to the acceleration\nFig. 1 bottom right panel, shows a correlation between\nof protons during the impulsive fast-rising phase of the\nR versus the geomagnetic Stormer rigidity cutoff of\nmax\nflarepeakingat18:09UT.ThedelaybetweentheX-ray\nsix different places (black circles), including the SAA-\nand proton flux peaks is because the proton velocity is\nCR (blue square). The solid red line is a linear fit, and\nslightly less than c, and the proton path is longer. The"
  },
  {
    "id": 50,
    "text": "and proton flux peaks is because the proton velocity is\nCR (blue square). The solid red line is a linear fit, and\nslightly less than c, and the proton path is longer. The\nthe two dotted red lines delimit the region with signifi-\nsecondpeakaretheprotonsacceleratedbyCMEshocks,\ncance of ±1σ. Only two places are out from the ±1.0σ\npeakingat18:14UT,andit’stheso-calledgradualphase\nsignificance region, the Thule (Groenlandia) in the low-\nand is characterized by its long duration, up to several\nest rigidity region and SAA-CR in the highest rigidity\ndays.\nregion. ThehighStormer’srigidityofSAA-CRdoesnot\nFig. 4 shows the temporal coincidence between the\ncorrespond to the high value of R expected by the\nmax\nGOESprotonfluxintheimpulsivephaseandNew-Tupi"
  },
  {
    "id": 51,
    "text": "4\nFigure 5. From top to bottom: GOES-16 proton flux in\nthreeenergybands,2ndto4thpanels,countingrateatNew-\nFigure 3. Top panel: GOES-18 X-ray flux in two wave-\nTupi muon detector, Yan ba Jing muon telescope, and Ker-\nlengths. Bottompanel: GOES-16protonfluxinthreeenergy\nguelen Neutron Monitor, respectively. To five consecutive\nbands. Both on July 17-18, 2023. The orange area at the\ndays, from July 15-19, 2023.\nbottom(isavisualguideonly)highlightstheprotonprompt\nemission during the flare impulsive phase.\nmuon excess. Particles (mostly proton) are accelerated\nin this phase exclusively by the flare, during the fast-\nrising until to reach the first peak (orange sector) in\nFig. 4.\nHowever, in the so-called gradual phase, protons ac-\ncelerated by CME’s shocks, the proton flux does not\nreachtheGeVenergyrangebecausetherearenoexcess\nmuons at ground level.\n4. SPECTRAL ANALYSIS\nWe perform a Monte Carlo simulation of air showers\ninitiated by SEP (protons) using the CORSIKA code"
  },
  {
    "id": 52,
    "text": "reachtheGeVenergyrangebecausetherearenoexcess\nmuons at ground level.\n4. SPECTRAL ANALYSIS\nWe perform a Monte Carlo simulation of air showers\ninitiated by SEP (protons) using the CORSIKA code\n(Heck et al. 2012), together with the FLUKA interac-\ntion model (Battistoni et al. 2008), that works well at\nGeV and sub-GeV energies, including secondary parti-\ncle decay. The surviving particles are tracked through\ntheatmosphereuntiltheyreachgroundlevel(sealevel).\nMost particles are muons with a small contribution of\nelectrons and nucleons.\nThe aim is to obtain the yield function, S (E ), that\nµ P\nis,thenumberofmuonsatsealevelperprimaryproton,\nfor an estimate of the upper limit of the integral proton\nFigure 4. Toppanel: GOES-16protonfluxinthreeenergy\nflux in the GeV energy range, associated with the im-\nbands. Bottom panel: New-Tupi counting rate expressed in\npulsive phase of M-6-class flare with onset on July 17,\nvariation(%). BothonJuly17-18,2023. Theorangeareaat\n2023, at ∼ 18 UT."
  },
  {
    "id": 53,
    "text": "bands. Bottom panel: New-Tupi counting rate expressed in\npulsive phase of M-6-class flare with onset on July 17,\nvariation(%). BothonJuly17-18,2023. Theorangeareaat\n2023, at ∼ 18 UT.\nthetop(isavisualguideonly)highlightstheprotonprompt\nemission during the flare impulsive phase. Fig. 6 (black squares) shows the Monte Carlo output\nundertheNew-Tupigeomagneticconditionsandvertical\nproton incidence, and fitting as"
  },
  {
    "id": 54,
    "text": "5\nFigure6. Yieldfunction,asthenumberofmuonsatthesea\nlevelperproton(verticalincidence),asafunctionofincident\nproton energy, from CORSIKA-FLUKA simulations, taking\ninto account the SAA’s central region magnetic conditions.\nFigure 8. Integralprotonflux: theredcirclesrepresentthe\nThe red dashed curve shows a fit function.\nGOES-16 data. It corresponds to the GOES-16 proton flux\nprompt emission (orange area in Fig. 3. The black square\nrepresents the proton flux obtained from the muon excess\non the New-Tupi detector (and Monte Carlo calculations),\nobserved in coincidence with the radiation storm.\nInaddition,weassumeherethattheenergyspectrum\nof solar protons in the GeV energy range, which is in\nthe high-energy tail of the SEP spectrum, can be fitted\nby a single power-law function.\nJ (E )=A Eβ. (2)\nP P P P\nThere are two unknown quantities in the above power-\nlaw function: the coefficient A and the spectral index\nP\nβ.\nAconvolutionbetweentheyieldfunctionS (E )and\nµ P"
  },
  {
    "id": 55,
    "text": "J (E )=A Eβ. (2)\nP P P P\nThere are two unknown quantities in the above power-\nlaw function: the coefficient A and the spectral index\nP\nβ.\nAconvolutionbetweentheyieldfunctionS (E )and\nµ P\ntheprotonspectrumJ (E )givestheresponsefunction\nP P\n(Augustoetal.2016b),whichisthenumberofmuonsin\ntheexcesssignalatNew-Tupidetectorgeneratedbythe\nSEP during the period T. We express this convolution\nFigure 7. Correlation between the coefficient Ap and the\nas\nspectralindexβ. AllpossiblevaluesofApandβ compatible\nwith the New-Tupi muon counting rate excess (black solid\n(cid:90) ∞\ncurve) and the high-energy GOES-16 proton fluence F (red J = S (E )F(θ)A EβdE , (3)\nµ µ P P P P\ndot curve). These quantities are defined by equations 3 and Emin\n5, respectively.\nwhereF(θ)∼exp(θ/C)isthepitchangledistribution\n(Shea & Smart 1982; Miroshnichenko et al. 2005). In\nthe central region of SAA (New-Tupi), the transverse\nS (E )=A Eνexp(−E /E ), (1) geomagnetic component is only B = 18.3 mT, that\nµ P µ P 0 P ⊥"
  },
  {
    "id": 56,
    "text": "(Shea & Smart 1982; Miroshnichenko et al. 2005). In\nthe central region of SAA (New-Tupi), the transverse\nS (E )=A Eνexp(−E /E ), (1) geomagnetic component is only B = 18.3 mT, that\nµ P µ P 0 P ⊥\nis, almost 80% smaller than the transverse component,\nwhere A = (6.8±1.4)×10−3, ν = 1.18±0.24, and at the same latitude, but outside from the SAA region,\nµ\nE =10.2±2.1GeV.Fig.6showsthefits(reddotline). favoring the focusing factor of the geomagnetic parallel\n0"
  },
  {
    "id": 57,
    "text": "6\ngeomagnetic on the incident solar protons (small pitch detectorattheSAAcentralregion. MuonsatNew-Tupi\nangles). For the present event, we found exp(θ/C)∼1. are produced by protons (ions) interaction in the upper\nThemuonexcessassociatedwithprotonsemitteddur- atmosphere reaching the Earthwith a magnetic rigidity\ning the impulsive phase (see Fig. 4), and considering above 3 GV (∼ 3 GeV for protons).\nan effective angular aperture of 60 degrees around the In most cases, SEP (mostly protons) detected by the\nzenith of the New-Tupi detector, the counting rate ex- GOES-16 spacecraft shows two steps. An impulsive\ncess is phase, wheretheaccelerationofprotons(ions)isbythe\nprompt emission of flare, followed by a gradual phase of\nJ µ =(3.20±2.51)×10−3muons/(cm2 ssr), (4) long duration, where the acceleration of protons (ions)\nis by the associated CME shock waves.\nFurthermore, we also obtain the integrated time pri-\nWe want to point out that the muon excess produced\nmary fluence as"
  },
  {
    "id": 58,
    "text": "is by the associated CME shock waves.\nFurthermore, we also obtain the integrated time pri-\nWe want to point out that the muon excess produced\nmary fluence as\nby SEPs with an effective rigidity above the cutoff (∼\n(cid:20)(cid:90) ∞ (cid:21) 3 GV) at the New-Tupi muon detector is in temporal\nF =T dE P A P E P βE P . (5) coincidence only with the GOES proton flux from the\nEmin\nimpulsivephase(seeFig.4). Consequently,inthegrad-\nFor the present case, the GOES-proton fluence in the ual phase, the protons accelerated by CME’s shocks do\nhigh-energy region (Ep≥50) MeV is not reach the GeV energy range because a muon excess\nis absent at ground level.\nF ∼(46.0±34.9)MeV/(cm2sr), (6) A marginal muon excess also appears on the Yan ba\nJingS-21(pointing21degreessouth)muontelescope(in\nThe terms on the left side of Eq. 1 and Eq. 5 are\nTibet). Also, a marginal particle excess is seen only in\nknown. Thus, we can consider all possible values of β\nthe French Kerguel NM (close to the South polar re-"
  },
  {
    "id": 59,
    "text": "Tibet). Also, a marginal particle excess is seen only in\nknown. Thus, we can consider all possible values of β\nthe French Kerguel NM (close to the South polar re-\nand A compatible with the observed muon counting\nP gion). In both cases, the excesses are in (temporal) co-\nrateexcess,J ,andtheintegratedGOES-protonfluence\nµ incidencewiththeGOESprotonflux(impulsivephase).\nF. Fig. 7 summarizes the situation. Giving:\nHowever, it is hard to verify whether these excesses are\nAp=(1.20±0.96)×10−3/(cm2ssr GeV)\ngenuine due to low confidence or simply fluctuations in\nand β =1.89±1.10.\nthe detectors’ count rate.\nTo make a comparison with satellite GOES data, we\nFrom a Monte Carlo analysis, we show that the SAA\nobtain the integral proton flux in the GeV energy range\ncentral region is favourable to the observation of tran-\nas\n(cid:90) ∞ sientsolarevents,especiallySEP,becausethemagneto-\nJ P (>E P )= A P E P −βdE P . (7) sphere has a ”dip´´ in this region, weakening the geo-\nEmin magnetic"
  },
  {
    "id": 60,
    "text": "as\n(cid:90) ∞ sientsolarevents,especiallySEP,becausethemagneto-\nJ P (>E P )= A P E P −βdE P . (7) sphere has a ”dip´´ in this region, weakening the geo-\nEmin magnetic\nFig. 8 shows the results of the integral proton flux ob-\nfield strength and allowing the entrance of charged\ntained from the New-Tupi muon excess observed in co-\nparticlesatlargedeeps inaregionnotfarfromthe geo-\nincidence with the radiation. The red circles represent\ngraphic Equator, giving a rigidity sub-cutoff around 3.1\nthe GOES-16 data, and the black squares represent the\nGV in a place where the conventional Stormer geomag-\nexpected proton flux in the GeV energy range obtained\nnetic rigidity cutoff is around 10 GV.\nfrom Monte Carlo, muon excess at the New-Tupi detec-\ntor, and the GOES-proton fluence. The origin of this\ntransienteventwasthesolareruption, anM6-classflare\n(promptemission),acceleratingprotons(ions)uptorel- 6. ACKNOWLEDGMENTS"
  },
  {
    "id": 61,
    "text": "tor, and the GOES-proton fluence. The origin of this\ntransienteventwasthesolareruption, anM6-classflare\n(promptemission),acceleratingprotons(ions)uptorel- 6. ACKNOWLEDGMENTS\nativistic energies, GeV energy range. This work is supported by the Rio de Janeiro\nResearch Foundation (FAPERJ) under Grant E-\n5. CONCLUSIONS\n26/010.101128/2018. We thank to NMDB Database\nWe have reported evidence of SEPs accelerated up to (www.nmdb.eu), founded under the European Union\nGeV energies during the eruptive phase of the M6-class FP7 Program (Contract No. 213007) by provide NMs\nsolar flare on July 17, 2023. The result comes from data and the Space Weather Prediction Center from\na timing analysis of a muon excess from the New-Tupi NOAA for its open data policy.\nAPPENDIX"
  },
  {
    "id": 62,
    "text": "7\nA. NEW-TUPI DETECTOR\nThe New-Tupi telescope is built with four identical particle detectors, forming two telescopes, as shown in Fig. 8\nfrom Augusto et al. (2016a). Each detector consisting of an Eljen EJ-208 plastic scintillator slab of 150 cm x 75 cm x\n5cmandaHamamatsuR877photomultiplierof127millimetersindiameter, packagedinapyramidalbox. ThePMT\nhigh voltage divider, amplifier, and high voltage power supplier are in the ORTEC ScintiPackTM Photomultiplier\nBase 296.\nFrom February 6, 2023, we have implemented a data acquisition system using a VERTILON high-speed pulse\ncounting system (MCPC618-8 Channel). allowing for direct connection with the PMTs without the need for external\npreamplifiers, with a 250 MHz count rate per channel.\nNowthedetectorisworkingonlyinscalermodeorsingleparticletechnique(Agliettaetal.1996),wherethesinglehit\nrates of all four PMTs, are recorded once a second. However, so far, only two detectors are working. The coincidences"
  },
  {
    "id": 63,
    "text": "rates of all four PMTs, are recorded once a second. However, so far, only two detectors are working. The coincidences\namong these detectors of each telescope will be implanted.\nAlso, the barometric coefficients for cosmic muon fluxes at the Earth’s surface can be obtained using the CORSICA\ncodeinKovylyaevaetal.(2013). ForNew-Tupidetectorconditionsandatsealevel,thebarometriccoefficientisabout\n-0.14% per mb, about eight to nine times less than the typical barometric coefficient in NMs."
  },
  {
    "id": 64,
    "text": "8\nREFERENCES\nAglietta, M., Alessandro, B., Antonioli, P., et al. 1996, Meyer, P., Parker, E., & Simpson, J. 1956, Physical\nAstrophysical Journal, 469, 305 Review, 104, 768\nAugusto, C., Navia, C., de Oliveira, M., Nepomuceno, A., Miroshnichenko, L., Klein, K.-L., Trottet, G., et al. 2005,\n& Fauth, A. 2016a, arXiv preprint arXiv:1603.08863\nJournal of Geophysical Research: Space Physics, 110\nAugusto, C., Navia, C., de Oliveira, M. N., Fauth, A., &\nMoraal, H., Belov, A., & Clem, J. 2000, Space Science\nNepomuceno, A. 2016b, Publications of the Astronomical\nReviews, 93, 285\nSociety of Japan, 68, 8\nPavo´n-Carrasco, F. J., & De Santis, A. 2016, Frontiers in\nBattistoni, G., Cerutti, F., Empl, A., et al. 2008, Nuclear\nEarth Science, 4, 40\nPhysics B-Proceedings Supplements, 175, 88\nCadeIII,W.B.,&Chan-Park,C.2015,SpaceWeather,13, Shea,M.,&Smart,D.1982,SpaceScienceReviews,32,251\n99 —. 2012, Space science reviews, 171, 161"
  },
  {
    "id": 65,
    "text": "Physics B-Proceedings Supplements, 175, 88\nCadeIII,W.B.,&Chan-Park,C.2015,SpaceWeather,13, Shea,M.,&Smart,D.1982,SpaceScienceReviews,32,251\n99 —. 2012, Space science reviews, 171, 161\nCasolino, M., De Simone, N., De Pascale, M. e., et al. 2009, Simpson, J. A. 2000, in Cosmic Rays and Earth:\nNuclear Physics B-Proceedings Supplements, 190, 293\nProceedings of an ISSI Workshop, 21–26 March 1999,\nChupp, E., Debrunner, H., Flu¨ckiger, E., et al. 1987, The\nBern, Switzerland, Springer, 11–32\nAstrophysical Journal, 318, 913\nSmart, D., & Shea, M. 2009, Advances in Space Research,\nFinlay, C. C., Kloss, C., Olsen, N., et al. 2020, Earth,\n44, 1107\nPlanets and Space, 72, 1\nVald´es-Galicia, J., Muraki, Y., Watanabe, K., et al. 2009,\nHeck, D., Pierog, T., & Knapp, J. 2012, Astrophysics\nSource Code Library, ascl Advances in Space Research, 43, 565\nHu, S., & Semones, E. 2022, Journal of Space Weather and Zhang, J., Tan, Y., Wang, H., et al. 2010, Nuclear"
  },
  {
    "id": 66,
    "text": "Source Code Library, ascl Advances in Space Research, 43, 565\nHu, S., & Semones, E. 2022, Journal of Space Weather and Zhang, J., Tan, Y., Wang, H., et al. 2010, Nuclear\nSpace Climate, 12, 5 Instruments and Methods in Physics Research Section A:\nKovylyaeva, A., Dmitrieva, A., Tolkacheva, N., & Accelerators, Spectrometers, Detectors and Associated\nYakovleva, E. 2013, Journal of Physics: Conference Equipment, 623, 1030\nSeries, 409, 012128"
  },
  {
    "id": 67,
    "text": "GENERATIVE INVERSE DESIGN OF METAMATERIALS WITH\nFUNCTIONAL RESPONSES BY INTERPRETABLE LEARNING\nWei(Wayne)Chen RachelSun\nJ.MikeWalker’66DepartmentofMechanicalEngineering DepartmentofMechanicalEngineering\nTexasA&MUniversity MassachusettsInstituteofTechnology\nCollegeStation,TX77843 Cambridge,MA02139\nw.chen@tamu.edu rmsun@mit.edu\nDoksooLee CarlosM.Portela\nDepartmentofMechanicalEngineering DepartmentofMechanicalEngineering\nNorthwesternUniversity MassachusettsInstituteofTechnology\nEvanston,IL60208 Cambridge,MA02139\ndslee@northwestern.edu cportela@mit.edu\nWeiChen\nDepartmentofMechanicalEngineering\nNorthwesternUniversity\nEvanston,IL60208\nweichen@northwestern.edu\nNovember1,2024\nABSTRACT\nMetamaterialswithfunctionalresponsescanexhibitvaryingpropertiesunderdifferentconditions\n(e.g.,wave-basedresponsesordeformation-inducedpropertyvariation). Thisworkaddressesthe\nrapidinversedesignofsuchmetamaterialstomeettargetqualitativefunctionalbehaviors,achallenge"
  },
  {
    "id": 68,
    "text": "(e.g.,wave-basedresponsesordeformation-inducedpropertyvariation). Thisworkaddressesthe\nrapidinversedesignofsuchmetamaterialstomeettargetqualitativefunctionalbehaviors,achallenge\nduetoitsintractabilityandnon-uniquesolutions. Unlikedata-intensiveandnon-interpretabledeep-\nlearning-based methods, we propose the Random-forest-based Interpretable Generative Inverse\nDesign(RIGID),asingle-shotinversedesignmethodforfastgenerationofmetamaterialdesigns\nwithon-demandfunctionalbehaviors. RIGIDleveragestheinterpretabilityofarandomforest-based\n“design→response”forwardmodel,eliminatingtheneedforamorecomplex“response→design”\ninverse model. Based on the likelihood of target satisfaction derived from the trained random\nforest, one can sample a desired number of design solutions using Markov chain Monte Carlo\nmethods. WevalidateRIGIDonacousticandopticalmetamaterialdesignproblems,eachwithfewer\nthan250trainingsamples. Comparedtothegeneticalgorithm-baseddesigngenerationapproach,"
  },
  {
    "id": 69,
    "text": "methods. WevalidateRIGIDonacousticandopticalmetamaterialdesignproblems,eachwithfewer\nthan250trainingsamples. Comparedtothegeneticalgorithm-baseddesigngenerationapproach,\nRIGID generates satisfactory solutions that cover a broader range of the design space, allowing\nforbetterconsiderationofadditionalfiguresofmeritbeyondtargetsatisfaction. Thisworkoffers\na new perspective on solving on-demand inverse design problems, showcasing the potential for\nincorporatinginterpretablemachinelearningintogenerativedesignundersmalldataconstraints.\n1 Introduction\nMetamaterialsareengineeredmaterials,typicallycomposedofperiodicallyarrangedbuildingblocks(i.e.,unitcells),\nthatexhibitpropertiesandfunctionalitiesbeyondtheirconstituentmaterials[1,2]. Wecanachievecertaineffective\nmaterialpropertiesofmetamaterialsbydesigningtheirstructure(i.e.,geometriesofunitcells)ratherthanchemical\ncomposition. Optical[3],acoustic[4],thermal[5],andmechanicalmetamaterials[6,2]aremetamaterialswithunique\n4202\ntcO\n03"
  },
  {
    "id": 70,
    "text": "composition. Optical[3],acoustic[4],thermal[5],andmechanicalmetamaterials[6,2]aremetamaterialswithunique\n4202\ntcO\n03\n]scitpo.scisyhp[\n6v30000.1042:viXra"
  },
  {
    "id": 71,
    "text": "propertiestailoredtomanipulatespecifictypesofwavesorenergy. Forexample,opticalmetamaterialsormetasurfaces\n(i.e.,thetwo-dimensionalformofmetamaterials)aredesignedtocontrolandmanipulateelectromagneticwaves,which\ncanleadtounusualpropertiessuchasanegativerefractiveindex[7]. Acousticmetamaterialsaredesignedtocontrol\nhowsound(acoustic)wavespropagatewhichenablesfunctionalitiessuchasnoisereduction[8].\nFunctional responses refer to varying properties or behaviors under different conditions. A notable example that\ninvolvesfunctionalresponsesismetamaterials,whichaltertheirelectromagnetic,acoustic,orelasticwavepropagation\nbehaviorsdependingonthewavelengthorfrequency[9]. Anotherexampleismetamaterialsthatexhibitchanging\npropertiesorfunctionalitiesduetodeformationinresponsetoexternalstimuliliketemperature[10]andmagnetic\nfields [11]. Tailoring functional responses of these metamaterials is of interest to applications such as sound and"
  },
  {
    "id": 72,
    "text": "fields [11]. Tailoring functional responses of these metamaterials is of interest to applications such as sound and\nvibrationcontrol,analogcomputing,medicalimaging,sensing,communication,andsoftrobotics. Inmanyusecases,\nratherthanpreciselycontrollingthecompletefunctionalresponses,weonlycareaboutqualitativebehaviorsunder\ncertainconditions.Forexample,acousticmetamaterialswereusuallydesignedtohavebandgapsatspecifiedfrequencies\ntoachievefunctionalitieslikewave-guiding[12,13],focusing[14,15],andvibrationmitigation[16,17,18]. However,\nit is unnecessary and computationally expensive to design for the whole dispersion relation [19, 20, 21, 22, 23].\nSimilarly, we may design optical metamaterials to qualitatively manipulate optical properties (e.g., high or low\nabsorption/reflection/transmission)undercertainwavelengths,withoutrequiringtheentirespectralresponsetomatch\nanexacttarget[24,25].\nIdentifyingmetamaterialdesignsfromagiventargetformsaninversedesignproblem. Unlikemanyforwardproblems"
  },
  {
    "id": 73,
    "text": "anexacttarget[24,25].\nIdentifyingmetamaterialdesignsfromagiventargetformsaninversedesignproblem. Unlikemanyforwardproblems\nwhereonecanobtainsolutions(e.g.,spectralresponsesormaterialpropertiesunderexternalstimuli)bymodeling\nthephysicsorconductingexperiments,inversedesignproblemsareusuallyintractable. Traditionally,theseproblems\naresolvedbyiterativeoptimization(i.e., minimizingthedifferencebetweentheactualquantityofinterestandthe\ntarget) [19, 20, 26, 22, 27]. This, however, requires repeatedly updating the design solution and solving forward\nproblems. Whenthedesigntargetchanges,oneneedstoreruntheentireoptimizationprocess. Thus,inversedesign\nby iterative optimization becomes impractical if solving the forward problem (by simulations or experiments) is\ntime-consumingorifthedesigntargetneedstochangefrequently. Forexample,inthecaseofacousticmetamaterial\ndesign,onemaywanttoobtainamaterialwaveguidecontainingspatially-varyingunitcellswithdifferentbandgap"
  },
  {
    "id": 74,
    "text": "design,onemaywanttoobtainamaterialwaveguidecontainingspatially-varyingunitcellswithdifferentbandgap\nrangesatdifferentlocationsinatessellation. Toacceleratetheoptimizationapproach,priorworksreplacedsimulations\nor experiments with machine learning models [28, 29]. However, the efficiency and quality of final solutions are\nhighlydependentonboththemachinelearningmodelandtheoptimizationalgorithm. Ontheotherhand,asingle\nrun of optimization usually only returns one final solution, although multiple designs might satisfy a given target\n(i.e., the non-uniqueness of solutions). For example, multiple acoustic metamaterial designs may have bandgaps\nwithinthesametargetfrequencyrange. Thisnon-uniquenessnatureofinversedesignproblemswasalsoshownfor\nopticalmetasurfaces[30,31,32,27],especiallywhenweonlycareaboutqualitativetargetresponses. Itisgenerally\nadvantageousforinversedesignapproachestoyieldadiversesetofsolutions,asthisallowsconsiderationofadditional"
  },
  {
    "id": 75,
    "text": "advantageousforinversedesignapproachestoyieldadiversesetofsolutions,asthisallowsconsiderationofadditional\nfiguresofmeritbeyondtargetsatisfaction. Forexample,derivingdiversesolutionsforacousticmetamaterialswillallow\nustoconsiderthekinematiccompatibilityofmultipledesignsinwaveguidedesignproblems. However,optimization\napproachescannotexplorediversealternativesolutionsefficiently.\nInthiswork,weaimtoachieveiteration-free,single-shotinversedesignformetamaterials—givenaqualitativetarget\nresponse,wewanttogeneratemultiplesatisfyingdesignsolutionsrapidly,withouttheneedforiterativelyevaluating\ndesigns(i.e.,runningsimulationsorconductingexperiments);andafterone-timeinitialtraining,noadditionaltraining\nordesignevaluationisrequiredtogeneratedesignsfordifferenttargets. Thisallowsfastandcomprehensiveexploration\nofthefeasibledesignspaceunderdifferenttargetresponses.\nPriorresearchattemptedtorealizeiteration-free,single-shotinversedesignusingmachinelearning. Therearethree"
  },
  {
    "id": 76,
    "text": "ofthefeasibledesignspaceunderdifferenttargetresponses.\nPriorresearchattemptedtorealizeiteration-free,single-shotinversedesignusingmachinelearning. Therearethree\nmainstreammethods. Themoststraightforwardapproachistolearnadirectinversemappingfromtheresponseto\ndesign variables. Neural networks are the most commonly used machine learning model for this purpose, due to\ntheirhighflexibilityinapproximatingarbitrarynonlinearinput-outputrelationships[33,21]. Despitethesimplicity\nofthedirectinversemapping,itsunderlyingassumptionoftheresponse-designmappingbeingone-to-onedoesnot\nhold in many cases dueto the non-uniqueness of solutions, as mentioned earlier. Such non-uniqueness will cause\nconflicting training instances where the same input (response) is associated with distinct outputs (designs), which\nwilldestabilizetheconvergenceduringneuralnetworktraining[30,34]. Toavoidthisissue,pastworkproposedthe"
  },
  {
    "id": 77,
    "text": "willdestabilizetheconvergenceduringneuralnetworktraining[30,34]. Toavoidthisissue,pastworkproposedthe\nTandemNeuralNetwork(T-NN)thatcascadesaninverse-designnetworkwithapretrainedforward-modelingnetwork\ntoavoidusingdesignsaslabelswhentrainingtheinverse-designnetworkandhencesolvedthetrainingconvergence\nissue[30,35,36,37,38,39,40]. Nonetheless,theoriginalT-NNsstilllearnaone-to-oneresponse-designmappingand\ncannotaccountforthenon-uniquenessofdesignsolutions. Tofundamentallysolvethisproblem,oneneedstolearna\none-to-manymapping. Forexample,Basteketal.[38]integratedstochasticsamplingintotheinverse-designnetwork\ntoallowthegenerationofmultiplefeasiblesolutions. Haetal.[41]usedmultipleT-NNstopredictmultipledesign\nsolutionsgivenatargetresponse. Wangetal.[42]adaptedtheT-NNtomimicthestructureofavariationalautoencoder\n2"
  },
  {
    "id": 78,
    "text": "Reduced wavevector F\nle\no\na\nra\nrn\nw\ni\na\nn\nr\ng\nd\nDesign\nvariable\ni (x\ni)Design\nvaria\nG\nble\ne\nj\nn\n(x\ne\n)\nr\nj\native\ndesign\n)zHM(\nycneuqerF\nAbsorbance\n)mn(\nhtgnelevaW\nAcoustic x = (x1, x2, ...)\nwaves\nLight x = (x1, x2, ...)\nS\nBandgap\nexistence\nS\nQualitative Interpretable machine\nDesign data Quantitative response response learning model\nHigh\nabsorbance\n)zHM(\nycneuqerF\nTarget\nbandgaps\nDesign Target\nTarget high-\nabsorbance regions\n)mn(\nhtgnelevaW\n0 y 1 Inverse\ninference Target-tailored\nLikelihood designs\n0 y 1\nDesign variable\ni (x\ni)Design\nvariable j (x)j\nFigure1: SchematicdiagramoftheRIGIDmethod. Wefirsttrainarandomforestonadesign-responsedatasettolearn\ntheforwarddesign-responserelation—predictingqualitativeresponses(e.g.,bandgapexistenceatanygivenwave\nfrequency)ofdesigns. Thengivenadesigntarget,wecaninferthelikelihoodofanydesignsatisfyingthetargetby\nprobingintothetrainedrandomforest. Newdesignswithtailoredresponsescanbegeneratedbysamplingthedesign\nspacebasedonthelikelihoodestimation."
  },
  {
    "id": 79,
    "text": "probingintothetrainedrandomforest. Newdesignswithtailoredresponsescanbegeneratedbysamplingthedesign\nspacebasedonthelikelihoodestimation.\nthatiscapableoflearningtheconditionaldistributionsofdesignsgiventargetresponses. Alargebodyofrecentworks\nachieved the goal of learning one-to-many mapping by using conditional generative models, typically conditional\ngenerativeadversarialnetworks(cGANs)[43,44,45,46,32],conditionalvariationalautoencoders(cVAEs)[31,47,48],\nandconditionaldiffusionmodels[49,50,51]. Thesemodelscangeneratemultipledesignsgivenatargetresponseby\nlearningthedistributionofdesignsconditionedontheresponse. Weincludemoredetailedexplanationsofthesethree\nmainstreammethodsinSIAppendix,Sec.S1andFig.S1.\nWereferinterestedreaderstoRef.[34,52,53,54,55,56,57]forcomprehensivereviewsofexistingmetamaterial\ndesignmethods. Tothebestofourknowledge,almostalltheexistingiteration-free,single-shotmetamaterialinverse"
  },
  {
    "id": 80,
    "text": "designmethods. Tothebestofourknowledge,almostalltheexistingiteration-free,single-shotmetamaterialinverse\ndesignmethodsarebasedondeeplearning,whichhasmanycommonissuessuchashighdatademand,exhaustive\nhyperparametertuning,slowtraining,andlowinterpretability,especiallycomparedtotraditionalmachinelearning\nmodelslikedecisiontreesandrandomforests. Ontheotherhand,Elzoukaetal.[58]proposedtousethedecision\ntreeasamoreinterpretablemodeltosolveboththeforwardpredictionandinversedesignproblem. Aftertraininga\ndecisiontreeforforwardprediction,onecanidentifyexplicitdesignrules(i.e.,feasibleregionsinthedesignspace)\nbytracingfromtargetleafnodestotherootnode. Thisapproachalsocapturestheone-to-manymappingnatureof\ninversedesignproblemssinceitgivesfeasibledesignvariablerangesratherthanasinglesolution. However,there\nremainsomelimitations. Firstly,forsolutionsidentifiedbythedesignrules,themethoddoesnotdifferentiatetheir"
  },
  {
    "id": 81,
    "text": "remainsomelimitations. Firstly,forsolutionsidentifiedbythedesignrules,themethoddoesnotdifferentiatetheir\nlikelihoodoftargetsatisfaction. Yetinreality,solutionsalwayshavedifferentlikelihoodsduetotheuncertaintyof\nmodelestimation. Secondly,themethodhastotraintwomodels: arandomforestwastrainedfirsttoensuremodel\naccuracyandrobustness,andthenalargedecisiontreewastrainedtoemulatetheperformanceoftherandomforest\nandprovidedesignrules. Thisisduetothechallengeofderivingexplicitdesignrulesfromanensemblemodellike\ntherandomforest. Finally,themethodwasdemonstratedonaproblemwithmorethan104 trainingdata,whilethe\neffectivenessonsmallerdatasets(i.e.,datawithordersofmagnitudesmallersamplesizes)wasnotstudied.\nThisworkaimstoaddresstheaforementionedproblemsbyproposingamethodcalledRandom-forest-basedInter-\npretableGenerativeInverseDesign(RIGID).Figure1showsanoverviewofthismethod. Specifically,wefirsttrain\naforwardpredictionrandomforest. Thengivenadesigntarget,wecanprobethetrainedrandomforesttoinferthe"
  },
  {
    "id": 82,
    "text": "pretableGenerativeInverseDesign(RIGID).Figure1showsanoverviewofthismethod. Specifically,wefirsttrain\naforwardpredictionrandomforest. Thengivenadesigntarget,wecanprobethetrainedrandomforesttoinferthe\nlikelihood of any design satisfying the target. To generate new designs tailored to the target, we can sample from\nthedesignspaceaccordingtothelikelihood. Comparedtothemostwidelystudiedneural-network-basedmethods,\nRIGIDhasamuchlowercostintrainingandhyperparametertuning,andworksmorerobustlyonsmall-sizedatasets\n(asrandomforestsarelesspronetooverfitting). Similartodeepgenerativemodels,itcangenerateadesirednumberof\nsolutions,allowingonetoexplorealternativesolutionsthatmighthavedesiredpropertiesorfunctionalitiesbeyond\ntheonesconsideredasthetarget. Theexplicitlikelihoodestimationalsooffersaninterpretablecharacterizationof\nadesign’stargetsatisfactionprobabilityandallowsanexploitation-explorationtrade-offwhenselectinggenerated"
  },
  {
    "id": 83,
    "text": "adesign’stargetsatisfactionprobabilityandallowsanexploitation-explorationtrade-offwhenselectinggenerated\ndesigns. WevalidatetheRIGIDmethodontwometamaterialdesignexamples—anacousticmetamaterialdesign\n3"
  },
  {
    "id": 84,
    "text": "example, where the target is to generate metamaterials with specific bandgaps, and an optical metasurface design\nexample,wherethetargetistogeneratemetasurfaceswithhighabsorbanceatspecifiedwavelengths.\nOurcontributionsarethree-fold. First,weproposeaniteration-free, single-shotinversedesignmethodthatisfast,\ngenerative, interpretable, and small-data-compatible. Secondly, we demonstrate the effectiveness of the proposed\nmethodonacousticandopticalmetamaterialdesignexamples, andproposebothqualitativeandquantitativeways\nof assessing our method. Finally, we create two synthetic test cases for fast examination and validation of model\nperformance. Thesetestcasescanbeusedforfuturebenchmarkingstudiesofrelatedmethods.\n2 Methods\nThe functional response of metamaterials can be modeled as y = f(x,s), where x denotes metamaterial design\nvariables(e.g.,materialsandgeometryparameters),sisanauxiliaryvariablerepresentingtheindependentvariable(or"
  },
  {
    "id": 85,
    "text": "variables(e.g.,materialsandgeometryparameters),sisanauxiliaryvariablerepresentingtheindependentvariable(or\nthe“x-axis”)oftheresponse(e.g.,thefrequency/wavelengthortheexternalstimulisuchastemperature),andyindicates\nthevalueoftheresponseassociatedwithourdesigntarget. Inthispaper,weassumey ∈{0,1}sinceweonlyfocus\nonqualitativebehaviorsatspecifiedfrequencies(e.g.,foranacousticmetamaterialoranopticalmetamaterialdesign,\nwhetherabandgapexistsorwhethertheenergyabsorbanceishigherthanathresholdwithinarangeoffrequencies).\nWeleavethemorechallengingproblemoftailoringquantitativebehaviorsasfuturework.\nOurgoalistosolvetheinverseproblem—findasetofdesignsolutionsthatsatisfyf(x,s) = 1whensisinside\na target domain Ω, i.e., {x∗|f(x∗,s) = 1,∀s ∈ Ω}. Here f(x,s) = 1 could mean, for instance, the existence of\nbandgapsforacousticmetamaterialsorhighabsorptionforopticalmetamaterials. ThedomainΩcanbeanyfrequency"
  },
  {
    "id": 86,
    "text": "bandgapsforacousticmetamaterialsorhighabsorptionforopticalmetamaterials. ThedomainΩcanbeanyfrequency\norwavelengthinterval(s). Notethatthisdesigntargetonlyrequiresthesatisfactionofbehaviorswithinthespecified\ndomainΩ,whilebehaviorsoutsidethisdomainareoutofconcerninthisstudy,althoughitisstraightforwardtoadopt\nourmethodtothisproblemsetting.\nWeusearandomforesttoapproximatethefunctionf. Arandomforestisanensemblelearningmethodthatcombines\nthepredictionsofmultipledecisiontreestoimproveaccuracyandreduceoverfitting[59]. Thetrainedrandomforest\nservesasaforwardpredictionmodelthatpredictstheoutcomeygivendesignvariablesxandtheauxiliaryvariables.\nComparedtothewidelyusedneuralnetworks,therandomforestasaforwardpredictionmodel(1)offerssignificantly\nfastertraining,(2)hasfewerhyperparameterstotune,(3)islesssusceptibletooverfittingonsmalldata,and(4)has\nhigherinterpretability(i.e.,thedecision-makingofeachtreeintherandomforestistransparent). Moreimportantly,this"
  },
  {
    "id": 87,
    "text": "higherinterpretability(i.e.,thedecision-makingofeachtreeintherandomforestistransparent). Moreimportantly,this\ninterpretabilityalsoallowsustorealizeinversedesignwithouttrainingaseparateinversemodel.\nFigure 2 shows how, by probing the trained random forest, one can estimate a likelihood distribution for target\nsatisfaction of solutions over the entire design space and sample (generate) new designs based on this likelihood\ndistribution. Sincewetargetqualitative(binary)behaviorsatspecifieds(e.g.,abandgapin3-4MHzfrequencyorhigh\nabsorptionatawavelengthof400-500nm),wefirstidentifytheleafnodes(oneachdecisiontreeintherandomforest)\nthatarerelevanttothesinthetarget(Fig.2,Step1). Wedothisbytracingdowneachtree,checkingonlythenodes\nthatusesasthesplittingfeature,andpruningthebranchesthatareirrelevanttothesinthetarget. Forexample,as\nshowninFig.2,therearetwotreenodesusingsasthesplittingfeature,withsplittingcriteriaats ≤ 5ands ≤ 7."
  },
  {
    "id": 88,
    "text": "thatusesasthesplittingfeature,andpruningthebranchesthatareirrelevanttothesinthetarget. Forexample,as\nshowninFig.2,therearetwotreenodesusingsasthesplittingfeature,withsplittingcriteriaats ≤ 5ands ≤ 7.\nGiventhetargetfrequencyrangeof3≤s≤4,wecanremovetherightbranchesofbothnodesasthesebranchesare\nonlyrelevanttos>5ands>7,respectively,whichconflictswiththetargetrangeof3≤s≤4. Afterpruningthese\nbranches,weendupwithasetofleavesrelevanttothetarget(highlightedinFig.2,Step1). Whenwehaveacombined\ntarget(e.g.,bandgapsinboth3-4MHzand6-7MHz,asshowninFig.2),weneedtogettheintersectionofallthesets\nofrelevantleavesandusethatasthefinalsetofrelevantleaves(highlightedinFig.2,Step2). Notethatacombined\ntargetincludescaseswheretherearemultiplenonadjacenttargetranges(e.g.,3-4MHzand6-7MHz)orwhenatarget\nrangeissplitbyatreenode(e.g.,atargetrangeof4-6MHzcanbesplitbythenode“s≤5”,thusweneedtoconsider\nitasthecombinationoftwotargetranges—4-5MHzand5-6MHz). AmoredetaileddiscussionofthisstepisinSI\nAppendix,Sec.S2."
  },
  {
    "id": 89,
    "text": "itasthecombinationoftwotargetranges—4-5MHzand5-6MHz). AmoredetaileddiscussionofthisstepisinSI\nAppendix,Sec.S2.\nThenextstepistotraceupthetreefromtheN relevantleaves,obtainedbyStep2,totherootnode(Fig.2,Step3). This\nwillresultinN decisionpaths,alongwhicharenodesindicatingsplittingcriteriafordesignvariablesx. Thus,each\ndecisionpathrepresentsasetofdesignvariableranges,orinotherwords,aregioninthedesignspace. Weassigneach\nregionascoreequaltothepredictedprobabilityateachcorrespondingleaf. Thisprobabilityislearnedfromthetraining\ndataandequalstheproportionofpositivedatainaleaf. Itindicatesthetree’sbeliefintheprobabilityofadesignx\nsatisfyingthetargetT ={f(x∗,s)=1|∀s∈Ω}(Ω=[3,4]∪[6,7]inthisexample)ifthedesignfallsinthedesign\nspaceregioncorrespondingtotheleaf. Therefore,withasingledecisiontreem,wealreadyhavethemapoflikelihood\nL (x|T)=P (T|x)fortargetsatisfaction: eachoftheN regionshasauniformlydistributedlikelihoodequaltothe\nm m"
  },
  {
    "id": 90,
    "text": "L (x|T)=P (T|x)fortargetsatisfaction: eachoftheN regionshasauniformlydistributedlikelihoodequaltothe\nm m\npredictedprobabilityatthecorrespondingleaf,andtherestofthedesignspacehasalikelihoodof0(Fig.2,Step3).\n4"
  },
  {
    "id": 91,
    "text": "Target : Generate acoustic metamaterials with bandgap(s) in 3-4 and 6-7 MHz frequency ranges\nStep 1. Tracing down the tree to check s Step 2. Getting intersection Step 3. Tracing up the tree\nF 0.3 of relevantleaves F 0.3\nF s ≤ 7 T 0.6 F s ≤ 7 T 0.6\n3 ≤ s≤ 4\nF\nx1 ≤ 4\nT x2 ≤ 5 F\nT 0\n0\n.\n.\n2\n9 0.3 0.3 F\nx1 ≤ 4\nT x2 ≤ 5 F\nT 0\n0\n.\n.\n2\n9\nx0 ≤ 2 F 1.0 0.6 0.6 x0 ≤ 2 F 1.0\nT F\nx2 ≤ 3\nT 0.3 T F\nx2 ≤ 3\nT 0.3\ns ≤ 5 0.9 0.9 s ≤ 5\nF 0.2 F 0.2\nT x1 ≤ 1\nT 0.7 0.2 0.2\nT x1 ≤ 1\nT 0.7\nF 0.3\ns ≤ 7 1.0 1.0\nF T 0.6\n6 ≤ s ≤\nx\n7\n0 ≤ 2\nF\nx1 ≤ 4\nT x2 ≤ 5\nF\nF\nT\n1\n0\n0\n.\n.\n.\n0\n2\n9\n0\n0\n.\n.\n2\n3\n0\n0\n.\n.\n2\n3\nxj\n0.2 0.9\n0.0\nT s ≤ 5 F\nx2 ≤ 3\nF T 0 0 . . 2 3 0.7 0.7 0.6\nT x1 ≤ 1\nT 0.7 xi\nStep 4. Averaging likelihood over all trees in random forest\n...\nStep 5. Generating designs based on likelihood\nLikelihood of\nDesigns Target and actual bandgaps\nmeeting target\n0.4\nxj\n0.8\n0.6\nxi\nFinallikelihoodmap 3 4 6 7\nFrequency (MHz)"
  },
  {
    "id": 92,
    "text": "...\nStep 5. Generating designs based on likelihood\nLikelihood of\nDesigns Target and actual bandgaps\nmeeting target\n0.4\nxj\n0.8\n0.6\nxi\nFinallikelihoodmap 3 4 6 7\nFrequency (MHz)\nFigure2:TheinversedesignpipelineoftheproposedRIGIDmethod(usingtheinversedesignofacousticmetamaterials\nasanexample). Givendesignparametersxandtheauxiliaryvariables(e.g.,wavefrequency),atrainedrandomforest\npredictstheprobabilityofthequalitativeresponsey(e.g.,bandgapexistence). Eachtreeintherandomforestsplits\nthejointspaceofxandsintoregions,eachassociatedwithaspecificprediction(shownonleafnodes). Thesplitting\ncriteriaareencodedintreenodes. “T”meansmeetingacriterionand“F”meansnotmeetingit. RIGIDfirstidentifies\nleafnodesthatarerelevanttotheconsideredrangeofauxiliaryvariablesbycheckingsplittingcriteriarelatedtosand\npruningtreebranchesthatareirrelevant(Step1). Iftheconsideredrangeofshasmultipleparts,werepeatthisstepfor\neachpartandtaketheintersectionofrelevantleaves(Step2). Eachrelevantleafnodecorrespondstoadecisionpath"
  },
  {
    "id": 93,
    "text": "eachpartandtaketheintersectionofrelevantleaves(Step2). Eachrelevantleafnodecorrespondstoadecisionpath\nindicatingaregioninthedesignspace,aswellasapredictedprobabilityoftargetsatisfaction,whichisascorewe\nassigntothecorrespondingdesignspaceregion(Step3). Withmultipletreesinarandomforest,wecanaveragethe\nscorespredictedbyeachtreeandusetheaveragescoreasourlikelihoodestimation(Step4). Wecanthensamplefrom\nthedesignspacebasedonthelikelihooddistributiontogeneratenewdesignstailoredtothetarget(Step5). Notethat\nthe2-dimensionallikelihoodmapsareonlyforvisualizationpurposes. Theactualdimensionwillbethesameasthe\ndesignspacedimension(i.e.,thenumberofdesignvariables).\nSinceasingledecisiontreeusuallylacksaccuracy,robustness,andawaytoquantifyestimationuncertainty,westill\nwanttotakeadvantageoftherandomforestasanensemblemodelforinversedesign. WeuseSteps1-3toderivethe\nlikelihooddistributionforeachoftheM treesintherandomforest,andsimplyusetheaverageoftheseM likelihood"
  },
  {
    "id": 94,
    "text": "wanttotakeadvantageoftherandomforestasanensemblemodelforinversedesign. WeuseSteps1-3toderivethe\nlikelihooddistributionforeachoftheM treesintherandomforest,andsimplyusetheaverageoftheseM likelihood\ndistributionsasthefinallikelihoodfortargetsatisfaction,L(x|T)=\n(cid:80)M\nL (x|T)/M,whichisamorecomplex\nm=1 m\nandsmoothfunction(Fig.2,Step4). Ifmoretreesbelieveadesignxhasahigherlikelihoodofsatisfyingthetarget,\nthenthedesignwillhaveahigherlikelihoodL(x|T). Finally,togeneratenewdesigns,wecansamplefromL(x|T)\nusingMarkovchainMonteCarlo(MCMC)methodssuchasMetropolis-Hastings[60](Fig.2,Step5).\nWecanalsoderivetheposteriorofadesignxgiventhetargetifweknowthepriorp(x)basedonBayes’theorem:\np(x|T)∝L(x|T)p(x)\n5"
  },
  {
    "id": 95,
    "text": "Inthiswork, weassumeauniformprior, makingtheposteriordirectlyproportionaltothelikelihoodL(x|T), and\nsamplingfromtheposteriorisequivalenttosamplingfromthelikelihood.\nComparedtopriorworks,RIGIDprovidesthefollowinguniquebenefits:\n1. Itiseffectiveonsmalldataproblemsastherandomforestislesssusceptibletooverfitting.\n2. Thetrainingisfast(insecondsofwalltime)anddoesnotrequirecomputationally-demandinghyperparameter\ntuning. Oncethetrainingisdone,nofurthertrainingoriterativeoptimizationisrequiredtogeneratedesigns\nfordifferenttargets.\n3. Themethodestimatestheexplicitlikelihoodoftargetsatisfactionforeverypossiblesolutioninthedesign\nspace. Givenadesigntargetofspecificfunctionalbehavior,wecangenerateanunlimitednumberofsolutions\nbasedonthelikelihood, allowingustoexplorealternativesolutionsthatmighthavedesiredpropertiesor\nfunctionalitiesbeyondtheonesconsideredasthetarget.\n4. Themethodoffersahighleveloftransparencyasonecaneasilyprobethetrainedmodeltounderstandits"
  },
  {
    "id": 96,
    "text": "functionalitiesbeyondtheonesconsideredasthetarget.\n4. Themethodoffersahighleveloftransparencyasonecaneasilyprobethetrainedmodeltounderstandits\nreasoningbehindanydecision-making(i.e.,whyadesignhasahigh/lowlikelihood).\n5. When generating design solutions, one can use a single parameter — the sampling threshold — to easily\ntunethetrade-offbetweenexploitation(i.e.,generateddesignshavehigherchancesofsatisfyingthetarget)\nandexploration(i.e.,generateddesignscoverabroaderareaofthedesignspace),aswewilldemonstratein\nResults.\n3 Results\nWedemonstrateourRIGIDmethodonanacousticmetamaterialdesignproblem,anopticalmetasurfacedesignproblem,\nandtwosyntheticdesignproblems. BasedonarecentreviewarticlebyLeeetal.[34]andotherrelatedworks(e.g.,\n[38]),existingiteration-free,single-shotinversedesignmethodsweredemonstratedontrainingdatasizerangingfrom\n103to106inscale. Hereweshowthatourmethodcanworkwithmuchsmaller-scaledatasets(lessthan250training\nsamples)."
  },
  {
    "id": 97,
    "text": "103to106inscale. Hereweshowthatourmethodcanworkwithmuchsmaller-scaledatasets(lessthan250training\nsamples).\nForallthetestproblems,weusedthesamerandomforesthyperparametersettingsanddidnotperformhyperparameter\ntuning. Specifically,eachrandomforestcontains1,000treeswithaminimumof2samplesrequiredtosplitaninternal\nnodeandaminimumof1samplerequiredtobeataleafnode. Giniimpurity[61]wasusedasthesplittingcriterionat\ntreenodes. Thetrain-testsplitratiowas4:1. Sincethepositive/negativetrainingdatacanbehighlyimbalanced(e.g.,the\nfrequencyrangeswithbandgapsaremuchnarrowerthanthosewithout),weusedtheSyntheticMinorityOver-sampling\nTEchnique(SMOTE)[62]toover-samplethepositiveclass. Forallthecasestudies,therandomforesttrainingtookless\nthan10secondsonanIntelCorei5-9300HCPU2.4GHzand8GBmemory. Aftertraining,wegeneratenewdesignsby\nsamplingfromtheresultinglikelihooddistributionusingMetropolis-Hastings. Inpractice,Metropolis-Hastingscan"
  },
  {
    "id": 98,
    "text": "than10secondsonanIntelCorei5-9300HCPU2.4GHzand8GBmemory. Aftertraining,wegeneratenewdesignsby\nsamplingfromtheresultinglikelihooddistributionusingMetropolis-Hastings. Inpractice,Metropolis-Hastingscan\ngenerateidenticalsamples,whichprovidesnobenefitsfordesignpurposes. Thusinthiswork,werejectthedesigns\nidenticaltopreviousonesduringsampling.\nWecompareRIGIDtoacommonlyusedlow-costgenerativedesignstrategy—surrogatemodelingcombinedwitha\ngeneticalgorithm(GA)[63,64,65,66,67]. Wechoosethisbaselineaccordingtothefollowingcriteriatoensurethat\nitsefficiencyandapplicationscenarioarecomparabletothoseofRIGID:1)thebaselinemethodshouldgeneratedesign\nsolutionsquickly(i.e.,noadditionaltrainingordesignevaluationisrequiredgivendifferentdesigntargets),2)the\nbaselinemethodcangeneratemultiplesatisfyingsolutionstotheinversedesignproblem,and3)thebaselinemethod\ncanhandlesmalldata. ThesecriteriaexcludesequentialdesignstrategieslikeBayesianoptimization(astheyviolatethe"
  },
  {
    "id": 99,
    "text": "canhandlesmalldata. ThesecriteriaexcludesequentialdesignstrategieslikeBayesianoptimization(astheyviolatethe\nfirstcriterion)andanyexistingsingle-shotinversedesignmethodsmentionedinSec.1(asdirectinversemappingand\nT-NNviolatethesecondcriterionandconditionalgenerativemodel-basedmethodsusuallyviolatethelastcriterion).\nGenetic algorithms are optimization methods that iteratively evolve a population of candidate solutions, based on\nbiologicallyinspiredoperatorssuchasmutation,crossover,andselection,towardbettersolutions[68]. Asweusually\nneedtoevaluatealargepopulationofcandidatesolutionsateachiteration(referredtoasageneration),asurrogate\nmodel(usuallyamachinelearningmodel)isoftenconstructedtoacceleratedesignresponseprediction,therebyspeeding\nuptheoptimizationprocess. Forthesurrogatemodelinthebaselinemethod,weusethesametrainedrandomforest\nasinRIGIDforconsistency,eliminatingdifferencesduetoforwardpredictionaccuracyandfocusingoncomparing"
  },
  {
    "id": 100,
    "text": "uptheoptimizationprocess. Forthesurrogatemodelinthebaselinemethod,weusethesametrainedrandomforest\nasinRIGIDforconsistency,eliminatingdifferencesduetoforwardpredictionaccuracyandfocusingoncomparing\ntheinversedesignmethods. WesettheobjectiveofGAtomaximizetheaggregatedprobabilityoftargetsatisfaction\nwithinthetargetdomainΩ,i.e.,max (cid:80) f′(x,s ),where{s }n arediscretizedauxiliaryvariablevalues(e.g.,\nx si∈Ω i i i=1\ndiscretevaluesoffrequenciesorwavelengths)andf′ representsthetrainedrandomforest. Weincludethedetailed\nconfigurationsofGAinSec.6. Afteroptimization,werankthesolutionsgeneratedthroughouttheoptimizationprocess\nbasedontheirobjectivevaluesandselectthetopsolutionsasfinalgenerateddesigns. Wecomparethesedesignsto\n6"
  },
  {
    "id": 101,
    "text": "thosegeneratedbyRIGID.Fromthispoint,wewillcalldesignsgeneratedbyRIGIDandGAasRIGIDdesignsandGA\ndesigns,respectively.\n3.1 ApplyingRIGIDtoDesignAcousticMetamaterialswithTargetBandgaps\nHereweconsideracousticmetamaterialsthatcancontrolelasticwavepropagationatultrasound(MHz)frequencies.\nVaryingthemicroscalegeometriesofacousticmetamaterialschangesthedynamicpropertiesofamaterial,suchas\nbandgaps[18](i.e.,forbiddenfrequencyrangesofamaterial)andwavepropagationdirection[12]. Thesematerials\npromise applications in waveguides [12, 13], lenses [14, 15], and vibration mitigation [17]. Designing acoustic\nmetamaterialswithtargetbandgapsischallenging,asmanythree-dimensionalarchitecturesdonotnaturallyhavefull\nbandgaps. Wepresentthebracedcubicdesignframework(Fig. 3A-B)asamethodtotunethesizeandlocationof\nbandgaps(Fig.3C).Inparticular,sphericalmicro-inertiaareaddedtothecenterandcornerofabracedcubicunitcell\nwithstrutradiusr . Micro-inertiaplacedatthecenterofthebracehasradiusr whilemicro-inertiaplacedatthe"
  },
  {
    "id": 102,
    "text": "withstrutradiusr . Micro-inertiaplacedatthecenterofthebracehasradiusr whilemicro-inertiaplacedatthe\nstrut center\ncornerofthecubicunitcellhasradiusr .\ncorner\nWerandomlycreated284setsofgeometricparametersx=(r ,r ,r )with4≤r ≤6.41,0≤r ≤\nstrut center corner strut center\n20,and0≤r ≤20(unit: µm). Theunitcellsizewassetata=60µm. Foreachofthesedesigns,weperformed\ncorner\nBloch-waveanalysistocomputeitsacousticdispersionrelation. Bandgaplocationandwidthwereextractedforeach\ndesignbasedonitsdispersionrelation.\nOutofthe284setsofdesignvariablesandbandgapdata,weused227samplesastrainingdata. Wefirstdiscretized\ntheentirefrequencyrangeinto100intervals,andtrainedarandomforesttopredictbandgapexistencey ∈{0,1}ata\nspecificintervalsforagivendesignx. ThetrainedmodelhasatestF1scoreof0.82. Theresultingconfusionmatrix\nontestdataisshowninSIAppendix,Tab.S1.\nTotesttheinversedesigncapabilityofRIGID,werandomlycreated10targets,eachcontaining1-2frequencyranges"
  },
  {
    "id": 103,
    "text": "ontestdataisshowninSIAppendix,Tab.S1.\nTotesttheinversedesigncapabilityofRIGID,werandomlycreated10targets,eachcontaining1-2frequencyranges\ninwhichbandgap(s)shouldexist. Wegenerated30designsforeachtargetbysamplingfromtheresultinglikelihood\ndistributionoverthedesignspace1. BandgapswereidentifiedfromdispersionrelationscomputedusingBloch-wave\nanalysis.\nFigure3Dshowsthekerneldensityestimation(KDE)forthelikelihoodofthe300RIGIDdesigns,conditionedon\ntheirtargetsatisfaction. WeuseDandD torepresentthecompletesetofgenerateddesignsandthesetofgenerated\nfeas\ndesignsthatactuallysatisfythetarget,respectively. ThenD\\D denotesthesetofgenerateddesignsthatcannot\nfeas\nfulfillthetargetinreality. Inanidealscenario,allsolutionsinDwouldsatisfythetarget,whichmeansD =D ,and\nfeas\ntheirdensityprofilesshouldcoincide. However,thisidealscenarioisimpossibleinrealityduetobothlimitedmodel\naccuracyanduncertainty. AsRIGIDgeneratesdesignsbysamplingtheentiredesignspacebasedonthelikelihood"
  },
  {
    "id": 104,
    "text": "accuracyanduncertainty. AsRIGIDgeneratesdesignsbysamplingtheentiredesignspacebasedonthelikelihood\nvalues,itisexpectedfornon-satisfactorydesignstobegeneratedaslongasthelikelihoodvaluesforthesedesigns\narenon-zeros. Therefore,thefactthatadesignisgenerateddoesnotmeanthatthemodeliscertainaboutthedesign\nsatisfyingthetarget. Wehavetolookattheestimatedlikelihoodtoknowhowlikelythetargetwillbesatisfiedand\nmakedecisionsfromthere. RIGIDthusincorporatedthemodel’sconfidenceintotheinversedesignprocess. Fora\nreasonablemodel,mostdesignsinD\\D shouldhavelowestimatedlikelihoodvalues. Consequently,thedensityof\nfeas\nD ’slikelihoodisaresultofshiftingsomeofD’sdensityfromleft(lowlikelihood)toright(highlikelihood). This\nfeas\nexpectationalignswiththeobservationinFig.3D.\nWhensamplingnewdesignsorselectingsolutionsfromgenerateddesigns,wecanputasamplingthresholdτ ∈(0,1)\nonthelikelihoodvaluestofilterout“lesspromising”solutions. Tofurtherexaminemodelbehaviorandquantifyhowτ"
  },
  {
    "id": 105,
    "text": "onthelikelihoodvaluestofilterout“lesspromising”solutions. Tofurtherexaminemodelbehaviorandquantifyhowτ\naffectstheinversedesignoutcome,wedefinethefollowingmetrics:\n|D |\nSelectionRate= ϕ≥τ ,\n|D|\n|D ∩D |\nSatisfactionRate= ϕ≥τ feas ,\n|D | (1)\nϕ≥τ\n1\n|D\n(cid:88)\nϕ≥τ|\nAverageScore= q ,\n|D | i\nϕ≥τ\ni=1\nwhereD isthesetofgenerateddesignswiththelikelihoodofatleastτ (i.e.,theselecteddesigns)andq denotesthe\nϕ≥τ i\npercentageoverlapbetweenthetargetandtheactualbehaviorofselecteddesigns. Thesatisfactionrateevaluateshow\nmanyselecteddesignssatisfythetargetbasedonabinarycriterion(i.e.,whetherornotadesignsatisfiesthecomplete\n1Notethatitispossibleforthelikelihoodtobezeroeverywhereinthedesignspacewhenthemodelbelievesthetargetis\nunachievable.Weignorethesecasesasitismeaninglessandimpossibletosampledesignsfromsuchlikelihooddistribution.\n7"
  },
  {
    "id": 106,
    "text": "Target not satisfied\nAll designs\nTarget satisfied\nEstimated likelihood\nytisneD\nr\ncorner r\ncenter\nr\nstrut\na\nReduced wavevector\nr = 4.32 r = 4.39 r = 4.35 r = 4.76 r = 4.70\nstrut strut strut strut strut\nr = 14.15 r = 18.26 r = 14.60 r = 16.32 r = 17.15\ncenter center center center center\nr = 8.42 r = 9.27 r = 2.18 r = 7.53 r = 0.04\ncorner corner corner corner corner\nLikelihood: 0.484 Likelihood: 0.436 Likelihood: 0.334 Likelihood: 0.328 Likelihood: 0.321\n)zHM(\nycneuqerF\n)zHM(\nycneuqerF\nAverage score\nSatisfaction rate (GA)\nSatisfaction rate\nSelection rate\nSampling threshold\nReduced wavevector\nscirteM\nA C D\nE\nB\nF\nFigure3: Acousticmetamaterialdesignproblemconfigurationandresults. (A)Designvariablesofcenterandcorner\nmassradii(r andr )andstrutradius(r ). (B)HighsymmetrypointsofthecubicirreducibleBrillouinzone.\ncenter corner strut\n(C)Asampledispersionrelationandbandgap(markedbythehighlightedzone). Thedesignobjectiveistogenerate"
  },
  {
    "id": 107,
    "text": "center corner strut\n(C)Asampledispersionrelationandbandgap(markedbythehighlightedzone). Thedesignobjectiveistogenerate\nnewacousticmetamaterialdesignswithtargetbandgaps. (D)KDEoftheestimatedlikelihoodforgenerateddesigns.\n(E)Satisfactionrates,averagescores,andselectionratesforRIGIDdesignsundervaryingsamplingthresholds(solid\nlines),incomparisontothesatisfactionrateofGAdesigns(horizontaldashedline). Thehorizontaldottedlineindicates\n100% satisfaction. (F) Geometries and corresponding dispersion relations of five RIGID designs with the highest\nlikelihoodofsatisfyingaspecifiedtargetbandgap(markedashighlightedfrequencyregions). Alltheradii(r ,r ,\nstrut center\nandr )haveaunitofµm. Hereonlythefourthdesignfailstomeetasmallportion(ataround6MHz)ofthetarget\ncorner\nbandgap,whereastheothersmeetit. GenerateddesignsforothertargetsareshowninSIAppendix,Figs.S2-S6.\ntarget),whereastheaveragescoreprovidesasoftmeasurewherepartialsatisfactionisalsocounted. Theaveragescore\nislower-boundedbythesatisfactionrate."
  },
  {
    "id": 108,
    "text": "target),whereastheaveragescoreprovidesasoftmeasurewherepartialsatisfactionisalsocounted. Theaveragescore\nislower-boundedbythesatisfactionrate.\nAsshowninFig.3E,theselectionratedecreaseswhenτ increasessincemoresolutionsarefilteredout. Ontheother\nhand,boththesatisfactionrateandtheaveragescoreincreasewithτ,whichindicatesahighcorrelationbetweenthe\nestimatedlikelihoodofasolutionanditsprobabilityofactuallyachievingthetarget. Asτ reaches0.6,thesatisfaction\nrateandtheaveragescorereach1,indicatingthatallgenerateddesignssatisfytheirtargets. Whensamplingorselecting\nnewsolutions,wecanusethesamplethresholdτ totunethetrade-offbetweenexploitationandexploration—alowτ\nfavorsexplorationassampledsolutionswillcoveralargerareaofthedesignspace,whileahighτ favorsexploitation\nassampledsolutionswillhaveahigherchanceofsatisfyingthetarget.\nInthecomparativestudy,wegeneratedthesamenumberofdesignsforthesametargetsbyapplyingGA.Wecomputed"
  },
  {
    "id": 109,
    "text": "assampledsolutionswillhaveahigherchanceofsatisfyingthetarget.\nInthecomparativestudy,wegeneratedthesamenumberofdesignsforthesametargetsbyapplyingGA.Wecomputed\nthesatisfactionrateofGAdesignsandfoundthatthevalueisclosetothesatisfactionrateofRIGIDdesignswhen\n8"
  },
  {
    "id": 110,
    "text": "settingτ to0.2. Thismeansthatifwesetτ tobeabove0.2,RIGIDwillmostlikelyachieveahighersatisfactionrate\nthanGA.ThedistributionsofgeneratedsatisfactorysolutionsshowninFig.4indicatethatGAdesignsarehighly\nlocalizedwhileRIGIDcancapturethehighdiversityoftheinversedesignsolutions,andsometimesdiscoversolutions\nverydifferentfromtheoriginalfeasiblesolutionsfromdata. Besides,Fig.4alsoshowsanexamplewhereGAfailsto\nfindanysatisfactorysolutionwhileRIGIDcanstillgenerateaverydiversesetofsatisfactorysolutions.\nStrut radius Center mass Corner mass\nradius radius\nssam\nretneC\nssam\nrenroC\nsuidar\nturtS\nsuidar\nsuidar\nTarget bandgap in [6.14, 6.61] and [8.52, 8.96] (MHz) Target bandgap in [5.01, 5.90] (MHz)\nStrut radius Center mass Corner mass\nradius radius\nsuidar\nturtS\nssam\nretneC\nssam\nrenroC\nsuidar\nsuidar\nData\nRIGID\nGA\nFigure4: Distributionsofsatisfactorysolutionsfortwobandgaptargets. Theoff-diagonalplotsshowthepairwise"
  },
  {
    "id": 111,
    "text": "radius radius\nsuidar\nturtS\nssam\nretneC\nssam\nrenroC\nsuidar\nsuidar\nData\nRIGID\nGA\nFigure4: Distributionsofsatisfactorysolutionsfortwobandgaptargets. Theoff-diagonalplotsshowthepairwise\nbivariatedistributionsofdesignvariables,andthediagonalplotsshowthemarginaldistributionsofthedataineach\ncolumn. TheleftpanelshowsthatGAdesignsarehighlylocalizedwhileRIGIDcanleadtodiversesolutions. Theright\npanelindicatesthatnoneoftheGAdesignssatisfythetarget,whilesatisfactoryRIGIDdesignsarediverseandcanbe\nverydifferentfromfeasibledesignsfromdata. Solutionsfromdataincludefeasibledesignsinbothtrainingandtest\ndata.\nFigure3FvisualizesthegeometriesanddispersionrelationsofRIGIDdesignsgeneratedbasedonarandomlycreated\nbandgaptarget. Onlythetopfivedesignswiththehighestlikelihoodvaluesareshown. Inthisexample,ourmethod\ngeneratesgeometricallydifferentdesignsthathaveahighprobabilityofachievingtargetbandgaps,eachyieldinga"
  },
  {
    "id": 112,
    "text": "generatesgeometricallydifferentdesignsthathaveahighprobabilityofachievingtargetbandgaps,eachyieldinga\nslightlydifferentdispersionrelation. Thisispromisingindesignapplicationsrequiringothermaterialproperties,such\nasdynamicwavevelocityorquasi-staticstiffness,inwhichtheusercanselectfromamenuofdesignswiththesame\ntargetbandgapbutothervaryingproperties. Generateddesignsbasedontheotherninebandgaptargetscanbefoundin\nSIAppendix,Figs.S2-S6.\n3.2 ApplyingRIGIDtoDesignOpticalMetasurfaceswithTargetHigh-AbsorbanceWavelengths\nOpticalmetasurfacesareartificiallyengineeredsystemsthatcansupportexoticlightpropagationbuildingonsubwave-\nlengthinclusions[7,69,70,71,72,73]. Amongadiversearrayofdevices,metamaterialabsorbers[74,75,76,77,78,\n79,80]havebeenintenselystudiedformedicalimaging,sensing,andwirelesscommunications.\nInthiscasestudy,weconsiderfourtypesofcross-sections(c∈{1,2,3,4})chosenfromtheliterature(Fig.5B).Itis"
  },
  {
    "id": 113,
    "text": "79,80]havebeenintenselystudiedformedicalimaging,sensing,andwirelesscommunications.\nInthiscasestudy,weconsiderfourtypesofcross-sections(c∈{1,2,3,4})chosenfromtheliterature(Fig.5B).Itis\nassumedthata3Dgeometricinstanceiscomposedofastackofthreelayersofprismaticunitcells,eachofwhichis\nverticallyextrudedandstacked(Fig.5A).Thegeometriesconstructedinthiswaycanberegardedasaninstantiationof\nmultilayeredmetasurfaces[81,82,83,84,85],whichofferricherdesignfreedomthanthesingle-layercounterpart.\nThethicknessofeachlayer(h ,l=1,2,3)isallowedtocontinuouslyvarybetween50and150nm. Hereinwedonot\nl\nconsiderparametricvariationsofagiventypeofunitcellcross-section;yetthosecanbetriviallyincorporatedinthe\nproposeddesignframeworkifnecessary.\n9"
  },
  {
    "id": 114,
    "text": "A B C\nMaterial 3 t\nMaterial 2 h\n3\nMaterial 1 h\n2\nh\n1\nD E\nTarget not satisfied\nAll designs\nTarget satisfied\nF\nG\necnabrosbA\nWavelength (nm)\nEstimated likelihood\nWavelength (nm)\nytisneD\necnabrosbA\ntnuoC\nSatisfaction rate\nSelection rate\nSampling threshold\nh 1 = 123.47 h 1 = 141.34 h 1 = 126.21 h 1 = 140.66 h 1 = 109.85\nh 2 = 97.30 h 2 = 141.74 h 2 = 124.31 h 2 = 116.79 h 2 = 78.19\nh 3 = 139.03 h 3 = 141.01 h 3 = 143.99 h 3 = 145.87 h 3 = 146.44\nLikelihood: 0.397 Likelihood: 0.393 Likelihood: 0.391 Likelihood: 0.388 Likelihood: 0.379\nscirteM\nSatisfaction rate (GA)\nAverage score\nMaterial 1 Material 2 Material 3 Geometric type\nytisneD\nRIGID GA\nThickness 1 Thickness 2 Thickness 3\nFigure 5: Optical metasurface design problem configuration and results. (A-B) Design variables (materials, layer\nthicknesses, and cross-section geometry types). (C) A sample absorbance spectrum and the wavelength intervals"
  },
  {
    "id": 115,
    "text": "thicknesses, and cross-section geometry types). (C) A sample absorbance spectrum and the wavelength intervals\n(highlightedwavelengthregions)correspondingtoabsorbanceabovethethresholdt. Thedesignobjectiveistogenerate\nnew optical metasurface designs that exhibit higher absorbance than a threshold t at the user-defined wavelength\ninterval(s). (D)KDEoftheestimatedlikelihoodforgenerateddesigns. (E)Satisfactionrates, averagescores, and\nselectionratesforRIGIDdesignsundervaryingsamplingthresholds(solidlines),incomparisontothesatisfaction\nrate of GA designs (horizontal dashed line). (F) Designs (geometries and material selections) and corresponding\nabsorbancespectraoffivemetasurfacesgeneratedbyRIGID.Thesefivesolutionsaregenerateddesignswiththehighest\nlikelihoodofsatisfyingspecifiedtargethigh-absorbanceregions(markedashighlightedwavelengthregions). Allthe\nlayerthicknesses(h ,l=1,2,3)haveaunitofnm. Hereallfivedesignssatisfythetarget. Generateddesignsforother\nl"
  },
  {
    "id": 116,
    "text": "layerthicknesses(h ,l=1,2,3)haveaunitofnm. Hereallfivedesignssatisfythetarget. Generateddesignsforother\nl\ntargetsareshowninSIAppendix,Figs.S7-S9. (G)Distributionsofdesignvariablesforsatisfactorysolutionsgenerated\nbyRIGIDandGA(forthesametargetdefinedinPanelF).GAdesignsarehighlylocalizedandlackdiversitycompared\ntoRIGIDdesigns.\n10"
  },
  {
    "id": 117,
    "text": "We also design the material of each layer (m ,l = 1,2,3). Three dielectric materials of interest, each of which is\nl\nassignedtoadifferentcolorinFig.5A,areTi(red),Si(blue),andGe(yellow). Thechallengeassociatedwiththis\ndesignproblemisitsmixed-variabledesignspacecontainingthreecontinuousvariables(i.e.,layerthicknesses)and\nfourcategoricalvariables(i.e.,materialchoicesateachlayerandthegeometrytype),whichmayleadtopotentially\ncomplicatedunderlyingdesign-responserelations(e.g.,thoseacrossdifferentgeometryormaterialtypes). Ingene√ral,a\ndielectricmaterialischaracterizedthroughacomplexrefractiveindexn˜ ∈Cdefinedasn˜ =n+jk,wherej = −1\nistheimaginaryunit,n∈Rinvolvesthespeedatwhichthelightpropagatesthroughthematerial,andk ∈Risthe\nextinctioncoefficientthatdictatestheenergylossduetothematerial. Withinthefrequencyregimeofinterest,those\nexhibitnonlineardispersion;boththerealandimaginarytermsingeneralareanon-analyticfunctionofexcitation"
  },
  {
    "id": 118,
    "text": "exhibitnonlineardispersion;boththerealandimaginarytermsingeneralareanon-analyticfunctionofexcitation\nwavelengths,i.e.,n(s)andk(s). Inaddition,theimpactofthesamematerialchoiceonthespectralresponseA(s)\nvariesdependingonthelayerlocationatwhichthematerialisplaced.\nBasedontheaboveconfiguration,werandomlysampled258setsofdesignvariablesx=(c,h ,h ,h ,m ,m ,m )\n1 2 3 1 2 3\nandcomputedtheircorrespondingabsorbancespectrausingwaveanalysis. Wesett=0.9astheabsorbancethreshold,\nsothat“high”absorbancemeanstheabsorbanceA(s)isnolessthan0.9. Wetrainedarandomforeston206training\ndata (i.e., 80% of the 258 designs and corresponding absorbance spectra) to predict whether “high” absorbance is\npresented(i.e.,thebinaryindicatory =1)atawavelengthsforagivendesignx. Thetrainedrandomforesthasatest\nF1scoreof0.83. TheconfusionmatrixontestdataisshowninSIAppendix,Tab.S2.\nNotethatthisprobleminvolvesinversedesignwithbothcontinuousandcategoricalvariables,whichcommonoptimiza-"
  },
  {
    "id": 119,
    "text": "F1scoreof0.83. TheconfusionmatrixontestdataisshowninSIAppendix,Tab.S2.\nNotethatthisprobleminvolvesinversedesignwithbothcontinuousandcategoricalvariables,whichcommonoptimiza-\ntionandgenerativemodeling-basedinversedesigncannothandlewellwithoutspecialtreatment[86,87,88]. Onthe\notherhand,ourrandomforest-basedmethodcannaturallyaddresssuchmixed-variableproblemswithoutanyissues.\nSimilartotheacousticmetamaterialdesignproblem,weuse10randomlycreatedtargetstoevaluatetheinversedesign\nperformanceofRIGID,exceptthathereatargetisrepresentedasthewavelengthrange(s)withinwhichabsorbance\nshouldbeatleast0.9. Wegenerated100designsforeachtargetbysamplingfromtheestimatedlikelihooddistribution.\nAmongthe1,000generatedsolutions,wesuccessfullyconductedwaveanalysisfor911designsandobtainedtheir\nabsorbancespectra. Figure5DshowstheKDEforthelikelihoodofthese911designs, conditionedontheirtarget\nsatisfaction. Thedensitiessharesimilarbehaviorasintheacousticproblem(Fig.3D)—unsatisfied/infeasibledesigns"
  },
  {
    "id": 120,
    "text": "satisfaction. Thedensitiessharesimilarbehaviorasintheacousticproblem(Fig.3D)—unsatisfied/infeasibledesigns\nD\\D areconcentratedatlowlikelihoodregions,whichcausesthelikelihooddensityofsatisfied/feasibledesigns\nfeas\nD tobearesultofshiftingsomeofD’sdensityfromleft(lowlikelihood)toright(highlikelihood). Thesampling\nfeas\nthresholdandmetricsrelationshowninFig.5Ealsofollowthesametrendasintheacousticproblem(Fig.3E),which\nagaindemonstratesastrongpositivecorrelationbetweentheestimatedlikelihoodandtheprobabilityofgenerated\ndesignsactuallyachievingtheirtargets.\nWealsogeneratedthesameamountofdesignsunderthesametargetsbyusingGA.WefoundthatGAdesignscan\ngenerate100%satisfactionrate(Fig.5E).However,similartotheacousticmetamaterialdesignexample,thegenerated\ndesignsarelocalizedcomparedtoRIGIDdesignswhichcovermuchwiderfeasibleregion(s)inthedesignspace. This\ncanbereflectedbythedesignvariabledistributionsofgeneratedfeasiblesolutionsshowninFig.5G,whichcorresponds"
  },
  {
    "id": 121,
    "text": "canbereflectedbythedesignvariabledistributionsofgeneratedfeasiblesolutionsshowninFig.5G,whichcorresponds\ntogenerateddesignsforarandomlycreatedtarget(thetargetisshownasthehighlightedwavelengthregionsinFig.5F).\nForexample,alltheGAdesignshavethesamecross-sectiongeometrictypewhileRIGIDgeneratedsatisfactorydesigns\nwithdifferentgeometrictypes. Itiseasytoseethat,withoutconsideringsolutiondiversity,achievingahightarget\nsatisfactionrateistrivial—aslongaswediscoveronesatisfactorysolution,wecanaddsufficiently-smallperturbations\ntothatsolutiontogenerateaninfinitenumberofalternativesolutionsthathavesimilarresponsesandthusarealsolikely\ntosatisfythetarget. Notethatcomparedtotheacousticproblem,thisopticalproblemislesschallengingasthereisan\naverageof28.5%dataalreadysatisfyingthetentargets(incontrasttoanaverageof9.3%satisfactorydesignsfromdata\nintheacousticproblem). Therefore,findingafewsatisfactorysolutionsfortheopticalproblemisrelativelyeasy;the"
  },
  {
    "id": 122,
    "text": "intheacousticproblem). Therefore,findingafewsatisfactorysolutionsfortheopticalproblemisrelativelyeasy;the\nchallengeliesindiscoveringallfeasiblesolutions,whereRIGIDexcelsbyidentifyingawiderangeofthem. Wecan\nalsoseethatwhenthesamplingthresholdis0.8,RIGIDachievesasatisfactionrateof100%,demonstratingitsability\ntotradeoffthediversityofgenerateddesignsforhigherfeasibility.\nFigure5FshowsgeneratedRIGIDdesignswiththetopfivelikelihoodestimationsforarandomlycreatedtarget. While\nthematerials,cross-sectiongeometries,andlayerthicknessesofgenerateddesignscanbedifferent,allthedesigns\nsatisfythetarget. GenerateddesignsbasedontheotherninetargetscanbefoundinSIAppendix,Figs.S7-S9.\n3.3 SyntheticDesignProblemsforRapidValidationandVisualization\nWhiletheabovemetamaterialdesignproblemsrepresentpracticalusecases,thevalidationstudyistime-consuming\nduetotheexpensivecomputationofmetamaterials’responsessuchasdispersionrelationsandabsorbancespectra. To"
  },
  {
    "id": 123,
    "text": "duetotheexpensivecomputationofmetamaterials’responsessuchasdispersionrelationsandabsorbancespectra. To\nallowfastvalidationoftheproposedmethodandeasierinspectionoftheestimatedlikelihoodinthedesignspace,we\ncreatetwosyntheticcasestudies. Bothproblemshave2-dimensional“designspaces”thatalloweasyvisualization.\n11"
  },
  {
    "id": 124,
    "text": "Superposed Sine Functions Synthetic Ranges\nt\nz Design ID\ns\nSquared Exponential Functions Synthetic Ranges\nt\nz\nB\ns\nA\nDesign ID\nFigure6: Syntheticdatacreationfor(A)theSqExpproblemand(B)theSupSinproblem. Foreachproblem,theleft\npanelshows100functionswithrandomlysampledparametersaandb. Wetreataandbassyntheticdesignvariables,\nandthecorrespondingfunctionsasquantitativeresponses(e.g.,absorbancespectraofopticalmetasurfaces). Theright\npanelshowsqualitativeresponses(e.g.,high-absorbancewavelengthrangesorbandgaps)aresimulatedbysynthetic\nranges,derivedbythresholdingthe100functions(Equations2and3withthresholdt=0.9).\nSqExp Problem To construct the first synthetic problem, we used a squared exponential function with tunable\nparametersaandbtomimicthequantitativefunctionalresponseofmetamaterials. Thequalitativeresponse(e.g.,“high”\nor“low”energyabsorptionatawavelength)isdefinedas:\n (cid:18) (cid:16) (cid:17)2 (cid:19)\n0, ifz =exp − s−a <t\nI(a,b;s)= 0.3b+0.1 , (2)\n1, otherwise"
  },
  {
    "id": 125,
    "text": "or“low”energyabsorptionatawavelength)isdefinedas:\n (cid:18) (cid:16) (cid:17)2 (cid:19)\n0, ifz =exp − s−a <t\nI(a,b;s)= 0.3b+0.1 , (2)\n1, otherwise\nwherezrepresentsquantitativeresponseandtisathresholdthatconvertszintoaqualitativeresponseI(a,b;s). Specif-\nically,I(a,b;s)=1canmeantheexistenceofabandgaporhighabsorbanceatafrequencys. Then{s|I(a,b;s)=1}\nrepresentsarangeofsthatmimicsourinterestedmaterialbehavior,suchasthefrequencyrangeofthebandgaporthe\nwavelengthrangeofhighabsorbance. Wecallthisthepositiverange. Byvaryingaandb,wecanproducedifferent\nsyntheticresponsesandpositiveranges. Therefore,wecanuseaandbassyntheticdesignvariables. Thereisaclear\nrelationbetweenthesedesignvariablesandthepositiverangethatEq.2creates—aandbcontrolthecenterlocation\nandthewidthofthepositiverange,respectively.\nInthisdesignproblem,wesampled100setsofaandbuniformlyatrandom. Wesettas0.9. BasedonEq.2,we\nobtainedthecorrespondingresponses(Fig.6A).Thesesetsofa,b,andresponsesconstituteadatasetfortrainingand"
  },
  {
    "id": 126,
    "text": "Inthisdesignproblem,wesampled100setsofaandbuniformlyatrandom. Wesettas0.9. BasedonEq.2,we\nobtainedthecorrespondingresponses(Fig.6A).Thesesetsofa,b,andresponsesconstituteadatasetfortrainingand\ntestingourmodel.\nSupSinProblem Anothersyntheticdesignproblemwasconstructedbyreplacingthesquaredexponentialfunction\nin the SqExp problem with a superposed sine function. Given synthetic design variables a and b, we can produce\nqualitativeresponsesusingthefollowingequation:\n(cid:26)\n0, ifz =sin(2π(s+a))+sin(3π(s+b))<t\nI(a,b;s)= . (3)\n1, otherwise\n12"
  },
  {
    "id": 127,
    "text": "Same as in the SqExp problem, we set t = 0.9 and created a dataset with 100 sets of synthetic design variables\nandcorrespondingrangesderivedfromsyntheticresponses(Fig.6B).Unlikethesquaredexponentialfunction,the\nsuperposed sine function can be multimodal, which means it can result in multiple synthetic ranges to mimic, for\nexample,multiplebandgaps. Thebandgaplocationsarecontrolledbyaandb.\nA Ω = [0.09, 0.13] Ω = [0.25, 0.28] Ω = [0.47, 0.51] Ω = [0.84, 0.88]\nΩ = [0.22, 0.26]\nΩ = [0.16, 0.2] Ω = [0.47, 0.52] Ω = [0.87, 0.9]\nU [0.31, 0.34]\n1 Tree 10 Trees 100 Trees 1000 Trees\npxEqS\nniSpuS\ndoohilekil\ndetamitsE\nB\npxEqS\nniSpuS\nC\nSampling threshold\nscirteM\nSampling threshold\nSupSin\nscirteM\nSqExp\nAverage score\nRIGID\nGA\nSatisfaction rate\nSelection rate\nAverage score\nSatisfaction rate\nSelection rate\nFigure7: Visualizationofestimatedlikelihoodandvalidationmetricsforsyntheticproblems. (A)Likelihoodfunction"
  },
  {
    "id": 128,
    "text": "RIGID\nGA\nSatisfaction rate\nSelection rate\nAverage score\nSatisfaction rate\nSelection rate\nFigure7: Visualizationofestimatedlikelihoodandvalidationmetricsforsyntheticproblems. (A)Likelihoodfunction\nvaluesforrandomlycreateddesigntargets. Orangelinesshowboundariesofactualfeasibleregionsassociatedwith\nthetargetsT = {I(a,b;s) = 1|∀s ∈ Ω}. PointsshowsatisfactoryRIGIDdesignsandGAdesigns. (B)Likelihood\nfunctionvaluesestimatedbyrandomforestswithvaryingnumbersofdecisiontrees. ThedesigntargetissetasT =\n{I(a,b;s) = 1|∀s ∈ [0.45,0.48]}fortheSqExpproblemandT = {I(a,b;s) = 1|∀s ∈ [0.63,0.68]∪[0.69,0.71]}\nfortheSupSinproblem. (C)Validationmetricsforinversedesigngeneration.\nForeachsyntheticexample,wesplitthedatainto80trainingdataand20testdata. Wetrainedarandomforestwith\nthesamehyperparametersettingsastheotherproblems,topredictthebinaryindicatorsI(a,b;s). TheF1scoresare\n0.85and0.86fortheSqExpandtheSupSinproblems,respectively. TheresultingconfusionmatricesareshowninSI"
  },
  {
    "id": 129,
    "text": "thesamehyperparametersettingsastheotherproblems,topredictthebinaryindicatorsI(a,b;s). TheF1scoresare\n0.85and0.86fortheSqExpandtheSupSinproblems,respectively. TheresultingconfusionmatricesareshowninSI\nAppendix,TablesS3-S4. Weevaluatedtheinversedesignperformancewiththetrainedmodels.\nDuetothefastevaluationofEquations2and3,wecanexhaustallthepossiblesolutionsinthedesignspacetoobtain\ntheground-truthfeasibleregion(s)foratarget. Figure7Ashowstheestimatedlikelihoodvaluesandtheground-truth\nfeasibleregionsunderrandomlycreatedtargets. Ingeneral,high-likelihoodregionsmatchactualfeasibleregionswell,\nwhichfurtherdemonstratestheeffectivenessofRIGID.WecanalsoobservethatfeasibleregionsintheSqExpandthe\nSupSinproblemsfollowdistinctpatterns. IntheSqExpproblem,aandbcontrolthecenterlocationandthewidthof\nthepositiverange,respectively. Therefore,thepositionofthefeasibleregionalongthea-axismoveswiththelocation"
  },
  {
    "id": 130,
    "text": "thepositiverange,respectively. Therefore,thepositionofthefeasibleregionalongthea-axismoveswiththelocation\nofthetargetrange,whilethefeasibleregiongraduallyshrinksasbdecreasessincethedecreaseofb(i.e.,positiverange\nwidth)causesthechoiceofa(i.e.,positiverangecenterlocation)tobemorerestrictedtofitthetargetrange. Inthe\nSupSinproblem,theremightbemultiplepositiverangesappearingatthepeaksofthesuperposedsinefunctioninEq.3.\nDesignvariablesaandbcontrolpositiverangelocationsbytranslatingeachsinefunction. Duetothesinefunction’s\n13"
  },
  {
    "id": 131,
    "text": "periodicity,wecanobtainmultiplefeasibleregionsalongbotha-andb-axes. Figure7Ashowsthatthelikelihood\nestimationbyRIGIDsuccessfullycapturedtheabove-mentionedpatternsoffeasibleregions.\nWealsovisualizethedistributionsofsatisfactorydesignsgeneratedbybothRIGIDandGAinFig.7A.Again,we\nobserveamuchbroadercoverageofthefeasibledesignspacebyRIGIDdesigns,whileGAdesignsaremerelyminor\nperturbationsofoneortwofeasiblesolutions.\nFigure7Bdemonstrateshowtheestimatedlikelihoodvarieswhenincreasingthenumberoftreesinarandomforest.\nWithasingledecisiontree,theestimatedlikelihoodfunctionisalmostabinaryfunctionandhighlyinaccurate. The\nlikelihood in the SqExp case is even zero everywhere, which makes it impossible to sample designs based on the\nlikelihood. Asthenumberoftreesincreases,thelikelihoodfunctionbecomessmootherandeventuallyconverges.\nBesidesthesequalitativevisualinspections,wealsocalculatedthemetricsproposedinEq.1,asshowninFig.7C.For"
  },
  {
    "id": 132,
    "text": "Besidesthesequalitativevisualinspections,wealsocalculatedthemetricsproposedinEq.1,asshowninFig.7C.For\neachofthetwosyntheticproblems,thesemetricswerecomputedon500designsgeneratedbygivingfiverandomtarget\nranges. Again,thesatisfactionrateandtheaveragescoreincreasewiththesamplingthreshold,indicatingastrong\ncorrelationbetweenthesamplingthresholdandtheprobabilityofgenerateddesignsactuallyachievingtheirtargets. In\nbothproblems,alltheselecteddesignssatisfytheirtargets(i.e.,thesatisfactionratesandaveragescoresreach1)when\nthesamplingthresholdreaches0.8.\n4 Discussion\nInthissection,wediscussthebenefitsandnecessitiesofRIGIDintermsofitsinterpretability,datademand,theability\ntodealwithdiscretedesignvariables,andthediversityofgenerateddesigns.\n4.1 Interpretability\nRIGID offers two levels of interpretability. Firstly, RIGID is more transparent than deep learning models. The\ntransparencyoftherandomforestasaforwardmodelhelpsusunderstandhowthepredictionsaremade. Theinverse"
  },
  {
    "id": 133,
    "text": "transparencyoftherandomforestasaforwardmodelhelpsusunderstandhowthepredictionsaremade. Theinverse\ndesignprocessbasedonlikelihoodestimationandsamplingisalsotransparent. Thetransparencyofbothforward\nandinverseprocessesoffersinsightsintothedecision-makingprocessanddiagnosticcapabilities. Secondly,owingto\ntheinterpretabilityoftheforwardpredictionmodel,wecanderivethelikelihood,whichtheinversedesignrelieson,\nbyprobingintoeachdecisionpathoftreesintherandomforest. Thelikelihoodisaninterpretablemetrictohelpus\nunderstandtheprobabilityofagenerateddesignsatisfyingthegiventargetandthusaidthedecision-makingprocess\nwhenselectingfinalsolutions. Theresults(Figures3D,3E,5D,5E,and7C)havedemonstratedahighcorrelation\nbetweenthelikelihoodandactualprobabilityoftargetsatisfaction. Thelikelihoodalsoservesasanindicatorofthe\nneedforadjustingthedesignspace. Whenthelikelihoodislowacrosstheentiredesignspace,themodelbelieves"
  },
  {
    "id": 134,
    "text": "needforadjustingthedesignspace. Whenthelikelihoodislowacrosstheentiredesignspace,themodelbelieves\nthetargetisalmostimpossibletoachieve,whichmayindicateaneedforexpandingthedesignspace(i.e.,extending\ndesignspaceboundsoraddingnewdesignfreedom)toallowwidercoverageoftheresponsespaceandeasierdiscovery\nofsatisfyingsolutions. PleaseseemorediscussioninSIAppendix, Sec.S4. Anotherbenefitofhavingtheexplicit\nlikelihoodasaninterpretablemetricisthatwecanadjustthelikelihoodthresholdtotrade-offbetweenexplorationand\nexploitation—higher(lower)thresholdmeansmorefocusonexploitation(exploration). Bothlevelsofinterpretability\nwereabsentfrommostpriorworksofiteration-free,single-shotinversedesignmethods.\n4.2 DataDemand\nRIGIDisparticularlyusefulwhendatacollectionisexpensive,asinmanycaseswherehigh-fidelitysimulationor\nexperimentaldataareneeded. Despitetheadvancementofsurrogatesolversandhardware-acceleratedsimulation,a"
  },
  {
    "id": 135,
    "text": "experimentaldataareneeded. Despitetheadvancementofsurrogatesolversandhardware-acceleratedsimulation,a\npersistenttrade-offbetweencomputationalresourcesandaccuracyremains,exacerbatedbychallengesinaccessing\nhigh-performanceinfrastructureandcollectinghigh-fidelitydata. Withmostofthepriorinversedesignworksbased\nondeeplearningwithahighdemandfordataandcomputationalresources,itisusefultocreateamethodthatworks\nunderscenariosattheotherendofthedata/computationalresourcerequirementspectrum. Additionally,whentheonly\nwaytoacquirehigh-fidelitydataisthroughexperiments,datadeficiencywillbecomeapressingchallengefordeep\nlearning-basedmethods. Whilethereisalargecollectionofdeeplearningmethodstoaddresssingle-shotinversedesign\nproblemswithlargedatasets,thesettingofsmalldataseemstobeunder-studiedbypriorworks,despitethissetting’s\npracticalityunderconstrainedbudgets. RIGIDfillsthisgapbyofferingamuchmoredata-efficientandinterpretable\nalternativetodeep-learning-basedsingle-shotinversedesignmethods.\n14"
  },
  {
    "id": 136,
    "text": "4.3 DiversityversusFeasibilityofGeneratedDesigns\nTheMCMCinRIGIDcansamplenewdesignsintheentiredesignspacebasedontheestimatedlikelihood,whichisnot\nlimitedtolocaloptimalityandhasthepotentialtodiscovermultipleverydifferentsatisfyingsolutions(ifany)inone\nshot. ThisisclearlydemonstratedintheSupSinexamplewheretherearemultipledisconnectedfeasibleregionsinthe\ndesignspaceandthelikelihoodestimatedbyRIGIDsuccessfullycoincideswiththesefeasibleregions(Fig.7A).The\nabilitytogeneratemultiplesatisfyingsolutionsallowsustoconsiderotherfiguresofmerit,suchasmanufacturability,\ncompatibility,andcost,inadditiontothemainobjectiveofsatisfyingthetargetresponse. Besidesenablingflexibilityof\ndesignchoice,generatingacomprehensivesolutionsetcanalsobringmoreinsightsintothestructure-propertyrelation\nassociatedwiththetarget. Forexample, afterobtainingadiversesetofdesignssatisfyingthesametarget, wecan\ninvestigatethedifferencesamongthesedesignstoidentifycriticalfeaturesthatleadtothesatisfactionofthetarget."
  },
  {
    "id": 137,
    "text": "investigatethedifferencesamongthesedesignstoidentifycriticalfeaturesthatleadtothesatisfactionofthetarget.\nForallfourproblemsdemonstratedinthiswork,whenthesamplingthresholdissetto0.8,RIGIDachievesasatisfaction\nrateof100%. AlthoughGAcanalsoleadtoahightargetsatisfactionrateforrelativelysimpleinversedesignproblems\n(e.g.,theopticalmetasurfacedesignprobleminSec.3.2),thegeneratedsolutionslackdiversity. Incontrast,RIGID\nprovidesauniquewaytocontrolthetrade-offbetweenthediversityandthefeasibilityofgenerateddesignsbytuning\nthe sampling threshold. It is important to note that, without considering diversity, a high target satisfaction rate\nbecomestrivial,aswecansimplyapplysufficiently-smallperturbationstoonefeasiblesolutiontogenerateaninfinite\nnumberofsimilarsolutionsthathavesimilarresponsesandhencearealsolikelytomeetthetarget. Inmanyinverse\ndesignproblemswheresolutionsarenon-unique, findingonesolutioncanbesimple, buttherealchallengeliesin"
  },
  {
    "id": 138,
    "text": "numberofsimilarsolutionsthathavesimilarresponsesandhencearealsolikelytomeetthetarget. Inmanyinverse\ndesignproblemswheresolutionsarenon-unique, findingonesolutioncanbesimple, buttherealchallengeliesin\ndiscoveringfeasiblesolutionsthatcoverabroadrangeofthedesignspace—aproblemwhereRIGIDwilloutperform\noptimization-basedmethodsasdemonstratedbyourresults.\n4.4 DiscreteDesignVariables\nTheuseofarandomforestinRIGIDallowseasyconsiderationofdiscretedesignvariables. Forexample,ouroptical\nmetasurfacedesignproblemhasthegeometrytypeandthematerialchoiceforeachlayerascategoricaldesignvariables.\nIncontrast, thesediscretevariableswillposechallengesforotherinversedesignmethodsbasedonsomemachine\nlearningmodels(e.g.,conditionalgenerativemodels)orgradient-basedoptimization.\n5 SummaryandOutlook\nWeproposedRIGID,aniteration-free,single-shotinversedesignmethodthatgeneratesmetamaterialstosatisfyqualita-\ntivebehaviorsoffunctionalresponses. Suchqualitativebehaviorsareimportantdesigntargetsinmanyapplications"
  },
  {
    "id": 139,
    "text": "tivebehaviorsoffunctionalresponses. Suchqualitativebehaviorsareimportantdesigntargetsinmanyapplications\nsuchastailoringbandgapsofacousticmetamaterialsforwave-guiding,focusing,andvibrationmitigation,ortailoring\ntheabsorptionlevelofopticalmetasurfacesatcertainwavelengthsformedicalsensing,imaging,andcommunication\napplications. Unlikemostexistingmachinelearning-basedinversedesignmethodsthatrequiretraininganinverse\nmodeltomaptargetstodesigns,theRIGIDmethodtakesadvantageoftherandomforest’sinterpretabilityandderives\nthelikelihoodoftargetsatisfactionbyprobingthetrainedforwardmodel. IncorporatedwithMCMC,onecansamplea\ndesirednumberofnewdesignsbasedontheestimatedlikelihood. Therefore,RIGIDfunctionsasagenerativemodel\nthatcancapturetheconditionaldistributionofsatisfyingdesignsgivenatarget,orinotherwords,theone-to-many\nmappingfromthetargettosatisfyingdesigns. Usingbothreal-worldandsyntheticdesignproblems,wedemonstrated"
  },
  {
    "id": 140,
    "text": "mappingfromthetargettosatisfyingdesigns. Usingbothreal-worldandsyntheticdesignproblems,wedemonstrated\nthatRIGIDisefficientandeffectiveondatasetswithtrainingsamplesizessmallerthan250. Weusedbothqualitative\nandquantitativeapproachestovalidatetheproposedmethod. Thequantitativeresultsrevealedastrongcorrelation\nbetweentheestimatedlikelihoodofasolutionanditsprobabilityofactuallyachievingthetarget,whichdemonstrated\ntheeffectivenessofthelikelihoodestimation. Duetothefastevaluationofoutputresponsesandthetransparencyof\nground-truthsolutions,theproposedsyntheticproblemscanbeusedforfuturebenchmarkingstudiesofmetamaterial\ndesignproblems.\nWhileweaddressqualitativedesigntargetsinthisstudy,asimilarideahasthepotentialtogeneralizetoquantitative\ntargets.Suchproblemscanbe,forexample,generatingopticalmetasurfacedesignswithspecificopticalspectra[89,31],\ngenerating functional materials with target nonlinear constitutive relations [49, 90], or generating programmable"
  },
  {
    "id": 141,
    "text": "generating functional materials with target nonlinear constitutive relations [49, 90], or generating programmable\nmetamaterialswithprescribedfunctionalresponses[91,92]. Itisalsostraightforwardtoadjustthetargettoachieve\nmultifunctionality(e.g.,negative/positivePoisson’sratiounderlow/highcompressionrate[93]).\nAlthough this study only demonstrates the RIGID method on parametric design (i.e., designs are represented by\ngeometricand/ormaterialparameters),themethodalsoappliestoshapeortopologicaldesignproblemswheretheshape\nortopologyofdesignscanvarywithoutbeingrestrictedtoalimitednumberofgeometricparameters[89,43,31,45,94].\nInthosecases,asvaliddesignsonlylieonalower-dimensionalmanifoldofthedesignspace,thelikelihoodoftarget\nsatisfactionwillbezeroalmosteverywhereintheoriginaldesignspace. ThusbeforeapplyingRIGID,weneedtoobtain\n15"
  },
  {
    "id": 142,
    "text": "alatentrepresentationthatcompactlycapturesthemanifoldofvaliddesigns[95,96],andusethelatentrepresentation\nasdesignvariablesforinversedesign.\n6 ExperimentalSection\nConfigurationsforGA TheinitialpopulationforGAisgeneratedbyrandomlysamplingsolutionswithinthedesign\nvariablebounds. Ateachgeneration,weselectindividualstoreproduceforanewgenerationbyrandomlypickingthree\nindividualsfromthepopulation,selectingthebestone,andrepeatingthisprocess300times. Wesettheprobability\nforcrossoverandmutationtobe0.5and0.2,respectively. Onceanindividualismutating,thereisa5%chancefor\neachdesignvariableinthisindividualtomutate. Foracontinuousdesignvariable,wedefinethemutationoperationas\naddingGaussiannoisewithameanof0andastandarddeviationequalto1%ofthevariable’srange. Foracategorical\ndesignvariable,mutationisdefinedasrandomlyreassigningthecategoryofthevariable. WeranGAfor10generations,\neachwithapopulationof300. Finally,weselectasetofsolutionswiththehighestobjectivevaluesamongallthe"
  },
  {
    "id": 143,
    "text": "eachwithapopulationof300. Finally,weselectasetofsolutionswiththehighestobjectivevaluesamongallthe\nindividualscreatedduringGA,andusetheseselectedsolutionsasgenerateddesigns.\nComputationofAcousticDispersionRelations WeperformedBloch-waveanalysisinCOMSOLMultiphysics\ntocomputethedispersionrelationsofacousticmetamaterials. Poisson’sratioof0.49,Young’smodulusof2.7GPa,\nanddensityof1170kg/m3 weresetasmaterialpropertieswith∼ 1.5×104 meshelementsperunitcell. Weused\nFloquet-Blochperiodicboundaryconditionstoobtainthefirst60eigenfrequenciesalongallsymmetrydomainsofthe\ncubicirreducibleBrillouinzone(Fig. 3B)foralllattices,thusgeneratingadispersionrelation.\nComputationofOpticalAbsorbanceSpectra Wecomputedtheabsorbancespectraforopticalmetasurfacesusing\nwaveanalysisinspiredbyZhangetal.[85]. TheRFModuleofCOMSOLMultiphysics®[97]wasusedtoevaluatethe\nspectralresponseofconcern,whichistheenergyabsorbanceA(s)inthevisibleregime(380-700nm). Anabsorbance"
  },
  {
    "id": 144,
    "text": "waveanalysisinspiredbyZhangetal.[85]. TheRFModuleofCOMSOLMultiphysics®[97]wasusedtoevaluatethe\nspectralresponseofconcern,whichistheenergyabsorbanceA(s)inthevisibleregime(380-700nm). Anabsorbance\nspectrumiscomputedwithrespectto33wavelengthcomponentss thatareuniformlydiscretizedoverthespecified\nk\nrange. Anincidentplanewaveisassumedtobegivenfromtheport,locatedatthetopfaceoftheanalysisdomain.\nWesettheperiodicityoftheanalysisdomainas400nm. Theperiodicboundaryconditiononelectromagneticfields\nis imposed on the lateral faces of the analysis domain. A substrate made of SiO is placed right below a given\n2\nunitcellinstance(theblacklayersinFig.5A-B).Withfullelectricfieldscomputedthroughthewaveanalysis,the\nenergy absorbance at a single wavelength s is quantified as A(s) = 1−|S (s)|2, where S is the component of\n11 ij\ntheS-parametermatrixthatspecifiesenergytransferbetweenports. WeusethedatapresentedinRef.[98]tosetthe\nmaterialdispersionofthedielectric.\nAcknowledgement"
  },
  {
    "id": 145,
    "text": "11 ij\ntheS-parametermatrixthatspecifiesenergytransferbetweenports. WeusethedatapresentedinRef.[98]tosetthe\nmaterialdispersionofthedielectric.\nAcknowledgement\nThisworkwassupportedbythestartupfundsfromtheJ.MikeWalker’66DepartmentofMechanicalEngineeringat\nTexasA&MUniversity,theNationalScienceFoundation(NSF)BRITEFellowprogram(CMMI2227641),theNSF\nCSSIprogram(OAC1835782),theKansasCityNationalSecurityCampus(PDRD#705288),andNSFCAREER\nAward(CMMI-2142460). R.S.acknowledgesfinancialsupportfromtheNSFGraduateResearchFellowshipProgram.\nDataAvailabilityStatement\nSourcecodeanddataforthisworkareavailableathttps://github.com/DIGITLab23/RIGID.\nKeywords\nmetamaterials,inversedesign,generativedesign,interpretablemachinelearning,functionalresponse\nReferences\n[1] MuamerKadic,TiemoBückmann,RobertSchittny,andMartinWegener. Metamaterialsbeyondelectromagnetism.\nReportsonProgressinphysics,76(12):126501,2013.\n[2] KatiaBertoldi,VincenzoVitelli,JohanChristensen,andMartinVanHecke. Flexiblemechanicalmetamaterials."
  },
  {
    "id": 146,
    "text": "ReportsonProgressinphysics,76(12):126501,2013.\n[2] KatiaBertoldi,VincenzoVitelli,JohanChristensen,andMartinVanHecke. Flexiblemechanicalmetamaterials.\nNatureReviewsMaterials,2(11):1–11,2017.\n16"
  },
  {
    "id": 147,
    "text": "[3] Costas M Soukoulis and Martin Wegener. Past achievements and future challenges in the development of\nthree-dimensionalphotonicmetamaterials. Naturephotonics,5(9):523–530,2011.\n[4] StevenACummer,JohanChristensen,andAndreaAlù. Controllingsoundwithacousticmetamaterials. Nature\nReviewsMaterials,1(3):1–13,2016.\n[5] Robert Schittny, Muamer Kadic, Sebastien Guenneau, and Martin Wegener. Experiments on transformation\nthermodynamics: moldingtheflowofheat. Physicalreviewletters,110(19):195901,2013.\n[6] JohanChristensen,MuamerKadic,OliverKraft,andMartinWegener. Vibranttimesformechanicalmetamaterials.\nMrsCommunications,5(3):453–462,2015.\n[7] JohnBrianPendry. Negativerefractionmakesaperfectlens. Physicalreviewletters,85(18):3966,2000.\n[8] NanshaGao,ZhichengZhang,JieDeng,XinyuGuo,BaozhuCheng,andHongHou. Acousticmetamaterialsfor\nnoisereduction: areview. AdvancedMaterialsTechnologies,7(6):2100698,2022.\n[9] ShanJiang,XuejunLiu,JianpengLiu,DongYe,YongqingDuan,KanLi,ZhoupingYin,andYongAnHuang."
  },
  {
    "id": 148,
    "text": "noisereduction: areview. AdvancedMaterialsTechnologies,7(6):2100698,2022.\n[9] ShanJiang,XuejunLiu,JianpengLiu,DongYe,YongqingDuan,KanLi,ZhoupingYin,andYongAnHuang.\nFlexiblemetamaterialelectronics. AdvancedMaterials,34(52):2200070,2022.\n[10] JWilliamBoley,WimMVanRees,CharlesLissandrello,MarkNHorenstein,RyanLTruby,ArdaKotikian,\nJenniferALewis,andLMahadevan. Shape-shiftingstructuredlatticesviamultimaterial4dprinting. Proceedings\noftheNationalAcademyofSciences,116(42):20856–20862,2019.\n[11] ChunpingMa,YilongChang,ShuaiWu,andRuikeReneeZhao. Deeplearning-accelerateddesignsoftunable\nmagneto-mechanicalmetamaterials. ACSAppliedMaterials&Interfaces,14(29):33892–33902,2022.\n[12] FilippoCasadei,TommasoDelpero,AndreaBergamini,PaoloErmanni,andMassimoRuzzene. Piezoelectric\nresonatorarraysfortunableacousticwaveguidesandmetamaterials. JournalofAppliedPhysics,112(6):064902,\n092012.\n[13] Guang-ShengLiu,YangZhou,Ming-HaoLiu,YingYuan,Xin-YeZou,andJian-ChunCheng.Acousticwaveguide"
  },
  {
    "id": 149,
    "text": "092012.\n[13] Guang-ShengLiu,YangZhou,Ming-HaoLiu,YingYuan,Xin-YeZou,andJian-ChunCheng.Acousticwaveguide\nwithvirtualsoftboundarybasedonmetamaterials. ScientificReports,10(1):981,2020.\n[14] GunhoKim,CarlosMPortela,PaoloCelli,AntonioPalermo,andChiaraDaraio. Poroelasticmicrolatticesfor\nunderwaterwavefocusing. ExtremeMechanicsLetters,49:101499,2021.\n[15] YangboXie,YangyangFu,ZhetaoJia,JunfeiLi,ChenShen,YadongXu,HuanyangChen,andStevenACummer.\nAcousticimagingwithmetamaterialluneburglenses. Scientificreports,8(1):16188,2018.\n[16] Kathryn H Matlack, Anton Bauhofer, Sebastian Krödel, Antonio Palermo, and Chiara Daraio. Composite\n3d-printedmetastructuresforlow-frequencyandbroadbandvibrationabsorption. ProceedingsoftheNational\nAcademyofSciences,113(30):8386–8390,2016.\n[17] SebastianKrödel,NicolasThomé,andChiaraDaraio. Wideband-gapseismicmetastructures. ExtremeMechanics\nLetters,4:111–117,2015.\n[18] AlirezaBayatandStavrosGaitanaros. Wavedirectionalityinthree-dimensionalperiodiclattices. Journalof"
  },
  {
    "id": 150,
    "text": "Letters,4:111–117,2015.\n[18] AlirezaBayatandStavrosGaitanaros. Wavedirectionalityinthree-dimensionalperiodiclattices. Journalof\nAppliedMechanics,85(1):011004,2018.\n[19] HenrikRonellenfitsch,NorbertStoop,JosephineYu,AdenForrow,andJörnDunkel. Inversedesignofdiscrete\nmechanicalmetamaterials. PhysicalReviewMaterials,3(9):095201,2019.\n[20] HeedongGohandLoukasFKallivokas. Inversebandgapdesignofelasticmetamaterialsforpandsvwave\ncontrol. ComputerMethodsinAppliedMechanicsandEngineering,370:113263,2020.\n[21] XiangLi, ShaowuNing, ZhanliLiu, ZimingYan, ChengchengLuo, andZhuoZhuang. Designingphononic\ncrystal with anticipated band gap through a deep learning based data-driven method. Computer Methods in\nAppliedMechanicsandEngineering,361:112737,2020.\n[22] XiaopengZhang,JianXing,PaiLiu,YangjunLuo,andZhanKang. Realizationoffullanddirectionalbandgap\ndesignbynon-gradienttopologyoptimizationinacousticmetamaterials. ExtremeMechanicsLetters,42:101126,\n2021."
  },
  {
    "id": 151,
    "text": "designbynon-gradienttopologyoptimizationinacousticmetamaterials. ExtremeMechanicsLetters,42:101126,\n2021.\n[23] ZhiChen,AlexanderOgren,ChiaraDaraio,LCatherineBrinson,andCynthiaRudin. Howtoseehiddenpatterns\ninmetamaterialswithinterpretablemachinelearning. ExtremeMechanicsLetters,57:101895,2022.\n[24] RobinVismara,NilsOdeboLänk,RuggeroVerre,MikaelKäll,OlindoIsabella,andMiroZeman. Solarharvesting\nbasedonperfectabsorbingall-dielectricnanoresonatorsonamirror. OpticsExpress,27(16):A967–A980,2019.\n[25] YongzhiCheng,FuChen,andHuiLuo. Plasmonicchiralmetasurfaceabsorberbasedonbilayerfourfoldtwisted\nsemicirclenanostructureatopticalfrequency. NanoscaleResearchLetters,16:1–9,2021.\n17"
  },
  {
    "id": 152,
    "text": "[26] Zhaocheng Liu, Dayu Zhu, Kyu-Tae Lee, Andrew S Kim, Lakshmi Raju, and Wenshan Cai. Compounding\nmeta-atomsintometamoleculeswithhybridartificialintelligencetechniques. AdvancedMaterials,32(6):1904790,\n2020.\n[27] DoksooLee,LuZhang,YueYu,andWeiChen. Deepneuraloperatorenabledconcurrentmultitaskdesignfor\nmultifunctionalmetamaterialsunderheterogeneousfields. AdvancedOpticalMaterials,12(15):2303087,2024.\n[28] SandeepInampudiandHosseinMosallaei. Neuralnetworkbaseddesignofmetagratings. AppliedPhysicsLetters,\n112(24),2018.\n[29] MaksymVZhelyeznyakov,SteveBrunton,andArkaMajumdar. Deeplearningtoacceleratescatterer-to-field\nmappingforinversedesignofdielectricmetasurfaces. ACSPhotonics,8(2):481–488,2021.\n[30] DianjingLiu,YixuanTan,ErfanKhoram,andZongfuYu. Trainingdeepneuralnetworksfortheinversedesignof\nnanophotonicstructures. AcsPhotonics,5(4):1365–1369,2018.\n[31] WeiMa,FengCheng,YihaoXu,QinlongWen,andYongminLiu. Probabilisticrepresentationandinversedesign"
  },
  {
    "id": 153,
    "text": "nanophotonicstructures. AcsPhotonics,5(4):1365–1369,2018.\n[31] WeiMa,FengCheng,YihaoXu,QinlongWen,andYongminLiu. Probabilisticrepresentationandinversedesign\nofmetamaterialsbasedonadeepgenerativemodelwithsemi-supervisedlearningstrategy. AdvancedMaterials,\n31(35):1901111,2019.\n[32] SensongAn,BowenZheng,HongTang,MikhailY.Shalaginov,LiZhou,HangLi,MyungkooKang,KathleenA.\nRichardson,TianGu,JuejunHu,ClaytonFowler,andHualiangZhang. MultifunctionalMetasurfaceDesignwith\naGenerativeAdversarialNetwork. AdvancedOpticalMaterials,9(5):1–10,2021.\n[33] ItzikMalkiel,MichaelMrejen,AchiyaNagler,UriArieli,LiorWolf,andHaimSuchowski. Plasmonicnanostruc-\nturedesignandcharacterizationviaDeepLearning. Light: ScienceandApplications,7(1),2018.\n[34] DoksooLee,WeiChen,LiweiWang,Yu-ChinChan,andWeiChen. Data-drivendesignformetamaterialsand\nmultiscalesystems: Areview. AdvancedMaterials,36(8):2305254,2024.\n[35] SensongAn,ClaytonFowler,BowenZheng,MikhailYShalaginov,HongTang,HangLi,LiZhou,JunDing,"
  },
  {
    "id": 154,
    "text": "multiscalesystems: Areview. AdvancedMaterials,36(8):2305254,2024.\n[35] SensongAn,ClaytonFowler,BowenZheng,MikhailYShalaginov,HongTang,HangLi,LiZhou,JunDing,\nAnuradhaMurthyAgarwal,ClaraRivero-Baleine,etal.Adeeplearningapproachforobjective-drivenall-dielectric\nmetasurfacedesign. ACSPhotonics,6(12):3196–3207,2019.\n[36] SiddhantKumar,StephanieTan,LiZheng,andDennisMKochmann. Inverse-designedspinodoidmetamaterials.\nnpjComputationalMaterials,6(1):1–10,2020.\n[37] ChristopherYeung,JuMingTsai,BrianKing,BenjaminPham,DavidHo,JuliaLiang,MarkW.Knight,and\nAaswathP.Raman. Multiplexedsupercellmetasurfacedesignandoptimizationwithtandemresidualnetworks.\nNanophotonics,10(3):1133–1143,12021.\n[38] Jan-HendrikBastek,SiddhantKumar,BastianTelgen,RaphaëlNGlaesener,andDennisMKochmann. Inverting\nthestructure–propertymapoftrussmetamaterialsbydeeplearning. ProceedingsoftheNationalAcademyof\nSciences,119(1):e2111505119,2022.\n[39] ChenXie,HaonanLi,ChenyangCui,HaodongLei,YingjieSun,ChiZhang,YaqiangZhang,HongxingDong,"
  },
  {
    "id": 155,
    "text": "Sciences,119(1):e2111505119,2022.\n[39] ChenXie,HaonanLi,ChenyangCui,HaodongLei,YingjieSun,ChiZhang,YaqiangZhang,HongxingDong,\nandLongZhang. Deeplearningassistedinversedesignofmetamaterialmicrowaveabsorber. AppliedPhysics\nLetters,123(18),2023.\n[40] KMahesh, SKumarRanjith, andRSMini. Adeepautoencoderbasedapproachfortheinversedesignofan\nacoustic-absorber. EngineeringwithComputers,40(1):279–300,2024.\n[41] Chan Soo Ha, Desheng Yao, Zhenpeng Xu, Chenang Liu, Han Liu, Daniel Elkins, Matthew Kile, Vikram\nDeshpande,ZhenyuKong,MathieuBauchy,etal. Rapidinversedesignofmetamaterialsbasedonprescribed\nmechanicalbehaviorthroughmachinelearning. NatureCommunications,14(1):5765,2023.\n[42] Ze-WeiWang,AnChen,Zi-XiangXu,JingYang,BinLiang,andJian-ChunCheng. On-demandinversedesignof\nacousticmetamaterialsusingprobabilisticgenerationnetwork. ScienceChinaPhysics,Mechanics&Astronomy,\n66(2):224311,2023.\n[43] JiaqiJiang,DavidSell,StephanHoyer,JasonHickey,JianjiYang,andJonathanAFan. Free-formdiffractive"
  },
  {
    "id": 156,
    "text": "66(2):224311,2023.\n[43] JiaqiJiang,DavidSell,StephanHoyer,JasonHickey,JianjiYang,andJonathanAFan. Free-formdiffractive\nmetagratingdesignbasedongenerativeadversarialnetworks. ACSnano,13(8):8872–8878,2019.\n[44] SunaeSoandJunsukRho. Designingnanophotonicstructuresusingconditionaldeepconvolutionalgenerative\nadversarialnetworks. Nanophotonics,8(7):1255–1261,2019.\n[45] FufangWen,JiaqiJiang,andJonathanAFan.Robustfreeformmetasurfacedesignbasedonprogressivelygrowing\ngenerativenetworks. ACSPhotonics,7(8):2098–2104,2020.\n[46] CaglarGurbuz,FelixKronowetter,ChristophDietz,MartinEser,JonasSchmid,andSteffenMarburg. Generative\nadversarialnetworksforthedesignofacousticmetamaterials. TheJournaloftheAcousticalSocietyofAmerica,\n149(2):1162–1174,2021.\n18"
  },
  {
    "id": 157,
    "text": "[47] WeiMa,YihaoXu,BoXiong,LinDeng,Ru-WenPeng,MuWang,andYongminLiu. Pushingthelimitsof\nfunctionality-multiplexing capability in metasurface design based on statistical machine learning. Advanced\nMaterials,34(16):2110022,2022.\n[48] Zhi-DanLei,Yi-DuoXu,ChengLei,YanZhao,andDuWang. Dynamicmultifunctionalmetasurfaces: aninverse\ndesigndeeplearningapproach. PhotonicsResearch,12(1):123–133,2024.\n[49] AndrewJLewandMarkusJBuehler. Single-shotforwardandinversehierarchicalarchitectedmaterialsdesign\nfornonlinearmechanicalpropertiesusinganattention-diffusionmodel. MaterialsToday,64:10–20,2023.\n[50] ZezhouZhang,ChuanchuanYang,YifengQin,HaoFeng,JiqiangFeng,andHongbinLi. Diffusionprobabilistic\nmodelbasedaccurateandhigh-degree-of-freedommetasurfaceinversedesign. Nanophotonics,12(20):3871–3881,\n2023.\n[51] Jan-HendrikBastekandDennisMKochmann. Inversedesignofnonlinearmechanicalmetamaterialsviavideo\ndenoisingdiffusionmodels. NatureMachineIntelligence,5(12):1466–1475,2023."
  },
  {
    "id": 158,
    "text": "2023.\n[51] Jan-HendrikBastekandDennisMKochmann. Inversedesignofnonlinearmechanicalmetamaterialsviavideo\ndenoisingdiffusionmodels. NatureMachineIntelligence,5(12):1466–1475,2023.\n[52] Sunae So, Jungho Mun, Junghyun Park, and Junsuk Rho. Revisiting the design strategies for metasurfaces:\nfundamentalphysics,optimization,andbeyond. AdvancedMaterials,35(43):2206399,2023.\n[53] XiaoyangZheng,XuboZhang,Ta-TeChen,andIkumuWatanabe. Deeplearninginmechanicalmetamaterials:\nfrompredictionandgenerationtoinversedesign. AdvancedMaterials,35(45):2302530,2023.\n[54] YabinJin,LiangshuHe,ZhihuiWen,BohayraMortazavi,HongweiGuo,DanielTorrent,BahramDjafari-Rouhani,\nTimon Rabczuk, Xiaoying Zhuang, and Yan Li. Intelligent on-demand design of phononic metamaterials.\nNanophotonics,11(3):439–460,2022.\n[55] WeiMa,ZhaochengLiu,ZhaxylykAKudyshev,AlexandraBoltasseva,WenshanCai,andYongminLiu. Deep\nlearningforthedesignofphotonicstructures. NaturePhotonics,15(2):77–90,2021."
  },
  {
    "id": 159,
    "text": "[55] WeiMa,ZhaochengLiu,ZhaxylykAKudyshev,AlexandraBoltasseva,WenshanCai,andYongminLiu. Deep\nlearningforthedesignofphotonicstructures. NaturePhotonics,15(2):77–90,2021.\n[56] JiaqiJiang,MingkunChen,andJonathanAFan. Deepneuralnetworksfortheevaluationanddesignofphotonic\ndevices. NatureReviewsMaterials,6(8):679–700,2021.\n[57] YihaoXu,BoXiong,WeiMa,andYongminLiu. Software-definednanophotonicdevicesandsystemsempowered\nbymachinelearning. ProgressinQuantumElectronics,89:100469,2023.\n[58] MahmoudElzouka,CharlesYang,AdrianAlbert,RaviSPrasher,andSeanDLubner. Interpretableforwardand\ninversedesignofparticlespectralemissivityusingcommonmachine-learningmodels. CellReportsPhysical\nScience,1(12),2020.\n[59] LeoBreiman. Randomforests. Machinelearning,45:5–32,2001.\n[60] W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications. Biometrika,\n57(1):97–109,1970.\n[61] LeoBreiman,JeromeFriedman,CharlesJStone,andRAOlshen. ClassificationandRegressionTrees. Chapman\nandHall/CRC,1984."
  },
  {
    "id": 160,
    "text": "57(1):97–109,1970.\n[61] LeoBreiman,JeromeFriedman,CharlesJStone,andRAOlshen. ClassificationandRegressionTrees. Chapman\nandHall/CRC,1984.\n[62] NiteshVChawla,KevinWBowyer,LawrenceOHall,andWPhilipKegelmeyer. Smote: syntheticminority\nover-samplingtechnique. Journalofartificialintelligenceresearch,16:321–357,2002.\n[63] ZhaochengLiu,LakshmiRaju,DayuZhu,andWenshanCai. Ahybridstrategyforthediscoveryanddesignof\nphotonicstructures. IEEEJournalonEmergingandSelectedTopicsinCircuitsandSystems,10(1):126–135,\n2020.\n[64] AnthonyPGarland,BenjaminCWhite,ScottCJensen,andBradLBoyce. Pragmaticgenerativeoptimizationof\nnovelstructurallatticemetamaterialswithmachinelearning. Materials&Design,203:109632,2021.\n[65] Junming Zhang, Guowu Wang, Tao Wang, and Fashen Li. Genetic algorithms to automate the design of\nmetasurfacesforabsorptionbandwidthbroadening. ACSAppliedMaterials&Interfaces,13(6):7792–7800,2021.\n[66] SabrinaChin-yunShenandMarkusJBuehler. Nature-inspiredarchitectedmaterialsusingunsuperviseddeep"
  },
  {
    "id": 161,
    "text": "metasurfacesforabsorptionbandwidthbroadening. ACSAppliedMaterials&Interfaces,13(6):7792–7800,2021.\n[66] SabrinaChin-yunShenandMarkusJBuehler. Nature-inspiredarchitectedmaterialsusingunsuperviseddeep\nlearning. CommunicationsEngineering,1(1):37,2022.\n[67] SangryunLee,ZhizhouZhang,andGraceXGu. Generativemachinelearningalgorithmforlatticestructureswith\nsuperiormechanicalproperties. MaterialsHorizons,9(3):952–960,2022.\n[68] MelanieMitchell. Anintroductiontogeneticalgorithms. MITpress,1998.\n[69] NanfangYu,PatriceGenevet,MikhailAKats,FrancescoAieta,Jean-PhilippeTetienne,FedericoCapasso,and\nZenoGaburro. Lightpropagationwithphasediscontinuities:generalizedlawsofreflectionandrefraction. science,\n334(6054):333–337,2011.\n19"
  },
  {
    "id": 162,
    "text": "[70] AlexanderVKildishev,AlexandraBoltasseva,andVladimirMShalaev. Planarphotonicswithmetasurfaces.\nScience,339(6125):1232009,2013.\n[71] TieJunCui,MeiQingQi,XiangWan,JieZhao,andQiangCheng. Codingmetamaterials,digitalmetamaterials\nandprogrammablemetamaterials. Light: science&applications,3(10):e218–e218,2014.\n[72] SyedSBukhari,JYiannisVardaxoglou,andWilliamWhittow.Ametasurfacesreview:Definitionsandapplications.\nAppliedSciences,9(13):2727,2019.\n[73] JieHu,SankhyabrataBandyopadhyay,Yu-huiLiu,andLi-yangShao. Areviewonmetasurface: fromprincipleto\nsmartmetadevices. FrontiersinPhysics,8:586087,2021.\n[74] N Iê Landy, S Sajuyigbe, Jack J Mock, David R Smith, and Willie J Padilla. Perfect metamaterial absorber.\nPhysicalreviewletters,100(20):207402,2008.\n[75] Xianliang Liu, Tatiana Starr, Anthony F Starr, and Willie J Padilla. Infrared spatial and frequency selective\nmetamaterialwithnear-unityabsorbance. Physicalreviewletters,104(20):207403,2010."
  },
  {
    "id": 163,
    "text": "[75] Xianliang Liu, Tatiana Starr, Anthony F Starr, and Willie J Padilla. Infrared spatial and frequency selective\nmetamaterialwithnear-unityabsorbance. Physicalreviewletters,104(20):207403,2010.\n[76] JiamingHao, JingWang, XianliangLiu, WillieJPadilla, LeiZhou, andMinQiu. Highperformanceoptical\nabsorberbasedonaplasmonicmetamaterial. AppliedPhysicsLetters,96(25),2010.\n[77] ClaireMWatts,XianliangLiu,andWillieJPadilla. Metamaterialelectromagneticwaveabsorbers. Advanced\nmaterials,24(23):OP98–OP120,2012.\n[78] YanxiaCui,YingranHe,YiJin,FeiDing,LiuYang,YuqianYe,ShouminZhong,YinyueLin,andSailingHe.\nPlasmonicandmetamaterialstructuresaselectromagneticabsorbers. Laser&PhotonicsReviews,8(4):495–520,\n2014.\n[79] XinyuLiu,KebinFan,IlyaVShadrivov,andWillieJPadilla. Experimentalrealizationofaterahertzall-dielectric\nmetasurfaceabsorber. Opticsexpress,25(1):191–201,2017.\n[80] Gun-YealLee,GwanhoYoon,Seung-YeolLee,HansikYun,JaebumCho,KyookeunLee,HwiKim,JunsukRho,"
  },
  {
    "id": 164,
    "text": "metasurfaceabsorber. Opticsexpress,25(1):191–201,2017.\n[80] Gun-YealLee,GwanhoYoon,Seung-YeolLee,HansikYun,JaebumCho,KyookeunLee,HwiKim,JunsukRho,\nandByounghoLee. Completeamplitudeandphasecontroloflightusingbroadbandholographicmetasurfaces.\nNanoscale,10(9):4237–4245,2018.\n[81] YouZhou,IvanIKravchenko,HaoWang,JRyanNolen,GongGu,andJasonValentine. Multilayernoninteracting\ndielectricmetasurfacesformultiwavelengthmetaoptics. Nanoletters,18(12):7529–7537,2018.\n[82] You Li, Qunsheng Cao, and Yi Wang. A wideband multifunctional multilayer switchable linear polarization\nmetasurface. IEEEAntennasandWirelessPropagationLetters,17(7):1314–1318,2018.\n[83] GiuseppeMarino,DavideRocco,CarloGigli,GrégoireBeaudoin,KonstantinosPantzas,StéphanSuffit,Pascal\nFilloux, Isabelle Sagnes, Giuseppe Leo, and Costantino De Angelis. Harmonic generation with multi-layer\ndielectricmetasurfaces. Nanophotonics,10(7):1837–1843,2021."
  },
  {
    "id": 165,
    "text": "Filloux, Isabelle Sagnes, Giuseppe Leo, and Costantino De Angelis. Harmonic generation with multi-layer\ndielectricmetasurfaces. Nanophotonics,10(7):1837–1843,2021.\n[84] StephanieCMalek,AdamCOvervig,AndreaAlù,andNanfangYu. Multifunctionalresonantwavefront-shaping\nmeta-opticsbasedonmultilayerandmulti-perturbationnonlocalmetasurfaces. Light: Science&Applications,\n11(1):246,2022.\n[85] YuanjianZhang,YingtingYi,WenxinLi,ShiriLiang,JingMa,ShuboCheng,WenxingYang,andYougenYi.\nHigh absorptivity and ultra-wideband solar absorber based on ti-al2o3 cross elliptical disk arrays. Coatings,\n13(3):531,2023.\n[86] YichiZhang,DanielWApley,andWeiChen. Bayesianoptimizationformaterialsdesignwithmixedquantitative\nandqualitativevariables. Scientificreports,10(1):4924,2020.\n[87] ChaoMa,SebastianTschiatschek,RichardTurner,JoséMiguelHernández-Lobato,andChengZhang. Vaem: a\ndeepgenerativemodelforheterogeneousmixedtypedata. AdvancesinNeuralInformationProcessingSystems,\n33:11237–11247,2020."
  },
  {
    "id": 166,
    "text": "deepgenerativemodelforheterogeneousmixedtypedata. AdvancesinNeuralInformationProcessingSystems,\n33:11237–11247,2020.\n[88] LeiXuandKalyanVeeramachaneni. Synthesizingtabulardatausinggenerativeadversarialnetworks. arXiv\npreprintarXiv:1811.11264,2018.\n[89] ZhaochengLiu,DayuZhu,SeanPRodrigues,Kyu-TaeLee,andWenshanCai. Generativemodelfortheinverse\ndesignofmetasurfaces. Nanoletters,18(10):6570–6576,2018.\n[90] NikolaosNVlassisandWaiChingSun. Denoisingdiffusionalgorithmforinversedesignofmicrostructureswith\nfine-tunednonlinearmaterialproperties. ComputerMethodsinAppliedMechanicsandEngineering,413:116126,\n2023.\n[91] WeichenLi,FengwenWang,OleSigmund,andXiaojiaShellyZhang. Digitalsynthesisoffree-formmultimaterial\nstructuresforrealizationofarbitraryprogrammedmechanicalresponses. ProceedingsoftheNationalAcademyof\nSciences,119(10):e2120563119,2022.\n20"
  },
  {
    "id": 167,
    "text": "[92] XinLin,FeiPan,YongMa,YulingWei,KangYang,ZihongWu,JuanGuan,BinDing,BinLiu,JinwuXiang,\netal. Mechanicalfouriertransformforprogrammablemetamaterials. ProceedingsoftheNationalAcademyof\nSciences,120(37):e2305380120,2023.\n[93] AleksiBossart,DavidMJDykstra,JopVanderLaan,andCorentinCoulais. Oligomodalmetamaterialswith\nmultifunctionalmechanics. ProceedingsoftheNationalAcademyofSciences,118(21):e2018610118,2021.\n[94] SensongAn,BowenZheng,HongTang,MikhailYShalaginov,LiZhou,HangLi,MyungkooKang,KathleenA\nRichardson,TianGu,JuejunHu,etal. Multifunctionalmetasurfacedesignwithagenerativeadversarialnetwork.\nAdvancedOpticalMaterials,9(5):2001433,2021.\n[95] LiweiWang,Yu-ChinChan,FaezAhmed,ZhaoLiu,PingZhu,andWeiChen. Deepgenerativemodelingfor\nmechanistic-basedlearninganddesignofmetamaterialsystems. ComputerMethodsinAppliedMechanicsand\nEngineering,372:113377,2020.\n[96] WeiChen,DoksooLee,OluwaseyiBalogun,andWeiChen. Gan-duf: Hierarchicaldeepgenerativemodelsfor"
  },
  {
    "id": 168,
    "text": "Engineering,372:113377,2020.\n[96] WeiChen,DoksooLee,OluwaseyiBalogun,andWeiChen. Gan-duf: Hierarchicaldeepgenerativemodelsfor\ndesignunderfree-formgeometricuncertainty. JournalofMechanicalDesign,145(1):011703,2023.\n[97] COMSOLAB. Comsolmultiphysics(version5.6). https://www.comsol.com,2020. Stockholm,Sweden.\n[98] DavidEAspnesandAAStudna. Dielectricfunctionsandopticalparametersofsi,ge,gap,gaas,gasb,inp,inas,\nandinsbfrom1.5to6.0ev. PhysicalreviewB,27(2):985,1983.\n21"
  },
  {
    "id": 169,
    "text": "Supporting Information for\nGenerative Inverse Design of Metamaterials with Functional\nResponses by Interpretable Learning\nS1 ExistingApproachesonDeepLearning-BasedIteration-Free,Single-ShotInverseDesign\nThethreemainstreamdeeplearningmethodsforiteration-free,single-shotmetamaterialinversedesignaredirectinverse\nmapping,TandemNeuralNetwork(T-NN),andconditionalgenerativemodels. Mostpriorworksfallintooneofthe\nthreecategoriesorattheintersectionoftwocategories. Herewegivemoredetailsofeachmethod. Thedirectinverse\nmappingapproach(Fig.S1A)simplylearnsaone-to-onemappingfromresponsestodesigns. Itfailstoaccountfor\nnon-uniquesolutionsoftheinverseproblem,whichleadstoconflictinglabels(i.e.,designs)forthesameinput(i.e.,\nresponse)andhencecausestrainingconvergenceissues.\nTheT-NN(Fig.S1B)wasdesignedtosolvethisnon-uniquenessissue. Itstrainingissplitintotwosteps: (1)pretraining\ntheforward-modelingnetworktoapproximatethedesign-responsemappingand(2)trainingthecascadednetworkby"
  },
  {
    "id": 170,
    "text": "theforward-modelingnetworktoapproximatethedesign-responsemappingand(2)trainingthecascadednetworkby\nfreezingtheweightsofthepretrainedforward-modelingnetwork. Thereisnolossfunctionthatforcesdesignsatthe\nintermediatelayertomatchdata(whichcontainsconflictinginstances),hencethetrainingconvergenceissueisavoided.\nHowever,theoriginalT-NNsstilllearnaone-to-oneresponse-designmappingandcannotgeneratemultiplesatisfying\nsolutions. VariantsofT-NNthatlearnone-to-manymappingsarementionedinthemaintext.\nTheconditionalgenerativemodels(Fig.S1C)arethemostcommonapproachtolearningaone-to-manymappingfrom\nresponsestodesigns. Differentgenerativemodelshavedistinctwaysoflearningconditionaldistributions. Ingeneral,\nthisisrealizedbytrainingneuralnetworkstotransformresponsesandrandomnoise(orlatentvariables)intodesigns,\nsothatthetrainednetworkcangenerateanon-deterministicdesignsolutionfromagiventargetresponseandrandomly\nsamplednoise,whichisequivalenttosamplingfromaconditionaldistribution."
  },
  {
    "id": 171,
    "text": "sothatthetrainednetworkcangenerateanon-deterministicdesignsolutionfromagiventargetresponseandrandomly\nsamplednoise,whichisequivalenttosamplingfromaconditionaldistribution.\nA B C Conditional Generative Model\nDirect Inverse Mapping Tandem Neural Network\n(only showing generation network)\nPretrained Forward Network\nResponse\nNoise or\nResponse Response Predicted Latent\nResponse Variables\nDesign Design Design\nFigureS1: Threemajormachinelearningmodelsforsingle-shotinversedesign. Notethattheseareonlyconceptual\nplotsandthenetworkscanbemorecomplicatedinreality. Also,duetothediversityandcomplexityofconditional\ngenerativemodelarchitectures,weonlyshowthenetworkfordesigngenerationbasedoncommonlyusedmodels(e.g.,\ngeneratorsincGANsordecodersincVAEs),whileomittingnetworksthatarenecessaryfortraining(e.g.,discriminators\nincGANsorencodersincVAEs).\nS2 ExplanationofStep2inRIGID\nThepurposeofStep2istoobtaintheintersectionofrelevantdesignspaceregionsforalltherangesofsinatarget. We"
  },
  {
    "id": 172,
    "text": "incGANsorencodersincVAEs).\nS2 ExplanationofStep2inRIGID\nThepurposeofStep2istoobtaintheintersectionofrelevantdesignspaceregionsforalltherangesofsinatarget. We\nareapproximatingthisgoalbysimplyobtainingtheintersectionofrelevantleaves. However,somenon-intersecting\nleavesmaystillhaveintersectingdesignspaceregions. Whenassigningtheprobabilitytotheintersectingregionof\ntwo non-intersecting leaves A and B, we need to consider the predicted probabilities at both leaves (P and P ).\nA B\nSpecifically,theassignedprobabilityatthisintersectingregionshouldbeP P ,whichcanbesmall. Therefore,we\nA B\nadoptthesimplificationofonlyconsideringtheintersectionofrelevantleavesandignoringtheintersectingregions\nassociatedwithnon-intersectingleaves. Theresultsalsodemonstratethatthisisareasonableapproximation.\nS-1"
  },
  {
    "id": 173,
    "text": "S3 TestPerformanceofForwardPrediction\nTestdataforallthedesignproblemsaredesignsx(thattherandomforestshaveneverseenduringtraining)andtheir\ncorrespondingqualitativebehaviorsy ∈{0,1}basedonfunctionalresponses.\nIntheacousticmetamaterialdesignproblem,wehave57testdesigns,yielding5,700testpointsastheentirefrequency\nrangeofdispersionrelationsisdiscretizedinto100intervalsforeachdesign. Intheopticalmetasurfacedesignproblem,\nwehave52testdesigns,witheachfunctionalresponsediscretizedinto33points,whichyields1,716testpointsintotal.\nIneachofthesyntheticdesignproblems,wehave20synthetictestdesigns,witheachsyntheticresponsediscretized\ninto100points. Thisresultsin2,000testdatapointsintotal. Confusionmatricesshowingtestperformancesarein\nTablesS1-S4.\nTableS1: Confusionmatrixontestdataintheacousticmetamaterialdesignproblem. ThetestF1scoreis0.82.\nPredicted\nYes No\nlautcA Yes 393 131\nNo 46 5130\nTableS2: Confusionmatrixontestdataintheopticalmetasurfacedesignproblem. ThetestF1scoreis0.83.\nPredicted\nYes No"
  },
  {
    "id": 174,
    "text": "Predicted\nYes No\nlautcA Yes 393 131\nNo 46 5130\nTableS2: Confusionmatrixontestdataintheopticalmetasurfacedesignproblem. ThetestF1scoreis0.83.\nPredicted\nYes No\nlautcA Yes 776 157\nNo 151 632\nTableS3: ConfusionmatrixontestdataintheSqExpproblem. ThetestF1scoreis0.85.\nPredicted\nYes No\nlautcA Yes 229 68\nNo 13 1690\nTableS4: ConfusionmatrixontestdataintheSupSinproblem. ThetestF1scoreis0.86.\nPredicted\nYes No\nlautcA Yes 355 81\nNo 37 1527\nS4 GeneratedDesignsandCorrespondingResponses\nIneachoftheacousticandopticalmetamaterialdesignproblems,werandomlycreated10designtargets(i.e.,frequency\nrangeswithinwhichbandgapsexistorwavelengthrangeswithinwhichabsorptionishigh). Giveneachdesigntarget,\nwegeneratemultipledesignsolutionsbutonlyvisualizefivesolutionshavingthehighestlikelihoodvalues. Themain\ntextshowsthefivesolutionsforthefirstdesigntargetineachproblem. FiguresS2-S9showthetopsolutions(rankedby\nlikelihood)fortheotherdesigntargets."
  },
  {
    "id": 175,
    "text": "textshowsthefivesolutionsforthefirstdesigntargetineachproblem. FiguresS2-S9showthetopsolutions(rankedby\nlikelihood)fortheotherdesigntargets.\nWhiletheselimitedexamplescannotrevealthesamedegreeofgeneralizableinsightsasthesatisfactionrateandthe\naveragescoredo,theystillofferanintuitiveglimpseintothemethod’sperformanceandfunctionality. Mostofthese\ngenerateddesignssatisfytheircorrespondingtargets. Somedistinctdesignscanachievethesametarget,demonstrating\nthecapabilityofRIGIDinsolvingthenon-uniquenessissueofinversedesign. Forsometargets,eventhemaximum\nlikelihoodofgenerateddesignsisextremelysmall(e.g.,TargetIVshowninthefirstrowinFig.S4),indicatingsmall\nlikelihoodvaluesacrosstheentiredesignspace. Thiscanbeowingtothedifficultyofachievingthetargetunderthe\nlimitofthedesignspace. Forexample,allthetopfivegenerateddesignsforTargetIVhavealikelihoodof0.001. This\nmeansthatthemodelbelievessatisfyingthetargetisalmostimpossible,whichmaybeduetothewidespanofthe"
  },
  {
    "id": 176,
    "text": "limitofthedesignspace. Forexample,allthetopfivegenerateddesignsforTargetIVhavealikelihoodof0.001. This\nmeansthatthemodelbelievessatisfyingthetargetisalmostimpossible,whichmaybeduetothewidespanofthe\ncombinationoftwotargetbandgaps. Thisdifficultymayresultfromthelimitationofthecurrentdesignspace. Alarger\nS-2"
  },
  {
    "id": 177,
    "text": "r = 4.31 r = 4.45 r = 4.19 r = 4.63 r = 4.44\nstrut strut strut strut strut\nr = 6.47 r = 2.86 r = 2.99 r = 6.94 r = 4.40\ncenter center center center center\nr = 19.33 r = 19.99 r = 19.34 r = 19.74 r = 19.02\ncorner corner corner corner corner\nLikelihood: 0.07 Likelihood: 0.057 Likelihood: 0.05 Likelihood: 0.034 Likelihood: 0.026\nr = 4.55 r = 4.27 r = 4.68 r = 4.08 r = 4.46\nstrut strut strut strut strut\nr = 19.64 r = 18.56 r = 19.64 r = 18.34 r = 20.00\ncenter center center center center\nr =1.32 r = 0.73 r = 9.09 r = 4.64 r = 8.66\ncorner corner corner corner corner\nLikelihood: 0.789 Likelihood: 0.771 Likelihood: 0.747 Likelihood: 0.652 Likelihood: 0.637\nFigureS2: Geometriesandcorrespondingdispersionrelationsofgeneratedacousticmetamaterialdesignsforbandgap\ntargetsIIandIII(markedasgray-shadedfrequencyregions). Eachrowshowsfivedesignswiththehighestlikelihood\nofsatisfyingatarget. Alltheradii(r ,r ,andr )haveaunitofµm.\nstrut center corner"
  },
  {
    "id": 178,
    "text": "targetsIIandIII(markedasgray-shadedfrequencyregions). Eachrowshowsfivedesignswiththehighestlikelihood\nofsatisfyingatarget. Alltheradii(r ,r ,andr )haveaunitofµm.\nstrut center corner\ndesignspacecanleadtoawidercoverageoftheresponsespaceandhencemorelikelytocontainsatisfactorysolutions.\nBasedontheestimatedlikelihood,wecandecidewhetherexpandingthedesignspace(i.e.,extendingdesignspace\nboundsoraddingnewdesignfreedom)isnecessarytodiscoversatisfyingsolutions.\nS5 InvestigationofGeometryTypesinGeneratedOpticalMetasurfaceDesigns\nAlthoughthegeneratedmetasurfacedesignswithtoplikelihoodvaluesinthemaintextandFigs.S7-S9donotcontain\ntherectangularring(c=1)andtheH-shaped(c=4)types,bothtypesweregeneratedinourinversedesignprocesses.\nBasedonthehistogramof1,000generateddesignsfromallfourgeometrytypes(Fig.S10A),thereisarelativelylower\nS-3"
  },
  {
    "id": 179,
    "text": "r = 5.06 r = 5.04 r = 5.04 r = 5.03 r = 5.00\nstrut strut strut strut strut\nr = 0.46 r = 5.16 r = 4.76 r = 2.28 r = 3.57\ncenter center center center center\nr = 18.79 r = 19.67 r = 19.09 r = 19.42 r = 19.10\ncorner corner corner corner corner\nLikelihood: 0.001 Likelihood: 0.001 Likelihood: 0.001 Likelihood: 0.001 Likelihood: 0.001\nr = 4.04 r = 4.27 r = 5.13 r = 5.21 r = 4.01\nstrut strut strut strut strut\nr =2.24 r = 8.45 r = 7.90 r = 8.34 r = 0.41\ncenter center center center center\nr = 19.78 r = 19.74 r = 19.43 r = 16.94 r = 17.54\ncorner corner corner corner corner\nLikelihood: 0.783 Likelihood: 0.653 Likelihood: 0.616 Likelihood: 0.442 Likelihood: 0.41\nFigureS3: Geometriesandcorrespondingdispersionrelationsofgeneratedacousticmetamaterialdesignsforbandgap\ntargetsIVandV(markedasgray-shadedfrequencyregions). Eachrowshowsfivedesignswiththehighestlikelihood\nofsatisfyingatarget. Alltheradii(r ,r ,andr )haveaunitofµm.\nstrut center corner\nS-4"
  },
  {
    "id": 180,
    "text": "r = 4.15 r = 4.21 r = 4.16 r = 4.14 r = 4.18\nstrut strut strut strut strut\nr = 18.20 r = 19.03 r = 17.94 r = 16.23 r = 16.74\ncenter center center center center\nr = 4.58 r = 11.38 r = 5.61 r = 2.58 r = 3.27\ncorner corner corner corner corner\nLikelihood: 0.234 Likelihood: 0.233 Likelihood: 0.22 Likelihood: 0.219 Likelihood: 0.158\nr = 5.76 r = 5.14 r = 4.95 r = 6.11 r = 4.72\nstrut strut strut strut strut\nr = 5.15 r = 4.11 r = 1.98 r = 0.30 r = 0.61\ncenter center center center center\nr = 19.66 r = 19.10 r = 19.13 r = 19.77 r = 18.94\ncorner corner corner corner corner\nLikelihood: 0.761 Likelihood: 0.715 Likelihood: 0.667 Likelihood: 0.587 Likelihood: 0.551\nFigureS4: Geometriesandcorrespondingdispersionrelationsofgeneratedacousticmetamaterialdesignsforbandgap\ntargetsVIandVII(markedasgray-shadedfrequencyregions). Eachrowshowsfivedesignswiththehighestlikelihood\nofsatisfyingatarget. Alltheradii(r ,r ,andr )haveaunitofµm.\nstrut center corner\nS-5"
  },
  {
    "id": 181,
    "text": "r = 5.43 r = 4.07 r = 5.83 r = 6.04 r = 5.44\nstrut strut strut strut strut\nr = 3.98 r = 8.87 r = 2.95 r = 7.12 r = 6.22\ncenter center center center center\nr = 19.66 r = 18.46 r = 17.63 r = 17.63 r = 17.78\ncorner corner corner corner corner\nLikelihood: 0.834 Likelihood: 0.472 Likelihood: 0.444 Likelihood: 0.432 Likelihood: 0.364\nr = 5.55 r = 5.48 r = 6.02 r = 5.17 r = 5.94\nstrut strut strut strut strut\nr = 5.67 r = 5.06 r = 4.21 r = 1.04 r = 8.22\ncenter center center center center\nr = 19.11 r = 19.84 r = 19.30 r = 19.44 r = 18.74\ncorner corner corner corner corner\nLikelihood: 0.79 Likelihood: 0.757 Likelihood: 0.69 Likelihood: 0.688 Likelihood: 0.685\nFigureS5: Geometriesandcorrespondingdispersionrelationsofgeneratedacousticmetamaterialdesignsforbandgap\ntargetsVIIIandIX(markedasgray-shadedfrequencyregions). Eachrowshowsfivedesignswiththehighestlikelihood\nofsatisfyingatarget. Alltheradii(r ,r ,andr )haveaunitofµm.\nstrut center corner\nS-6"
  },
  {
    "id": 182,
    "text": "r = 4.22 r = 4.20 r = 4.21 r = 4.01 r = 4.07\nstrut strut strut strut strut\nr = 18.77 r = 18.09 r = 17.42 r = 18.24 r = 17.26\ncenter center center center center\nr = 0.36 r = 0.06 r = 11.08 r = 2.70 r = 5.76\ncorner corner corner corner corner\nLikelihood: 0.002 Likelihood: 0.001 Likelihood: 0.001 Likelihood: 0.001 Likelihood: 0.001\nFigureS6: Geometriesandcorrespondingdispersionrelationsofgeneratedacousticmetamaterialdesignsforbandgap\ntarget X (marked as gray-shaded frequency regions). Each row shows five designs with the highest likelihood of\nsatisfyingatarget. Alltheradii(r ,r ,andr )haveaunitofµm.\nstrut center corner\npercentageofdesignsfromtherectangularringandtheH-shapedtypes(the1stand4thtypes). Thekerneldensity\nestimation(KDE)plot(Fig.S10B)showsthatthesetwotypeshavefewerhigh-likelihooddesignscomparedtothe\nothertwotypes(the2ndand3rdtypes). Thisexplainswhytheyareabsentfromtheexamplesofgenerateddesignswith"
  },
  {
    "id": 183,
    "text": "estimation(KDE)plot(Fig.S10B)showsthatthesetwotypeshavefewerhigh-likelihooddesignscomparedtothe\nothertwotypes(the2ndand3rdtypes). Thisexplainswhytheyareabsentfromtheexamplesofgenerateddesignswith\ntoplikelihoodvaluesshowninthemaintextandFigs.S7-S9. Regardlessoftherelativelylowerlikelihoodvalues,there\naresatisfyingdesignsfromtherectangularringandtheH-shapedtypes. SomeexamplesareshowninFig.S11.\nThelowestimatedlikelihoodcanbecausedbythemodel’slackofbeliefregardingthepossibilityofthedesignmeeting\nthetarget,andthisbeliefstemmedfromlearningfromthetrainingdataset. Uponfurtherinvestigationintoouroptical\nmetasurfacedataset,wefoundthatthepercentagesofthehighabsorbanceregion(absorbancenotlessthanthe0.9\nthreshold)withintheconsideredwavelengthrangefortherectangularringandtheH-shapedgeometrytypesare29.5%\nand18.8%,respectively,whilethepercentagesfortheothertwotypesareabove80%. Thismayexplainthemodel’s"
  },
  {
    "id": 184,
    "text": "threshold)withintheconsideredwavelengthrangefortherectangularringandtheH-shapedgeometrytypesare29.5%\nand18.8%,respectively,whilethepercentagesfortheothertwotypesareabove80%. Thismayexplainthemodel’s\nlowconfidenceinbelievingtherectangularringandtheH-shapedgeometrytypessatisfyingthetarget,whichfurther\nledtothelowlikelihood.\nS-7"
  },
  {
    "id": 185,
    "text": "FigureS7: Designsandcorrespondingabsorbancespectraofgeneratedopticalmetasurfacedesignsfordesigntargets\nII-IV(markedasblue-shadedwavelengthregions).Eachrowshowsfivedesignswiththehighestlikelihoodofsatisfying\natarget. Allthelayerthicknesses(h ,l=1,2,3)haveaunitofnm.\nl\nS-8\n\nFigureS8: Designsandcorrespondingabsorbancespectraofgeneratedopticalmetasurfacedesignsfordesigntargets\nV-VII (marked as blue-shaded wavelength regions). Each row shows five designs with the highest likelihood of\nsatisfyingatarget. Allthelayerthicknesses(h ,l=1,2,3)haveaunitofnm.\nl\nS-9\n\nFigureS9: Designsandcorrespondingabsorbancespectraofgeneratedopticalmetasurfacedesignsfordesigntargets\nVIII-X (marked as blue-shaded wavelength regions). Each row shows five designs with the highest likelihood of\nsatisfyingatarget. Allthelayerthicknesses(h ,l=1,2,3)haveaunitofnm.\nl\nS-10"
  },
  {
    "id": 186,
    "text": "A B\nFigureS10: Statisticsof1,000generatedopticalmetasurfacedesigns. (A)Histogramshowingthecountsofgenerated\ndesigns in four geometry types. (B) KDE plot showing distributions of estimated likelihood values for different\ngeometrytypes.\nFigureS11: Examplesofgenerateddesignsthatsatisfythetargets(markedasblue-shadedwavelengthregions)and\nbelongtotherectangularringandH-shapedgeometrytypes. Allthelayerthicknesses(h ,l=1,2,3)haveaunitofnm.\nl\nS-11"
  },
  {
    "id": 187,
    "text": "Informational non-reductionist theory of consciousness\nthat providing maximum accuracy of reality prediction\nVityaev Е.Е.\nSobolev institute of mathematics, Novosibirsk, Russia\nvityaev@math.nsc.ru\nAnnotation.\nThe paper considers a non-reductionist theory of consciousness, which is not reducible to\ntheories of reality and to physiological or psychological theories. Following D.I.Dubrovsky's\n\"informational approach\" to the \"Mind-Brain Problem\", we consider the reality through the prism of\ninformation about observed phenomena, which, in turn, is perceived by subjective reality through\nsensations, perceptions, feelings, etc., which, in turn, are information about the corresponding brain\nprocesses. Within this framework the following principle of the Information Theory of Consciousness\n(ITS) development is put forward: the brain discovers all possible causal relations in the external\nworld and makes all possible inferences by them. The paper shows that ITS built on this principle:"
  },
  {
    "id": 188,
    "text": "(ITS) development is put forward: the brain discovers all possible causal relations in the external\nworld and makes all possible inferences by them. The paper shows that ITS built on this principle:\n(1) also base on the information laws of the structure of external world; (2) explains the structure and\nfunctioning of the brain functional systems and cellular ensembles; (3) ensures maximum accuracy\nof predictions and the anticipation of reality; (4) resolves emerging contradictions and (5) is an\ninformation theory of the brain's reflection of reality.\nKeywords. reality, consciousness, mind-brain problem, brain, information.\nThe non-reductionist theory of consciousness is not reducible to any theory of\nreality and to any physiological or psychological theory.\nAt the Seventh International Conference on Cognitive Science, K.V.Anokhin said\n\"The problem is not that the existing neurophysiological theories are imperfect ... The"
  },
  {
    "id": 189,
    "text": "At the Seventh International Conference on Cognitive Science, K.V.Anokhin said\n\"The problem is not that the existing neurophysiological theories are imperfect ... The\ncorrelative approaches used in them simply cannot answer questions about the nature\nof mind and subjective experience ... This requires a non-reductionist fundamental\ntheory\" [3].\nMax Tegmark in his book [14] also write that\nbetween the external reality \"External Reality\" and\n\"Internal Reality\" there should be an intermediate\n\"Consensus Reality\", describing the external reality in\nphysical terms and at the same time is reflected in the\ninternal reality.\nFollowing D.I. Dubrovsky [10], we will use an\n\"informational approach\" to the description of\n\"Internal Reality\" as a subjective reality. Herewith, the\nsubjective reality is the reality of an individual's\nconscious states – sensations, perceptions, feelings,\nthoughts, intentions, desires, etc. At the same time, the\nphenomena of subjective reality are considered as"
  },
  {
    "id": 190,
    "text": "conscious states – sensations, perceptions, feelings,\nthoughts, intentions, desires, etc. At the same time, the\nphenomena of subjective reality are considered as\ninformation related to the corresponding brain process\nas its carrier. Fig. 1. Consciousness and Brain"
  },
  {
    "id": 191,
    "text": "Thus, within this framework of the information approach to the \"Mind-Brain\nProblem\", we get the following scheme fig. 1., where reality is described through\ninformation about observed phenomena, which are perceived by subjective reality\nthrough sensations, perceptions, feelings, etc., which in turn are information about the\ncorresponding brain processes.\nWhat is the purpose of this information? Most precisely, it is defined in the\n\"principle of the evolution of the living world\", formulated by P.K. Anokhin: \"There\nwas one universal pattern in the adaptation of organisms to external conditions, which\nlater developed rapidly throughout the evolution of the living world: a highly rapid\nreflection of the slowly unfolding events of the external world\" [5].\nLet us reveal this principle and formulate the principles of creating an information\ntheory of consciousness.\nI. First, following the \"principle of the evolution of the living world\", this"
  },
  {
    "id": 192,
    "text": "Let us reveal this principle and formulate the principles of creating an information\ntheory of consciousness.\nI. First, following the \"principle of the evolution of the living world\", this\ninformation theory should anticipate the events of the outside world.\nHowever, if the external world were accidental, then anticipation of it would be\nimpossible. But our world is well structured. If there are any laws in the information\nstructure of reality, then it is natural to assume that in the process of evolution there\nwere developed such neurobiological mechanisms that would use these structures to\nproduce the most accurate reflection of reality. Therefore, the following principle is\nnecessary.\nII. The information theory of reflection should be based on the laws of the\nstructure of the external world and describe simultaneously both the\ninformation structure of reality and neurophysiological and other mechanisms\nthat ensure the reflection of this structure in terms of subjective reality."
  },
  {
    "id": 193,
    "text": "information structure of reality and neurophysiological and other mechanisms\nthat ensure the reflection of this structure in terms of subjective reality.\nThe following laws of the information structure of reality will be given. The\ncorresponding neurobiological mechanisms that use these structures were indicated by\nK.V. Anokhin in his report \"Cognitome – hypernetwork model of the brain\". These are\nKOGi (Cognitive Groups of Neurons), generalizing the ideas of the functional systems\ntheory and D. Hebb's cellular ensembles [17].\nWe propose the following fundamental principle of the Information Theory of\nConsciousness (ITS), which is sufficient to explain the basic information processes:\nTHE PRINCIPLE OF unlimited inference: The brain detects all possible\ncausal connections in the external world and makes all possible conclusions on\nthem.\nIt turns out that this principle is sufficient to build an ITS, which:\n1. Explains the structure and functioning of KOGs of functional systems and D."
  },
  {
    "id": 194,
    "text": "them.\nIt turns out that this principle is sufficient to build an ITS, which:\n1. Explains the structure and functioning of KOGs of functional systems and D.\nHebb's cellular ensembles.\n2. Based on the following information laws of the structure of the external world.\n3. Provides maximum accuracy of predictions and anticipations of reality.\n4. Resolves emerging contradictions.\n5. It is an information theory of reflection of reality by the brain."
  },
  {
    "id": 195,
    "text": "Let's consider the first law of the information structure of the external world – its\ncausality. Causality is a consequence of physical determinism: \"for any isolated\nphysical system, some of its state determines all subsequent states\" [12]. But consider\na car accident [12]. What is the reason for it? This may be the condition of the road\nsurface, its humidity, the position of the sun relative to the driver, reckless driving, the\npsychological state of the driver, brake malfunction, etc. Obviously, there is no definite\nreason in this case.\nIn the philosophy of science, causality is reduced to prediction and explanation.\n\"Causality means predictability ... if the entire previous situation is known, the event\ncan be predicted ... if all the facts and laws of nature related to this event are given\"\n[12]. It is clear that to know all the facts, the number of which, as in the case of an\naccident, is potentially infinite and all the laws are impossible. In addition, humans and"
  },
  {
    "id": 196,
    "text": "[12]. It is clear that to know all the facts, the number of which, as in the case of an\naccident, is potentially infinite and all the laws are impossible. In addition, humans and\nanimals learn the laws of the outside world through training. Therefore, causality is\nreduced to prediction by inductive statistical inference, when the prediction is derived\nfrom facts and statistical laws with some probability.\nIn addition, causal relationships in the form of statistical laws found on real data\nor as a result of training face to the problem of statistical ambiguity – contradictory\npredictions can be derived from them [18]. To avoid this ambiguity, Hempel introduced\nthe requirement of maximum specificity [18], informally consisting in the fact that\nstatistical laws should include the maximum available information.\nWe solved the problem of statistical ambiguity and determined the Maximum\nSpecific Causal Relationships (MSCR), for which it was proved that inductive"
  },
  {
    "id": 197,
    "text": "We solved the problem of statistical ambiguity and determined the Maximum\nSpecific Causal Relationships (MSCR), for which it was proved that inductive\nstatistical inference using them does not lead to contradictions [28,32] and thereby\nmost accurately implement the principle of evolution of the living world by\nP.K.Anokhin. We have developed a special semantic probabilistic inference [28,32]\nthat detects MSCR. In particular, it satisfies Cartwright's definition of a probabilistic\ncausal relationship with respect to some background, which consists in the fact that\neach condition of the premise of causal relationship strictly increases the conditional\nprobability of the conclusion. In addition, we have developed a formal neuron model\n[30] satisfying the Hebb`s rule, which implements this inference and detects MSCR\ncausal relationships [28,32].\nThus, accurately analyzing the concept of causality, we get the informational law"
  },
  {
    "id": 198,
    "text": "[30] satisfying the Hebb`s rule, which implements this inference and detects MSCR\ncausal relationships [28,32].\nThus, accurately analyzing the concept of causality, we get the informational law\nof reality that can be represented in ITS by detecting causal connections at the level of\na neuron, ensuring its plasticity and manifesting itself, in particular, by conditional\nreactions. At the same time, they can be as accurate as possible through the use of\nMSCR conditional links.\nThis provides the first level of the maximum accuracy of predictions and\nanticipations of reality, implementing the principle of anticipatory reflection of reality.\n3. \"Natural\" classification\nLet's move on to the next law of the informational structure of the external world\nobjects – the \"natural\" classification. The first rather detailed analysis of the \"natural\"\nclassification belongs to J.S. Mill [21]. First, we will separate the \"artificial\""
  },
  {
    "id": 199,
    "text": "objects – the \"natural\" classification. The first rather detailed analysis of the \"natural\"\nclassification belongs to J.S. Mill [21]. First, we will separate the \"artificial\"\nclassifications from the \"natural\" ones: \"Let's take any attribute, and if some things"
  },
  {
    "id": 200,
    "text": "have it and others do not, then we can base the division of all things into two classes\non it.\", \"But if we turn to ... the class of \"animal\" or \"plant\", ... then we will find that in\nthis respect some classes are very different from others. ... have so many features that\nthey cannot be ... enumerated\" [21].\nJ.S. Mill defines the \"natural\" classification as follows: \"Most of all, it\ncorresponds to the goals of scientific (natural) classification when objects are combined\ninto such groups regarding which the greatest number of general proposals can be\nmade\" [21]. Based on the concept of \"natural\" classification, J. S. Mill defines the\nconcept of an \"image\" of a class as a certain pattern that has all the characteristics of\nthis class.\nNaturalists wrote that the creation of a \"natural\" classification consists in\n\"indication\" – from an infinitely large number of features it is necessary to move to a\nlimited number of them, which would replace all other features [13]. This means that"
  },
  {
    "id": 201,
    "text": "\"indication\" – from an infinitely large number of features it is necessary to move to a\nlimited number of them, which would replace all other features [13]. This means that\nin \"natural\" classes, the attributes are strongly correlated, for example, if there are 128\nclasses and the attributes are binary, then only 7 attributes can be independent\n\"indicator\" attributes among them, since 27 = 128, and other attributes can be predicted\nby the values of these 7 attributes. We can choose various 7-10 attributes as \"indicator\"\nand then other attributes, of which there are potentially infinitely many, can be\npredicted from these selected attributes. Therefore, there is an exponential (relative to\nthe number of attributes) number of causal relationships linking the attributes of objects\nof \"natural\" classes.\nSuch redundancy of information, but already when perceiving objects of the\nexternal world, is confirmed in cognitive sciences when considering \"natural\"\nconcepts."
  },
  {
    "id": 202,
    "text": "of \"natural\" classes.\nSuch redundancy of information, but already when perceiving objects of the\nexternal world, is confirmed in cognitive sciences when considering \"natural\"\nconcepts.\n4. \"Natural\" concepts in cognitive sciences\nThe highly correlated structure of the objects of the external world is also revealed\nby the theory of \"natural\" concepts. \"Natural\" classification reveals the structure of the\nobjects of the external world, and \"natural\" concepts, studied in cognitive sciences,\ndetermine the perception of these \"natural\" objects as elements of subjective reality.\nIn the works of Eleanor Rosch, the following principle of categorization of\n\"natural\" categories was formulated: «Perceived World Structure … is not an\nunstructured total set of equiprobable co-occurring attributes. Rather, the material\nobjects of the world are perceived to possess … high correlational structure …\ncombinations of what we perceive as the attributes of real objects do not occur"
  },
  {
    "id": 203,
    "text": "objects of the world are perceived to possess … high correlational structure …\ncombinations of what we perceive as the attributes of real objects do not occur\nuniformly. Some pairs, triples, etc., are quite probable, appearing in combination …\nwith one, sometimes another attribute; others are rare; others logically cannot or\nempirically do not occur» [25].\nDirectly perceived objects (basic objects) are information–rich bundles of\nobservable properties that create categorization (an image in the J.S. Mill definition):\n«Categories can be viewed in terms of their clear cases if the perceiver places emphasis\non the correlational structure of perceived attributes … By prototypes of categories\nwe have generally meant the clearest cases of category membership» [24]."
  },
  {
    "id": 204,
    "text": "In further research, it was found that models based on features, similarities and\nprototypes are not enough to describe \"natural\" classes. Considering these studies, Bob\nRehder put forward a theory of causal models, according to which: \"people's intuitive\ntheories about categories of objects consist of a model of the category in which both a\ncategory's features and the causal mechanisms among those features are explicitly\nrepresented\" [23]. In the theory of causal models, the relation of an object to a category\nis no longer based on a set of signs and proximity by signs, but on the basis of the\nsimilarity of the generative causal mechanism.\nBob Rehder used Bayesian networks to represent causal knowledge [22].\nHowever, they do not support cycles and therefore cannot model cyclic causal\nrelationships. The formalization we propose further in the form of probabilistic formal\nconcepts directly models cyclic causal relationships [5-8,28-29,32]."
  },
  {
    "id": 205,
    "text": "relationships. The formalization we propose further in the form of probabilistic formal\nconcepts directly models cyclic causal relationships [5-8,28-29,32].\n5. The integrated information theory by G.Tononi\nThe theory of integrated information by G.Tononi is also based on the highly\ncorrelated structure of the external world [20,26-27]. If the \"natural\" classification\ndescribes objects of the external world, and cognitive sciences describe the perception\nof objects of the external world, then the theory of integrated information analyzes the\ninformation processes of the brain on the perception of objects of the external world.\nIntegrated information is considered by G.Tononi as a property of a system of\ncyclic causal relationships: «Indeed, a “snapshot” of the environment conveys little\ninformation unless it is interpreted in the context of a system whose complex causal\nstructure, over a long history, has captured some of the causal structure of the world,"
  },
  {
    "id": 206,
    "text": "information unless it is interpreted in the context of a system whose complex causal\nstructure, over a long history, has captured some of the causal structure of the world,\ni.e. long-range correlations in space and time» [27].\nThe relationship of integrated information with reality G.Tononi describes as\nfollows: «Cause-effect matching … measures how well the integrated conceptual\nstructure … fits or ‘matches’ the cause-effect structure of its environment», «…\nmatching should increase when a system adapts to an environment having a rich,\nintegrated causal structure. Moreover, an increase in matching will tend to be\nassociated with an increase in information integration and thus with an increase in\nconsciousness» [26-27].\nG.Tone defines consciousness as a primary concept that has the following\nphenomenological properties: composition, information, integration, exclusion [20,26-\n27]. We present the formulations of these properties together with our interpretation of"
  },
  {
    "id": 207,
    "text": "phenomenological properties: composition, information, integration, exclusion [20,26-\n27]. We present the formulations of these properties together with our interpretation of\nthese properties (given in parentheses) from the point of view of \"natural\" classification\nof the external world objects.\n1. composition – elementary mechanisms (causal interactions) can be combined\ninto higher-order ones (\"natural\" classes form a hierarchy);\n2. information – only mechanisms that specify 'differences that make a difference'\nwithin a system count (only the system of \"resonating\" causal relationships forming\nthe class is significant);\n3. integration – only information irreducible to non-independent components\ncounts (only the system of \"resonating\" causal relationships is significant, which not"
  },
  {
    "id": 208,
    "text": "reducible to the information of individual components, indicating an excess of\ninformation and the perception of a highly correlated structure of a \"natural\" object);\n4. exclusion – only maxima of integrated information count (only values of\nfeatures that are maximally interconnected by causal relationships form an \"image\" or\n\"prototype\").\nUnlike G.Tononi, we consider these properties not as internal properties of the\nsystem, but as the ability of the system to reflect the \"natural\" classification of the\nobjects of the external world. Then consciousness, unlike G.Tononi, is defined not by\nthe phenomenological properties of neural structures, but as the ability of the brain,\nusing the integrated information of neural structures, to reflect the world represented\nby a hierarchical \"natural\" classification and the system of \"natural concepts\" and their\ncausal models.\n6. Formalization of \"natural\" classification, \"natural\" concepts and\nconsciousness as integrated information by G.Tononi"
  },
  {
    "id": 209,
    "text": "causal models.\n6. Formalization of \"natural\" classification, \"natural\" concepts and\nconsciousness as integrated information by G.Tononi\nIn accordance with the Principle of unlimited inference, the brain carries out all\npossible conclusions on causal relationships. These causal relationships, of which there\nis an exponential number, in the process of perceiving \"natural\" objects, loop on\nthemselves, forming a certain \"resonance\", which is a system with highly integrated\ninformation in the sense of G.Tononi. At the same time, \"resonance\" occurs if and only\nif these causal relationships reflect some \"natural\" object in which a potentially infinite\nset of features mutually assume each other. The resulting cycles of conclusions on\ncausal relationships are mathematically described by \"fixed points\", which are\ncharacterized by the fact that further application of conclusions to the properties under\nconsideration does not predict new properties. The set of mutually related properties"
  },
  {
    "id": 210,
    "text": "characterized by the fact that further application of conclusions to the properties under\nconsideration does not predict new properties. The set of mutually related properties\nobtained at a fixed point gives the \"image\" of the class or \"prototype\" of the concept\nand its \"causal model\". Therefore, the brain perceives a \"natural\" object not as a set of\nfeatures, but as a \"resonating\" system of causal connections that close on themselves\nthrough the simultaneous inference of the entire set of features of the \"image\" or\n\"prototype\" forming a \"causal model\".\nIt can be shown that the MSCR causal relationships organized into cellular\nensembles make it possible to identify objects of the external world as reliably as\npossible and then predict the properties of these objects as accurately as possible using\nthis identification, since only MSPS causal relationships related to this class are used\nfor predictions. This forms a second, even more accurate, from the point of view of"
  },
  {
    "id": 211,
    "text": "this identification, since only MSPS causal relationships related to this class are used\nfor predictions. This forms a second, even more accurate, from the point of view of\nforecasting, level of organization of information processes.\nWe propose a fundamentally new mathematical apparatus for determining\nintegrated information, \"natural\" classification and \"natural\" concepts. Our\nformalization is based on a probabilistic generalization of the formal concepts analysis\n[8,29-32]. Formal concepts can be defined as fixed points of deterministic rules (with\nno exceptions) [19]. But, as J. Mill wrote: \"Natural groups ... are determined by\nfeatures, ... while taking into account not only the features that are certainly common\nto all the objects included in the group, but the whole set of those features, of which all\noccur in most of these objects, and the majority in all.\" Therefore, it is necessary to get"
  },
  {
    "id": 212,
    "text": "away from deterministic rules and replace them with probabilistic ones in order to\ndetermine the features not exactly, but for the majority. Therefore, we generalize\nformal concepts to the probabilistic case, replacing deterministic rules with MSPS\ncausal relationships and defining probabilistic formal concepts as fixed points of these\nmaximally specific rules [8,29-32]. Due to the fact that the conclusions, based on the\nmost specific causal relationships are consistent, the resulting fixed point will also be\nconsistent and will not contain both a feature and its negation, i.e. such a definition of\nprobabilistic formal concepts is correct.\nIt can be shown [9] that probabilistic formal concepts adequately formalize\n\"natural\" classification and, in moreover, the resulting \"natural\" classification satisfies\nall the requirements that naturalists imposed on \"natural\" classifications [9].\nLet's consider an example of computer simulation of the \"natural\" classes,"
  },
  {
    "id": 213,
    "text": "all the requirements that naturalists imposed on \"natural\" classifications [9].\nLet's consider an example of computer simulation of the \"natural\" classes,\n\"natural\" concepts and integrated information discovery for the encoded digits. Let\nX(a)– be the set of properties of object a given by some set of predicates, and let\n(P &...&P ⇒P )∈MS(X) – be the set of MSPS of causal relationships performed\ni i i\n1 k 0\nfor properties X, {P ,...,P }⊂ X then the prediction operator Pr and the fixed point can\ni i\n1 k\nbe written as follows [6,9]:\nPr(X)=Φ (X∪{P |(P &...&P ⇒P )∈MS(X)}∪{¬P |(P &...&P ⇒¬P )∈MS(X)}),\nKrit i i i i i i i i\n0 1 k 0 0 1 k 0\nwhere Φ (X) – is an operator that modifies the set of features X by adding or removing\nKrit\nsome feature, so that a certain criterion Krit of mutual consistency of causal\nrelationships by mutual prediction of features from X is maximal [6,9]. The Krit\ncriterion measures the informational integration of features according to the system of"
  },
  {
    "id": 214,
    "text": "relationships by mutual prediction of features from X is maximal [6,9]. The Krit\ncriterion measures the informational integration of features according to the system of\ncausal relationships differently than it is done in the G.Tononi’s theory. A fixed point\nis reached when\nPrn+1(X(a))=Prn(X(a)),\nfor some n, where – n is a multiple\napplication of the operator Pr. Since with each application of the operator Pr, the value\nof the Krit criterion increases and reaches a local maximum at a fixed point, then a\nfixed point, which reflect some \"natural\" object, has a maximum of integrated\ninformation and the \"exclusion\" property according to G.Tononi.\nLet us encode the digits as shown in fig. 2. and form a training set, consisting of\n360 shuffled digits (12 digits of fig. 2 duplicated in 30 copies without specifying where\nwhich digit is). On this set, a semantic probabilistic inference revealed 55089 MSCR\ncausal relationships – general statements about objects that J.S. Mill spoke about.\nb)\na)"
  },
  {
    "id": 215,
    "text": "which digit is). On this set, a semantic probabilistic inference revealed 55089 MSCR\ncausal relationships – general statements about objects that J.S. Mill spoke about.\nb)\na)\nc)\nFig. 2. Encoding of digits"
  },
  {
    "id": 216,
    "text": "Fig. 3. The fixed point of the digit 6.\nAccording to these causal relationships, exactly 12 fixed points were found that\ncorrespond to numbers.\nAn example of a fixed point for the digit 6 is shown in fig. 3. Consider what this\nfixed point is. Let's number the signs of the digits as shown in Fig. 2. The first pattern\nof figure 6 in fig. 3, represented in the first rectangle after the curly bracket, means that\nif there is a sign 6 in square 13 (let's denote it as 13-6), then there should be a sign 2 in\nsquare 3 (let's denote it as (3-2)). The predicted sign is indicated by a dotted line. Let's\nwrite this causal relationship as (13-6 ⇒ 3-2). It is not difficult to verify that this causal\nrelationship is carried out on all figures. The second causal relationship means that\nfrom the sign (9-5) and the negation of the value 5 of the first sign ¬(1-5) (the first sign\nshould not be equal to 5), the sign (4-7) follows. Negation is indicated in the figure by"
  },
  {
    "id": 217,
    "text": "from the sign (9-5) and the negation of the value 5 of the first sign ¬(1-5) (the first sign\nshould not be equal to 5), the sign (4-7) follows. Negation is indicated in the figure by\na dotted line, as shown at the bottom of fig. 3. We get a causal relationship (9-5&¬(1-\n5) ⇒ 4-7). The next 3 causal relationships in the first row of the digits 6 will be\nrespectively (13-6 ⇒ 4-7), (17-5&¬(13-5) ⇒ 4-7), (13-6 ⇒ 16-7).\nFig. 3 shows that the causal relationships and the signs of the number 6 form a\nfixed point – mutually predict each other. Note that the causal connections used in the\nfixed point are fulfilled on all digits, and the fixed point itself identifies only the digit\n6. This illustrates the phenomenological property 2 'differences that make a difference',\nin which the system of causal connections perceives \"realizes\" an integral object.\nTherefore, the figures are distinguished not by causal relationships in themselves, but\nby their systemic relationship."
  },
  {
    "id": 218,
    "text": "Therefore, the figures are distinguished not by causal relationships in themselves, but\nby their systemic relationship.\nA fixed point forms a \"prototype\" according to Eleanor Rosch or an \"image\"\naccording to J. S. Mill. The program does not know in advance which combinations of\nfeatures are maximally correlated with each other.\nProbabilistic formal concepts describe not only \"natural\" concepts, but also\ncontexts. Contexts also have the property of maximum prediction accuracy – causal\nrelationships found on a certain context and its causal model will more accurately\npredict the properties of this context."
  },
  {
    "id": 219,
    "text": "Fig. 4. Two contexts - Numbers and Letters\nConsider the following example on fig. 4, containing both numbers and letters.\nYou can learn only on numbers and build probabilistic formal concepts of numbers,\nyou can learn on letters and build probabilistic concepts of letters only, and you can\nlearn both on numbers and letters and build formal concepts of numbers and letters. In\neach of these cases, various MSCR causal relationships will be found, but MSCR\ncausal relationships describing numbers and letters together will contain additional\nsigns separating them from each other, which is obtained automatically by MSCR\ncausal relationships. When considering (in context) both letters and numbers of MSCR\ncausal relationships will have a higher probability, then MSCR on numbers or letters\nand therefore they will be triggered in the formal model of the neuron. Our formal\nneuron model, which detects the most specific causal connections [30], follows the"
  },
  {
    "id": 220,
    "text": "and therefore they will be triggered in the formal model of the neuron. Our formal\nneuron model, which detects the most specific causal connections [30], follows the\nwell–known physiological property of neurons - more probable conditional stimuli are\ntriggered faster in time.\n7. Theory of functional systems\nThe formalization of the second type KOGs – the KOGs of functional systems, is\nbased on the consideration of purposeful behavior, which is carried out by developing\nconditional (causal) links between the actions and its results. These conditional\nconnections are sufficient for modeling functional systems and developing animats.\nP.K. Anokhin wrote that \"We are talking about the collateral branches of the\npyramidal tract, diverting to many neurons \"copies\" of impulsations that go to the\npyramidal tract\" [4-5]. Thus, when a motor neuron sends a signal to the muscles about\nsome action, copies of this excitation are sent, including to the projection zones, which"
  },
  {
    "id": 221,
    "text": "pyramidal tract\" [4-5]. Thus, when a motor neuron sends a signal to the muscles about\nsome action, copies of this excitation are sent, including to the projection zones, which\ncan record the result of the action performed. Therefore, the brain detects all causal\nconnections between actions and their results.\nWe show in the diagram fig. 5 that this is sufficient to explain the basic\nmechanisms of the functional systems of the brain formation [7,31]. Let's assume that\nwe have no experience yet and a motivational excitement has arisen, shown by the\nblack triangle. Then, to meet the need by trial and error, we can do some action that\nwill be activated by some neuron, indicated by a white triangle. Simultaneously with\nthe activation of this action, a \"copy\" of the excitation of this neuron will be sent to the\nprojection zones, where there will be a neuron that will react to the result of the action"
  },
  {
    "id": 222,
    "text": "received from the outside world. Since this neuron will first receive excitation from the\nactivation of an action by a white neuron, it will form a conditional relation between\nthe activation of an action by a white neuron and the result obtained. If now, after\nreceiving this result that has changed the situation, we carry out some next action, also\nindicated by a white triangle, then we will get the following result, for which there will\nalso be a neuron that will react to the result of this action. If, as a result, the need was\nsatisfied and the goal is achieved, then the entire chain of active neurons and\nconditional connections that led to the result will be reinforced and stored in the\nmemory. Thus, there will be an internal contour of forecasting the results achievement\nby the causal relationships. Then, at the next occurrence of motivational excitement,\nthis chain of actions will be extracted from memory and will predict the achievement"
  },
  {
    "id": 223,
    "text": "by the causal relationships. Then, at the next occurrence of motivational excitement,\nthis chain of actions will be extracted from memory and will predict the achievement\nof the result along the inner contour even before any actions. So an action plan will be\nformed, which, according to the inner contour, as stated in the quote by P.K. Anokhin,\nactivates neurons waiting for the results of actions, which will form an acceptor of the\nresults of actions, studied in detail in the theory of functional systems. Thus, the\nformation and operation of the functional system can be explained by the formation of\ncausal relationships between the action and its results.\nIn terms of MSCR causal relationships, the scheme of functional systems is as\nfollows fig. 6. [7,31]. We consider the need as a request to the functional system to\nachieve the goal indicated by the predicate PG . This request enters the afferent\n0\nsynthesis block and, for functional systems that do not have functional subsystems,"
  },
  {
    "id": 224,
    "text": "achieve the goal indicated by the predicate PG . This request enters the afferent\n0\nsynthesis block and, for functional systems that do not have functional subsystems,\nextracts causal relationships of the form P ,…,P ,A ,…,A => PG from memory,\ni1 im k1 kl 0\nleading to the goal PG achievement, where P ,…,P are the properties of the\n0 i1 im\nenvironment necessary to achieve the goal achievement, and A ,…,A – a sequence\nk1 kl\nof actions leading to the goal. At the same time, the properties of P ,…,P must be\ni1 im\npresent in the properties of the environment P ,…,P entering the afferent synthesis\n1 n\nblock. For hierarchically organized functional systems, this query extracts causal\nrelationships of a more complex type P ,…,P ,PG ,…,PG ,A ,…,A => PG from\ni1 im j1 jn k1 kl 0\nthe MSCR memory, including requests to achieve the sub-goals PG ,…,PG .\nj1 jn\nBrain\nMotivation tluseR\nActions in the outside world\nFig. 5. Formation of MSCR of conditional connections\nbetween actions actions and results"
  },
  {
    "id": 225,
    "text": "S N F\nRESULT\ni e o\nR\nt e r\n0\nu d e\nG\nDECISION- ACCEPTOR\nAFFERENT o\nMAKING ACTION\nSYNTHESIS a\nSelect actions RESULTS\nSet MSR rules l\nA ,…,A or Waiting for a result\nP ,…,P ,PG ,…,PG , k1 kl\nPG\ni1 im j1 jn subgoals 0\nA ,…,A →PG\nk1 kl 0 PG ,…,PG\nj1 jn\npredicting goal PG\n0 providing the G R\nachievement in a situation maximum\nP ,…,P probability of\ni1 im ASSESSMENT OF\nachieving the\nRESULTS OF ACTION\ngoal PG\n0 PG ≈ R\n0 0\nClarification of patterns\nReverse afferentation\nS Actions\nabout achieving a\nu\nresult R\nb A 0\nFig. 6. Functional system diagram\nThen the extracted rules are sent to the decision-making block, where forecast of\nthe goal achievement is made for each rule and a probability estimation of the goal\nachievement is calculated. Prediction according to the rules, where only actions are\nperformed, is carried out according to the probability of the rule itself. The forecast\naccording to the rules which requests sub-goals is carried out by sending these requests"
  },
  {
    "id": 226,
    "text": "performed, is carried out according to the probability of the rule itself. The forecast\naccording to the rules which requests sub-goals is carried out by sending these requests\nto functional subsystems, decision-making in them and receiving from them the\nprobability estimations of the corresponding sub-goals achievement. The resulting\nprobability of the forecast is calculated by the product of the probability of the rule on\nthe probability of achieving of its sub-goals. After that, a decision is made by the rule\nselecting that has the maximum probability estimation of the goal achievement.\nThen an action plan is formed, including all the actions included in the rule and\nall the actions that are in the functional subsystems. Simultaneously with the action\nplan, the acceptor of the actions result is formed, including the expectation of all\npredicted sub-results in functional subsystems, as well as in the functional system itself."
  },
  {
    "id": 227,
    "text": "plan, the acceptor of the actions result is formed, including the expectation of all\npredicted sub-results in functional subsystems, as well as in the functional system itself.\nAfter that, the action plan begins to be implemented, and the expected results are\ncompared with the results obtained.\nIf all the sub-results and the final result are achieved and coincide with the\nexpected results, then the rule itself and all the rules of the functional subsystems that\nwere selected in the decision-making process are reinforced and their probability\nincreases. If the result is not achieved in some subsystem, then the corresponding rule,\nselected by decision-making block, of this functional subsystem is penalized. Then\nthere is a tentative research reaction that revises the decision. This model has been\nsuccessfully used to model animates [11,16,31]."
  },
  {
    "id": 228,
    "text": "Fig. 7. Multi-faceted reality.\n8. Consciousness as the tool for resolving contradictions\nLet us consider the third level of prediction accuracy provided by the information\ntheory of consciousness – consciousness as a mechanism for resolving contradictions.\nThe world is multifaceted like a diamond fig. 7 and there is no single consistent\ndescription of it, and the function of consciousness is to choose the appropriate context\ncorrectly, within which you can get the most accurate prediction. In science, such\ncontexts are paradigms that form a certain view, point of view and the corresponding\nsystem of concepts of a particular theory. These paradigms, as a rule, are not\ncompatible with each other.\nThis point of view on consciousness is also expressed by V.M. Allakhverdov [2].\nIn his work [1] he writes: \"Consciousness, faced with contradictory information, tries\nto remove this information from the surface of consciousness or modify it so that the"
  },
  {
    "id": 229,
    "text": "In his work [1] he writes: \"Consciousness, faced with contradictory information, tries\nto remove this information from the surface of consciousness or modify it so that the\ncontradiction disappears or ceases to be perceived as a contradiction.\" In this work, he\ncites 7 cases of resolving contradictions by consciousness. All these cases are explained\nby the properties or interaction of probabilistic formal concepts that define the concepts\nor contexts in question. Let 's consider two of them for brevity:\n1. Case 1. The easiest way to get rid of contradiction or ambiguity is to choose\none interpretation for awareness, and not to realize all the others (incompatible\nwith it) (negatively choose).\nExample. The phenomenon of binocular competition, when different stimuli are\nsimultaneously presented to the different eyes to the subject. If two images are\npresented, one of which is more likely or familiar, the subjects mostly see only it."
  },
  {
    "id": 230,
    "text": "simultaneously presented to the different eyes to the subject. If two images are\npresented, one of which is more likely or familiar, the subjects mostly see only it.\nExplanation. A probabilistic formal concept mutually predicts the properties\nincluded in the concept, as well as the negation of other properties that should not\nbe in it, and thereby inhibits alternatives.\n2. Case 2. When realizing the different sides of the contradiction, an attempt is\nmade to find a way of explanation — the connection of different sides into a\nconsistent whole."
  },
  {
    "id": 231,
    "text": "Example. In the conditions of binocular competition, if you present a red circle\non one eye and a black triangle on the other, the subject will see a black triangle\non a red background.\nExplanation. If the perceived features do not contradict each other and do not\ninhibit each other, then they can form a combined probabilistic formal concept\nand be perceived accordingly.\nIn all cases, it is possible to correctly choose the most appropriate probabilistic formal\nconcept or context to resolve the contradiction and obtain the most accurate prediction\nin accordance with the causal relationships of the chosen concept or context.\nReferences\n1. V.M. Allakhverdov, O.V. Naumenko, M.G. Filippova, O.B. Shcherbakova, M.O. Avanesyan,\nE.Y. Voskresenskaya, A.S. Starodubtsev. How consciousness gets rid of contradictions //\nSHAGI/STEPS 2015(1) The journal of the School of advanced studies in Humanities (in Russian)"
  },
  {
    "id": 232,
    "text": "E.Y. Voskresenskaya, A.S. Starodubtsev. How consciousness gets rid of contradictions //\nSHAGI/STEPS 2015(1) The journal of the School of advanced studies in Humanities (in Russian)\n2. Allakhverdov V.M. Consciousness as a paradox. Exp. Psy. St. Pet.: DNA, 2000. (in Russian)\n3. Anokhin K.V. Cognitom: in search of a general theory of cognitive science // The Sixth\nInternational Conference on Cognitive Science, Kaliningrad, 2014, pp. 26-28. (in Russian)\n4. Anokhin P.K. The problem of decision-making in psychology and physiology // Problems of\ndecision-making. Moscow: Nauka, 1976. pp. 7-16. (in Russian)\n5. Anokhin P.K. Biology and neurophysiology of the conditioned reflex and its role in adaptive\nbehavior. Oxford a.o.: Pergamon press, 1974. 574 p.\n6. Vityaev E.E., Neupokoev N.V. Formal model of perception and image as a fixed point of\nanticipation. Neuroinformatics. 2012, volume 6, No. 1, pp. 28-41. (in Russian)"
  },
  {
    "id": 233,
    "text": "6. Vityaev E.E., Neupokoev N.V. Formal model of perception and image as a fixed point of\nanticipation. Neuroinformatics. 2012, volume 6, No. 1, pp. 28-41. (in Russian)\n7. Vityaev E.E. Logic of brain work // Approaches to modeling thinking. ed. V.G. Redko. URSS\nEditorial, Moscow, 2014, pp. 120-153. (in Russian)\n8. Vityaev E.E., Demin A.V., Ponomarev D.K. Probabilistic generalization of formal concepts //\nProgramming. Vol.38, No.5, 2012, pp. 219-230. (in Russian)\n9. Vityaev E.E., Martynovich V.V. Formalization of \"natural\" classification and systematics through\nfixed points of predictions // SEMR. News. V. 12, IM SB RAS, 2015, pp. 1006-1031. (in Russian)\n10. Dubrovsky D.I. The problem of \"consciousness and the brain\": An information approach.\nKnowledge, Understanding, Skills. 2013, №4. (in Russian)\n11. Mukhortov V.V., Khlebnikov S.V., Vityaev E.E. Improved algorithm of semantic probabilistic\ninference in the 2-dimensional animate problem // Neuroinformatics, 6(1). 50-62. (in Russian)"
  },
  {
    "id": 234,
    "text": "11. Mukhortov V.V., Khlebnikov S.V., Vityaev E.E. Improved algorithm of semantic probabilistic\ninference in the 2-dimensional animate problem // Neuroinformatics, 6(1). 50-62. (in Russian)\n12. Rudolf Carnap. Philosophical foundations of Physics. M., \"Progress\", 1971, p. 388. (in Russian)\n13. Smirnov E.S. The construction of a species from a taxonomic point of view. Zool. Journal. (1938).\n17:3, pp. 387-418. (in Russian)\n14. Max Tegmark. Our mathematical universe. ACT, 2016, p. 592. (in Russian)\n15. Cartwright, N. Causal Laws and Effective Strategies. Noûs. (1979) 13(4): 419-437.\n16. Demin A.V., Vityaev E.E. Learning in a virtual model of the C. elegans nematode for locomotion\nand chemotaxis // Biologically Inspired Cognitive Architectures. 2014, v.7, pp.9–14.\n17. Hebb D.O. The Organization of Behavior. Wiley: New York; 1949.\n18. Hempel C.G. Maximal Specificity and Lawlikeness in Probabilistic Explanation. Philosophy of\nScience. (1968) 35, pp. 116-33."
  },
  {
    "id": 235,
    "text": "17. Hebb D.O. The Organization of Behavior. Wiley: New York; 1949.\n18. Hempel C.G. Maximal Specificity and Lawlikeness in Probabilistic Explanation. Philosophy of\nScience. (1968) 35, pp. 116-33.\n19. Ganter B. Formal Concept Analysis: Methods, and Applications in Computer Science. TU\nDresden (2003).\n20. Masafumi Oizumi, Larissa Albantakis, Giulio Tononi. From the Phenomenology to the\nMechanisms of Consciousness: Integrated Information Theory 3.0 // PLOS Computational\nBiology. May 2014, V.10. Issue 5.\n21. Mill J.S. System of Logic. Ratiocinative and Inductive. L., 1983."
  },
  {
    "id": 236,
    "text": "22. Bob Rehder, Jay B. Martin. Towards A Generative Model of Causal Cycles // 33rd Annual\nMeeting of the Cognitive Science Society 2011, (CogSci 2011), Boston, Massachusetts, USA,\n20-23 July 2011, V.1 pp. 2944-2949.\n23. Rehder B. Categorization as causal reasoning // Cognitive Science, 27. 2003, pp. 709–748.\n24. Rosch E.H. Natural categories // Cognitive Psychology 4. 1975, P. 328-350.\n25. Rosch E. Principles of Categorization // Rosch E.&Lloyd B.B. (eds), Cognition and\nCategorization. Lawrence Erlbaum Associates, Publishers, 1978. pp. 27–48.\n26. Tononi G. Information integration: its relevance to brain function and consciousness. Arch. Ital.\nBiol., 148: 299-322, 2010.\n27. Tononi G. Integrated information theory of consciousness: an updated account. Arch Ital Biol\n150, 2012, 56–90.\n28. Vityaev E.E. The logic of prediction. // Proceedings of the 9th Asian Logic Conference\nMathematical Logic in Asia. World Scientific, Singapore. 2005, pp. 263–276."
  },
  {
    "id": 237,
    "text": "150, 2012, 56–90.\n28. Vityaev E.E. The logic of prediction. // Proceedings of the 9th Asian Logic Conference\nMathematical Logic in Asia. World Scientific, Singapore. 2005, pp. 263–276.\n29. Vityaev E.E., Martinovich V.V. Probabilistic Formal Concepts with Negation // A. Voronkov, I.\nVirbitskaite (Eds.). PCI 2014, LNCS 8974, 2015, pp. 385-399.\n30. Vityaev E.E. A formal model of neuron that provides consistent predictions // Biologically\nInspired Cognitive Architectures 2012 // Advances in Intelligent Systems and Computing, v.196,\nSpringer, 2013, pp. 339-344.\n31. Vityaev E. Purposefulness as a Principle of Brain Activity // Anticipation: Learning from the Past.\nCognitive Systems Monographs, V.25, Springer, 2015, pp. 231-254.\n32. Vityaev, E., Odintsov, S. How to predict consistently? Trends in Mathematics and Computational\nIntelligence // Studies in Computational Intelligence 796. Mara Eug. Cornejo (ed), 2019, 35-41."
  },
  {
    "id": 238,
    "text": "Consciousness as a logically consistent and\nprognostic model of reality1\nEvgenii Vityaev 1,2,*\n1 Sobolev institute of mathematics, Koptuga 4, Novosibirsk, Russia; vityaev@math.nsc.ru\n2 Novosibirsk State University, Pirogova 2, Novosibirsk, Russia; vityaev@math.nsc.ru\n* Correspondence: vityaev@math.nsc.ru; Tel.: +7-913-945-4867\nAbstract: The work demonstrates that brain might reflect the outer world causal relationships in\nthe form of a logically consistent and prognostic model of reality, which shows up as\nconsciousness. The paper analyses and solves the problem of statistical ambiguity and provides a\nformal model of causal relationships as probabilistic maximally specific rules. We suppose that\nbrain makes all possible inferences from causal relationships. We prove that the suggested formal\nmodel has a property of an unambiguous inference: from consistent premises we infer a consistent\nconclusion. It enables a set of all inferences to form a consistent model of the perceived world."
  },
  {
    "id": 239,
    "text": "model has a property of an unambiguous inference: from consistent premises we infer a consistent\nconclusion. It enables a set of all inferences to form a consistent model of the perceived world.\nCausal relationships may create fixed points of cyclic inter-predictable properties. We consider the\n“natural” classification introduced by John St. Mill and demonstrate that a variety of fixed points of\nthe objects’ attributes forms a “natural” classification of the outer world. Then we consider notions\nof “natural” categories and causal models of categories, introduced by Eleanor Rosch and Bob\nRehder and demonstrate that fixed points of causal relationships between objects attributes, which\nwe perceive, formalize these notions. If the “natural” classification describes the objects of the\nexternal world, and “natural” concepts the perception of these objects, then the theory of integrated\ninformation, introduced by G. Tononi, describes the information processes of the brain for"
  },
  {
    "id": 240,
    "text": "external world, and “natural” concepts the perception of these objects, then the theory of integrated\ninformation, introduced by G. Tononi, describes the information processes of the brain for\n“natural” concepts formation that reflects the “natural” classification. We argue that integrated\ninformation provides high accuracy of the objects identification. A computer-based experiment is\nprovided that illustrates fixed points formation for coded digits.\nKeywords: clustering; categorization; natural classification; natural concepts; integrated\ninformation; concepts\n1. Introduction\nThe work demonstrates that the human brain may reflect the outer world causality in the form\nof a logically consistent and prognostic model of reality that shows up as consciousness.\nThe work analyses and solves such problem of causal reflection of the outer world as a\nstatistical ambiguity (Section 2.3). The problem is solved in such a way that it is possible to obtain a"
  },
  {
    "id": 241,
    "text": "The work analyses and solves such problem of causal reflection of the outer world as a\nstatistical ambiguity (Section 2.3). The problem is solved in such a way that it is possible to obtain a\nformal model of causal relationships, which provides a consistent and prognostic model of the outer\nworld. To discover these causal relationships by the brain, a formal model of neuron that is in line\nwith Hebb rule (Hebb, 1949), is suggested. We suppose that brain makes all possible\ninferences/predictions from those causal relationships. We prove (see Section 2.5) that the suggested\nformal model of causal relationships has a property of an unambiguous inference/predictions,\nnamely, consistent implications are drawn out from consistent premises. It enables a set of all\ninferences/predictions, which brain makes from causal relationships, to form a consistent and\npredictive model of the perceived world. What is particularly important is that causal relationships"
  },
  {
    "id": 242,
    "text": "inferences/predictions, which brain makes from causal relationships, to form a consistent and\npredictive model of the perceived world. What is particularly important is that causal relationships\nmay create fixed points of cyclic inter-predictable properties that create a certain “resonance” of\ninter-predictions. In terms of interconnections between neurons, these are cellular assemblies of\nneurons that mutually excite each other and form the systems of highly integrated information. In\n1 The work is supported by the Russian Science Foundation grant №17-11-01176 in part concerning the\nmathematical results in 2.4-2.6 and by Russian Foundation for Basic Research # 19-01-00331-a in other parts."
  },
  {
    "id": 243,
    "text": "the formal model these are logically consistent fixed points of causal relationships. We argue\n(Section 2.1) that if attributes of the outer world objects are taken regardless of how persons perceive\nthem, a complex of fixed points of the objects’ attributes forms a “natural” classification of the outer\nworld. If the fixed points of causal relationships of the outer world objects, which persons perceive,\nare taken, they form “natural” concepts described in cognitive sciences (Section 2.2).\nIf the “natural” classification describes objects of the external world, and “natural” concepts are\nthe perception of these objects, then the theory of integrated information (Tononi, 2004, 2016,\nOzumi, Albantakis and Tononi, 2014) describes the information processes of the brain when these\nobjects are perceived.\nG. Tononi defines consciousness as a primary concept, which has the following\nphenomenological characteristics: composition, information, integration, exclusion (Ozumi,"
  },
  {
    "id": 244,
    "text": "objects are perceived.\nG. Tononi defines consciousness as a primary concept, which has the following\nphenomenological characteristics: composition, information, integration, exclusion (Ozumi,\nAlbantakis and Tononi, 2014). For a more accurate determination of these properties G. Tononi\nintroduces the concept of integrated information: \"integrated information characterizing the\nreduction of uncertainty is the information, generated by the system that comes in a certain state\nafter the causal interaction between its parts, which is superior information generated independently\nby its parts themselves\" (Tononi, 2004).\nThe process of reflection of causal relationships of the outer world (Fig. 1) shall be further\nconsidered. It includes:\n1. The objects of the outer world (cars, boats, berths) which relate to certain \"natural\" classes;\n2. The process of brain reflection of objects by causal relations marked by blue lines;"
  },
  {
    "id": 245,
    "text": "1. The objects of the outer world (cars, boats, berths) which relate to certain \"natural\" classes;\n2. The process of brain reflection of objects by causal relations marked by blue lines;\n3. Formation of the systems of interconnected causal relationships, indicated by green ovals.\nIn G. Tononi's theory only the third point of reflection is considered. The totality of the excited\ngroups of neurons form a maximally integrated conceptual structure that defined by G. Tononi as\nqualia. Integrated information is also considered as a system of cyclic causality. Using integrated\ninformation the brain is adjusted to perceiving \"natural\" objects of the outer world.\nIn terms of integrated information, phenomenological properties are formulated as follows. In\nbrackets an interpretation of these properties from the point of view of \"natural\" classification is\ngiven.\n1. Composition – elementary mechanisms (causal relationships) can be combined into the"
  },
  {
    "id": 246,
    "text": "brackets an interpretation of these properties from the point of view of \"natural\" classification is\ngiven.\n1. Composition – elementary mechanisms (causal relationships) can be combined into the\nhigher-order ones (\"natural\" classes in the form of causal loops produce a hierarchy of \"natural\"\nclasses);\n2. Information – only mechanisms that specify “differences that make a difference” within a\nsystem shall be taken into account (only a system of “resonating” causal relationships, forming\na class and “differences that make a difference” is important. See illustration in the computer\nexperiment below);\n3. Integration – only information irreducible to non-interdependent components shall be taken\ninto account (only system of \"resonating\" causal relations, indicating an excess of information\nand perception of highly correlated structures of \"natural\" object is accounted for);\n4. Exclusion – only maximum of integrated information counts (only values of attributes that are"
  },
  {
    "id": 247,
    "text": "and perception of highly correlated structures of \"natural\" object is accounted for);\n4. Exclusion – only maximum of integrated information counts (only values of attributes that are\n\"resonating\" at the fix-point and, thus, mostly interrelated by causal relationships, form a\nFigure 1. Brain reflection of causal relationships between objects attributes"
  },
  {
    "id": 248,
    "text": "\"natural\" class or \"natural\" concept).\nThese properties are defined as the intrinsic properties of the system. We consider these\nproperties as the ability of the system to reflect the complexes of external objects’ causal relations,\nand consciousness as the ability of a complex hierarchical reflection of a \"natural\" classification of the\nouter world.\nTheoretical results on consistency of inference and consistency of fixed points of our formal\nmodel are supposing that a probability measure of events is known. However, if we discover causal\nrelationships on the training set, and intend to predict properties of a new object out of the training\nset and belonging to a wider general population, or to recognize a new object as a member of some\n“natural” concept, there might be inconsistencies. Here, a certain criterion of maximum consistency\nis employed (see Section 2.8), which is based upon information measure, close in meaning to an"
  },
  {
    "id": 249,
    "text": "“natural” concept, there might be inconsistencies. Here, a certain criterion of maximum consistency\nis employed (see Section 2.8), which is based upon information measure, close in meaning to an\nentropic measure of integrated information (Tononi, 2004). The process of recognizing “natural”\nclasses is described in Section 2.9.\nIn section 3 a computer modeling of \"fixed points\" discovering for the coded digits is provided.\nFurther we shell describe the idea of the work in more detail before the complicated\nmathematical part.\nCausality is a result of physical determinism: “for every isolated physical system some fixed\nstate of a system determines all the subsequent states” (Carnap, 1966). But, an automobile accident\nshall be taken as an example (Carnap, 1966). What was the reason for it? It might be a road surface\ncondition or humidity, position of sun with respect to drivers’ looks, reckless driving, psychological"
  },
  {
    "id": 250,
    "text": "condition or humidity, position of sun with respect to drivers’ looks, reckless driving, psychological\nstate of driver, functionality of brakes, etc. It is clear that there is no any certain cause in this case.\nIn the philosophy of science causality is reduced to forecasting and explaining. “Causal relation\nmeans predictability … in that if the entire previous situation is known, an event may be predicted\n…, if all the facts and laws of nature, related to the event, are given” (Carnap, 1966). It is clear that\nnobody knows all the facts, which number in case of an accident is potentially infinite, and all the\nlaws of nature. In case of a human being and animals, the laws are obtained by training (inductive\nreasoning). Therefore, causality is reduced to predicting pursuant to inductive-statistical (I-S)\nreasoning, which involves logical inference of predictions from facts and statistical laws with some\nprobabilistic assessment."
  },
  {
    "id": 251,
    "text": "reasoning, which involves logical inference of predictions from facts and statistical laws with some\nprobabilistic assessment.\nWhen discovering causal relationships and laws on real data or by training, a problem of\nstatistical ambiguity appears – contradictions (contradictory predictions) may be inferred from these\ncausal relationships. See example in the section 2.3. To avoid inconsistences, Hempel (1965, 1968)\nintroduced a requirement of maximal specificity (see Sections 2.3, 2.4), which implies that a\nstatistical law should incorporate maximum of information, related to the predictable property.\nSection 2.5 presents a solution to the problem of statistical ambiguity. Following Hempel, a\ndefinition of maximally specific rules is given (Section 2.4, 2.5), for which it is proved (Section 2.6)\nthat (I-S) inference that uses only maximum specific rules, does not result in inconsistencies (see also"
  },
  {
    "id": 252,
    "text": "that (I-S) inference that uses only maximum specific rules, does not result in inconsistencies (see also\nVityaev, 2006). A special semantic probabilistic inference is developed that discover all maximum\nspecific rules, which might be considered as the most precise causal relationships (that have\nmaximum conditional probability and use maximum available information). Work (Vityaev, 2013)\nshows that the semantic probabilistic inference might be considered as a formal model of neuron\nthat satisfy the Hebb rule, in which the semantic probabilistic reasoning discover all most precise\nconditional relationships. A set of such neurons might create a consistent and prognostic model of\nthe outer world. Causal relationships may form fixed points of cyclic inter-predictable properties\nthat produce a certain “resonance” of mutual predictions. Cycles of inferences about causal\nrelations, are mathematically described by the \"fixed points\". These points are characterized by the"
  },
  {
    "id": 253,
    "text": "that produce a certain “resonance” of mutual predictions. Cycles of inferences about causal\nrelations, are mathematically described by the \"fixed points\". These points are characterized by the\nproperty, that using inferences with respect to considered properties they don’t predict some new\nproperty.\nNeuron transmits its excitation to the other neurons through multiple both excitatory and\ninhibitory synapses. Inhibitory synapses may slow down other neurons and stop their activity. It is\nimportant for “inhibiting” alternate perception images, attributes, properties, etc. Within our formal\nmodel it is accomplished by discovering “inhibitory” conditional relationships that predict absence"
  },
  {
    "id": 254,
    "text": "of an attribute/property of the object (the perceived object shall not have the respective\nattribute/property) as compared to the other objects, where this characteristic is found. A formal\nmodel specifies it by predicates’ negations for corresponding attribute/property. Absence of\ninconsistencies at the fixed point means that there are no two causal relationships simultaneously\npredicting both availability of some attribute/property with an object, and its absence.\nThe structure of the outer world objects was analyzed in the form of \"natural\" classifications\n(Section 2.1). It was noted that \"natural\" classes of animals or plants have a potentially infinite\nnumber of different properties (Mill, 1983). Naturalists, who were building \"natural\" classifications,\nalso noted that construction of a \"natural\" classification was just an indication: from an infinite\nnumber of attributes you need to pass to the limited number of them, which would replace all other"
  },
  {
    "id": 255,
    "text": "also noted that construction of a \"natural\" classification was just an indication: from an infinite\nnumber of attributes you need to pass to the limited number of them, which would replace all other\nattributes (Smirnof, 1938). This means that in \"natural\" classes these attributes are strongly\ncorrelated, for example, if there are 128 classes, and their attributes are binary, then the independent\n\"indicator\" attributes among them will be about 7 attributes as 27 = 128, and others can be predicted\nbased on these 7 attributes. The set of mutually predicted properties, obtained at the fixed point,\nproduces some \"natural\" class.\nHigh correlation of attributes for \"natural\" classes was also confirmed in cognitive studies (see.\nSection 3). Eleanor Rosch formulated the principles of categorization, one of which is as follows: \"the\nperceived world is not an unstructured set of properties found with equal probability, on the"
  },
  {
    "id": 256,
    "text": "Section 3). Eleanor Rosch formulated the principles of categorization, one of which is as follows: \"the\nperceived world is not an unstructured set of properties found with equal probability, on the\ncontrary, the objects of perceived world have highly correlated structure\" (Rosch, 1973, 1978, Rosch\nat al. 1976). Therefore, directly perceived objects (so called basic objects) are observed and functional\nligaments, rich with information. Later, Bob Rehder suggested a theory of causal models, in which\nthe relation of an object to a category is based not on a set of attributes but on the proximity of\ngenerating causal mechanisms: \"the object is classified as a member of a certain category to the extent\nthat its properties are likely to have been generated by this category of causal laws\" (Rehder, 2003).\nThus, the structure of causal relationships between the attributes of objects is taken as a basis of"
  },
  {
    "id": 257,
    "text": "Thus, the structure of causal relationships between the attributes of objects is taken as a basis of\ncategorization. Therefore, brain perceives a \"natural\" object not as a set of attributes, but as a\n\"resonant\" system of causal relationships, closing upon itself through simultaneous inference of the\ntotal aggregate of the \"natural\" concept features. At the same time, \"resonance\" occurs, if and only if\nthese causal relationships reflect some integrity of some \"natural\" class, in which a potentially\ninfinite number of attributes mutually presuppose each other. To formalize causal models, Bob\nRehder proposed to use causal graphical models (CGMs). However, these models are based on\n«deployment» of Bayesian networks, which do not allow cycles and cannot formalize cyclic causal\nrelationships.\nIt should be specially noted that the \"resonance\" of mutual predictions of the properties of\nobjects is carried out continuously in time and therefore the predicted properties are properties that"
  },
  {
    "id": 258,
    "text": "It should be specially noted that the \"resonance\" of mutual predictions of the properties of\nobjects is carried out continuously in time and therefore the predicted properties are properties that\nhave just been perceived as stimuli from the object. Therefore, it is always a prediction in time. The\nabsence of contradictions in the predictions is also the absence of contradictions between the\npredicted stimulus and the really received stimulus.\nHere we suggest a new mathematical apparatus for formalizing cyclic causal relationships.\n2. Materials and Methods\n2.1. \"Natural\" classification\nThe first fairly detailed philosophical analysis of \"natural\" classification was carried out by John\nSt. Mill (1983). This analysis describes all the basic properties of \"natural\" classifications, which were\nlater confirmed by naturalists who were building \"natural\" classifications and also by those\nresearchers in the field of cognitive sciences, who were investigating \"natural\" concepts."
  },
  {
    "id": 259,
    "text": "later confirmed by naturalists who were building \"natural\" classifications and also by those\nresearchers in the field of cognitive sciences, who were investigating \"natural\" concepts.\nAccording to John St. Mill (1983) \"artificial\" classifications differ from the \"natural\" ones in that\nthey may be based on any one or more attributes, so that different classes differ only in inclusion of\nobjects, having different values of these attributes. But if the classes of \"animals\" or \"plants\" are\nconsidered, they differ by such a large (potentially infinite) number of properties that they can't be\nenumerated. Furthermore, all these properties are based on statements, confirming this distinction."
  },
  {
    "id": 260,
    "text": "At the same time, among the properties of some \"natural\" classes, there are both observable and\nnon-observable attributes. To take into account hidden attributes, their causal exhibitions in the\nobserved attributes should be found. \"For example, the natural classification of animals should be\nbased mainly on their internal structure; however, it would be strange, as noted by A. Comte, if we\nwere able to determine the family and specie of one or another animal, only by killing it\" (Mill, 1983).\nJames. St. Mill gives the following definition of \"natural\" classification: it is such a classification,\nwhich combines the objects into the groups, about which we can make the greatest number of\ncommon propositions, and it is based on such properties, which are causes of many others … He\nalso defines an \"image\" of a class, which is the forerunner of \"natural\" concepts: \"our concept of class,\nthe way in which this class is represented in our mind, is the concept of some sample, having all"
  },
  {
    "id": 261,
    "text": "the way in which this class is represented in our mind, is the concept of some sample, having all\nattributes of this class in the highest ... degree\".\nJohn St. Mill's arguments were confirmed by naturalists. For example, Rutkowski (1884) writes\nabout inexhaustible number of general properties of the \"natural\" classes: \"The more essential\nattributes are similar in comparable objects, the more likely they are the similar in other attributes\".\nSmirnov E.S. (1938) makes a similar statement: \"The taxonomic problem is in indication: \"from an\ninfinite number of attributes we need to pass to a limited number of them, which would replace all\nother attributes.\" As a result, there were formulated a problem of specifying \"natural\" classifications,\nwhich is still being discussed in the literature (Wilkins and Ebach, 2013).\nThese researches are uncover a high degree of over determination of information for “natural”"
  },
  {
    "id": 262,
    "text": "which is still being discussed in the literature (Wilkins and Ebach, 2013).\nThese researches are uncover a high degree of over determination of information for “natural”\nclasses. If for specification of 128 classes with binary characteristics only 7 binary attributes are\nrequired, since 27 = 128, then for hundreds or even thousands of attributes as it is the case of\n“natural” classes, they are much more highly overdetermined. Since over determination of features\nis high, it is possible to find, for example, 20 attributes for our classes, which are identical for the\nobjects of the same class and entirely different in other classes. As each class is described by the fixed\nset of values of any 7 from 20 binary attributes, then remaining 13 attributes are uniquely\ndetermined. It implies that there are, at least, 13* C 720 = 1007760 dependencies between attributes.\nThis property of “natural” classification was called as “taxonomic saturation” (Kogara, 1982). For"
  },
  {
    "id": 263,
    "text": "This property of “natural” classification was called as “taxonomic saturation” (Kogara, 1982). For\nsuch systems of attributes there is no a problem of a “curse of dimensionality”, when as\ndimensionality of the attribute space increases, accuracy of Machine Learning algorithms decreases.\nConversely it was shown in (Nedelko, 2015) that accuracy of the classification algorithm increases, if\nit properly incorporates redundancy of information. The classification and recognition algorithm in\nSections 2.8, 2.9 incorporate this redundant information. As it is shown in the works on “natural”\nconcepts in cognitive studies (see next section), human perception also incorporates this redundancy\nof information that allows identifying objects with practically with very high accuracy.\n2.2. Principles of categorization in cognitive sciences\nIn the works of Eleanor Rosch (1973, 1978, Rosch at al. 1976) the principles of categorization of"
  },
  {
    "id": 264,
    "text": "2.2. Principles of categorization in cognitive sciences\nIn the works of Eleanor Rosch (1973, 1978, Rosch at al. 1976) the principles of categorization of\n\"natural\" categories, confirming statements of John St. Mill and naturalists, are formulated on the\nbasis of the experiments:\n“Cognitive Economy. The first principle contains the almost common-sense notion that, as an\norganism, what one wishes to gain from one's categories is a great deal of information about the\nenvironment while conserving finite resources as much as possible. To categorize a stimulus means\nto consider it, for purposes of that categorization, not only equivalent to other stimuli in the same\ncategory but also different from stimuli not in that category. On the one hand, it would appear to the\norganism's advantage to have as many properties as possible predictable from knowing any one\nproperty, a principle that would lead to formation of large numbers of categories with as fine"
  },
  {
    "id": 265,
    "text": "organism's advantage to have as many properties as possible predictable from knowing any one\nproperty, a principle that would lead to formation of large numbers of categories with as fine\ndiscriminations between categories as possible».\n«Perceived World Structure. The second principle of categorization asserts that … perceived\nworld – is not an unstructured total set of equiprobable co-occurring attributes. Rather, the material\nobjects of the world are perceived to possess … high correlational structure. … In short,\ncombinations of what we perceive as the attributes of real objects do not occur uniformly. Some"
  },
  {
    "id": 266,
    "text": "pairs, triples, etc., are quite probable, appearing in combination sometimes with one, sometimes\nanother attribute; others are rare; others logically cannot or empirically do not occur».\nIt is understood that the first principle is not possible without the second one – the cognitive\neconomy is not possible without a structured world. «Natural» objects (basic objects) are rich with\ninformation ligaments of observed and functional properties, which form a natural discontinuity,\ncreating categorization. These ligaments form a \"prototypes\" of the objects of different classes\n(\"images\" of John St. Mill): «Categories can be viewed in terms of their clear cases if the perceiver\nplaces emphasis on the correlational structure of perceived attributes … By prototypes of categories\nwe have generally meant the clearest cases of category membership» (Rosch, 1973, 1978). «Rosch and\nMervis have shown that the more prototypical of a category a member is rated, the more attributes it"
  },
  {
    "id": 267,
    "text": "we have generally meant the clearest cases of category membership» (Rosch, 1973, 1978). «Rosch and\nMervis have shown that the more prototypical of a category a member is rated, the more attributes it\nhas in common with other members of the category and the fewer attributes in common with\nmembers of the contrasting categories» (Rosch & Mervis, at al., 1976).\nFor the future the theory of \"natural\" concepts was suggested by Eleanor Rosch, called the\nPrototypical Theory of Concepts (Prototype Theory). Its main features are described as follows: «The\nprototype view (or probabilistic view) keeps the attractive assumption that there is some underlying\ncommon set of features for category members but relaxes the requirement that every member have\nall the features. Instead, it assumes there is a probabilistic matching process: Members of the\ncategory have more features, perhaps weighted by importance, in common with the prototype of"
  },
  {
    "id": 268,
    "text": "all the features. Instead, it assumes there is a probabilistic matching process: Members of the\ncategory have more features, perhaps weighted by importance, in common with the prototype of\nthis category than with prototypes of other categories» (Ross, et al., 2008).\nIn further studies it was found that the models based on attributes, similarities and prototypes\nare not sufficient to describe classes. It is therefore necessary to take into account the theoretical,\ncausal and ontological knowledge, relating to the objects of classes. For example, people do not only\nknow that birds have wings and can fly, and build their nests in trees, but also facts, that the birds\nbuild their nests in trees, because they can fly, and fly because they have wings.\nMany researchers believe that the most important theoretical knowledge is the knowledge of\ncausal dependencies because of its functionality. It allows the organism to interfere in external"
  },
  {
    "id": 269,
    "text": "Many researchers believe that the most important theoretical knowledge is the knowledge of\ncausal dependencies because of its functionality. It allows the organism to interfere in external\nevents and to gain control over the environment. Studies have shown that people's knowledge of\ncategories isn't limited by the list of properties, and includes a rich set of causal relationships\nbetween these properties, which can be ranked. The importance of these properties also depends on\nthe category causal relationships. It was shown in some experiments (Ahn, at al., 2000, Sloman, Love\nand Ahn, 1998), that property is more important in classification, if it is more involved in causal\nnetwork of relationships between the attributes. In the other experiments it was shown, that the\nproperty is more important, if it has more reasons (Rehder & Hastie, 2001).\nConsidering causal dependencies, Bob Rehder proposed the theory of causal models, according"
  },
  {
    "id": 270,
    "text": "property is more important, if it has more reasons (Rehder & Hastie, 2001).\nConsidering causal dependencies, Bob Rehder proposed the theory of causal models, according\nto which: «people’s intuitive theories about categories of objects consist of a model of the category, in\nwhich both the category’s features and the causal mechanisms among those features are explicitly\nrepresented. In other words, theories might make certain combinations of features either sensible\nand coherent … in light of the relations linking them, and the degree of coherence of a set of features\nmight be an important factor determining membership in a category» (Rehder, 2003).\nIn the theory of causal models, the relationship of the object to a category is based not on a set of\nattributes or similarity based on attributes, but on the basis of similarity of generating causal\nmechanisms: «Specifically, a to-be-classified object is considered a category member to the extent"
  },
  {
    "id": 271,
    "text": "attributes or similarity based on attributes, but on the basis of similarity of generating causal\nmechanisms: «Specifically, a to-be-classified object is considered a category member to the extent\nthat its features were likely to have been generated by the category’s causal laws, such that\ncombinations of features that are likely to be produced by a category’s causal mechanisms are\nviewed as good category members and those unlikely to be produced by those mechanisms are\nviewed as poor category members. As a result, causal-model theory will assign a low degree of\ncategory membership to objects that have many broken feature correlations (i.e., cases where causes\nare present but effects absent or vice versa). Objects that have many preserved correlations (i.e.,\ncauses and effects both present or both absent) will receive a higher degree of category membership\nbecause it is just such objects that are likely to be generated by causal laws» (Rehder, 2003)."
  },
  {
    "id": 272,
    "text": "causes and effects both present or both absent) will receive a higher degree of category membership\nbecause it is just such objects that are likely to be generated by causal laws» (Rehder, 2003).\nTo represent causal knowledge, some researchers have used Bayesian networks and causal\nmodels (Cheng, 1997, Gopnik, at al., 2004, Griffiths &Tenenbaum, 2009). However, these models"
  },
  {
    "id": 273,
    "text": "cannot simulate cyclic causality because Bayesian networks do not support cycles. In his work Bob\nRehder (Rehder & Martin, 2011) proposed a model of causal cycles, based on a \"disclosure\" of causal\ngraphical models. The \"disclosure\" is fulfilled by creating a Bayesian network which unfolds over\ntime. But this work did not formalize the cycles of causality.\nOur model is directly based on cyclic causal relationships, represented by fundamentally new\nmathematical models – fixed points of predictions on causations. To formalize such fixed points, a\nprobabilistic generalization of formal concepts was defined (Vityaev, et al., 2012, 2015). Formal\nconcepts emerging in the Formal Concept Analysis (FCA) may be specified as fixed points of\ndeterministic implications (with no exceptions) (Ganter and Wille, 1999, Ganter, 2003). We\ngeneralize formal concepts for probabilistic case through introducing probabilistic implications and"
  },
  {
    "id": 274,
    "text": "deterministic implications (with no exceptions) (Ganter and Wille, 1999, Ganter, 2003). We\ngeneralize formal concepts for probabilistic case through introducing probabilistic implications and\ndefining fixed points for probabilistic implications (Vityaev, et al., 2012, 2015). Generalization is\nmade so, that in certain conditions probabilistic formal concepts and formal concepts coincide. A\ncomputer experiment is performed (Vityaev, et al., 2012, 2015), which demonstrates that\nprobabilistic formal concepts might “restore” formal concepts after superimposition of noise.\nThe formalization given in the section 2.6 is more general, then in works (Vityaev, et al., 2012,\n2015). When there is a fixed set of objects, and there is no general population, for which we need to\nrecognize a new object from general population as belonging to one of available formal concepts,\nformalization in section 2.6 provides probabilistic formal concepts. But when set of objects is a"
  },
  {
    "id": 275,
    "text": "recognize a new object from general population as belonging to one of available formal concepts,\nformalization in section 2.6 provides probabilistic formal concepts. But when set of objects is a\nsample from the general population, and it is necessary to recognize new objects to the one of the\nprobabilistic formal concepts, this formalization provides “natural” classification of the objects of\ngeneral population.\n2.3. Statistical ambiguity problem\nA problem of inconsistent predictions obtained from inductively deduced knowledge is called\na problem of statistical ambiguity. The classical example is the following. Suppose that we have the\nfollowing statements.\nL1 Almost all cases of streptococcus infection clear up quickly after the administration of\npenicillin.\nL2 Almost no cases of penicillin resistant streptococcus infection clear up quickly after the\nadministration of penicillin.\nC1 Jane Jones had streptococcus infection.\nC2 Jane Jones received treatment with penicillin."
  },
  {
    "id": 276,
    "text": "administration of penicillin.\nC1 Jane Jones had streptococcus infection.\nC2 Jane Jones received treatment with penicillin.\nC3 Jane Jones had a penicillin resistant streptococcus infection.\nOn the base of L1 and C1∧C2 one can explain why Jane Jones recovered quickly (E). The second\nargument with L2 and C2∧C3 explains why Jane Jones did not (¬E). The set of statements {C1, C2,\nC3} is consistent. However, the conclusions contradict each other, making these arguments rival\nones. Hempel hoped to solve this problem by forcing all statistical laws in an argument to be\nmaximally specific – they should contain all relevant information with respect to the domain in\nquestion. In our example, then, the statement C3 invalidates the first argument L1, since this\nargument is not maximally specific with respect to all information about Jane Jones. So, we can only\nexplain ¬E, but not E. Let us consider the problem of statistical ambiguity the notion of maximal\nspecificity in more detail."
  },
  {
    "id": 277,
    "text": "explain ¬E, but not E. Let us consider the problem of statistical ambiguity the notion of maximal\nspecificity in more detail.\nThere are two types of predictions (explanations): deductive-nomological (D-N) and\ninductive-statistical (I-S). The employed laws in D-N predictions are supposed to be true, whereas\nin I-S predictions they are supposed to be statistical.\nDeductive-nomological model may be presented in the form of a scheme"
  },
  {
    "id": 278,
    "text": "L\nC\nG\n,\n1\n,\n1\n,\n,\nL\nC\nm\nn\nwhere: L\n1\n,,L\nm\n- set of laws; C\n1\n, , C\nn\n- set of facts; G – predicted statement;\nL,,L ,C,,C G ; set\n1 m 1 n\nL\n1\n, , L\nm\n, C\n1\n, , C\nn\nis consistent; L\n1\n, , L\nm\nG , C\n1\n, , C\nn\nG ; laws\nL\n1\n,,L\nm\ncontain only universal quantifiers; set of facts C1,…,Cn – quantifier-free formulas.\nInductive-statistical model is identical to deductive-nomological with the difference that laws\nare statistical and shall meet the Requirement of Maximal Specificity (RMS), to avoid\nin-consistencies.\nRequirement of Maximal Specificity is defined by Carl Hempel (1965, 1968) as follows:\nF G, pG;F= r\nF(a)\n[r].\nG(a)\nRule F  G is the maximum specific with the state of knowledge K, if for each class H, for\nwhich both of the below statements belong to K\nx(H(x)F(x)), H(a)\nthere is a statistical law p ( G ; H ) = r ' in K such that r  r ' . RMS requirement implies that if both F"
  },
  {
    "id": 279,
    "text": "which both of the below statements belong to K\nx(H(x)F(x)), H(a)\nthere is a statistical law p ( G ; H ) = r ' in K such that r  r ' . RMS requirement implies that if both F\nand H contain object a, and H is a subset of F, then H has more specific information about object a,\nthan F and, therefore, law p ( G ; H ) shall prevail over law p ( G ; F ) . However, for maximum specific\nrules law p(G;H) should have the same probability as law p ( G ; F ) and thus H not adds any\nadditional information.\nThe Requirement of Maximal Specificity had not been investigated by Hempel and its\nsuccessors formally, and it had not been proved that it can avoid inconsistencies. The next section\ncontains a formal definition of the maximal specificity, for which we prove that I-S inference that use\nonly maximal specific rules, is consistent.\n2.4. Requirement of Maximal Specificity\nLet us introduce a language of the first order L of signature   \n1\n, ,\nk\n , which contains only"
  },
  {
    "id": 280,
    "text": "only maximal specific rules, is consistent.\n2.4. Requirement of Maximal Specificity\nLet us introduce a language of the first order L of signature   \n1\n, ,\nk\n , which contains only\nunitary predicates symbols for the objects properties and stimulus description. Let U (  ) shall\ndenote a set of all atomic formulas. Atomic formulas or their negations shall be called liters, and a\nmultitude of all liters shall be denoted as Lit. Closure of all liters with respect to logical operations\n& ,  ,  shall be called a set of sentences  (  ) .\nAlso we need an empirical system MA,W of the signature  for representing the set of\nobjects A and set of predicates W   P\n1\n, ..., P\nk\n defined on A. Set of all M-true sentences from  (  )\nshall be called a theoryTh(M)of M. It shall be further supposed that theoryTh(M)– is a set of\nuniversally quantified formulas. It is known that any set of universal formulas is logically equivalent\nto the set of rules as follows\n(1)"
  },
  {
    "id": 281,
    "text": "universally quantified formulas. It is known that any set of universal formulas is logically equivalent\nto the set of rules as follows\n(1)\nC(A &...&A A ),k0,A {A,...,A },\n1 k 0 0 1 k\nwhere A ,A ,...,A – liters. Formulas of typeA ,k0are considered as rules\n0 1 k 0\n( T  A\n0\n) , where T –\ntruth. Hence, without loss of generality, it can be assumed that theory Th(M) is a set of the (1) type\nrules."
  },
  {
    "id": 282,
    "text": "A rule might be true on empirical system M only because the premise of the rule is always\nfalse. Furthermore, a rule may also be true, since its certain “sub-rule” that involves a part of\npremise, is true on empirical system. These observations are summarized in the following theorem.\nТеорема 1 8. (Vityaev, 2006). Rule C  ( A\n1\n& ... & A\nk\n A\n0\n) logically follows from rules:\n1. (A &...&A A ) С, {A ,...,A ,A }{A ,...,A }, 0  h < k;\ni1 ih i0 i1 ih i0 1 k\n2. (A &...&A A ) С, {A ,...,A }{A ,...,A }, 0  h < k,\ni1 ih 0 i1 ih 1 k\nwhere is prof.\nDefinition 1. Sub-rule of rule C shall be any rule of 1 or 2 type, specified in theorem 1.\nCorollary 1. If sub-rule of rule C is true on M, then rule C is true on M.\nDefinition 2. Any rule C, true on M, each sub-rule of which is already not true on M, shall be\ncalled a law of empirical system M. Rule (  A\n0\n) is true on M, if M A\n0\n. Rule (A ) true on\n0\nМ, is a law on M."
  },
  {
    "id": 283,
    "text": "called a law of empirical system M. Rule (  A\n0\n) is true on M, if M A\n0\n. Rule (A ) true on\n0\nМ, is a law on M.\nLet Law – be the set of all laws on M. Then, theory T h ( M ) logically follows from Law.\nTheorem 2 (Vityaev, 2006). (See prof in Appendix A). L a w T h ( M ) and for each rule\nC  T h ( M ) its sub-rule exists, which is a law on M.\nStatistical laws shall now be defined. For the sake of simplicity a probability shall be\ndetermined on empirical system M   A ,W  as on a general population, where А is a set of objects\nof a general population, and W is a set of unitary predicates, defined on A.\nProbability : A  0 ,1    shall be defined on A (general case is considered in Halpern, 1990):\n(a)1,(a)0, aA.\naA\n(2)\n(B)(b), B A.\nbB\nProbability\n(B)(b)\nbB\nn  on ( A ) n , shall be naturally defined:\nn ( a\n1\n, ..., a\nn\n) ( a\n1\n) ... ( a\nn\n)       .\nInterpretation of language L shall be determined as mapping I :   W , where predicate\nP\nj"
  },
  {
    "id": 284,
    "text": "bB\nn  on ( A ) n , shall be naturally defined:\nn ( a\n1\n, ..., a\nn\n) ( a\n1\n) ... ( a\nn\n)       .\nInterpretation of language L shall be determined as mapping I :   W , where predicate\nP\nj\n W , j  1 , ..., k corresponds to each predicate symbol\nj\n  . Mapping ν:XA of a set of\nvariables X of language L to the set of objects shall be called attribution. Composition of mappings\nI , where () specifies a formula obtained from  by replacement of predicate symbols of\nby predicates W through interpretation I and replacement of variables from  with objects from A\nby attributing . Probability  of sentence φ(a,,b)() shall be defined as:\nη(φ)=μn({a ,…,a |M νIφ,νa=a ,…,νb=a }). (3)\n1 n 1 n\nDefinition 3. Rule C(A &...&A A ) of (1) type, with\n1 k 0\n( A\n1\n& ... & A\nk\n) 0   and conditional\nprobability ( C ) ( A\n0\n/A\n1\n& ... & A\nk\n) 0     strictly higher than conditional probabilities of all its"
  },
  {
    "id": 285,
    "text": "1 k 0\n( A\n1\n& ... & A\nk\n) 0   and conditional\nprobability ( C ) ( A\n0\n/A\n1\n& ... & A\nk\n) 0     strictly higher than conditional probabilities of all its\nsub-rules, shall be called a probabilistic law on M. Conditional probability of sub-rule C(A )\n0\nshall be defined as (C)(A /T)(A ). All rules of type(A ), (A )0 are probabilistic\n0 0 0 0\nlaws.\nA set of all probabilistic laws shall be designated as LP."
  },
  {
    "id": 286,
    "text": "Definition 4. Probabilistic law C  ( A\n1\n& ... & A\nk\n A\n0\n) , which is not a sub-rule of any other\nprobabilistic law, shall be called the strongest probabilistic law (SPL-rule) on M. A set of all SPL-rules\nshall be designated as SPL.\nIt shall be proved that a concept of probabilistic law generalizes a concept of a law, which is\ntrue on M.\nTheorem 3. (See prof in Appendix A). Law  SPL  LP.\nDefinition 5 (Vityaev, 2006). A semantic probabilistic inference (SP-inference) of some SPL-rule,\npredicting liter A\n0\n, shall be such sequence of probabilistic laws C\n1\nC\n2\n... C\nn\n, as:\n1. C\n1\n (  A\n0\n) ;\n2. C\n1\n, C\n2\n, ..., C\nn\n L P , C\ni\n ( A i1 & ... & A iki  A\n0\n) , k\ni\n 0 ;\n3. C\ni\nis a sub-rule of C\ni 1\n, i.e. {Ai&...&Ai}{Ai1&...&Ai1},\n1 ki 1 ki1\nk\ni\n k\ni 1\n;\n4. ( C\ni 1\n) ( C\ni\n)  \n\n ;\n5. C\nn\n(4)\n– SPL-rule.\nA set of all SP-inferences, predicting liter A shall be considered. This set may be presented\n0\nas a semantic probabilistic inference tree for liter A\n0\n."
  },
  {
    "id": 287,
    "text": ") ( C\ni\n)  \n\n ;\n5. C\nn\n(4)\n– SPL-rule.\nA set of all SP-inferences, predicting liter A shall be considered. This set may be presented\n0\nas a semantic probabilistic inference tree for liter A\n0\n.\nLemma 2. (See prof in Appendix A). Any probabilistic law C  ( A\n1\n& ... & A\nk\n A\n0\n) belongs to\nsome semantic probabilistic inference, which predicts liter A\n0\nand, hence, to the tree of semantic\nprobabilistic inference of literA .\n0\nDefinition 6. A maximum specific rule MS(A ) on M for predicting literA shall be an SPL-rule\n0 0\nthat has a maximum value of conditional probability among all SPL-rules of a semantic\nprobabilistic inference tree for liter A\n0\n. If there are several rules with identical maximum value, all\nof them are maximum specific.\nA set of all maximum specific rules shall be denoted as MSR.\nProposition 1. Law  MSR  SPL  LP.\n2.5. Resolving the problem of statistical ambiguity\nA requirement of maximal specificity shall be defined in the general case. It shall be supposed"
  },
  {
    "id": 288,
    "text": "Proposition 1. Law  MSR  SPL  LP.\n2.5. Resolving the problem of statistical ambiguity\nA requirement of maximal specificity shall be defined in the general case. It shall be supposed\nthat statement H in the Hempel’s formulation of the requirement of maximal specificity is a\nsentence H   (  ) of the language L.\nDefinition 7. Rule C  ( F  G ) , F   (  ) , G  L it meets the requirement of maximal\nspecificity (RMS), if it follows from H() and F(a)&H(a) for some a  A that rule\nC '  ( F & H  G ) has the same probability (G/F&H)(G/F)r.\nIn other words, RMS states that there is no sentenceH(), which would increase (or\ndecrease, see the Lemma below) conditional probability (G/F)r of the rule by adding it to the\npremise of the rule.\nLemma 3. (See prof in Appendix A). If statement H() decreases probability of rule i.e.\n(G/F&H)(G/F) then the statement H increases it and ( G / F & H ) ( G / F )     ."
  },
  {
    "id": 289,
    "text": "premise of the rule.\nLemma 3. (See prof in Appendix A). If statement H() decreases probability of rule i.e.\n(G/F&H)(G/F) then the statement H increases it and ( G / F & H ) ( G / F )     .\nLemma 4. (See prof in Appendix A). For each rule C(A &...&A A ) of form (1), there\n1 k 0\nshall be found a probabilistic law C'(B &...&B A ), k'k, on M, for which (C')(C).\n1 k' 0\nТеорема 4 (Vityaev, 2006). (See prof in Appendix A). Any maximum specific rule\nMS(G)(FG), F(),GLit meets the requirement of maximal specificity."
  },
  {
    "id": 290,
    "text": "2.6. Fixed points of predictions based on MSR rules. Solution of a statistical ambiguity problem.\nIt shall be proved that any I-S inference is consistent for any set of rules { L\n1\n, , L\nm\n}  M S R . To\ndo it, an inference by MSR rules and fixed points of inference as per MSR rules shall be considered.\nDefinition 8. An immediate successor operator Pr shall be defined by rules from P  M S R\nwith a set of liters L as follows:\nPr (L)=L{A |C(A &...&A A ),{A,...,A }L,СP}\nP 0 1 k 0 1 k\nDefinition 9. A fixed point of operator Pr of immediate successor with respect to a set of liters\nL shall be a closure of this set of liters with respect to the immediate successor operator P  rP ( L ) ,\nwhence it follows that P rP ( P  rP ( L ) )  L .\nDefinition 10. A set of liters L  { L\n1\n, ..., L\nk\n} is called a compatible set, if ( L\n1\n& ... & L\nk\n) 0   .\nDefinition 11. A set of liters L is consistent, if it does not contain simultaneously atom G and\nits negation  G ."
  },
  {
    "id": 291,
    "text": "1\n, ..., L\nk\n} is called a compatible set, if ( L\n1\n& ... & L\nk\n) 0   .\nDefinition 11. A set of liters L is consistent, if it does not contain simultaneously atom G and\nits negation  G .\nProposition 2. (See prof in Appendix A). If L is compatible, L is consistent.\nIt shall be shown that an immediate prediction retains the property of compatibility.\nТеорема 5 (Vityaev and Martinovich 2015). (See prof in Appendix A). If L is compatible, P rP ( L )\nis also compatible, P  M S R .\nTo prove the theorem, the following lemma shall first be proved.\nЛемма 5. (See prof in Appendix A). If for rules A = ( A  G ) , B = ( B   G ) ,\nA=A &...&A ,\n1 k\nB = B\n1\n& ... & B\nm\n, ( A & B ) 0    , k  0 , m  0 inequality ( G / A & B ) ( G / A )    \nis valid, a rule exists that has a strictly higher conditional probability than rule A.\nCorollary 2. If L is compatible, then P rP ( L ) is consistent for P  M S R ."
  },
  {
    "id": 292,
    "text": "is valid, a rule exists that has a strictly higher conditional probability than rule A.\nCorollary 2. If L is compatible, then P rP ( L ) is consistent for P  M S R .\nCorollary 3. (Solution to a problem of statistical ambiguity). I-S inference is consistent for any\nset of laws P = { L\n1\n, , L\nm\n}  M S R and set of facts { C\n1\n, ..., C\nn\n} .\nCorollary 4. Fixed points Pr(L) for a compatible set of liters L are compatible and consistent.\nP\nA set of all fixed points L=Pr (N) , obtained by all maximum specific rules on all\nMSR\ncompatible sets of liters N, shall be designated using Class(M).\n2.7. “Natural” classification as a fixed points of prediction by a maximum specific rules.\nProbabilistic formal concepts and “natural” classification shall be specified, using fixed points\npursuant to maximum specific rules. If empirical system M   A ,W  is defined on some fixed set of\nobjects A, the below defined probabilistic formal concepts and “natural” classifications for this set"
  },
  {
    "id": 293,
    "text": "objects A, the below defined probabilistic formal concepts and “natural” classifications for this set\nare in agreement. It can be shown (Vityaev et al., 2005, Vityaev & Martynovich, 2015), that this\ndefinition of “natural” classification satisfies all the requirements, which natural scientists imposed\non “natural” classification.\nDefinition 12. Probabilistic formal concepts and “natiral” classification (Vityaev &\nMartynovich, 2015).\n1. A set of all fixed points of Class(M) shall be called a set of all probabilistic formal concepts and\n“natural” classes of this empirical system M.\n2. Each class/concept L specifies in empirical system M a set of objects, which belong to a\nclass/concept M ( L )  { b  A | B\nb\nL } , whereB b,W is a sub-model of model\nb\nM   A , W \ngenerated by object bA.\n3. A lawful model of class/concept СL,Z  shall be defined as a set of liters LClass(M) of\nL\nfixed point and a set of rules Z MSR, applicable to liters from L.\nL"
  },
  {
    "id": 294,
    "text": "4. A generating set of some class/concept C   L , Z\nL\n shall be such a subset of liters N  L , that\nL=Pr (N).\nMSR\n5. A set S of atomic sentences (j a ) , j=1,…s, s  k shall be called a system-forming, if for each\nclass from Class(M) there is a generating set of liters, which are obtained from the\nsystem-forming set of atomic sentences by taking or not taking the negation.\n6. A systematics shall be defined as a set S,Z ,{Z } , where S is a system-forming\nS Li LiClass(M)\nset of atomic sentences, Z\nS\n– a law of systematization, which defines an order of taking\nnegations for atomic sentences from S, { Z }\nLi L i C lass(M )\n, a set of the rules for fixed points from\nClass(M).\n2.8. Method of “natural” data classification\nIn the case of “natural” classification, an empirical system M  A,W as a general\npopulation, is unknown. Only a data sampled from a general population are known. Therefore, to"
  },
  {
    "id": 295,
    "text": "In the case of “natural” classification, an empirical system M  A,W as a general\npopulation, is unknown. Only a data sampled from a general population are known. Therefore, to\ndevelop a method for “natural” classification, the questions of making “natural” classification and\nfixed points by data samplings should be considered. By a sampling from a general population\nwe shall mean a sub-model, B\nB\n  B , W  , where B is a set of objects randomly selected from\ngeneral population A. A frequency probability  shall be specified within a sampling, by\nB\nassuming\nB\n( a ) 1 / N , N | B |    , according to which probability  on sentences from\nB\n (  ) is\ndetermined. On sampling B\nB\n  B , W  , as on a sub-model, a theory Th(B), a set of laws Law(B), a\nset of probabilistic laws LP(В), a set of the strongest probabilistic laws SPL(B), and a set of\nmaximum specific laws MSR(B) might be obtained.\nProposition 3. T h ( M )  T h ( B ) .\nSince each set of liters, S\nb\n  L  L it B\nb"
  },
  {
    "id": 296,
    "text": "maximum specific laws MSR(B) might be obtained.\nProposition 3. T h ( M )  T h ( B ) .\nSince each set of liters, S\nb\n  L  L it B\nb\nL  is compatible, since it is obtained on real objects,\nwhich have a nonzero probability, hence, a set C la s s ( B )  { L = P  rM\nSR\n( N ) | N  S\nb\n, b  B } shall be a\nset of all fixed points on sampling B\nB\n. Classes Class(B) shall be the analogues of probabilistic\nformal concepts (Vityaev, et al., 2012, 2015). However, a potential identification and recognition of\n“natural” classes of a general population by “natural” classes discovered on a sampling, is of\nconcern, rather than probabilistic formal concepts, defined on sampling B\nB\n M . In this case,\nprobability  of a general population is unknown, but frequency probability  within a\nB\nsampling is known.\nMaximum specific laws MSR(B) within sampling B may be not the same on a general\nB\npopulation. They can be considered as approximations of laws MSR(M) in the following sense."
  },
  {
    "id": 297,
    "text": "B\nsampling is known.\nMaximum specific laws MSR(B) within sampling B may be not the same on a general\nB\npopulation. They can be considered as approximations of laws MSR(M) in the following sense.\nDetermining maximum specific laws using semantic probabilistic inference shall be recalled. In the\nprocess of inferring a premise of a rule is stepwise developed through strictly enhancing its\nconditional probability and involving as much relevant information as possible, on accordance\nwith a requirement of maximal specificity, to ensure maximum probable and consistent prediction.\nSampling B might facilitate building a tree of semantic probabilistic inference through\nB\nincreasing the premise and applying some statistical criteria for checking the strict increase of the\nconditional probability  on general population M A,W . For this purpose, an exact"
  },
  {
    "id": 298,
    "text": "independence criterion of Fischer for contingency tables shall be used. Here, by sampling\nB\nB\n  B , W  a set L P ( M ) of probabilistic laws with some confidence level  might be\n\ndiscovered, where each probabilistic inequality shall be statistically validated with confidence level\n. By the set L P ( M ) , a set of the strongest probabilistic laws\n\nS P L ( M ) might be found, with\n\nconfidence level .\nInference by S P L ( M ) rules may be inconsistent, hence, to build fixed points by set\n\nS P L ( M ) , it is necessary to use a weaker criterion of consistency of probabilistic laws in mutual\n\npredictions, which assumes occurrence of inconsistencies.\nAn operator of direct inference r\nS P L (M )\n( L )\n\n  shall be defined for this case. A set of laws,\nverified within a set of liters L shall be defined\nSat(L){C|CSPL (M),C(A &...&A A),{A ,...,A }L,A L},\n 1 k 0 1 k 0\nand a set of laws, disprovable at a set of liters L\nF a l( L ) { C | C S P L ( M ) , C ( A\n1\n& ... & A\nk\nA"
  },
  {
    "id": 299,
    "text": "Sat(L){C|CSPL (M),C(A &...&A A),{A ,...,A }L,A L},\n 1 k 0 1 k 0\nand a set of laws, disprovable at a set of liters L\nF a l( L ) { C | C S P L ( M ) , C ( A\n1\n& ... & A\nk\nA\n0\n) , { A\n1\n, ..., A\nk\n} L , A\n0\nL }\n\n       , (A  A ).\n0 0\nA criterion Kr of mutual consistency between laws from SPL shall be defined on a set of\n\nliters L as:\nK r\nSP L (M )\n( L )\nC Sat(L )\n( C )\nC F al(L )\n( C )  \n\n  \n\n , where ( C ) lo g (1\nB\n( C ) )      .\nFunction lo g (1\nB\n( C ) )    incorporates not a probability itself, but its closeness to 1, since it\ncharacterizes a predictive power of regularity more accurately.\nCriterion K r\nSP L (M )\nspecifies not only information measure of consistency between\n\nregularities, but also an information measure of mutual integration between causal relationships\nwithin a set of liters L. Therefore, this measure is very close in meaning to entropy measure of\nintegrated information stated in (Tononi, 2004).\nOperator r\nS P L (M )\n( L )\n"
  },
  {
    "id": 300,
    "text": "within a set of liters L. Therefore, this measure is very close in meaning to entropy measure of\nintegrated information stated in (Tononi, 2004).\nOperator r\nS P L (M )\n( L )\n\n  functions as follows: it either adds one of liters Lit to a set L, which is\npredicted by regularities from S a t( L ) , or deletes one of liters of a set L, predictable by disprovable\nlaws from F a l( L ) . Here, a compatibility of laws (their information measure) applicable to a set L\nshall strictly increase, i.e. at every step an inequality shall be satisfied\nK r\nSP L\n( r\nSP L\n( L ) ) K r\nSP L\n( L )\n  \n   .\nHere, that liter is added/deleted, which results in a maximum increase in the criterion\n(information measure). If adding/deleting a liter does not increase a criterion, a set L remains\nunchanged and is a fixed point. Changes in criterion, when adding/deleting an element, are,\nrespectively:\n( L )\nA 0 P\nm\nrSPL\na x(L\n), A 0\n{L K r\nSP L (M )\n( L A\n0\n) K r\nSP L (M )\n( L ) } \n  \n \n \n  ,"
  },
  {
    "id": 301,
    "text": "respectively:\n( L )\nA 0 P\nm\nrSPL\na x(L\n), A 0\n{L K r\nSP L (M )\n( L A\n0\n) K r\nSP L (M )\n( L ) } \n  \n \n \n  ,\n(L) max {Kr (L\\A )Kr (L)}.\nA0PrSPL (L),A0L\nSPL(M) 0 SPL(M)"
  },
  {
    "id": 302,
    "text": "Operator r\nS P L (M )\n( L )\n\n  adds/deletes that element, which maximizes the value of a criterion.\nAdded/deleted elements are defined as follows:\n( A\n0\n)\nA 0 P\na r g\nrSPL\nm\n(M\na x\n(L ), ) A 0 L\n( K r\nSP L (M )\n( L A\n0\n) )\n \n \n \n ,\n( A\n0\n)\nA 0 P\na r g\nrSPL\nm\n(M\na x\n(L ), ) A 0 L\n( K r\nSP L (M )\n( L \\ A\n0\n) )\n \n \n \n.\nIn each case of employing, operator r\nS P L (M )\n( L )\n\n  adds/deletes that element, which increases\na criterion to the maximum, i.e. adds element ( A\n0\n)  , if ( L ) ( L )      , ( L ) 0    and deletes\nelement ( A\n0\n)  , if (L)(L), ( L ) 0    .\nThus, operator r\nS P L (M )\n( L )\n\n  is determined as follows:\nr\nSP L (M )\n( L )\nL\nL \\\n(\n(\nA\nA\n0\n0\n)\n)\nif\nif\nL , e ls\n(\n(\ne\nL\nL\n)\n)\n(\n(\nL\nL\n)\n)\n,\n,\n(\n(\nL\nL\n)\n)\n0\n0 .\n  \n  \n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFixed point r\nSP L (M )\n( L ) L\n\n   shall be obtained in the third case, when adding/deleting an"
  },
  {
    "id": 303,
    "text": ")\n(\n(\nL\nL\n)\n)\n,\n,\n(\n(\nL\nL\n)\n)\n0\n0 .\n  \n  \n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFixed point r\nSP L (M )\n( L ) L\n\n   shall be obtained in the third case, when adding/deleting an\nelement does not increase the criterion. A set of all such fixed points obtained on all compatible\nsets of liters L, shall be defined as a set of “natural” classes Class (M).\n\nTo obtain a regular model of class LClass (M) a set of regularities\n\nZ\nL\ninterpredicting the\nclass attributes should be specified. Regularities S a t( L ) represent such regularities. At the fixed\npoint L disprovable predictions across regularities Fal(L) are overlapped with the verified\npredictions across regularities S a t( L ) . Since, theoretically, with the known measure , there\nshouldn’t be any inconsistencies, regularities F a l( L ) shall be considered obtained due to\nrandomness of sampling B\nB\n. Regularities from S P L ( M ) that were not included into any set\n"
  },
  {
    "id": 304,
    "text": "shouldn’t be any inconsistencies, regularities F a l( L ) shall be considered obtained due to\nrandomness of sampling B\nB\n. Regularities from S P L ( M ) that were not included into any set\n\nS a t( L ) of any class LClass (M), are considered as obtained as a result of retraining by virtue of\n\nsampling B randomness, therefore, they will be deleted from\nB\nS P L ( M ) .\n\nDefinition 13. A set C   L , S a t ( L )  shall be called a regular model of class LClass (M).\n\nFor “natural” classes, definitions of generating set, system forming set, and systematics from\ndefinition 12 remain unchanged.\nThe functions of information measure in terms of accuracy of identifying objects of the outer\nworld shall be noted:\n1. It enables to cope with retraining and exclude regularities, which are probably obtained\ndue to randomness of sampling;\n2. Extraction of the maximum value of information measure facilitates extracting all\noverdetermined information, inherent in “natural” classes;"
  },
  {
    "id": 305,
    "text": "due to randomness of sampling;\n2. Extraction of the maximum value of information measure facilitates extracting all\noverdetermined information, inherent in “natural” classes;\n3. Recognition of objects of the outer world using overdetermined information and makes\nthis recognition maximally accurate and solves the problem of “dimensionality curse”.\n2.9. Recognition of “natural” classes"
  },
  {
    "id": 306,
    "text": "Regular models of classes allow recognizing them on control objects, chosen from a general\npopulation. For this purpose, regular models of classes C   L , S a t ( L )  in the form of regular\nmatrices shall be defined. For each liter A\n0\n L the power of its prediction shall be estimated\npursuant to regularities from S a t( L ) and liters from L as\nK r\nSat(L )\n( A\n0\n)\nC Sat(L ),C (A &1 ...& A k A0 )\n( C )  \n \n\n\n.\nDefinition 14. Regular matrix of class C   L , S a t ( L )  shall be defined as a tuple\nM\nC\n  L ,{ K r\nSat(L )\n( A\n0\n) }\nA 0 L\n .\nProposition 4. For each class C   L , S a t ( L )  an equation is true\nKr (L)  Kr (A ).\nSPL(M) Sat(L) 0\nA0L\nUsing regular matrices, new objects of a general population may be recognized and relegated\nto the “natural” classes. Pertinence of object bB to class C   L , S a t ( L )  shall be assessed. To do\nthat, it shall be estimated, to what degree object b  B is conform with regularities of classSat(L)."
  },
  {
    "id": 307,
    "text": "that, it shall be estimated, to what degree object b  B is conform with regularities of classSat(L).\nDefinition 15. Pertinence of object b  B to class C   L , S a t ( L )  shall be defined as\nS c o r e ( b / C ) \nA 0\n(Sb\n L )\nK r\nSat(L )\n( A\n0\n) \nA 0 L\n,\nA 0 S b\nK r\nSat(L )\n( A\n0\n) .\nRecognition by regular matrices is made the same way as by weight matrices. To do that, for\nall objects of positive and negative sampling shall be calculated the S c o r e ( b / C ) for each class, and\na threshold value shall be computed that provides the required values of the first and second type\nerrors. Then new objects of a general set might be recognized and relegated to the class, if value\nS c o r e ( b / C ) of this object is higher than the threshold value.\n3. Results\nLet's illustrate the formation of fixed points for the \"natural\" classes/concepts by the computer\nexperiment on coded digits. Digits shall be encoded, as it is shown in Fig. 2. Attributes of digits shall"
  },
  {
    "id": 308,
    "text": "experiment on coded digits. Digits shall be encoded, as it is shown in Fig. 2. Attributes of digits shall\nbe enumerated, as indicated in Table 1. A training set, consisting of 360 shuffled digits (12 digits of\nFig. 2, which are duplicated in 30 copies without specifying where any digit is) shall be formed. On\nthis set 55089 maximum specific rules were found by semantic probabilistic inference. They are\ngeneral statements about the objects, mentioned by John St. Mill and W. Whewell."
  },
  {
    "id": 309,
    "text": "Let X ( a ) be a set of properties (stimulus) of an object а, defined by a set of predicates, and\n(P &...&P P )MSR (X) – the set of the maximum specific rules satisfied for properties X,\ni1 ik i0 \n{P,...,P } X with confidential level . Then we can define operator\ni1 ik\nr\nM SR (X ) \n  of direct\ninference. The fixed point is reached when r\nM SR (X\nn) 1 ( X ( a ) ) r\nM SR (X\nn) ( X ( a ) )\n \n      , for some n,\nwhere r\nM SR (X\nn)\n\n  – n multiple application of the operator. Since in each application of operator\nr the value of criterion Kr increases and at the fixed point reaches a local\nMSR(X) MSR(X)\nmaximum, then the fixed point, when reflecting some \"natural\" object, has a maximum information\nmeasure K r\nM SR (X )\nFigure 2. Digits coding.\nand property of «exclusion» by G. Tononi.\n\nTable 1. Encoding of digital's fields.\n1 2 3 4\n5 6 7 8\n9 10 11 12\n13 14 15 16\n17 18 19 20\n21 22 23 24\nFields codes."
  },
  {
    "id": 310,
    "text": "M SR (X )\nFigure 2. Digits coding.\nand property of «exclusion» by G. Tononi.\n\nTable 1. Encoding of digital's fields.\n1 2 3 4\n5 6 7 8\n9 10 11 12\n13 14 15 16\n17 18 19 20\n21 22 23 24\nFields codes.\nAccording to these regularities, exactly 12 fixed points are discovered, accurately\ncorresponding to the digits. An example of a fixed point for digit 6 is shown in Figure. 3. It shall be\nconsidered, what this fixed point is."
  },
  {
    "id": 311,
    "text": "Figure 3. Fix-point for digit 6.\nThe first regularity of digit 6 in Figure 3, represented in the first box after the brace, means that\nif in the square 13 (Table. 1) there is an attribute 6 (it is denote as 13-6), then it must be an attribute 2\nin square 3 (it is denote by (3-2) ). The predicted attribute is indicated by a dotted line. This\nregularity shall be written as (13-6  3-2). It is easy to see that this regularity is really implemented\nin all the digits. The second regularity means, that from the attribute (9-5) and the denial of the\nvalue 5 of the first attribute (1-5) (the first attribute must not be equal to 5) follows attribute (4-7).\nDenial is denoted by a dash line, as shown in the lower part of Figure 3. Thus, regularity\n(9-5&(1-5)  4-7) is obtained. The following 3 regularities in the first row of the digit 6, will be\n(13-6  4-7), (17-5&(13-5)  4-7), (13-6  16-7), respectively."
  },
  {
    "id": 312,
    "text": "(9-5&(1-5)  4-7) is obtained. The following 3 regularities in the first row of the digit 6, will be\n(13-6  4-7), (17-5&(13-5)  4-7), (13-6  16-7), respectively.\nFigure 3 shows, that these regularities and the attributes of digit 6 form a fixed point. They\nmutually predict each other. It is worth mentioning, that regularities, used in the fixed point, are\ndiscovered on all digits, but fixed point allocates only one digit. This illustrates the\nphenomenological property of “information” which states that “differences that make a difference”.\nThus, the system of causal relationships perceives (is conscious of) whole object. Thus, digits are\nidentified not by regularities themselves, but their system relationship. The fixed points forms a\n\"prototype\" by Eleanor Rosch, or \"image\" by John St. Mill. The program does not know in advance\nwhich possible combinations of attributes are correlated with each other.\n4. Discussion"
  },
  {
    "id": 313,
    "text": "\"prototype\" by Eleanor Rosch, or \"image\" by John St. Mill. The program does not know in advance\nwhich possible combinations of attributes are correlated with each other.\n4. Discussion\nTheoretical results obtained in the paper suggest that it is possible to create a mathematically\nprecise system of reflection of reality, based on the most specific rules and fixed points that use them.\nFirst of all, the question arises about the functioning of the neuron – is it really detects causal\nrelations in accordance with the semantic probabilistic inference. Unfortunately, this is still\nimpossible to verify, because connecting several contacts to one neuron is a deadly number for the\nneuron itself. However, it can be shown that the reflection of causal relationships is able to model a\nmultitude of cognitive functions in accordance with existing physiological and psychological\ntheories. The organization of purposeful behavior is modeled by causal relationships between"
  },
  {
    "id": 314,
    "text": "multitude of cognitive functions in accordance with existing physiological and psychological\ntheories. The organization of purposeful behavior is modeled by causal relationships between\nactions and their results (Vityaev, 2015), which fully corresponds to the theory of functional systems\n(Anokhin, 1974). Fixed points adequately model the process of perception (Vityaev & Neupokoev,"
  },
  {
    "id": 315,
    "text": "2014). A set of causal relationships models expert knowledge (Vityaev, Perlovsky Kovalerchuk,\nSperansky, 2013). Therefore, the verification of this formal model for compliance with the actual\nprocesses of the brain seems to be an important task.\nReferences\n1. Hebb, D. (1949). The organization of behavior. A neurophysiological theory, NY.\n2. Ozumi, M.; Albantakis, L.; Tononi, G. (2014) From the Phenomenology to the Mechanisms of\nConsciousness: Integrated Information Theory 3.0 // PLOS Computational Biology May, 10:5.\n3. Tononi, G.; Boly, M.; Massimini, M.; Koch, C. (2016). Integrated information theory: from\nconsciousness to its physical substrate. Nature Reviews Neuroscience, 450–461.\n4. Tononi G. An information integration theory of consciousness. (2004). BMC. Neurosci 5:42.\n5. Carnap, R. (1966). Philosophical Foundations of Physics. Basic Books.\n6. Hempel, C. (1965). Aspects of Scientific Explanation. In: Aspects of Scientific Explanation and other"
  },
  {
    "id": 316,
    "text": "5. Carnap, R. (1966). Philosophical Foundations of Physics. Basic Books.\n6. Hempel, C. (1965). Aspects of Scientific Explanation. In: Aspects of Scientific Explanation and other\nEssays in the Philosophy of Science, The Free Press: New York.\n7. Hempel, C. (1968). Maximal Specificity and Lawlikeness in Probabilistic Explanation. Philosophy\nof Science, 35, 16–33.\n8. Vityaev, E. (2006). The logic of prediction. In Mathematical Logic in Asia. Proceedings of the 9th\nAsian Logic Conference, Novosibirsk, Russia, August 16-19, 2005; Goncharov, S., Downey, R.,\nOno, H., Eds.; World Scientific: Singapore, pp. 263-276.\n9. Vityaev, E. (2013). A formal model of neuron that provides consistent predictions. In Biologically\nInspired Cognitive Architectures 2012. Proceedings of the Third Annual Meeting of the BICA Society;\nChella, A., Pirrone, R., Sorbello, R., Johannsdottir, K., Eds.; Advances in Intelligent Systems and\nComputing, V.196, Springer: Heidelberg, New York, Dordrecht, London, pp. 339-344."
  },
  {
    "id": 317,
    "text": "Chella, A., Pirrone, R., Sorbello, R., Johannsdottir, K., Eds.; Advances in Intelligent Systems and\nComputing, V.196, Springer: Heidelberg, New York, Dordrecht, London, pp. 339-344.\n10. Mill, J. (1983). System of Logic, Ratiocinative and Inductive. L.\n11. Smirnof, E. (1938). Constructions of forms from the taxonomic view. Zool. Jour., v.17(3), 387-418.\n(in Russian)\n12. Rosch, E. (1973). Natural categories. Cognitive Psychology, 4, 328-350.\n13. Rosch, E.; Mervis, C.; Gray, W.; Johnson, D.; Boyes-Braem, P. (1976). Basic objects in natural\ncategories. Cognitive Psychology, 8, 382–439.\n14. Rosch, E. (1978). Principles of Categorization. In Cognition and Categorization, Rosch, E., Lloyd, B.,\nEds.; Lawrence Erlbaum Associates, Publishers: Hillsdale, pp. 27–48.\n15. Rehder, B. (2003). Categorization as causal reasoning. Cognitive Science, 27, 709–748.\n16. Rutkovskii, L. (1884). Elementary logic textbook; Spt. (in Russian)"
  },
  {
    "id": 318,
    "text": "15. Rehder, B. (2003). Categorization as causal reasoning. Cognitive Science, 27, 709–748.\n16. Rutkovskii, L. (1884). Elementary logic textbook; Spt. (in Russian)\n17. Wilkins, J.; Ebach M. (2013). The Nature of Classification. Relationships and Kinds in the Natural\nSciences. Palgrave Macmillan, pp. 208.\n18. Kogara, V. (1982). Classification Functions. In Classification Theory and Data Analysis, part 1;\nNovosibirsk. (in Russian)\n19. Nedelko, V. (2015). On the question of the effectiveness of boosting in the classification problem.\nSiberian Journal of Pure and Applied Mathematics, 15:2, 72-89. (in Russian)\n20. Ross, B.; Taylor, E.; Middleton, E.; Nokes, T. (2008). Concept and Category Learning in Humans.\nIn Learning and Memory: A Comprehensive Reference; Ed. Byrne, J.; Elsevier: Oxford, V.2, pp.\n535-556.\n21. Ahn, W.; Kim, N.; Lassaline, M.; Dennis, M. (2000). Causal status as a determinant of feature\ncentrality. Cognitive Psychology, 41, 361–416."
  },
  {
    "id": 319,
    "text": "535-556.\n21. Ahn, W.; Kim, N.; Lassaline, M.; Dennis, M. (2000). Causal status as a determinant of feature\ncentrality. Cognitive Psychology, 41, 361–416.\n22. Sloman, S.; Love, B.; Ahn, W. (1998). Feature centrality and conceptual coherence. Cognitive\nScience, 22, 189–228.\n23. Rehder, B.; Hastie, R. (2001). Causal knowledge and categories: The effects of causal beliefs on\ncategorization, induction, and similarity. Journal of Experimental Psychology: General, 130, 323–360.\n24. Cheng, P. (1997). From covariation to causation: A causal power theory. Psychological Review,\n104, 367-405.\n25. Gopnik, A.; Glymour, C.; Sobel, D.; Schulz, E.; Kushnir, T. (2004). A theory of causal learning in\nchildren: Causal maps and Bayes nets. Psychological Review, 111, 3-23."
  },
  {
    "id": 320,
    "text": "26. Griffiths, T.; Tenenbaum, J. (2009). Theory-based causal induction. Psychological Review, 116, 56.\n27. Rehder, Bob; Martin, J. (2011). Towards A Generative Model of Causal Cycles. In 33rd Annual\nMeeting of the Cognitive Science Society 2011, Boston, Massachusetts, V.1; pp. 2944-2949.\n28. Vityaev, E.; Demin, A.; Ponomaryov, D. (2012). Probabilistic Generalization of Formal Concepts.\nProgramming and Computer Software, V. 38:5, 219–230.\n29. Vityaev, E.; Martinovich, V. (2015). Probabilistic Formal Concepts with Negation. In: PCI 2014,\nVoronkov, A., Virbitskaite, I., Eds.; LNCS 8974, pp. 385-399.\n30. Ganter, B. (2003). Formal Concept Analysis: Methods, and Applications in Computer Science. TU:\nDresden, Germany.\n31. Ganter, B., Wille, R. (1999). Formal concept analysis - Mathematical Foundations. LNCS (LNAI).\nSpringer: Heidelberg.\n32. Halpern, J. (1990). An analysis of first-order logic of probability. Artificial Intelligence, 46, 311-350."
  },
  {
    "id": 321,
    "text": "Springer: Heidelberg.\n32. Halpern, J. (1990). An analysis of first-order logic of probability. Artificial Intelligence, 46, 311-350.\n33. Vityaev E.; Morozova, N.; Sutyagin, A.; Lapardin, K. (2005). Natural classification as a law of\nnature. In Structural regularities analysis (Comp. syst. issue 174); Novosibirsk, pp. 80-92. (in\nRussian)\n34. Vityaev, E.; Martynovich V. (2015). “Natural” classification and systematic formalization as a\nfix-point of predictions. Siberian Electronic Mathematical Reports, 12, 1006-1031.\n35. Vityaev, E. (2015). Purposefulness as a Principle of Brain Activity. In Anticipation: Learning from\nthe Past. Ed. Nadin, M., Cognitive Systems Monographs, V.25, Chapter No.:13, Springer, 231-254.\n36. Anokhin, P.K. (1974). Biology and neurophysiology of the conditioned reflex and its role in adaptive\nbehaviour, Oxford etc.: Pergamon press, pp. 574.\n37. Vityaev, E.; Neupokoev, N. (2014). Perception formal model based on fix-point of predictions. In"
  },
  {
    "id": 322,
    "text": "behaviour, Oxford etc.: Pergamon press, pp. 574.\n37. Vityaev, E.; Neupokoev, N. (2014). Perception formal model based on fix-point of predictions. In\nApproaches to mind modeling. Ed. Red’ko, V., URSS Editorials: Moscow, p. 155-172.\n38. Vityaev, E.; Perlovsky, L.; Kovalerchuk, B.; Speransky S. (2013). Probabilistic dynamic logic of\ncognition. Biologically Inspired Cognitive Architectures. Special issue: Papers from the Fourth\nAnnual Meeting of the BICA Society (BICA 2013), v.6, 159-168.\nAcknowledgments: The authors thank the editors of the special issue “Integrated Information Theory” for\npublishing this paper in open access.\nConflicts of Interest: The author declares no conflict of interest.\nAppendix A. Mathematical proofs.\nProof of the theorem 2. Rule C  T h ( M ) is either a law and belongs to Law, or a sub-rule\nexists for it, which is true on M. This sub-rule shall be taken, then again it is either a law, or there is"
  },
  {
    "id": 323,
    "text": "exists for it, which is true on M. This sub-rule shall be taken, then again it is either a law, or there is\na sub-rule for it, true on M, and so on. As a result, the law is obtained, which is a sub-rule of C.\nThen, by virtue of theorem 1, it can be deduced from this law ■\nProof of the theorem 3. The second exception follows from the definition. The first exception\nshall be considered. If rule C  L is of the form C  (  A\n0\n) , it belongs to LP by definition. It shall\nbe supposed that rule C  ( A\n1\n& ... & A\nk\n A\n0\n) is a law within M. It shall be proved that\n(A &...&A )0. If rule C is a law within M, the sub-rule\n1 k\n( A\n2\n& ... & A\nk\n  A\n1\n) is not always\ntrue within M, and, thus, in some cases conjunction A\n2\n& ... & A\nk\n& A\n1\nis true, whence it follows that\n( A\n2\n& ... & A\nk\n& A\n1\n) 0   . Then conditional probabilities of all sub-rules are defined, since\n(A &...&A )(A &...&A )0 follows from {A ,...,A }{A,...,A}. It shall be proved that\ni1 ih 1 k i1 ih 1 k\n(C)1."
  },
  {
    "id": 324,
    "text": "(C)(A /A &...&A )(A &A &...&A )/(A &...&A )\n0 1 k 0 1 k 1 k\n(A &A &...&A )/(A &A &...&A )(A &A &...&A ).\n0 1 k 0 1 k 0 1 k\nSince rule С is true on M, there are no cases on M, when conjunction (A &A &...&A ) is\n0 1 k\ntrue and, hence, ( A\n0\n& A\n1\n& ... & A\nk\n) 0    and ( C ) 1   .\nIt shall be proved that conditional probability of each sub-rule of rule C  ( A\n1\n& ... & A\nk\n A\n0\n)\nis strictly less than (C)1. Each sub-rule ( A\ni1\n& ... & A\nih\n L ) of rule C is false on M, where L is\neither liter  A , or A, for the 1st and 2nd type sub-rules. It means that ( A\ni1\n& ... & A\nih\n& L ) 0    . It\nfollows from the last inequality that:\nSince ( C ) 1   , rule C cannot be a sub-rule of any other probabilistic law, since in this case its\nconditional probability would be strictly less than conditional probability of this rule, what is\nimpossible ■\nProof of the lemma 2. If a probabilistic law has a form C  (  A\n0\n) , it shall be verified, if it is"
  },
  {
    "id": 325,
    "text": "impossible ■\nProof of the lemma 2. If a probabilistic law has a form C  (  A\n0\n) , it shall be verified, if it is\nthe strongest probabilistic law. If it is true, then a semantic probabilistic inference is found, if no,\nthere is a probabilistic law, for which this probabilistic law is a sub-rule. It shall be taken as the next\nrule of semantic probabilistic inference, and it shall be again verified, if it is the strongest law, and\nso on. A sub-rule, which is a probabilistic law, shall be found for probabilistic law\nC(A &...&A A), k1. It always exists, since rule\n1 k 0\nC  (  A\n0\n) is a probabilistic law. Since it\nis a sub-rule, its conditional probability will be less than a conditional probability of the rule as\nsuch. It shall be added as the previous rule of a semantic probabilistic inference, and the procedure\nshall be continued.\nProof of the lemma 3. Designations a ( G & F & H )   , b(F&H) , c ( G & F & H )    ,"
  },
  {
    "id": 326,
    "text": "shall be continued.\nProof of the lemma 3. Designations a ( G & F & H )   , b(F&H) , c ( G & F & H )    ,\nd  (F&H) shall be introduced. Then an original inequality (G/ F&H)(G/ F) shall be\nre-written as a / b  ( a  c ) /( b  d ) , from which it follows that\n(a  c)/(b  d)c/d(G/F)(G/F&H) ■\nProof of the lemma 4. Rule C  ( A\n1\n& ... & A\nk\n A\n0\n) is either a probabilistic law, or there is a\nsub-rule R   ( P\n1\n& … & P\nk '\n A\n0\n) , k'0, {P…P }{A…A}, k'k such, that an inequality\n1 k' 1 k\n(R)(A) is satisfied. Similarly for rule R  , it is either a probabilistic law, or there is a sub-rule\nwith identical properties for it ■\nProof of theorem 4. It should be proved that for any sentence H(), if F(a)&H(a),aA is\ntrue on M, inequality ( G / F & H ) ( G / F ) r     is valid. From the condition of truth F ( a ) & H ( a )\non M, it follows that (F&H)0 and, hence, a conditional probability is defined."
  },
  {
    "id": 327,
    "text": "true on M, inequality ( G / F & H ) ( G / F ) r     is valid. From the condition of truth F ( a ) & H ( a )\non M, it follows that (F&H)0 and, hence, a conditional probability is defined.\nA case shall be considered, when H is a liter (B or B). The opposite shall be assumed that\n(G/F&H)r. Then, as per lemma 3, one of inequalities (F&BG)r or ( F & B G ) r    \nshall be satisfied for one of the rules ( F & B  G ) or ( F &  B  G )\n(L/A &...&A )(A &...&A &L)/(A &...&A )\ni1 ih i1 ih i1 ih\n(A &...&A &L)/((A &...&A &L)(A &...&A &L))1.\ni1 ih i1 ih i1 ih\n. Then, as per lemma 4, a\nprobabilistic law C' exists, which is a sub-rule and has not lower conditional probability, then\nη(C')r. Hence, by lemma 2, probabilistic law C' belongs to some tree of semantic probabilistic\ninference and has a higher value of conditional probability than maximum specific rule MS(G),\npredicting G, which contradicts to maximum specificity MS(G)."
  },
  {
    "id": 328,
    "text": "A case shall be considered, when sentence H is a conjunction of two atoms B\n1\n& B\n2\n, for which a\ntheorem has already been proved. Also, the opposite shall be assumed that one of inequalities\n(G/F&B &B )>r ,\n1 2\n( G /F & B\n1\n& B\n2\n) > r   , ( G /F & B\n1\n& B\n2\n) > r   , ( G /F & B\n1\n& B\n2\n) > r    is valid.\nThen, by lemma 4 and lemma 2, a probabilistic law С' exists that belongs to the tree of a semantic\nprobabilistic inference and is a sub-rule of one of these rules, and is such, as η ( C ')  r is valid.\nHowever, it is impossible, since rule С  ( F & H  G ) is maximum specific. Hence, for all these\ninequalities only equality = or inequality < may take place. The last case is impossible due to the\nfollowing equality\nA case, when sentence H is a conjunction of several atoms or their negations, is called an\ninduction.\nIn the general case, sentence H   (  ) may be presented as a disjunction of conjunctions of"
  },
  {
    "id": 329,
    "text": "A case, when sentence H is a conjunction of several atoms or their negations, is called an\ninduction.\nIn the general case, sentence H   (  ) may be presented as a disjunction of conjunctions of\natoms or their negations. To complete the proof, suffice it to consider a case, when sentence H is a\ndisjunction of two non-intersecting sentences D  E , (D&E)  0, for which a theorem is already\nproved, i.e.\n( G / F & D ) ( G / F & E ) ( G / F ) r       .\nThen ( G / F & ( D E )\n( G &\n( F\nF\n&\n&\n( D\n( D\nE\nE\n)\n) ( G &\n(\nF\nF\n&\n&\nD\nD\n)\n)\n(\n(\nG\nF\n&\n&\nF\nE\n&\n)\nE )\nr\n  \n\n  \n \n\n\n\n\n\n .\nA disjunction case of multiple non-intersecting sentences is proved by induction ■\nProof of proposition 2. If L is compatible, atom GL and its negation  G  L cannot exist\nsimultaneously, since then ( & L ) ( G & G ) 0      , where & L is a conjunction of liters from L ■\nProof of the lemma 5. A conditional probability shall be written as follows\n( G / A & B ) ( G / A & ( B\n1\n... B\nm"
  },
  {
    "id": 330,
    "text": "Proof of the lemma 5. A conditional probability shall be written as follows\n( G / A & B ) ( G / A & ( B\n1\n... B\nm\n) )         .\nDisjunction B ...B shall be presented as disjunction of conjunctions\n1 m\ni(1,...1,0)\nV (Bi1 &...&Bim), where\n1 m\ni(0,...,0)\ni  ( i1 , ..., im ) , i1 , ..., im  { 0 ,1 } zero means that there is a negation with the\ncorresponding atom, and unity means that there is no negation. Disjunction does not involve set\n(1,…,1), which corresponds to conjunction B\n1\n& ... & B\nm\n.\nThen a conditional probability ( G / A & ( B\n1\n... B\nm\n) )      shall be rewritten as\nG /\ni\ni\n(1,...1,0 )(\n(0 ,...,0 )\nA & B i11 & ... & B imm ) \n \n\nV\n\n.\nIt shall be proved that if ( G / A & B ) ( G / A )     , one of inequalities shall be satisfied\n(G/A&Bi1 &...&Bim)(G/A),\n1 m\n( i1 , ..., im )  (1 , ...,1 ) .\nThe opposite shall be assumed: all inequalities are simultaneously true\n(G/A&Bi1 &...&Bim)(G/A),\n1 m\n( i1 , ..., im )  (1 , ...,1 )"
  },
  {
    "id": 331,
    "text": "1 m\n( i1 , ..., im )  (1 , ...,1 ) .\nThe opposite shall be assumed: all inequalities are simultaneously true\n(G/A&Bi1 &...&Bim)(G/A),\n1 m\n( i1 , ..., im )  (1 , ...,1 )\nin those cases, when (A&Bi1 &...&Bim)0. Since (A&B)0, there are cases, when\n1 m\n(A&Bi1 &...&Bim)0.\n1 m\nr\n( G\n(\n&\nF\nF\n)\n) ( G & F\n(\n&\nF\nB\n&\n1\n&\nB\n1\nB\n&\n2\n)\nB\n+\n2\n) +\n( G &\n( F\nF &\n&\nB\nB\n1\n1\n&\n&\nB\nB\n2\n2\n)\n)\n+\n+\n(\n(\nG\nF\n&\n&\nF &\nB\n1\nB\n&\n1\n&\nB\n2\nB\n)\n2\n+\n) +\n( F\n( G\n&\n& F\nB\n&\n1\n&\nB &\n1B\n2\n)\nB\n2\n)\n.\n    \n    \n \n\n \n\n\n\n\n"
  },
  {
    "id": 332,
    "text": "Then\n(G&A&Bi1 &...&Bim)(G/A)(A&Bi1 &...&Bim),\n1 m 1 m\n( i1 , ..., im )  (1 , ...,1 ) ,\nG /\ni\ni\n(1,...1,0 )(\n(0 ,...,0 )\nA & B i11 & ... & B imm )\ni\ni\n(1,...1,0 )(\nG\n(0 ,...,0 )\ni (1,...1,0 )(\ni (0 ,...,0 )\n&\nA\nA\n&\n&\nB\nB\ni11\ni11\n&\n&\n...\n...\n&\n&\nB\nB\nimm\nimm\n)\n) \n\n\n \n\nV\n\n\n \n\n\nV\n\n\nV\n\n\n\ni(1,...1,0) i(1,...1,0)\n (G&A&Bi1 &...&Bim) (G/A)  (A&Bi1 &...&Bim)\n1 m 1 m\ni(0,...,0)  i(0,...,0) (G/A),\ni(1,...1,0) i(1,...1,0)\n (A&Bi1 &...&Bim)  (A&Bi1 &...&Bim)\n1 m 1 m\ni(0,...,0) i(0,...,0)\nthat contradicts to inequality ( G / A & B ) ( G / A )     . Therefore, this assumption is not valid, and\nthere is a rule as follows\nA & B i11 & ... & B imm  G , ( i1 , ..., im )  (1 , ...,1 ) ,\nthat has a strictly higher assessment of conditional probability than A ■\nProof of theorem 5: It should be proved that, every time a rule from P  M S R is applied, a"
  },
  {
    "id": 333,
    "text": "that has a strictly higher assessment of conditional probability than A ■\nProof of theorem 5: It should be proved that, every time a rule from P  M S R is applied, a\ncompatible set of liters is once again obtained. The opposite shall be assumed that, when applying\nsome rule A = ( A\n1\n& ... & A\nk\n G ) , { A\n1\n, ..., A\nk\n}  L , k  1 to a set of liters L  { L\n1\n, ..., L\nk\n} , a liter G shall\nbe obtained, for which ( L\n1\n& ... & L\nn\n& G ) 0   .\nSince for rules inequalities ( G /A\n1\n& ... & A\nk\n) > ( G )   , (A &...&A )>0,(G)>0 are satisfied,\n1 k\nthen (G&A &...&A )>(G)(A &...&A )>0.\n1 k 1 k\nNegations of liters { B\n1\n, ..., B }t = { L\n1\n& ... & L\nn\n} \\{ A\n1\n, ..., A\nk\n} shall be added to rule А, and rule\n(A &...&A &(B &...&B)G) shall be obtained. The following designations shall be made\n1 k 1 t\n&A =A &...&A , &B B &...&B ,\ni 1 k j 1 t\n& L = L\n1\n& ... & L\nn\n.\nAs it was assumed, ( L\n1\n& ... & L\nn\n& G ) 0   and ( & A\ni\n& ( & B\nj\n) ) = ( & L ) > 0   . It shall be proved"
  },
  {
    "id": 334,
    "text": "1 k 1 t\n&A =A &...&A , &B B &...&B ,\ni 1 k j 1 t\n& L = L\n1\n& ... & L\nn\n.\nAs it was assumed, ( L\n1\n& ... & L\nn\n& G ) 0   and ( & A\ni\n& ( & B\nj\n) ) = ( & L ) > 0   . It shall be proved\nthat in this case ( & A\ni\n& ( & B\nj\n) ) 0    . The opposite shall be assumed that ( & A\ni\n& ( & B\nj\n) ) 0    ,\nthen\n( G & ( & A\ni\n) & ( & B\nj\n) ) ( & A\ni\n& ( & B\nj\n) ) = 0 .     \nWhence it follows that\n0(&L&G)(G&(&A )&(&B ))(G&(&A )(G&(&A )&(&B )))=\ni j i i j\n(G&A &...&A )>(G)(A &...&A )>0.\n1 k 1 k\nA comtradiction is obtained. Then\n( G\n( G\n( &\n/\n&\nA\n& A\n( &\n)\ni\ni\nA\n&\n) )\ni(\n&\n(\nA\n&\ni\nB\n(\n&\n&\n) )\nj\nL\n( &\n=\n&\nB\nG\nj\n(\n)\n) )\nG &\n( &\n( &\nA\n( &\nA\ni\n&\ni\nA\n)\ni\n&\n)\n( &\n( G\n( & B ) )\nj\nB ) )\nj\n& ( & A )\ni\n( & A &\ni\n)\n( &\n(\nB\nG\nj\n&\n) )\n( &\n( &\nA )\ni\nA\ni\n( G\n)\n)\n&\n( &\n( &\nA\n( G\n( &\nA\n)\ni\n&\nA\n)\ni\n)\n( &\n&\ni\nA )\ni\n( &\n( G\n&\nB\n/\n(\n)\nj\nA\n&\n)\n1\nB\n&\n) )\nj\n... & A\nk\n).\n  \n\n  \n   \n\n     \n\n\n\n\n\n\n\n\n\n\n\n"
  },
  {
    "id": 335,
    "text": "B\nG\nj\n&\n) )\n( &\n( &\nA )\ni\nA\ni\n( G\n)\n)\n&\n( &\n( &\nA\n( G\n( &\nA\n)\ni\n&\nA\n)\ni\n)\n( &\n&\ni\nA )\ni\n( &\n( G\n&\nB\n/\n(\n)\nj\nA\n&\n)\n1\nB\n&\n) )\nj\n... & A\nk\n).\n  \n\n  \n   \n\n     \n\n\n\n\n\n\n\n\n\n\n\n\nThen, by virtue of lemmas 2, 4, 5, it shall be found that there exists a probabilistic law with a higher\nconditional probability than rule A, which contradicts to a maximum specificity of rule A ■"
  },
  {
    "id": 336,
    "text": "Building Open-Ended Embodied Agent via Language-Policy\nBidirectional Adaptation\nShaopengZhai*1 JieWang*1 TianyiZhang*1 FuxianHuang*1\nQiZhang*1 MingZhou*1 JingHou*21 YuQiao1 YuLiu1\nAbstract (1)pre-trainingLLMstotranslatehuman-instructionsinto\nsub-tasks, for open-ended planning (Wang et al., 2023a;\nBuildingembodiedagentsonintegratingLarge\nOuyangetal.,2022),and(2)curriculumRLforopen-ended\nLanguage Models (LLMs) and Reinforcement\ncontrol(Teametal.,2021;Balduzzietal.,2019).\nLearning(RL)haverevolutionizedhuman-AIin-\nteraction: researcherscannowleveragelanguage Forpre-trainedLLMs,particularlythosewithclosedsource\ninstructions to plan decision-making for open- architectures, focus on resolving planning with general\nended tasks. However, existing research faces knowledge acquired during the pre-training stage (Wang\nchallenges in meeting the requirement of open- et al., 2023a). However, they share shortcomings like re-"
  },
  {
    "id": 337,
    "text": "challenges in meeting the requirement of open- et al., 2023a). However, they share shortcomings like re-\nendedness. TheytypicallyeithertrainLLM/RL lyingontask-orientedandhand-craftedprompting,strug-\nmodelstoadapttoafixedcounterpart,limitingex- glingtocomprehendinteractionsinspecialcontextssuch\nplorationofnovelskillsandhinderingtheefficacy as games and be incompetent for high real-time require-\nofhuman-AIinteraction. Tothisend,wepresent ments due to inefficient model computation. In contrast,\nOpenPAL, a co-training framework comprising curriculumRLconductsopen-endedlearninginanend-to-\ntwostages: (1)fine-tuningapre-trainedLLMto endmanner,developingindiversemethodologiessuchas\ntranslatehumaninstructionsintogoalsforplan- population-basedRL(Teametal.,2021),goal-conditioned\nning,andgoal-conditionedtrainingapolicyfor RL(GCRL)(Liuetal.,2022)andetc. DespiteRLexcels\ndecision-making;(2)co-trainingtoaligntheLLM inlearningnovelskillscomparedtorule-basedcontrol,it"
  },
  {
    "id": 338,
    "text": "ning,andgoal-conditionedtrainingapolicyfor RL(GCRL)(Liuetal.,2022)andetc. DespiteRLexcels\ndecision-making;(2)co-trainingtoaligntheLLM inlearningnovelskillscomparedtorule-basedcontrol,it\nandpolicy,achievinginstructionopen-endedness. lacksthedirectcapabilityforinteractionwithhumans. To\nWeconductedexperimentsusingContra,anopen- leverageadvantagesfrombothsides,i.e.,beingcapableof\nended FPS game, demonstrating that an agent interactinghumanandAIinsolvingreal-timetaskstowards\ntrainedwithOpenPALnotonlycomprehendsar- open-endedness,anidealimplementationistointegrating\nbitraryinstructionsbutalsoexhibitsefficientexe- LLMforplanningandRLpolicyfordecisionmaking. How-\ncution. TheseresultssuggestthatOpenPALholds ever, existing studies in this domain have focused on im-\nthepotentialtoconstructopen-endedembodied provingtrainingefficiencyorreducinginteractioncostsby\nagentsinpracticalscenarios. either independently training the RL policy or LLM (Hu\netal.,2023;Duetal.,2023)toadapttheother,resultingin"
  },
  {
    "id": 339,
    "text": "agentsinpracticalscenarios. either independently training the RL policy or LLM (Hu\netal.,2023;Duetal.,2023)toadapttheother,resultingin\noverfittingandfailingtoexplorenovelskillsinspecificenvi-\n1.Introduction\nronmentsthatnecessitatespecializedknowledgeandfalling\nshortofachievingtrueopen-endedness. Despiteprevious\nWiththeincreasingprevalenceofLLMssuchasChatGPT,\nworkresolvethisissuewithLLM-basedre-planning(Wang\nresearchershaveprogressivelyshiftedtheirfocustowards\netal.,2023a;b),itisinefficientforhigh-dimensionaltasks\nLLM-centered principles, building embodied agents that\nandthere-planningisstillintheexistingrangeofstrength.\ninteractwithhumanstotackleopen-endedtasks(Khandel-\nwaletal.,2022;Huangetal.,2023). Toachievethistarget, Toaddresstheabovechallenge,weproposeaco-training\nweneedtoresolvethechallengeofdevelopingAIagents framework,OpenPAL,struncturedasatwo-stagelearning\nwiththeabilitytocontinuouslylearnnewskills,whichis processtoimplementebi-directionaladaptation. Thisde-"
  },
  {
    "id": 340,
    "text": "withtheabilitytocontinuouslylearnnewskills,whichis processtoimplementebi-directionaladaptation. Thisde-\nrelated to adomain commonly referred toas open-ended signenablestheRLpolicycontinuouslyexplorenovelskills\nlearningthatisbroadlycategorizedintotwomainfactions: whiling align the LLM and the policy towards achieving\ninstruction open-endedness. In the first stage, we sepa-\n*Equal contribution 1Shanghai AI Laboratory\nratelytrainaChatGLM-6B(Duetal.,2022)asaplanner\n2Tongji University. Correspondence to: Shaopeng Zhai\n<zhaishaopeng@pjlab.org.cn>,YuLiu<liuyu@pjlab.org.cn>. (or goal generator) G llm and policy π g , where G llm gen-\neratesgoalswithgiveninstructionsandenvironmentcon-\nCopyright2024bytheauthor(s).Inprogress.\n1\n4202\nbeF\n6\n]IA.sc[\n3v60000.1042:viXra"
  },
  {
    "id": 341,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\nHuman Instructions Open-ended Environments\n(cid:31)Prepare to Ambush.” Observations Agent Actions\n(cid:31)Be Careful !”\n“Find Them Out !” (cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:26)(cid:25)(cid:24)(cid:23)(cid:24)(cid:25)(cid:22)(cid:30)(cid:24)(cid:21) (cid:20)(cid:19)(cid:22)(cid:30)(cid:29)(cid:18)(cid:19)(cid:17)(cid:24)(cid:19)(cid:16) (cid:15)(cid:14)(cid:24)(cid:19)(cid:16) (cid:13)(cid:16)(cid:12)(cid:24)(cid:29)(cid:11)(cid:10)(cid:9)(cid:8)(cid:23)(cid:24)(cid:29)(cid:26)\nCo-training\nGoals Planning\nLLM Policy\nRewards Feedback\nMulti-step Fine-tuning Goal-conditioned RL\nGoal\nRewards\nFigure1: Overviewofco-traininginOpenPAL.ThePolicyandLLMispre-trainedwithmulti-stepfine-tuningandgoal-\nconditionedRL,respectively. Then,theco-trainingalignsthemtowardsachievinginstructionopen-endedness.\ntext, and π learns to execute goals. To achieve that, we defines a task for the agent to execute. As for decision\ng"
  },
  {
    "id": 342,
    "text": "text, and π learns to execute goals. To achieve that, we defines a task for the agent to execute. As for decision\ng\nproposemulti-stepfine-tuningapre-trainedLLMwithGPT- making,πdenotesapolicyasπ :S ×G →∆(A),which\n4-generated instructions and goals, and open-ended goal is a distribution over the action space. To solve a M, or\ngenerationtolearnagoal-conditionedpolicy. Inthesecond achieveopen-endednessinotherwords,anagentwithpolicy\nstage,weimplementco-trainingtoalignG (planning) πneedstomaximizeitsaccumulativerewardoverthegoal\nllm\n(cid:104) (cid:105)\na\na\nn\nim\nd\ns\nπ\nt\ng\no\n(\na\nd\nc\ne\nh\nc\ni\ni\ne\nsi\nv\no\ne\nn\ni\n-\nn\nm\ns\na\ntr\nk\nu\ni\nc\nn\nt\ng\nio\n),\nn\na\no\ns\np\nil\ne\nl\nn\nu\n-\ns\ne\ntr\nn\na\nd\nt\ne\ne\nd\nd\nn\ni\ne\nn\nss\nF\n,\nig\na\nu\nli\nr\ng\ne\nn\n1\nin\n.\ng\nT\nt\nh\nh\ni\ne\ns spaceasE at∼π(·|st,g),(st,at)∼τ,g∼Pg (cid:80)T t=0 γtr(s t ,a t ,g) ,\nwhere γ ∈ [0,1) discounts a reward at each time step to\ninstructionspacewiththeopen-endedgoalspacethatthe\nensuretheconvergence. Normally,r(s ,a ,g)isbinaryas"
  },
  {
    "id": 343,
    "text": "where γ ∈ [0,1) discounts a reward at each time step to\ninstructionspacewiththeopen-endedgoalspacethatthe\nensuretheconvergence. Normally,r(s ,a ,g)isbinaryas\nagentachieved. Specifically,weimplementtheco-training t t\nas an interleaved execution of (1) Reinforcement Learn- (cid:40)\ningwithAgentFeedback(RLAF)forG llm and(2)GCRL r(s t ,a t ,g)= 1 0 o P t R h O er J w (s i t s + e 1 )=g . (1)\nfor π with goals generated by G , where RLAF cen-\ng llm\ntersaroundrewardingG llm withagentfeedbackandgoal To approximate (cid:80)T γtr(s ,a ,g), GRL suggests us-\nexecution. Thistwo-stagedapproachoptimizestheLLM t=0 t t\ning the Universal Value Function Approximator (UVFA)\nforcomprehendingenvironmentcontextundertheconsid-\nV(s,g). As for the solving of open-endedness, there are\nerationofdecision-making,whileconcurrentlyenhancing\ncriticalchallengesthatneedtoberesolved,includingbut\ndecision-makingforgoalsalignedwithhumaninstructions.\nnotlimitedto: (1)P isagnostictoanagent,and(2)G is"
  },
  {
    "id": 344,
    "text": "criticalchallengesthatneedtoberesolved,includingbut\ndecision-makingforgoalsalignedwithhumaninstructions.\nnotlimitedto: (1)P isagnostictoanagent,and(2)G is\nForevaluation,weemployContra,anopen-endedFPSgame. g\nuncountableandcontinuous. Toresolvetheseissues,exist-\nTheresultsdemonstratethatOpenPALachievesahighgoal\ningresearchiscenteredoncurriculumRL(Weng,2020),\ncompletionratioforopen-endedhuman-AIinteraction.\ni.e.,automaticallydiscoveringnovelgoalsfrompastlearn-\ning. Hindsightgoalrelabelling(Andrychowiczetal.,2017;\n2.Background\nFangetal.,2019;Zhengetal.,2022;Schauletal.,2015)\nimplicitlyimplementscurriculumlearningbyintroducing\nGoal-conditionedReinforcementLearning. Formally,\naprioritizedreplaymechanismandperformshighdataef-\nGCRLcouldbeformulatedasagoal-augmentedMarkov\nficiency. Despite numerous curriculum approaches, the\nDecision Process M (Liu et al., 2022). Denoting M a\nsampleinefficiencyduetothesettingofbinaryrewardsig-\ntuple ⟨S,A,G,P,R,γ⟩, where S, A, G the state, action"
  },
  {
    "id": 345,
    "text": "Decision Process M (Liu et al., 2022). Denoting M a\nsampleinefficiencyduetothesettingofbinaryrewardsig-\ntuple ⟨S,A,G,P,R,γ⟩, where S, A, G the state, action\nnals(Equation(4))hinderspolicylearning. Asasolution,\nandgoalspaces,respectively. Ingeneral,G isaprojection\nexistingresearchleveragesrewardshaping(Ngetal.,1999;\nof S, i.e., G = PROJ(S). P defines the state transition\nEcoffet et al., 2021; Ding et al., 2023; Trott et al., 2019)\nprobabilities, i.e., P : S × A → ∆(S), where ∆(·) a\nwhichisastraightforwardandefficientidea.\ndistribution. R : S × A × G → R defines the reward\nfunctionr(s,a,g). Atthebeginningofantrajectoryτ, a\nLLMsandHuman-AIInteraction. LLMs(Brownetal.,\ngoalg issampledfromadistributionP ,whichgenerally\ng\n2020; OpenAI, 2023) a class of neural networks that ex-\n2"
  },
  {
    "id": 346,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\necute in auto-regressive for text generation. Given a se- mizationtasksaredelegatedtotheLearner(GPUnode)for\nquenceoftexttokenswithlengthtasx =(x ,...,x ), policyupdates. Thisdistributedapproachsignificantlyen-\n1:t 1 t\nthe generation of a next token x could be formulated hancesrolloutthroughput,therebyimprovingoveralltrain-\nt+1\nas sampling from a probabilistic model P(·|x ). As for ingefficiency. Anotherefficiencychallengestemsfromthe\n1:t\nthe training of LLMs, the target is equivalently to find a iterativedevelopmentofContra. Duringtheperiodofour\nparametersetθ whichsatisfiestheoptimalgeneration, research, Contra’s environmental attributes continuously\nLLM\ni.e., θ = argmax E P(x |x ;θ). Beyond change as the programming development. Thus, policy\nLLM θ x1:t,xt+1 t+1 1:t\nthe research of LLMs, it is attractive to leverage LLMs retrainingwouldbenecessaryifthereisnoexplicitinter-"
  },
  {
    "id": 347,
    "text": "LLM θ x1:t,xt+1 t+1 1:t\nthe research of LLMs, it is attractive to leverage LLMs retrainingwouldbenecessaryifthereisnoexplicitinter-\nasaninterfacetointeracthumanwithagents(Jiangetal., vention. Toreducesuchanextracomputationburden,we\n2023;Hermannetal.,2017). Wecanroughlyreformulate employsurgery(Berneretal.,2019)toretainlearnedskills\nthegenerationasx ∼ P(·|ι,x )forhuman-AIinter- atthelowesttrainingcost,enablingadaptationtoachanging\nt+1 1:t\naction,whereιthelanguageinstructionasaprompt,x observation/goalspacewhileensuringcompatibilitywith\n1:t\nthecontext. Forthecaseshavelowreal-timerequirements, networkinputs. DetailedinformationonthedistributedRL\nx isacontrolsignalfordecisionmaking(Brohanetal., frameworkcanbefoundinAppendixF,andversionchanges\nt+1\n2022;Zitkovichetal.,2023;Nakanoetal.,2021). While arelistedinTable8. Inthefollowingcontent,wewillin-\nfor the cases have high real-time requirements, x is a troduceOpenPALintwostages,includingtheindependent\nt+1"
  },
  {
    "id": 348,
    "text": "for the cases have high real-time requirements, x is a troduceOpenPALintwostages,includingtheindependent\nt+1\ngoal and will be fed to a controller to guide the decision trainingatstageI(Section4.1∼4.3)andtheco-trainingat\nmaking(Wangetal.,2023b;a). Ourstudyfallswithinthe stageII(Section4.4).\nlattersituation,buildingopen-endededembodiedagentsin\nthecaseswithhighreal-timerequirements. 4.1.ExploringBasicSkillsviaNon-goalRL\nIn the realm of GCRL, the prevalent approach involves\n3.TheContra: ABattleRoyaleFPSGame\ncurriculumlearningagoal-conditionedpolicyfromscratch,\nlearning goal execution while exploring goals. However,\nContraseamlesslymergesthelast-man-standinggameplay\nitmaybeinefficientforanagenttoexplorethewholegoal\ndynamics with the survival, exploration, and scavenging\nspacewhenthereisalackofpriorknowledgeofthegoal\nelementsinherentinfirst-personshootinggames(Gautam\nspace. Thus,weoptforleveragingnon-goalRLforbasic\netal.,2021). Itunfoldswithmultiplehostileteams,neces-"
  },
  {
    "id": 349,
    "text": "elementsinherentinfirst-personshootinggames(Gautam\nspace. Thus,weoptforleveragingnon-goalRLforbasic\netal.,2021). Itunfoldswithmultiplehostileteams,neces-\nskill learning before goal-conditioned learning. For the\nsitatingplayerstocollaboratewithteammates, withstand\nimplementation,weemployProximalPolicyOptimization\nadversaries,andstrivetooutlastothersintheever-changing\n(PPO) (Schulman et al., 2017) with fine-grained reward\narena. Theagent’sobjectivesencompassindividualsurvival\nshapingas\nandtheeliminationofencounteredenemies. Anagentin\nContramandatesasequentialacquisitionofskills,starting\nr(s,a)=λ rb(s,a)+λ roa(s,a), (2)\nfrom fundamental abilities like running and item collec- 1 2\ntion. Asthelearningproceeds,anagentmustmastermore\nwhererb forskilllearningwhichfocusesontargetingthe\nintricate skills such as evading enemy projectiles and co-\nagenttowardswiningandsurvivingaslongaspossible,isa\nordinatingtacticswithteammates,anopen-endedlearning"
  },
  {
    "id": 350,
    "text": "intricate skills such as evading enemy projectiles and co-\nagenttowardswiningandsurvivingaslongaspossible,isa\nordinatingtacticswithteammates,anopen-endedlearning\nlinearcombinationofdiversebehaviorfactors,roaencour-\nprocess. TheprimaryrationalebehindchoosingContraas\nages the agent to avoid obstacles due to the agent is not\nourtestbedliesinitspossessionofproprietaryknowledge\nsensitivetoobstacleswhennavigating,andλ ,λ thefac-\n1 2\nnotencompassedingeneralknowledgerepositories. Con-\ntorstoweightthecontributionofeachitem. Thedetailsof\nsequently, we employ Reinforcement Learning (RL) for\ntherewardconstructionareincludedinTable5,AppendixA.\nknowledgediscovery,andco-trainingtoaligntheLanguage Then,thevaluefunctionforestimating (cid:80)T γl−tr(s ,a )\nModel(LLM)andRLincomprehendingtheenvironment. l=t l l\nisimplementedasamulti-headnetworkandsharestheback-\nboneofpolicy,i.e.,V(s )=λ Vb(s )+λ Voa(s ),where\nt 1 t 2 t"
  },
  {
    "id": 351,
    "text": "Model(LLM)andRLincomprehendingtheenvironment. l=t l l\nisimplementedasamulti-headnetworkandsharestheback-\nboneofpolicy,i.e.,V(s )=λ Vb(s )+λ Voa(s ),where\nt 1 t 2 t\n4.ACo-trainingFramework: OpenPAL Vb(s )andVoa(s )approximate (cid:80)T γl−trb(s ,a )and\nt t l=t l l\n(cid:80)T γl−troa(s ,a ),respectively.\nConsideringthetraininginthecontextofopen-endedlearn- l=t l l\ningisextensivedata-thirsty,wefirstintroducetwocritical\nengineeringdesignstoenhancetrainingefficiency. Specif- 4.2.LearningaGoal-conditionedPolicy\nically,OpenPALincorporatesadistributedRLframework\nWeconstructthegoalspaceusingvariousstateattributes\ninspired by AlphaStar (Vinyals et al., 2019) with modi-\nwhich can be determined and effected by interaction. In\nfications, resulting in the formation of the Actor-League-\ndetail,theyare(1)agentprivatestatesthatcanbedirectly\nLearnerarchitecture. Inthisarchitecture,theLeagueisre-\nchangedbytheagentorotherplayers,suchasfiring,walk-\nsponsiblefordistributingrollouttaskstoaclusterofActors"
  },
  {
    "id": 352,
    "text": "Learnerarchitecture. Inthisarchitecture,theLeagueisre-\nchangedbytheagentorotherplayers,suchasfiring,walk-\nsponsiblefordistributingrollouttaskstoaclusterofActors\ning, etc. (2) enemies states that can be effected through\n(CPUnodes)fordatacollectionandevaluation,whileopti-\ntheagentinteractions,suchasknockdownanenemies;and\n3"
  },
  {
    "id": 353,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\n(3)teammatesstatesthatcanbeeffectedbytheinteraction ing G\nrnd\n= Π6\ni=\n8\n1\nUNIFORM(Gi). This results in a goal\nbetweentheagentanditsteammates. Wesummarizethem generationg ∼G ∪G .\nrnd op\ninTable4. Withtheaboveconsideration,wefurthermodel\neachattributeasasub-goalspaceGi withmultiplecandi- IntrinsicRewardShaping. Asintroducedintheafore-\ndatesthatcanbeexpressedasasetofnormalizedindices mentioned, a critical challenge hinders the goal comple-\n{ j |j = 0,...,|Gi|}, where 0 an invalid attribute value tion is sparse rewarding. To mitigate this, we extend the\n|Gi|\nindicatesthecorrespondingattributenotbeselected. For reward function in Equation (2) with an intrinsic reward\nthegoalspace,thereare68sub-goalspacesthatshapeitas rg(s ,a ,g) that evaluates the degree of goal completion.\nt t\nG =Π68 Gi. Obviously,gwhichcomprisesofmorevalid Equation (4) shows the calculation of rg(s ,a ,g) as the\ni=1 t t"
  },
  {
    "id": 354,
    "text": "t t\nG =Π68 Gi. Obviously,gwhichcomprisesofmorevalid Equation (4) shows the calculation of rg(s ,a ,g) as the\ni=1 t t\nsub-goals,themoredifficulttocompleteforthepolicy. Euclideannormdifferencebetweentwoconsecutivestates\nandagoalas\nOpen-ended Goal Generation. Among existing\nGCRL research, hindsight goal relabelling and genera-\nrg(s\nt\n,a\nt\n,g)=∥g−PROJ(s\nt−1\n)∥\np\n−∥g−PROJ(s\nt\n)∥\np\n, (4)\ntion (Andrychowicz et al., 2017; Ren et al., 2019) are\nwhere∥·∥ indicatesthep-norm. Thisrewardprovidesa\neffectivegoal-conditionedlearningmethodsthatadvantage p\ndenserrewardsignalateachtimesteptotheagentaboutits\nfromafreeofgoalprior,comparedtoexplicitcurriculum.\nproximitytothegoal,offeringmorenuancedinformation\nHowever, there is a limitation of in-distribution goal\nthanabinarysignalindicatingwhetherithasreachedthe\nexploration (Bai et al., 2019), i.e., policy learning and\ngoalornot. Inourcurrentimplementation,wesetp = 1.\ngoal exploration shares the same training dataset, which"
  },
  {
    "id": 355,
    "text": "exploration (Bai et al., 2019), i.e., policy learning and\ngoalornot. Inourcurrentimplementation,wesetp = 1.\ngoal exploration shares the same training dataset, which\nThus,therewardfunctionforGCRLisformulatedas\nis inefficient in exploration as the range of goals are\nlimited by the scale of samples. Comparatively, if we r(s,a,g)=r(s,a)+λ rg(s,a,g), (5)\n3\ncan model the goal distribution, we can not only achieve\ndata efficiency akin to that of hindsight goal generation, where r(s,a) comes from Equation (2). And for the\nbut also progressively attain an open-ended goal space value function corresponds to Equation (5), we extend\nby adjusting the goal distribution. Therefore, we train a themulti-headV(s )withanewvalueheadVg(s ,g)as\nt t\nneural-basedgoalgeneratorG op overadatasetoftrajectory V(s t ,g)=V(s)+λ 3 Vg(s t ,g),whereVg(s t ,g)approxi-\nsegmentsD τ ={τ}exploredbythewell-trainedpolicyπ⋆ mates (cid:80)T i=t γi−trg(s t ,a t ,g).\nfromSection4.1asitistrainedforexploringbasicskills."
  },
  {
    "id": 356,
    "text": "segmentsD τ ={τ}exploredbythewell-trainedpolicyπ⋆ mates (cid:80)T i=t γi−trg(s t ,a t ,g).\nfromSection4.1asitistrainedforexploringbasicskills.\nWe assume that a goal g corresponding to a given initial AvoidingPolicyDegeneration. Letπ⋆ denotethewell-\nstate s can be represented by a 3-tuple x = (s,∆t,∆V), trained policy from the non-goal reinforcement learning\nwhere∆tthetimeslotrequiredtoachievegstartingfrom\nstep. However,wehaveobservedaperformancedegenera-\ns, and ∆V a vector of state values from s to g with the tiononbasicskillexecutionwhencontinuingthetraining\nconsideration of representation. As a result, we train the ofthegoal-conditionedpolicyπ startingfromπ⋆. This\ng,θ\ngoalgeneratorG\nop\ntotakeinputintheformofx,thereby\nisattributedtotwoaspects: (1)catastrophicforgettingon\nallowingvariationsin∆tand∆V toyielddifferentgoals thebasicskillsasthegoal-conditionedlearningcontinues;\nfor a given state s. For an implementation, we firstly (2) a change in the input of the policy network from s to"
  },
  {
    "id": 357,
    "text": "for a given state s. For an implementation, we firstly (2) a change in the input of the policy network from s to\nconstructadatasetD x,g = {(x,g)}fromD τ ,whereeach (s,g),wheregintroducesinterferenceindecision-making,\niteminD x,g satisfies: asthepolicyhasnotencounteredgoalinputsduringnon-\ngoal-conditionedtraining. Toaddressthesissues,wepro-\ns∼ UNIFORM(τ\n:150\n), (3)\nposeamodificationtothegoal-conditionedpolicylearning\nVb(s′|s′ ∈τ ) objectivebyintroducingaKL-divergenceregularizer,and\ng = PROJ(s′), s′ ∼P(s′)=\n(cid:80) V\n−\nb(\n2\ns\n0\n′\n:\n)\n,\nintroduce20%workersfornon-goalpolicylearningtoavoid\ns′∈τ−20:\ncatastrophicforgetting. Thisregularizerquantifiesthedis-\n∆t= thetimeslotfromstos′,\ntancebetweenπ⋆andπ whenπ conditionedong =∅\ng,θ g,θ\n∆V = (cid:2) Vb(s),Vb(s′) (cid:3) . asitisequivalentlytonon-goalpolicy:\nτ :150 the first 150 states of τ, τ −20: the last 20 maxE (s,g) (cid:2) J(π g,θ )−1 g=∅ ·D KL (π⋆ ∥π g,θ ) (cid:3) . (6)\nθ\nstates. Then we train G with a MSE loss as\nop"
  },
  {
    "id": 358,
    "text": "τ :150 the first 150 states of τ, τ −20: the last 20 maxE (s,g) (cid:2) J(π g,θ )−1 g=∅ ·D KL (π⋆ ∥π g,θ ) (cid:3) . (6)\nθ\nstates. Then we train G with a MSE loss as\nop\nmin E [∥G (x)−g∥ ].Whilevarying∆tand∆V J(π )thepolicylossinPPO,and1 indicatesthatthe\nGop Dx,g op 2 g,θ g=∅\nproducesdiversegoals,itremainschallengingtocompre- KL-divergencetermisonlyactivatedwhenanemptygoal\nhensively cover the entire goal space corresponding to inputforπ . Algorithm3,AppendixHsummarizesthe\ng,θ\na given state. As a supplement, we propose integrating learningprocess. Furthermore,weobservedthatoccasion-\ngoalgenerationwithauniformsampler,denotedasG , ally sampling experience from π⋆ to train π can also\nrnd g,θ\nwhichrandomlysamplesgoalsfromthegoalspaceG us- relievethedegeneration.\n4"
  },
  {
    "id": 359,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\n4.3.Fine-tuningaLLM-basedGoalGenerator improving the accuracy; and (3) Ensemble Fine-tuning\n(EFT): multiple checkpoints of G are utilized to gen-\nLetI representthesetofnaturallanguageinstructions,and llm\nerategoalcandidatesforeach(o,ι) ∈ D ,thensub-goals\nOthesetofabstractedenvironmentstatesintext. Ourob- x\nwithhighestcountsarereconstructedasagroundgoalto\njectiveistofine-tuneapre-trainedLLMasagoalgenerator,\nfine-tunethemodeltoenhancethegeneration.\ndenotedasG :O×I →G,whichmeansG generates\nllm llm\nagoalrelevanttoagiveninstructionwiththeconsideration\n4.4.CollaborativeTraining\nofcurrentenvironmentcontext,i.e.,astateabstraction.\nAfter completing the above training steps, we obtained a\nwell-trainedgoalgeneratorG andgoal-conditionedpol-\nDatasetConstruction. Toachievethat,wefirstconstruct llm\nicyπ thatsatisfactorilyadheretotheirrespectivegoaldistri-\nOusingstatescollectedbytheπ .Eachabstractiono∈O g\ng,θ"
  },
  {
    "id": 360,
    "text": "DatasetConstruction. Toachievethat,wefirstconstruct llm\nicyπ thatsatisfactorilyadheretotheirrespectivegoaldistri-\nOusingstatescollectedbytheπ .Eachabstractiono∈O g\ng,θ\nbutions. However,aninconsistencypersistsbetweenG\nencapsulates essential state features of its corresponding llm\nandπ stemmingfromtheirindependenttrainingobjectives,\nstates,andtheextractionrulesareoutlinedinAppendixB. g\nwhereG aimedtogenerategoalsthatsatisfygivenin-\nForthecreationofI,weleveragevariousinstructiongener- llm\nstructions,andπ focusedonexploringgoals. Therefore,\nationtoensureitsdiversityandscalability,aligningwithour g\nweintroduceco-trainingtoaddresstheaforementionedis-\noverarchinggoalofachievingopen-endednessintheinstruc-\nsueensuringthatthegoalsgeneratedbyG arenotonly\ntionspace. Specifically,I isderivedfromfourtypes. Most llm\nlinguisticallysoundbutalsoalignedwiththecapabilitiesof\noftheseareformulatedthroughatupleofaninitialstateand\nπ . Weformulatetheco-trainingasfollows:"
  },
  {
    "id": 361,
    "text": "linguisticallysoundbutalsoalignedwiththecapabilitiesof\noftheseareformulatedthroughatupleofaninitialstateand\nπ . Weformulatetheco-trainingasfollows:\natargetstate/trajectorycollectedbyπ ,whichaimstoalign g\ng\nG llm and π g at environmental comprehension. Then, we (cid:40) π =argmax E (cid:2) V (s,g) (cid:3)\nleveragethisdataandGPT-4(OpenAI,2023)togenerate g πg g∼PG|Gllm πg\nP =argmax E [V (s,g)],\nappropriateinstruction. Thisinstructionaimstodirectfrom G|Gllm PG|Gllm g∼PG|Gllm Gllm\nthe specified initial state to the intended target state, and (7)\nCoT(Weietal.,2023)isdeployedtoenhanceperformance.\nwhereP thegoaldistributionconditionedbyG ,\nSpecifically,thefourtypesofinstructiongenerationare(1)\nG|Gllm llm\nV(s,g)denotesanapproximateevaluationforπ orG ,\ng llm\nI (HumanInstructions,HI):human-annotatedinstruc-\nH ingeneral,astatevaluefunction. Itisnoteworthythatour\ntions; (2) I (State Instructions, SI): GPT-4-generated\nS co-trainingframeworkisclosetoahierarchicalreinforce-"
  },
  {
    "id": 362,
    "text": "H ingeneral,astatevaluefunction. Itisnoteworthythatour\ntions; (2) I (State Instructions, SI): GPT-4-generated\nS co-trainingframeworkisclosetoahierarchicalreinforce-\ninstructionsbygivingatupleofstates(s,s′)wherethes\nmentlearningframework(HRL)(Vezhnevetsetal.,2017),\ntheinitialstatethatsampledfromagenttrajectoriesands′\nwheretheManager(comparabletoG )plansgoalsforthe\nllm\nthetargetstatethatismanuallyconstructedbymodifying\nlearningoftheWorker(comparabletoπ ),withRLbeing\ng\nfeatures of the s; (3) I (Agent Instructions, AI): GPT-\nA performed for each. Inspired by HRL, we implement co-\n4-generatedinstructionsbygivingapairof(s,τ)wheres\ntrainingbyintegratingthegoal-conditionedtrainingofπ\ng\ntheinitialstate,τ theagenttrajectory;and(4)I (Random\nR andReinforcementLearningwithAgentFeedback(RLAF)\nInstructions,RI):amixtureoftheabovethreeinstruction\nforG . RLAFisbuiltuponPPO,witharewardshaping\nllm\nsetstoformasupplementarydataset. ByaccompanyingO\nthat considers (1) Rf the evaluation of goal completion,"
  },
  {
    "id": 363,
    "text": "forG . RLAFisbuiltuponPPO,witharewardshaping\nllm\nsetstoformasupplementarydataset. ByaccompanyingO\nthat considers (1) Rf the evaluation of goal completion,\nwithI,wefurtherconstructD ={(o,ι)|(o,ι)∈O×I}.\nx whereahighrewardindicatesthatagoaliscompletedor\nSubsequently, we employ GPT-4 to generate appropriate\nthe reachable probability from current state; (2) Re the\ngoalsGˆusingD aslabeleddatafortrainingG ,result-\nx llm evaluationofcrucialsub-goalcompletion,whichinvolves\ninginadatasetD = {(o,ι,g)|(o,ι,g) ∈ O×I ×Gˆ}. To\nexaminingcasesbypairinginstructionsinabatchwithaset\nensure that the goals generated by GPT-4 conform to the\nofessentialsub-goals;(3)Rmtheevaluationofoutputting\nformatwewant,acomprehensivepromptengineeringen-\nthepropergoalformat,withtheLLMbeingpenalizedbased\ndeavorwasconductedtoestablishasetofpredetermined\non edit distance. Then, we can express the reward func-\nrulesforGPT-4. Therule-basedpromptsthatguideGPT-4’s\ntion as R(s,ι,g) = Rf(s,g)+Re(s,g)+Rm(s,g) and"
  },
  {
    "id": 364,
    "text": "on edit distance. Then, we can express the reward func-\nrulesforGPT-4. Therule-basedpromptsthatguideGPT-4’s\ntion as R(s,ι,g) = Rf(s,g)+Re(s,g)+Rm(s,g) and\nresponses are documented in Table 10, with examples of\nAppendixKincludesmoredetails. Weobservedthetrain-\npromptsforgenerationprovidedinTable17.\ningwillleadG andπ compromisetoalocaloptimal,\nllm g\ni.e., G comfortsahighcompletionratioforπ butne-\nllm g\nMulti-stepFine-tuning. Wefine-tuneChatGLM-6Bwith glectconsistencywithinstructions,andπ simultaneously\ng\nLoRA (Hu et al., 2021) in three steps, as illustrated in rewardsG withahighcompletionratio. Furthermore,\nllm\nFigure8. Thestepsinclude(1)CoT-assistedfine-tuning as the policy training continuing, the evaluation for goal\n(CoFT):wesplittheCoTstepsofbuildingI intoindepen- generationisout-date. Tofixthisissue,weproposeape-\ndenttrainingdata,aimingtoexpandthevolumeoftraining riodicresetfortheRLAF,i.e.,theparametersoftheG\nllm"
  },
  {
    "id": 365,
    "text": "denttrainingdata,aimingtoexpandthevolumeoftraining riodicresetfortheRLAF,i.e.,theparametersoftheG\nllm\ndataaswellasenhancethegoalgenerator’sreasoningand willberesetforeverysetnumberofstepstoavoidbeing\nunderstandingtoD ;(2)SupervisedFine-tuning(SFT): trapped in a local convergence, achieving enhanced goal\nx\nstrictly formatting the LLM-generated goals and further completion, and keeping goals consistent with human in-\n5"
  },
  {
    "id": 366,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\nstructions. Consideringthetrainingefficiency,weconduct ure2(a),ourmethodsurpassesHERby≈3.4%. Figure2(b)\nLoRA (Hu et al., 2021) to update the model weights for evaluatesthegeneralizationonunseengoals,addressingthe\nG . Figure8illustratesthewholetrainingprocess,and secondaspectmentionedearlier. Itisnoteworthythatthe\nllm\nAlgorithm1summarizesthecorrespondingpseudo-code. unseen goals are re-combinations of goals obtained with\nHER and G . As indicated in Figure 2(b), our method\nllm\nexcelsoverthebaselineintermsofcompletionratio. Fig-\n5.Experiment\nure 2(c) answers the third point by comparing the use of\nWeconductempiricalexperimentstoevaluatetheefficacyof KL-divergenceregularizerforpolicylearning,considering\nbothstagesofourproposedOpenPAL.TomaketheContra changesinoverallperformanceandtheabilitytoeliminate\nsatisfy the learning requirements, we give well-designed enemies. Three metrics are designed for evaluation: (1)"
  },
  {
    "id": 367,
    "text": "satisfy the learning requirements, we give well-designed enemies. Three metrics are designed for evaluation: (1)\nspacesandrewardfunctionsasfollows. Meanbasicrewardperstep, whichindicateswhetherthe\ncurrent policy degenerates in performing basic skills per\nObservationSpace. Theobservationspaceencompasses stepagainstawell-trainednon-goalpolicy,andintentional\nmanyfactors,suchasunitfeaturesdetailingtheagentstates, to emphasize the agent’s immediate responsiveness over\nthose of other players and environmental features captur- finalresults;(2)#Enemieskilled,representingtheaverage\ninginteractionevents. Additionally,anagent-centricRGB numberofenemieskilledbytheagentperepisode;and(3)\nbird’s-eye-view(BEV)ofthelocalenvironmentisconsid- #Enemiesknockeddown,representingtheaveragenumber\nered. Table4includesdetailedinformation. ofenemiesknockeddownbytheagentperepisode.\n5.2.EvaluatingLLM-basedGoalGeneration\nAction Space. The action space is implemented on top"
  },
  {
    "id": 368,
    "text": "ered. Table4includesdetailedinformation. ofenemiesknockeddownbytheagentperepisode.\n5.2.EvaluatingLLM-basedGoalGeneration\nAction Space. The action space is implemented on top\nofContra’smicro-operationAPI,comprisingacollection We conducted evaluation of G through two compara-\nllm\nof multi-grained actions. These actions range from fine- tiveexperimentsonGPT-4-generatedinstructiondatasets,\ngrainedmovements,suchassix-degrees-of-freedommove- aiming to investigate the impact of different instruction\nment and weapon usage, to compound actions in coarse- datasetsandfine-tuningparadigms. Theevaluationmetrics\ngrainedcategories,suchasfiringatatarget,andeachaction employedencompassprecision, recall, andF1score. It’s\nis executed over a duration of 200ms, hence the control worthnotingthatapotentialissueindeterminingthepre-\nfrequencyis5Hz. Thetotalsizeoftheactionspaceis54. cisionofgeneratingsub-goalsthatarecloseinsemantics.\nFurtherdetailsinTable6. Forinstance,associatingthesub-goal“movingspeed”val-"
  },
  {
    "id": 369,
    "text": "frequencyis5Hz. Thetotalsizeoftheactionspaceis54. cisionofgeneratingsub-goalsthatarecloseinsemantics.\nFurtherdetailsinTable6. Forinstance,associatingthesub-goal“movingspeed”val-\nues“veryfast”versus“fast”maybeperceivedasanegative\nRewardFunctions. Acomprehensiverepresentationis instanceunderprecisionmeasurement. Consequently,we\nemployedfortherewardfunction,consideringvariousfac- arguethatthegenerationofsuchsub-goalsshouldweigh\ntorscontributingtogoal-conditionedpolicylearning. These moreinchoosingsub-goalthandeterminingvalues. Thus,\nfactorsareorganizedasalinearcombinationtoformulate we further propose three choice-based metrics: precision\ntherewardfunction.Furthermore,wedeterminetheweights (choice),recall(choice),andF1(choice). Table1provides\nforthecombinationwithatwo-foldprinciple: (1)assigning\nweightstorewarditemsbasedontheirscalesandempha- Precision Recall F1\nDataset Precision Recall F1\nsizingimportantfactors;(2)dynamicallyadjustingweights (Choice) (Choice) (Choice)"
  },
  {
    "id": 370,
    "text": "weightstorewarditemsbasedontheirscalesandempha- Precision Recall F1\nDataset Precision Recall F1\nsizingimportantfactors;(2)dynamicallyadjustingweights (Choice) (Choice) (Choice)\ninresponsetolearningfeedback,suchasdecreasingorin- HI 0.435 0.611 0.361 0.517 0.395 0.560\ncreasingtheweightsofcorrespondingfactors. Additional AI 0.474 0.611 0.419 0.532 0.445 0.569\nSI 0.444 0.601 0.413 0.539 0.428 0.568\ninformationisavailableinAppendixA.\nRI 0.499 0.633 0.414 0.526 0.453 0.574\nALL 0.555 0.685 0.505 0.621 0.529 0.652\n5.1.EvaluatingGoal-conditionedRL\nTable1: Evaluationondifferentdatasets. “ALL”thepropor-\nWeevaluatetheπ ofstageIfromthreedistinctperspectives\ng\ntionalmixtureofthefourbasedatasets.\ntoverifytheopen-endednessachievedonG: (1)thecomple-\ntionratio,(2)generalizationcapabilityconcerningunseen\ngoals,and(3)robustnesswhenintegratinggoal-conditioned acomparisonoffivetypesofinstructiondatasetsusedinthe"
  },
  {
    "id": 371,
    "text": "tionratio,(2)generalizationcapabilityconcerningunseen\ngoals,and(3)robustnesswhenintegratinggoal-conditioned acomparisonoffivetypesofinstructiondatasetsusedinthe\nlearningatopnon-goallearning. GiventhatGCRLinOpen- multi-stepfine-tuningprocessforG . Thecomparison\nllm\nPALcomprisesrandomandhindsightstages,ourevaluation reveals that utilizing a mixture significantly outperforms\ninvolvesacomparativeanalysiswithabaseline,HER,i.e., individual base datasets, which indicates a mixture aids\ntrainingtheRLagentwithhindsightgoalgeneration. Fig- G in capturing human preferences and understanding\nllm\nure2(a)presentsacomparisonofthegoalcompletionratio theimplicationsofeachabstractedstate,therebyenhancing\nacrossdifferentmethodsonavalidationdatasetwheregoals goalgeneration. Table2comparesfourkindsoffine-tuning\nare generated using G and G . As depicted in Fig- withtheproposedmulti-stepfine-tuning,including(1)SFT:\nrnd op\n6"
  },
  {
    "id": 372,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\nAlgorithm1COLLABORATIVETRAINING\n1: Input: θtheparametersofπ g,θ ;β ={β llm ,β LoRA }forG llm,β ,β llm thepre-trainedparametersofG llm ;β LoRA the\nfine-tunedLoRAparametersofG ;I =I ∪I ∪I ∪I theinstructionset\nllm H S A R\n2: Reloadingθ,β llm ,mergingβ LoRA intoβ llm\n3: forloop=1,2,... do\n4: Initializeanewβ LoRA andβ ={β llm ,β LoRA }\n5: foriteration=1,2,...,ndo\n6: Agentsfromabatchofworkerssendstates{s j |j =1,...,m}toG llm\n7: Randomsampleabatchofinstructions: I train ={ι j |j =1,...,m}⊂I\n8: GenerategoalsinstringwithLLM:G s ={g j ∼G llm,β (s j ,ι j )|j =1,...,m}andparseG s toformattedgoals: G\n9: DistributeG toagentsπ g,θ ,thencollecttrajectories{τ j }andreturns{R j }toform\nTj\n(cid:88)\n(τ,R)={(τ ,R )|j =1,...,m},whereτ ={g ,s ,a ,r ,...,s ,a ,r },R = r\nj j j j 1 1 1 Tj Tj Tj j t\nt=1\n10: UpdateθwithEquation(6)and(τ,R)\n11: FiltercompletedgoalsG c fromG,extractRthesetfrom(τ,R)asagentfeedbackrewardsRf"
  },
  {
    "id": 373,
    "text": "j j j j 1 1 1 Tj Tj Tj j t\nt=1\n10: UpdateθwithEquation(6)and(τ,R)\n11: FiltercompletedgoalsG c fromG,extractRthesetfrom(τ,R)asagentfeedbackrewardsRf\n12: Computeexaminationrewardbyevaluatingcrucialsub-goalcompletionasRe = REWARDe (G c ,I S ∩I train ,G),\nandformattingreward: Rm = REWARDm (g\ns\n,g),refertoAppendixK\n13: Updateβ LoRA withPPO,andmergeupdatedβ LoRA intoβ llm\n14: endfor\n15: endfor\nPrecision Recall F1 Instruction Goal(Beforeco-training) Goal(Afterco-training)\nTuning Precision (Choice) Recall (Choice) F1 (Choice) Whetherproneposition:True Averagevelocity:Static\nStop! Whetherproneposition:True\nSFT 0.547 0.663 0.490 0.602 0.517 0.632 Averagevelocity:Static Lengthofdistancemoved:NoMovement\nCoTF 0.533 0.652 0.487 0.599 0.509 0.624 Whetherproneposition:True\nCoTF→SFT 0.555 0.685 0.505 0.621 0.529 0.652 Getdown, Whetherproneposition:True Lengthofdistancemoved:NoMovement\nCoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629 stayhidden. Averagevelocity:Static Averagevelocity:Static"
  },
  {
    "id": 374,
    "text": "CoTF→SFT→EFT 0.590 0.694 0.501 0.593 0.516 0.629 stayhidden. Averagevelocity:Static Averagevelocity:Static\nWhetherseenbyenemy:False\nLengthofdistancemoved:long\nTable2: Evaluationondifferenttuningmethods. Whetherholdagun:True Averagevelocity:Fast\nWhetherhavebullets:True Whetherholdagun:True\nEnemy!\nHorizontaldirectionofview:SouthwestHorizontaldirectionofmovement:Southwest\nRushandfire.\nWhetherseenenemy:True Whetherseenenemy:True\nonlyusethetargetpromptwithoutCoTdatatosupervised Averagevelocity:Fast Damagetoenemy:High\nHorizontaldirectionofview:Southwest\nfine-tuning,whichcanberegardedasabaselineforanaive Lengthofdistancemoved:long\nWhetherproneposition:True\nSFT; (2) CoTF: only CoT-assisted fine-tuning; (3) CoTF Enemiesnearby, Averagevelocity:Fast Averagevelocity:Fast\nWhetherproneposition:False\nmovetodefend Whetherholdagun:True\n→SFT:furtherSFTtargetpromptafterCoTF;(4)CoTF Horizontaldirectionofmovement:North\nandavoiddamage.Healthlevel:Full\n(Positionofenemy:South)"
  },
  {
    "id": 375,
    "text": "Whetherproneposition:False\nmovetodefend Whetherholdagun:True\n→SFT:furtherSFTtargetpromptafterCoTF;(4)CoTF Horizontaldirectionofmovement:North\nandavoiddamage.Healthlevel:Full\n(Positionofenemy:South)\n→SFT→EFT:furtherensemblefine-tuningtargetprompt Whethertorestorehealth:True\nWhetherholdagun:True\nafterCoTF.Withthecomparison,weconcludethatCoTF\nTable3: Comparisonofgoal-generation. Cyanthehelpful,\nand SFT can improve each other and achieve better per-\npink the conflicting, and orange the critical sub-goals. It\nformance. Furthermore,ensemblefine-tuningsignificantly\nisevidentthatco-trainingenablesgoal-generationtoavoid\nenhancesprecisionwhilemarginallydecreasingrecall,mak-\nconflictsofsub-goalsandimprovesreasonabilitybyinclud-\ningitmoresuitableforgeneratingaccurateconcisegoals.\ninghelpfulandcriticalsub-goals.\n5.3.EvaluatingCo-training\nmentsmainlyliesin2 ≤ |g| ≤ 4, because|g| = 1istoo\nWeconductananalysisofthecompletionratiocorrespond- easy while |g| ≥ 5 is too hard to complete. Figure 3(b)"
  },
  {
    "id": 376,
    "text": "5.3.EvaluatingCo-training\nmentsmainlyliesin2 ≤ |g| ≤ 4, because|g| = 1istoo\nWeconductananalysisofthecompletionratiocorrespond- easy while |g| ≥ 5 is too hard to complete. Figure 3(b)\ningtothenumberofvalidsub-goalsduringtheco-training showsacaseof|g| = 3thatco-trainingindeedimproves\nprocess. Thoughthedimensionsizeofgoalspaceachieves the completion ratio as the green curve. It is noteworthy\n68,thenumberofsub-goalsforvalidgoalspredominantly that the performance suddenly downgrades at each reset.\nfallswithintherangeof1to7.Thisisrationalascompleting ThisphenomenonisattributedtotheresetofG breaks\nllm\nagoalwithanexcessivenumberofsub-goalsisexceedingly theadaptationwithπ ,avoidingbeingtrappedinlocalopti-\ng\nchallengingforapolicy, evenimpossibilityforhumanto mal. Meanwhile,theperformancetendstoconverge,which\nachieve. Furthermore,Figure3(a)showsthattheimprove- indicates the successor loops produce a better adaptation\n7"
  },
  {
    "id": 377,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\n30\n29 28\n27\n26\n25\n24\n0 0.1M0.2M0.3M0.4M0.5M0.6M0.7M0.8M0.9M1.0M\nTraining Steps\n)%(\noitaR\nnoitelpmoC\nlaoG\nOurs 32.5\nHER 30.0 27.5\n25.0\n22.5\n20.0\n17.5\n15.0\n0 0.1M0.2M0.3M0.4M0.5M0.6M0.7M0.8M0.9M1.0M\nTraining Steps\n(a)\n)%(\noitaR\nnoitelpmoC\nlaoG\n0.00\nOurs\nHER 0.02\n0.04\n0.06\n0.08\n0.10\n0 0.1M0.2M0.3M0.4M0.5M0.6M0.7M0.8M0.9M1.0M\nTraining Steps\n(b)\ndraweR\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\nscitsitatS\nemaG\nMean basic reward per step\nMean basic reward per step (No KL) #Enemies killed #Enemies knocked down\n#Enemies killed (No KL)\n#Enemies knocked down (No KL)\n(c)\nFigure2: (a)Thegoalcompletionrateontrainingdataset;(b)Thegoalcompletionrateonunseengoals,i.e.,thetestdataset;\n(c)Theevaluationofpolicylearningincasesofw/andw/oKL-divergenceregularizer.\nTrend of policy ability\nSmoothed Completion Rate Original Completion Rate\nReset Loops\n(a) Goalcompletionrate(1≤|g|≤7)\n)%(\noitaR\nnoitelpmoC\nlaoG\n0.200\n0.175\n0.150\n0.125\n0.100\n0.075"
  },
  {
    "id": 378,
    "text": "Trend of policy ability\nSmoothed Completion Rate Original Completion Rate\nReset Loops\n(a) Goalcompletionrate(1≤|g|≤7)\n)%(\noitaR\nnoitelpmoC\nlaoG\n0.200\n0.175\n0.150\n0.125\n0.100\n0.075\n1 2 3 4 5 6 7 g 4 S u b g g 8 o a l g 1 2 g 16 g20 Time in one loop 0\n0\n.\n.\n0\n0\n2\n5\n5\n0\n(b) Goalcompletionrate(|g|=3)\nytilibissoP\n(c) Sub-goalsdistribution\nFigure3: (a)Thecompletionratioofgoalswithdimensionsizerangesfrom1to7;(b)Thegoalcompletionratioofgoals\nthat |g| = 3, the trend curve reflects the improving completion ratio; (c) The sub-goals distribution changes along the\ntraininginoneloopofco-training,wherethedescriptionofeachgiisincludedinTable14.\nbetween LLM and policy than before. Additionally, we 6.Conclusion\ninvestigatedthechangeinthegenerationprobabilityofsub-\nIn this paper, we propose OpenPAL experts on learning\ngoals(Table14)duringco-training. Specifically,Figure3(c)\nopen-endedembodiedagentsforhuman-AIinteraction,ex-\nillustrateschangeswithinatrainingloop,whileFigure11(a)"
  },
  {
    "id": 379,
    "text": "goals(Table14)duringco-training. Specifically,Figure3(c)\nopen-endedembodiedagentsforhuman-AIinteraction,ex-\nillustrateschangeswithinatrainingloop,whileFigure11(a)\ncellinginachievinginstructionopen-endednessthrougha\nindicateschangesacrossloops. Astrainingprogresses,the\ntwo-stagelearningprocess. TheempiricalresultsonContra\nprobabilitiesassociatedwitheachgiundergogradualmodi-\nrepresentthatOpenPALshowsthepotentialasapractical\nfications. Forinstance,sub-goalswithgrowingprobabilities\nsolution for human-AI interaction in complex situations.\narecentraltotheagentprivatestatesduetotheirrelatively\nDespitethepositiveresults,weadmittherearestillsome\nattainablenatureandinfluenceinagentinteraction. Con-\nlimitations to our work that would be expected to be re-\nversely, sub-goalswithfallingprobabilitiesarecentralto\nsearched in the future—for instance, a truly open-ended\nother players’ states, as they are not directly changed by\ngoaldescriptioninsteadofthehandcraftedgoalspaceinthe"
  },
  {
    "id": 380,
    "text": "searched in the future—for instance, a truly open-ended\nother players’ states, as they are not directly changed by\ngoaldescriptioninsteadofthehandcraftedgoalspaceinthe\nagentactions,andG tendstogenerateoutputsforthese\nllm\ncurrentversion;supportingmulti-modalityinput/outputto\nsub-goalsonlywhenabsolutelynecessary. Toinvestigate\nfreefromexpensivefeatureengineering.\ntheimpactofco-trainingtoG ,wehavealsoidentified\nllm\nthechangesofgoal-generationforaninstruction,asshown\ninTable3. Evidently,afterco-training,G demonstrates AuthorContributionStatement\nllm\nits capacity to eliminate contradictory and irrational ele-\nTheauthorsconfirmtheircontributionasfollows:\nmentswithintheinitialobjectivesandexhibitstheabilityto\nintroducenewsub-goals,therebyrenderingtheoverallgoal Shaopeng Zhai: team leadership, open-ended learning,\nmoreattainable,allwhileretainingitsexceptionalsemantic LLM/RLAFtraining,agentanalysis,architecturedesign"
  },
  {
    "id": 381,
    "text": "moreattainable,allwhileretainingitsexceptionalsemantic LLM/RLAFtraining,agentanalysis,architecturedesign\ncomprehensioncapabilities. JieWang: infrastructure/frameworkengineering,non-goal\n8"
  },
  {
    "id": 382,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\nagenttraining,open-endedlearning,ablationstudies,fea- Du,Y.,Watkins,O.,Wang,Z.,Colas,C.,Darrell,T.,Abbeel,\ntureengineering P., Gupta, A., and Andreas, J. Guiding pretraining in\nTianyiZhang: non-goalagenttraining,open-endedlearn- reinforcementlearningwithlargelanguagemodels.arXiv\ning,featureengineering preprintarXiv:2302.06692,2023.\nFuxian Huang: non-goal agent training, paper writing,\nopen-endedlearning Du,Z.,Qian,Y.,Liu,X.,Ding,M.,Qiu,J.,Yang,Z.,and\nQi Zhang: LLM training, RLAF training, paper writing, Tang,J. Glm: Generallanguagemodelpretrainingwith\nablationstudies autoregressiveblankinfilling. InProceedingsofthe60th\nAnnual Meeting of the Association for Computational\nMingZhou: co-trainingframework,curriculumresearch,\npaperwriting\nLinguistics(Volume1: LongPapers),pp.320–335,2022.\nJingHou: LLMtraining,paperwriting\nEcoffet,A.,Huizinga,J.,Lehman,J.,Stanley,K.O.,and"
  },
  {
    "id": 383,
    "text": "MingZhou: co-trainingframework,curriculumresearch,\npaperwriting\nLinguistics(Volume1: LongPapers),pp.320–335,2022.\nJingHou: LLMtraining,paperwriting\nEcoffet,A.,Huizinga,J.,Lehman,J.,Stanley,K.O.,and\nClune,J. Firstreturn,thenexplore. Nature,590(7847):\nReferences\n580–586,2021.\nAndrychowicz,M.,Wolski,F.,Ray,A.,Schneider,J.,Fong,\nFang, M., Zhou, T., Du, Y., Han, L., and Zhang, Z.\nR.,Welinder,P.,McGrew,B.,Tobin,J.,PieterAbbeel,O.,\nCurriculum-guided hindsight experience replay. Ad-\nandZaremba,W. Hindsightexperiencereplay. Advances\nvances in neural information processing systems, 32,\ninneuralinformationprocessingsystems,30,2017.\n2019.\nBai,C.,Liu,P.,Zhao,W.,andTang,X. Guidedgoalgen-\nGautam, A., Jain, H., Senger, A., and Dhand, G. Battle\nerationforhindsightmulti-goalreinforcementlearning.\nroyale: First-person shooter game. In Proceedings of\nNeurocomputing,359:353–367,2019.\ntheInternationalConferenceonInnovativeComputing&\nCommunication(ICICC),2021."
  },
  {
    "id": 384,
    "text": "royale: First-person shooter game. In Proceedings of\nNeurocomputing,359:353–367,2019.\ntheInternationalConferenceonInnovativeComputing&\nCommunication(ICICC),2021.\nBalduzzi, D., Garnelo, M., Bachrach, Y., Czarnecki, W.,\nPerolat,J.,Jaderberg,M.,andGraepel,T. Open-ended\nHermann,K.M.,Hill,F.,Green,S.,Wang,F.,Faulkner,R.,\nlearninginsymmetriczero-sumgames. InInternational\nSoyer,H.,Szepesvari,D.,Czarnecki,W.M.,Jaderberg,\nConferenceonMachineLearning,pp.434–443.PMLR,\nM.,Teplyashin,D.,etal. Groundedlanguagelearningin\n2019.\nasimulated3dworld. arXivpreprintarXiv:1706.06551,\n2017.\nBerner,C.,Brockman,G.,Chan,B.,Cheung,V.,De˛biak,P.,\nDennison,C.,Farhi,D.,Fischer,Q.,Hashme,S.,Hesse,\nHu,B.,Zhao,C.,Zhang,P.,Zhou,Z.,Yang,Y.,Xu,Z.,and\nC., et al. Dota 2 with large scale deep reinforcement\nLiu,B.Enablingintelligentinteractionsbetweenanagent\nlearning,2019.\nandanllm: Areinforcementlearningapproach,2023.\nBrohan,A.,Brown,N.,Carbajal,J.,Chebotar,Y.,Dabis,J.,\nHu,E.J.,Shen,Y.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang,"
  },
  {
    "id": 385,
    "text": "learning,2019.\nandanllm: Areinforcementlearningapproach,2023.\nBrohan,A.,Brown,N.,Carbajal,J.,Chebotar,Y.,Dabis,J.,\nHu,E.J.,Shen,Y.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang,\nFinn,C.,Gopalakrishnan,K.,Hausman,K.,Herzog,A.,\nS.,Wang,L.,andChen,W. Lora: Low-rankadaptation\nHsu,J.,etal. Rt-1: Roboticstransformerforreal-world\noflargelanguagemodels,2021.\ncontrolatscale. arXivpreprintarXiv:2212.06817,2022.\nHuang,W.,Wang,C.,Zhang,R.,Li,Y.,Wu,J.,andFei-Fei,\nBrown,T.B.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,\nL. Voxposer: Composable 3d value maps for robotic\nJ.,Dhariwal,P.,Neelakantan,A.,Shyam,P.,Sastry,G.,\nmanipulationwithlanguagemodels. InConferenceon\nAskell,A.,Agarwal,S.,Herbert-Voss,A.,Krueger,G.,\nRobotLearning,pp.540–562.PMLR,2023.\nHenighan,T.,Child,R.,Ramesh,A.,Ziegler,D.M.,Wu,\nJ.,Winter,C.,Hesse,C.,Chen,M.,Sigler,E.,Litwin,M., Jiang,Y.,Gupta,A.,Zhang,Z.,Wang,G.,Dou,Y.,Chen,Y.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish, Fei-Fei,L.,Anandkumar,A.,Zhu,Y.,andFan,L. Vima:"
  },
  {
    "id": 386,
    "text": "Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, Fei-Fei,L.,Anandkumar,A.,Zhu,Y.,andFan,L. Vima:\nS.,Radford,A.,Sutskever,I.,andAmodei,D. Language Robotmanipulationwithmultimodalprompts. 2023.\nmodelsarefew-shotlearners,2020.\nKhandelwal, A., Weihs, L., Mottaghi, R., andKembhavi,\nBurda,Y.,Edwards,H.,Storkey,A.,andKlimov,O. Explo- A. Simplebuteffective: Clipembeddingsforembodied\nrationbyrandomnetworkdistillation. InInternational ai. InProceedingsoftheIEEE/CVFConferenceonCom-\nConferenceonLearningRepresentations,2018. puterVisionandPatternRecognition,pp.14829–14838,\n2022.\nDing, H., Tang, Y., Wu, Q., Wang, B., Chen, C., and\nWang,Z. Magneticfield-basedrewardshapingforgoal- Liu, M., Zhu, M., and Zhang, W. Goal-conditioned re-\nconditionedreinforcementlearning. IEEE/CAAJournal inforcement learning: Problems and solutions. arXiv\nofAutomaticaSinica,10(12):1–15,2023. preprintarXiv:2201.08299,2022.\n9"
  },
  {
    "id": 387,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\nNakano,R.,Hilton,J.,Balaji,S.,Wu,J.,Ouyang,L.,Kim, Wagner,R.A.andFischer,M.J. Thestring-to-stringcor-\nC.,Hesse,C.,Jain,S.,Kosaraju,V.,Saunders,W.,etal. rection problem. Journal of the ACM (JACM), 21(1):\nWebgpt: Browser-assistedquestion-answeringwithhu- 168–173,1974.\nmanfeedback. arXivpreprintarXiv:2112.09332,2021.\nWang,G.,Xie,Y.,Jiang,Y.,Mandlekar,A.,Xiao,C.,Zhu,\nNg, A. Y., Harada, D., and Russell, S. Policy invariance Y., Fan, L., and Anandkumar, A. Voyager: An open-\nunder reward transformations: Theory and application endedembodiedagentwithlargelanguagemodels. arXiv\nto reward shaping. In Icml, volume 99, pp. 278–287. preprintarXiv:2305.16291,2023a.\nCiteseer,1999.\nWang,Z.,Cai,S.,Liu,A.,Ma,X.,andLiang,Y. Describe,\nNikulin,A.,Kurenkov,V.,Tarasov,D.,andKolesnikov,S. explain,planandselect: Interactiveplanningwithlarge"
  },
  {
    "id": 388,
    "text": "Citeseer,1999.\nWang,Z.,Cai,S.,Liu,A.,Ma,X.,andLiang,Y. Describe,\nNikulin,A.,Kurenkov,V.,Tarasov,D.,andKolesnikov,S. explain,planandselect: Interactiveplanningwithlarge\nAnti-explorationbyrandomnetworkdistillation. arXiv languagemodelsenablesopen-worldmulti-taskagents.\npreprintarXiv:2301.13616,2023. arXivpreprintarXiv:2302.01560,2023b.\nOpenAI. Gpt-4technicalreport,2023. Wei,J.,Wang,X.,Schuurmans,D.,Bosma,M.,Ichter,B.,\nXia,F.,Chi,E.,Le,Q.,andZhou,D. Chain-of-thought\nOuyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C., prompting elicits reasoning in large language models,\nMishkin,P.,Zhang,C.,Agarwal,S.,Slama,K.,Ray,A., 2023.\net al. Training language models to follow instructions\nwithhumanfeedback. AdvancesinNeuralInformation Weng, L. Curriculum for reinforcement learn-\nProcessingSystems,35:27730–27744,2022. ing. lilianweng.github.io, Jan 2020. URL\nhttps://lilianweng.github.io/posts/\nRen,Z.,Dong,K.,Zhou,Y.,Liu,Q.,andPeng,J. Explo- 2020-01-29-curriculum-rl/."
  },
  {
    "id": 389,
    "text": "ProcessingSystems,35:27730–27744,2022. ing. lilianweng.github.io, Jan 2020. URL\nhttps://lilianweng.github.io/posts/\nRen,Z.,Dong,K.,Zhou,Y.,Liu,Q.,andPeng,J. Explo- 2020-01-29-curriculum-rl/.\nrationviahindsightgoalgeneration. AdvancesinNeural\nInformationProcessingSystems,32,2019. Zhang,T.,Xu,H.,Wang,X.,Wu,Y.,Keutzer,K.,Gonza-\nlez, J.E., andTian, Y. Noveld: Asimpleyeteffective\nSchaul,T.,Quan,J.,Antonoglou,I.,andSilver,D. Priori- exploration criterion. Advances in Neural Information\ntizedexperiencereplay.arXivpreprintarXiv:1511.05952, ProcessingSystems,34:25217–25230,2021.\n2015.\nZheng,Q.,Zhang,A.,andGrover,A. Onlinedecisiontrans-\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and former. Ininternationalconferenceonmachinelearning,\nKlimov, O. Proximal policy optimization algorithms, pp.27042–27059.PMLR,2022.\n2017.\nZitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,\nTeam,O.E.L.,Stooke,A.,Mahajan,A.,Barros,C.,Deck, Wu,J.,Wohlhart,P.,Welker,S.,Wahid,A.,etal. Rt-2:"
  },
  {
    "id": 390,
    "text": "2017.\nZitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F.,\nTeam,O.E.L.,Stooke,A.,Mahajan,A.,Barros,C.,Deck, Wu,J.,Wohlhart,P.,Welker,S.,Wahid,A.,etal. Rt-2:\nC.,Bauer,J.,Sygnowski,J.,Trebacz,M.,Jaderberg,M., Vision-language-actionmodelstransferwebknowledge\nMathieu,M.,etal. Open-endedlearningleadstogener- toroboticcontrol. In7thAnnualConferenceonRobot\nally capable agents. arXiv preprint arXiv:2107.12808, Learning,2023.\n2021.\nTrott, A., Zheng, S., Xiong, C., and Socher, R. Keeping\nyour distance: Solving sparse reward tasks using self-\nbalancingshapedrewards,2019.\nVanderMaaten,L.andHinton,G. Visualizingdatausing\nt-sne. Journalofmachinelearningresearch,9(11),2008.\nVezhnevets,A.S.,Osindero,S.,Schaul,T.,Heess,N.,Jader-\nberg, M., Silver, D., and Kavukcuoglu, K. Feudal net-\nworksforhierarchicalreinforcementlearning. InInterna-\ntionalConferenceonMachineLearning,pp.3540–3549.\nPMLR,2017.\nVinyals,O.,Babuschkin,I.,Czarnecki,W.M.,Mathieu,M.,\nDudzik,A.,Chung,J.,Choi,D.H.,Powell,R.,Ewalds,"
  },
  {
    "id": 391,
    "text": "tionalConferenceonMachineLearning,pp.3540–3549.\nPMLR,2017.\nVinyals,O.,Babuschkin,I.,Czarnecki,W.M.,Mathieu,M.,\nDudzik,A.,Chung,J.,Choi,D.H.,Powell,R.,Ewalds,\nT., Georgiev, P., etal. Grandmasterlevelinstarcraftii\nusingmulti-agentreinforcementlearning. Nature, 575\n(7782):350–354,2019.\n10"
  },
  {
    "id": 392,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\nA.Contra: TheEnvironment\nContraseamlesslymergesthelast-man-standinggameplaydynamicswiththesurvival,exploration,andscavengingelements\ninherentinfirst-personshootinggames(Gautametal.,2021). Thegameunfoldswithmultiplehostileteams,necessitating\nplayerstocollaboratewithteammates,withstandadversaries,andstrivetooutlastothersintheever-changingarena. The\nagent’sobjectivesencompassindividualsurvivalandtheeliminationofencounteredenemies. InContra,theagent’saction\ninterfaceisdesignedtomirrorhumancapabilities,encompassingbasicmovementsandactionslikemovingandshooting,\nwithactionintervalsaround200ms,similartothefrequencyofhumanoperations. Throughtheseactions,anagentinContra\nmandatesasequentialacquisitionofskills,startingfromfundamentalabilitieslikewalking,jumping,running,anditem\ncollection. Asthelearningproceeds,anagentmustmastermoreintricateskillssuchasevadingenemyprojectilesand"
  },
  {
    "id": 393,
    "text": "collection. Asthelearningproceeds,anagentmustmastermoreintricateskillssuchasevadingenemyprojectilesand\ncoordinatingtacticswithteammates. Thischaracteristicdefinesanopen-endedlearningprocesswheretheagentcontinually\nexploresthegameenvironmenttorefinemasteredskillsandacquirenewones.\nObservationSpace. Theobservationspaceencompassesvariousfactors,comprisingunitfeaturesdelineatingtheagent’s\nstatus,aswellasthatofotherplayers. Additionally,itincludesenvironmentalfeaturescharacterizinginteractionevents\nandanagent-centricRGBbird’s-eye-viewofthelocalobservation. Forthedetails,weincludetheminTable4. Giventhe\nheterogeneityintheshapesanddatatypesofthesefeatures,weadoptindependentfeatureprocessingforeachofthem,\nsubsequentlyconcatenatingthemtoserveasinputforthepolicyandvaluenetworks.InFigure4,wepresentthefoundational\nUnit Features\n([j�iii\nu\n-,\n-----------------'\nThe Agent RBG Bird's-eye-view\n-----------------'\nOther players Environmental features"
  },
  {
    "id": 394,
    "text": "Unit Features\n([j�iii\nu\n-,\n-----------------'\nThe Agent RBG Bird's-eye-view\n-----------------'\nOther players Environmental features\nFigure4: Preprocessingforanobservationwithfourtypesoffeatures.\nprocessforhandlinganobservation. Eachobservationconsistsoffourtypesoffeature,andeachtypeisassociatedwithan\nindependentnetworkdedicatedtoprocessingandoutputtingthecorrespondingembedding.\nActionSpace. AsintroducedinTable6,theinstantiationofactionspaceisachievedthroughtheutilizationofthemicro-\noperationAPIwithinContra.Thisprocessgivesrisetoacompilationofactionscharacterizedbydiverselevelsofgranularity.\nInadetailedbreakdown,theactionspacecomprisesseveraldistincttypes,namely(1)movementdirectionactionspace,it\nprovides16discretechoices,eachevenlydistributedacrossa360◦spectrum,(2)camerayawdirectionactionspace,it\noffers16choiceswithanequitabledivisionof360◦,(3)camerapitchdirectionactionspace,itencompassesthreedistinct"
  },
  {
    "id": 395,
    "text": "offers16choiceswithanequitabledivisionof360◦,(3)camerapitchdirectionactionspace,itencompassesthreedistinct\nvalues: −45◦,0◦,45◦,(4)bodyactionspace,itincorporatesninevalues: slide,stop,crouch,run,jump,ground,open\norclosedoor,rescue,andno-opand(5)attackingactionspace,itcomprises: fire,reloading,treat,pickupsupply,drop\nsupply,stopandfire,stopadjustandfire,and(6)weaponswitch,itmanifeststhreevalues: useweapon0,useweapon1,\nandno-weapon. Theaggregateddimensionalityoftheactionspaceisquantifiedat54intotal.\nReward Engineering. The primary objective of our training regimen is to equip the agent with the ability to play\nwith other players in Contra while concurrently optimizing its strength in eliminating opponents. To achieve this ob-\njective, we have formulated a diverse array of rewards designed to guide policy learning. However, the complexity\ninvolved in designing and fine-tuning these rewards is evident. To simplify the engineering, our design is character-"
  },
  {
    "id": 396,
    "text": "involved in designing and fine-tuning these rewards is evident. To simplify the engineering, our design is character-\nizedallocatingweightsbasedontheexpectedvalueofeachreward,ensuringaproportionateinfluenceonthelearning\nprocess. In accordance with the principle, we assume a referenced maximum return of 20, with different rewards as-\nsignedproportionsbasedontheirrelativeimportance. Specifically,forcriticalactionssuchasknockingdownorkilling\nan enemy, their values are set to approximately 4 (20% out of 20). Conversely, for less critical actions like scouting\n11"
  },
  {
    "id": 397,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\nor incurring time penalties, their values are set to less than 1 (5% out of 20). Detailed specifications are outlined in\nTable 5. In accordance with the aforementioned principles, we can now construct the reward function r(s,a) by lin-\nearly composing these factors, facilitating their collaborative influence on guiding policy learning. As delineated in\nAlgorithm 3, these factors are broadly classified into three categories: basic rewards rb, obstacle avoidance rewards\nroa, andgoalachievementrewardrg. Thebasicrewardsareprimarilydirectedatsteeringtheagenttowardsenhanced\ngame-playing performance, encompassing collaborative engagement with teammates and eliminating adversaries, etc.\nIn the case of roa, the objec-\ntiveistopromotenaturalnaviga-\nFeatureClass Field Description Dimension\ntionandforestalltheagentfrom\n1.Unitfeature Scalar Includesheroes,teammates,enemies,monster 527\nencounteringobstacles,suchas"
  },
  {
    "id": 398,
    "text": "tiveistopromotenaturalnaviga-\nFeatureClass Field Description Dimension\ntionandforestalltheagentfrom\n1.Unitfeature Scalar Includesheroes,teammates,enemies,monster 527\nencounteringobstacles,suchas\nAttribute CharacterID,teamID,size,skills 28\nstonesandtrees. Regardingthe Status HP,oxygen,speed,peektype,alivestate,bodystate,etc. 44\nimplementation, penalties are Heroes Pose Position,rotation,cameraposition,camerarotation,etc. 76\nItem Backpack,weapon 144\nimposedontheagentfordevia-\nAttribute CharacterID,teamID,size,skills 28\ntionsfromtheoptimalpath.This\nTeammates Status HP,oxygen,speed,peektype,alivestate,bodystate,etc. 30\noptimaltrajectoryisdetermined Pose Position,rotation,cameraposition,camerarotation,etc. 43\nbyassessingthecosinesimilar- Attribute CharacterID,teamID,size,skills 28\nity between the agent’s current Enemies Status HP,oxygen,speed,peektype,alivestate,bodystate,etc. 33\nPose Position,rotation,cameraposition,camerarotation,etc. 43\nmovement direction, a 2D unit"
  },
  {
    "id": 399,
    "text": "ity between the agent’s current Enemies Status HP,oxygen,speed,peektype,alivestate,bodystate,etc. 33\nPose Position,rotation,cameraposition,camerarotation,etc. 43\nmovement direction, a 2D unit\nAttribute Monstertype,size 8\nvector, provided as an environ- Monsters Status HP,maxHP,HPpercent,targettype 6\nmentalsignal,andtheexpected Pose Position,rotation,relativeposition,distance 16\nobstacle-free trajectory derived 2.Globalfeature Scalar Includescircle,event,doorandsupply 65\nfromtheactionsequenceinthe Status State,pain,radius 4\nCircle Position Bluecircle,whitecircle 6\ntrajectory:\nTime existencetime,resttime,totaltime,delaytime,appeartime 5\ndenv·d⋆ Attribute Type,damage,elapsedtime 8\nr t oa = ∥denv t ∥ ∗∥d t ⋆∥ −1, Event Position Occurredposition 3\nt 2 t 2\n(8) Attribute Type,state 5\nDoor Pose Position,relativeposition,rotation 8\nwhere denv the actual move-\nt Status Type,size,quantity,etc. 19\nmentdirectionoftheagentthat\nSupply Position Position,relativeposition,distance 7"
  },
  {
    "id": 400,
    "text": "Door Pose Position,relativeposition,rotation 8\nwhere denv the actual move-\nt Status Type,size,quantity,etc. 19\nmentdirectionoftheagentthat\nSupply Position Position,relativeposition,distance 7\ncan be directly accessed from\n3.Invisibleenemyfeature Scalar Invisible(nearby)enemyfeatureonlyforvalueestimation 104\nthe environment feedback, d⋆\nt Attribute CharacterID,teamID,size,skills 28\nthe expected movement direc- Invisibleenemies Status HP,oxygen,speed,peektype,alivestate,bodystate,etc. 33\ntion, which is derived by com- Pose Position,rotation,cameraposition,camerarotation,etc. 43\nbining the expected movement 4.Spatialfeature Scalar BEV 12288\ndirectionfromthepreviousmo- BEV Region Altitudemapandaerialviewmap 3x64x64\nmentwiththemovementaction\nTable4: Thedetailsoffeaturesintheobservationspace.\ntakenatthecurrentmoment. For\ntheconvenience,wesummarize\nthecorrespondingpseudo-codeinAlgorithm2. Fortheline5inAlgorithm2,weuse∆ todenotetheshiftingfromagent\nt"
  },
  {
    "id": 401,
    "text": "takenatthecurrentmoment. For\ntheconvenience,wesummarize\nthecorrespondingpseudo-codeinAlgorithm2. Fortheline5inAlgorithm2,weuse∆ todenotetheshiftingfromagent\nt\nmovementdirectionactiona tothecameravisiondegreeω. Toaddresstheissueoftheagentgettingstuckonobstaclesdue\nt\ntoshort-termactionsequences,weemployasmallerγ forthecorrespondingvaluehead. Specifically,wesetγ to0.92. This\nadjustmenthelpsmitigatetheimpactoftheobstacleavoidancerewardonlong-termcreditassignment,allowingforamore\nbalancedconsiderationofimmediateandfuturerewardsintheagent’sdecision-makingprocess. Asforthegoal-achieving\nreward,we’veintroducedinthemaintext,pleaserefertoSection4.2.\nB.EnvironmentalStateAbstractionandGoalSpace\nFor a comprehensive understanding of the game environment, a language model undergoes a fine-tuning process due\nto the scarcity of textual information within the simulation environment. The need arises to articulate non-linguistic"
  },
  {
    "id": 402,
    "text": "to the scarcity of textual information within the simulation environment. The need arises to articulate non-linguistic\nelements, and the interaction between an agent and the simulation environment is centered on continuously observ-\ning the environment’s state and generating corresponding actions. Therefore, the key aspects requiring verbalization\nprimarily involve the state and actions of the agent. However, given the abundance of possible observation states in\nthe simulation environment, it is impractical to use all of these states directly as prompts for the language model, es-\npecially considering token limitations. Consequently, there is a crucial need to extract and linguistically transform\nthe most significant meta-states to facilitate successful model interaction. It is noteworthy that smaller language\n12"
  },
  {
    "id": 403,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\nAlgorithm2CALCULATIONOFd⋆\nt\nInput: d⋆ ,a\nt−1 t\nOutput: d⋆\nt\n1: ifa t ∈movementdirectionactionspacethen\n(cid:20) (cid:21)\ncos(a )\n2: d⋆ ← t\nt sin(a )\nt\n3: elseifa t ∈yawdirectionactionspacethen\n4:\n5: ∆ t =a t −ω\n(cid:20) (cid:21)\ncos(∆ ) −sin(∆ )\n6: d⋆ ← t t d⋆\nt sin(∆ ) cos(∆ ) t−1\nt t\n7: elseifa t ∈{stop;rescue;stopandfire;stopadjustandfire}then\n(cid:20) (cid:21)\n0\n8: d⋆ ←\nt 0\n9: else\n10: d⋆ ←d⋆\nt t−1\n11: endif\nmodels have limitations in comprehending and manipulating numerical values effectively. To address this challenge,\na deliberate effort is made to minimize the use of numerical values during the environmental linguistics process.\nForexample,insteadofspecifyinganagent’sspeedwithspecificnumericmetrics\nlike“speed: 1m/s→3m/s”aqualitativerepresentationsuchas“speed: slower→ SubActionSpace DimSize\nfaster”isadopted. Thistechniquetransformstheoriginalcontinuousstateintoa movementdirection 16"
  },
  {
    "id": 404,
    "text": "like“speed: 1m/s→3m/s”aqualitativerepresentationsuchas“speed: slower→ SubActionSpace DimSize\nfaster”isadopted. Thistechniquetransformstheoriginalcontinuousstateintoa movementdirection 16\nlimited,discretemeta-state,therebyenhancingthelanguagemodel’sunderstanding.\nyawdirection 16\nSimilarly,forexpeditinglanguagemodelunderstanding,adiscreteactionspaceis\npitchdirection 3\nadopted,witheachactionaccompaniedbyasuccinctartificiallanguagedescription.\nbodyaction 9\nThisdiscreetarticulationofactionscontributestotheoverallinterpretabilityofthe\nbasicaction 7\nlanguagemodelwithinthesimulationenvironment. WelistthedetailsinTable7.\nswitchweaponaction 3\nC.PolicyNetwork Table6: Actionspace.\nFigure5isthenetworkarchitectureusedfornon-goalRL,correspondingtothe\npolicy π. On top of observation preprocessing in Appendix A, we introduce a\nbackboneimplementedwithafully-connectedlayerfollowedbythreeResidualBlocks. Asforthepolicyheadandthree"
  },
  {
    "id": 405,
    "text": "policy π. On top of observation preprocessing in Appendix A, we introduce a\nbackboneimplementedwithafully-connectedlayerfollowedbythreeResidualBlocks. Asforthepolicyheadandthree\nvalueheads,weimplementedeachofthemastwoconnectedResidualBlocks. Itisnoteworthythattheinvisibleenemy\ninformation,suchasthenearestenemy’slocation,hasalsobeenintroducedasaninputtothevalueestimation,forthe\nconsiderationofstabilizingthepolicylearning(Vinyalsetal.,2019).\nFigure5: Networkstructureofourproposedpolicy.\n13"
  },
  {
    "id": 406,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\nFeature Weight Description\nenemydiscovery 0.02 rewardforseeanenemy\ndetectedbyenemy -0.002 punishmentforbeingseenbyanenemy\nscout 0.0001 rewardforsearchforanenemy\nno-op -0.0002 punishmentforstoppinganddoingnothing\nbullet 0.015 rewardforusingandrefillingbullets\nhealthpoint 0.03 rewardforhealthpointchanges\nbeknockeddown -2.5 punishmentforbeingknockeddown\ndead -3.5 punishmentforbeingkilled\ndamageenemy 0.1 rewardfordamaginganenemy\nknockdownenemy 4.5 rewardforknockingdownanenemy\nkillenemy 3.5 rewardforkillinganenemy\napproachadownedteammate 0.001 rewardforapproachingadownedteammate\nhelpadownedteammateup 0.8 rewardforhelpingupadownedteammate\nnotsaveadownedteammate -0.5 punishmentfornotsavingadownedteammate\ngotobluecircle 0.00015 rewardforgoingtobluecircle\nbeinwhitecircle -0.00005 smallpunishmentforbeingoutsideofwhitecircle\noutsidebluecircle -0.012 punishmentforbeingoutsideofbluecircle"
  },
  {
    "id": 407,
    "text": "gotobluecircle 0.00015 rewardforgoingtobluecircle\nbeinwhitecircle -0.00005 smallpunishmentforbeingoutsideofwhitecircle\noutsidebluecircle -0.012 punishmentforbeingoutsideofbluecircle\nteammatedamageenemy 0.03 rewardfromteammatedamagingenemies\nteammategetup 0.6 rewardfromteammategettingup\nIhelpteammateup 4 rewardforhelpingteammateup\ninterrupthelpingteammateup -0.05 punishmentfortheinterruptiontohelpteammateup\nobstacleavoidance 0.012 punishmentforbeingstuck\ngoal 1 rewardofgoalcompletion\nTable5: Theintroductionofdifferentrewards.\nGoal Processing. To construct the goal-\nDate Iteration #params Change\nconditioned policy π g on top of π, we need to 4/14/2023 1 1802702 Experimentstarted\nencodeagoalgthatisgeneratedfromG ,G 4/27/2023 1808552 1802702 Env-init:Randomweapons\nrnd op 5/8/2023 2829170 1803087 Action:Addafireactionforlongdistance\nand G . Thus, we propose a network F as an 5/10/2023 3034011 1803087 Env-init:Randomsafeareainthewholemap"
  },
  {
    "id": 408,
    "text": "rnd op 5/8/2023 2829170 1803087 Action:Addafireactionforlongdistance\nand G . Thus, we propose a network F as an 5/10/2023 3034011 1803087 Env-init:Randomsafeareainthewholemap\nllm 5/11/2023 3130353 1803855 Observation:Addnumberofremainingplayersinthegame\nencoder to achieve that. In practice, to improve 5/12/2023 3198564 2412975 Observation:AddBEVfeature\n5/16/2023 3673506 2418111 Observation:Addhistoryrotationfeature\ntherepresentation,wefurtherincludeotherinputs 5/22/2023 4519567 2418368 Observation:Addrotationchangefeature\nbesidesgas(1)goal_mode_info: a10-dimensional 5/29/2023 5442025 2418368 Reward:Addrewardsforteamwork\n6/2/2023 5899503 2418368 Updatenewgameversion\nvector that indicates whether current learning is 6/13/2023 7306607 3013409 Network:Addobstacleavoidancerewardandcorrespondingvaluehead\n6/14/2023 7404118 3015457 Observation:Adddistancefeaturetonearbyobstacles\ngoal-conditioned, the achieved times of g, and 6/16/2023 7628098 3015457 Env-init:Playernumbersperteamincreasedto4"
  },
  {
    "id": 409,
    "text": "6/14/2023 7404118 3015457 Observation:Adddistancefeaturetonearbyobstacles\ngoal-conditioned, the achieved times of g, and 6/16/2023 7628098 3015457 Env-init:Playernumbersperteamincreasedto4\n6/19/2023 7974450 3109267 Action:Useattentiontoselecttargettoattack\nthe rest time to complete this goal (as we preset\nthe maximum timestep for goal completion is\nTable8: Themajorchangesinthetrainingprocedure.\n30s), short in v ; (2) sub_goal_masking: a\ninfo\n68-dimensional 0-1 vector that indicates which\nsub-goalsaremaskedfortheircorrespondingvaluesare0,shortinv ;(3)expected_goal_of_current_state: namely,for\nmask\ns\nt\natt,wegenerateagoalg′ = PROJ(s\nt\n)whichisa68-dimensionalvectorasthesameasg,shorting′. Then,weformthe\ninputforF asx=(g,g′,v ,v ). Consideringthateachiteminxisheterogeneousonthedatatype,sowetransform\ninfo mask\nthemwithindependentFClayersandthenfollowsaResNetblockforeachofthem. Wecanexpressthisprocessas\ne\ng\n= RESNET(FC(g)), e\ng′\n= RESNET(FC(g′)), e\ninfo\n= RESNET(FC(v\ninfo\n)), e\nmask"
  },
  {
    "id": 410,
    "text": "info mask\nthemwithindependentFClayersandthenfollowsaResNetblockforeachofthem. Wecanexpressthisprocessas\ne\ng\n= RESNET(FC(g)), e\ng′\n= RESNET(FC(g′)), e\ninfo\n= RESNET(FC(v\ninfo\n)), e\nmask\n= RESNET(FC(v\nmask\n)),\nwheree ,e ,e ,e aretheembeddingrepresentationcorrespondingtoeachinputitem,andallofthemarethesame\ng g′ info mask\nindimensionsize. Then,weaggregatethemwithcomputingtheaverageofthemastogetafusionembeddingas\ne =(e +e +e +e )/4.\nfusion g g′ info mask\nWithsurgery,wenowconcatenatetheembeddingofbackboneofπwithe andfusingthemviaaFClayer,toforma\nfusion\nbackboneforπ .\ng\n14"
  },
  {
    "id": 411,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\nSub-goalClass Candidates\nDamagetoenemy [Zero,Low,Littlelow,Medium,Littlehigh,High]\nWhetherknockdownenemy [True,False]\nWhetherkillenemy [True,False]\nWhetherseenenemy [True,False]\nWhetherseenbyenemy [True,False]\nNumberofenemieshaveeverseen [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\nLengthofdistancemoved [Nomovement,Short,Medium,Long,Verylong]\nAveragevelocity [Static,Slow,Medium,Fast,Falling]\nHorizontaldirectionofmovement [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]\nHorizontaldirectionofview [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]\nPitchdirectionofview [Low,Littlelow,Medium,Littlehigh,High]\nHealthlevel [Empty,Low,Medium,High,Full]\nWhethertorestorehealth [True,False]\nWhetherthehealthisdamaged [True,False]\nWhetherrescuedteammate [True,False]\nWhetherbeknockeddown [True,False]\nWhetherproneposition [True,False]\nWhetherhaveagun [True,False]\nWhetherhavebullets [True,False]"
  },
  {
    "id": 412,
    "text": "Whetherrescuedteammate [True,False]\nWhetherbeknockeddown [True,False]\nWhetherproneposition [True,False]\nWhetherhaveagun [True,False]\nWhetherhavebullets [True,False]\nWhetherhavemedicalkits [True,False]\nDistancewithnearestenemy [Touch,Nearby,Moderate,Far,Outofreach,ExtremeFar]\nWhethercloserwithnearestenemy [True,False]\nWhethercrouchposition [True,False]\nWhetherholdagun [True,False]\nLengthofdistancefromagenttoteammate [Touch,Nearby,Moderate,Far,Outofreach,ExtremeFar]\nWhetherseenbyteammate [True,False]\nTeammate’spositionrelativetoagent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest]\nWhetherfollowwiththeviewsofteammate [True,False]\nWhethertargetthesameenemyasteammate [True,False]\nWhetherfollowwiththemovementdirectionofteammate [True,False]\nHorizontaldirectionofmovementofenemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]\nVelocityofenemy [Static,Slow,Medium,Fast,Falling,None]"
  },
  {
    "id": 413,
    "text": "Horizontaldirectionofmovementofenemy [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]\nVelocityofenemy [Static,Slow,Medium,Fast,Falling,None]\nEnemy’spositionrelativetoagent [West,Northwest,North,NorthEast,East,Southeast,South,Southwest,None]\nTable7: Overviewofsub-goalclasses,weshowapartofthemhere.\nD.Surgery\nAstheprojectproceeded,Contrawascontinuouslyimprovedtosatisfyricherfeaturedenvironmentdynamics. However,\nsuchaniterativedevelopmentposessomechallengestotheresearchofopen-endedlearninginanembodiedsituation,as\nthechangesinAPIandenvironmentattributeswillmakethetrainingbenon-stationary. Apopularsolutiontoresolvethis\nissueisthesurgeryintroducedbyBerneretal.(2019),whichsignificantlyreducestrainingtimebymaximizingretentionof\npreviouslylearnedabilities. Similarly,weleveragesurgeryinfouraspectstoensurethetrainingadaptstothenewchanges,\nincludingmodelarchitecture,observationspace,actionspace,andrewardfunctions. Table8illustratesthemainchanges"
  },
  {
    "id": 414,
    "text": "includingmodelarchitecture,observationspace,actionspace,andrewardfunctions. Table8illustratesthemainchanges\nweconductedandthecorrespondingparameters. Forthesurgeryofobservationspaceandmodelarchitecture,wehave\nintroducedadecoupledencodinginAppendixA;forthesurgeryofactionspace,wedirectlyextendthepolicyheadinwidth\ntosatisfythenewactionspace;fortherewardfunctions,theessentialsaretoincludethenewlyintroducedfeatureswhich\ncancontributetothelearning,asweintroducedinAppendixA,alinearcombinationhasbeenconsideredtosatisfythis\nrequirement. Inourexperiment,weproposethreenovelsurgerymethods,wheretwoformodelarchitectureandonefor\nobservationspace. Thegameenvironmenthaschangedseveraltimessincethetrainingstarted. Thechangesaremainly\naboutaddingplayercharacters,addingplayerskills,addingweapon,modifyingthemap,etc. Forallthesechanges,the\nproportionofnewenvironmentsinwhichwetrainourpolicygrowsslowlyfrom0%to100%. Inthiscase,thevarianceis"
  },
  {
    "id": 415,
    "text": "proportionofnewenvironmentsinwhichwetrainourpolicygrowsslowlyfrom0%to100%. Inthiscase,thevarianceis\nrelativelysmallandtheperformancewouldquicklyrecoverinthenewenvironment. Figure6evaluatestheutilityofsurgery,\nillustratingthechangesinbasicvalueduringthetraining. Itcanbeseenthatthevalueschangesmoothlyformostsurgeries.\nMeanwhile,thevaluesremainstableafterthesurgeries. Theseresultsprovetheeffectivenessofoursurgery.\n15"
  },
  {
    "id": 416,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\n5\n4\n3\n2\n1\n0\n-1\n04/14 04/28 05/12 05/26 06/09 06/23 07/07\nDates\neulaV\ncisaB\nFigure6: Thevaluechangesduringthetrainingprocess.\nE.DatasetsConstruction\nThe process of fine-tuning the language model is operationalized through a question and answer paradigm. In this\nframework,weprovidethelanguagemodelwithacomprehensivedepictionofthepresentconditionspertainingtothe\nagent,itscompanions,andadversaries. Additionally,wefurnishthemodelwiththelinguisticinstructionsfurnishedby\nits teammates. Subsequently, the language model is tasked with generating the agent’s subsequent target meta state in\naccordancewiththespecificsoutlinedinthequestionandanswerdataset,aselucidatedinTable17.Theresponseisgenerated\nbyGPT-4andsubsequentlysubjectedtoparsingandformattingprocessesfacilitatedbytherule-basedcodingmechanism.\nToensurealignmentbetweentheresponsesgeneratedbyGPT-4andthetargetedmeta-stateformat,acomprehensiveprompt"
  },
  {
    "id": 417,
    "text": "ToensurealignmentbetweentheresponsesgeneratedbyGPT-4andthetargetedmeta-stateformat,acomprehensiveprompt\nengineeringendeavorwasconductedtoestablishasetofpredeterminedrulesforGPT-4. Therule-basedprompts,which\nwereemployedtoguideGPT-4’sresponses,aremeticulouslydocumentedinTable10.\nInanefforttomitigatetheriskofoverfittingourmodeltoconciseformattedoutputs,therebypreservingitscapacityfor\nenvironmentalreasoning,weaugmentedourdatasetwithasubstantialvolumeofChainofThoughtdata. Thisaugmentation\napproach entails a systematic procedure whereby the large language model is guided through a step-by-step ideation\nprocess,ultimatelyculminatingintheattainmentoftheintendedtargetstate. Concretely,ourmethodologycommences\nwithaninitialinquiryintothesemanticinterpretationofthegiveninstruction,followedbytheidentificationofpertinent\nstates, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporal"
  },
  {
    "id": 418,
    "text": "states, contemplation of state adjustments, analysis of action propensities, and an estimation of the requisite temporal\nconsiderations. Comprehensive documentation of the detailed prompts and ensuing responses derived from the Chain\nofThoughtprocedurecanbefoundinTables16and9. ItisnoteworthythattraditionalChainofThoughtprocessesin\nexistinglargelanguagemodelsoftengeneratesequentialthoughts,amethodcharacterizedbyarelativelyprotractedtemporal\ntrajectory. Thissequentialreasoningapproachmaynotbewell-suitedtothehighreal-timedemandstypicallyencountered\ninfirst-personshooter(FPS)games. Furthermore,thesingular-stepreasoningcapabilitiesinherentinsmallerlanguage\nmodelsareintrinsicallymodestandpronetoerrors. Consequently,theamplificationoferrorprobabilitieswithintheChain\nofThoughtreasoningprocessmaynotyieldsuperioroutcomes. Inlightoftheseconsiderations,wehaveundertakena\nstrategythatamalgamatesChainofThoughtdatawiththefinaltargetstatedata,therebyenhancingthefine-tuningofour"
  },
  {
    "id": 419,
    "text": "strategythatamalgamatesChainofThoughtdatawiththefinaltargetstatedata,therebyenhancingthefine-tuningofour\nlanguagemodel. Inthecourseoftestreasoningexercises,thelanguagemodelpromptlygeneratestheultimatetargetstate,\nwiththeChainofThoughtinformationbeingimplicitlyencodedwithintheneuralnetworkparameters.\nInstructionDatasets. Tocoveracomprehensiverangeofinstructiontypesandstatedistributions,wegeneratedfour\ntypesofinstructionsets,which,whencombinedwithstatessampledfromtheenvironment,resultinfourdifferentdatasets.\nThesearetheHI(HumanInstruction)dataset,constructedbasedonhuman-annotatedcommands;theSI(StateInstruction)\ndataset,builtbyreverse-generatingcommandsbasedonstatetransitionsspecifiedbytheintelligentagent;theAI(Agent\nInstruction)dataset, constructedbymainkindsofinstructionwhichcanbecompletebypre-trainedAgent; andtheRI\n(RandomInstruction)dataset,generatedthroughrandomsamplingofagentstatetransitionsandrandomcommands."
  },
  {
    "id": 420,
    "text": "(RandomInstruction)dataset,generatedthroughrandomsamplingofagentstatetransitionsandrandomcommands.\n• I (HumanInstructions). Wegenerateopen-endedinstructionsmanually,whilethecorrespondingstatesaresampled\nH\nfromtheintelligentagent’sinteractionlogs. ThesearecombinedandannotatedusingGPT-4basedontheprompting\n16"
  },
  {
    "id": 421,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\nmethodpreviouslydescribed. Wefoundthatduetovaryingfrequenciesofstatechangesduringtheagent’sinteractions,\nsomestatesaredifficulttocapturecomprehensivelyonlyusingrandomsampling. Toensureamorecomprehensive\ndistributionofstatesinthedataandtofacilitatebetterunderstandingbythelanguagemodel,weemployamulti-round\nrejectionsamplingapproachtoconstructstateset. LetSbethesetofstateswaitingtobesampled. Weperformmultiple\nroundsofsamplingonS,withSgetrepresentingthesetofstatessampledinthei-thround,initiallyempty. Next,we\ni\nsampleastatesfromS withoutreplacementandcheckwhethershasanystatevaluesnotpresentinSget. Ifitdoes,\ni\nweacceptitandaddittoSget,otherwisewerejectit. OnceallstatesinS havebeensampled,oneroundiscompleted.\ni\nSgetistheresultofi-thround’ssampling,andS willberesetforthenextround. Thissamplingmethodisemployedto\ni\nenhancethecomprehensivenessofstatecoverageinalldatasetsexcepttheRandomInstructiondataset."
  },
  {
    "id": 422,
    "text": "i\nSgetistheresultofi-thround’ssampling,andS willberesetforthenextround. Thissamplingmethodisemployedto\ni\nenhancethecomprehensivenessofstatecoverageinalldatasetsexcepttheRandomInstructiondataset.\n• I (StateInstructions). Weaimtocoverabroaderrangeofstatechangesintheinstructionstoenhancethelanguage\nS\nmodel’sunderstandingofvariousstatetransitions. Toachievethis,wedesigncorrespondinggoalsandinstructionsfor\nallstates. Specifically,foreachvalueofeachstate,wegenerateaseriesofinstructionsthatrequirethecorresponding\nstateandvalue. ThesearethenannotatedusingGPT-4basedonthepromptingmethodspreviouslydescribed. The\nannotatedresultsarechecked;iftheydonothavecorrespondingstatesandvalues,manualannotationandmodification\nareperformedtoincludetherelevantstates.\n• I (AgentInstructions). Weaimtoinitiallyaligntheplanningcapabilitiesofthelanguagemodelwiththepre-trained\nA\nabilitiesofanintelligentagentbasedonreinforcementlearningpolicies. Todoso,wegeneratepotentialcorresponding"
  },
  {
    "id": 423,
    "text": "A\nabilitiesofanintelligentagentbasedonreinforcementlearningpolicies. Todoso,wegeneratepotentialcorresponding\ninstructionsbasedonactualstatechangesinagentinteractions. Specifically,wefirstsampleaseriesofagentstatepairs\nat5-secondintervals. Forasubsetofthese,wemanuallyannotatepossiblecorrespondinginstructions. Wethenuse\nthesemanualannotationsasaknowledgebaseandemploythe\"langchain\"methodtousetheseexamplestoguide\ntheannotationoftheremainingdatausingChatGPT-3.5. Finally,werepresentalltheinstructionsasvectorsusing\nOpenAI’sembeddingAPIandperformclustering. Weselectthe14mostrepresentativetypesofinstructionsandpair\nthemcyclicallywithtworoundsofsampledstates,ultimatelyconstructingadatasetthatbetterreflectsthefundamental\nexecutioncapabilitiesoftheintelligentagent.\n• I (RandomInstructions). Thissetisprimarilydesignedtoenrichthedatadistribution. Itisconstructedbyrandomly\nR\ngeneratinginstructionsandfullyrandomlysamplingstates,andthenannotatedusingGPT-4basedontheprompting\nmethodspreviouslydescribed."
  },
  {
    "id": 424,
    "text": "R\ngeneratinginstructionsandfullyrandomlysamplingstates,andthenannotatedusingGPT-4basedontheprompting\nmethodspreviouslydescribed.\nThequantatyoftheaforementionedfourtypesofdatasetsis507forHI,1098forSI,1441forAIand1382forRI.Moreover,\nthetestdatasetconstructinstructionsthatdifferfromthoseusedinthetrainingdata,thenutilizeGPT-4togeneratedraft\nlabelsofgoalsandmodifiedwithmanuallyfilteredandannotated. Thistestdatasetusedforevaluatingthemodel’sabilityto\nplanreasonablyinresponsetoinstructions. Andthesizeofdatasetforeachtuningstepis26,568forCoT-assitedfine-tuning,\n4,428forsupervisedfine-tuning,and4,994forensemblingfine-tuning.\nF.DistributedTrainingFramework\nToimprovethetrainingefficiency,weadoptadistributedtrainingsystem,showninFigure7. Inthissystem,theActorsrun\noverCPUnodestocollecttrainingdata,thensendthecollecteddatatotheLearnerwhichisdeployedonaGPUnode. We\nfurthertakeaLLMservertoenablemultiplereplicasofLLMforgoalgeneration,whichimprovethethroughputofrollout"
  },
  {
    "id": 425,
    "text": "furthertakeaLLMservertoenablemultiplereplicasofLLMforgoalgeneration,whichimprovethethroughputofrollout\nwhentheRLtrainingisswitchtogoal-conditionedcases.\nG.ParameterSettings\nSomeofthehyper-parametersusedinourexperimentareillustratedinTable11andotherdynamichyper-parametersare\nintroducedtheircorrespondingparts.\nH.Algorithms\nAlgorithm 3 lists the pseudo-code of goal-conditioned RL procedures in Stage I, where Rb, Roa and Rg represent the\nt t t\ndiscountedbasicreturn,obstacleavoidancereturnandgoal-reachingreturnfromtimestepttillthetermination,respectively.\n17"
  },
  {
    "id": 426,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\n,.---' --------..., 'I\n,..)\"'-----------\"\\'I\nl LLM Replica V !,I \\\nI I '\"------------.// \\\nI I\nI I\nI Goals /\nI\n;'\nState ab� 1 t ractions , , ;-- Actions\nI / -----------��====���\n1 /;___\n\\\n\\\n\\\nLearning tasks Checkpoints \\\nActors\nLearner League\nTraining outcomes Rollout outcomes\n+-\nModel parameters --+ Redis Model parameters --+ Environments\nFigure7: Thistrainingsystemhasfourkeyparts: Actor,Learner,LeagueandLLMreplicas. Actorsareresponsiblefor\ndatacollection,theLearnertrainsthepolicymodelusingthisdata,theLeaguecoordinatestheoveralltrainingprocessand\ndisplaysresults,andtheLLMReplicashandlegoalgenerationanddistributethemtodownstreamworkers.\nAlgorithm3GOAL-CONDITIONEDREINFORCEMENTLEARNING\n1: Input: θparameterizespolicyπandζ ={ζ 1 ,ζ 2 ,ζ 3 }parameterizesvalueheads,goalgeneratorsG rnd andG op\n2: fork=1,2,... do\n3: Resetenvironmentwithreturnedinitialstates 0 ∼P(s)\n4: Sampleagoal: g ∼(G rnd ∪G op )(s 0 ,∆t,∆V)"
  },
  {
    "id": 427,
    "text": "2: fork=1,2,... do\n3: Resetenvironmentwithreturnedinitialstates 0 ∼P(s)\n4: Sampleagoal: g ∼(G rnd ∪G op )(s 0 ,∆t,∆V)\n5: Runpolicyπ θk inenvironmentuntilbeterminated\n6: ActorscollecttrajectoriesD τ andsendthemtotheLearner\n7: Updatetheθ k toθ k+1 withEquation(6)\n(cid:104) (cid:105)\n8: Updateζ bymax ζ E s∼Dτ ∥R t b−V ζ b 1 (s t )∥ 2 +∥R t oa−V ζ o 2 a(s t )∥ 2 +1 gt̸=∅ ∥R t g−V ζ g 3 (s t )∥ 2\n9: endfor\nThecomputationofeachcanbeexpressedasfollows:\nT T T\n(cid:88) (cid:88) (cid:88)\nRb = γrb(s ,a ), Roa = γroa(s ,a ), Rg = γrg(s ,a ,g). (9)\nt i i t i i t i i\ni=t i=t i=t\nAsforJ(π )inEquation(6),wefollowthecomputationofpolicylossinPPOtoexpressitas\ng,θ\n(cid:18) (cid:19) (cid:40)\nπ (s,a) (1+ϵ)A, A≥0\nJ(π\ng,θ\n)=min\nπ\ng,θ\n(s,a)\nAπg,θk(s,a),g(ϵ,Aπg,θk(s,a)) , where(s,a)∼D\nτ\n,g(ϵ,A)=\n(1−ϵ)A, A>0\n.\ng,θk\n(10)\nAπ(s,a)indicatestheadvantageiscomputedundertheconditionofpolicyπ.\nI.Open-endedGoalGeneration"
  },
  {
    "id": 428,
    "text": "J(π\ng,θ\n)=min\nπ\ng,θ\n(s,a)\nAπg,θk(s,a),g(ϵ,Aπg,θk(s,a)) , where(s,a)∼D\nτ\n,g(ϵ,A)=\n(1−ϵ)A, A>0\n.\ng,θk\n(10)\nAπ(s,a)indicatestheadvantageiscomputedundertheconditionofpolicyπ.\nI.Open-endedGoalGeneration\nInspired by Hindsight Experience Replay (HER) (Andrychowicz et al., 2017), we adopt a similar method to utilize\nthe collected trajectories for learning a goal generator G which accepts a state as input. We conclude its train-\nop\ning in two steps: (1) constructing (s,g) pairs with collected trajectories as illustrated in Figure 11(b); (2) supervised\ntraining G with the above pairs and an MSE loss between the labeled goals and predicted goals. For step (1),\nop\nwe split trajectories into many segments with length of 200 timesteps. Then, we randomly sample a state s from\nthe first 150 steps and sample a state s′ from the last 20 steps to derive a goal g = PROJ(s′), with a distribution"
  },
  {
    "id": 429,
    "text": "the first 150 steps and sample a state s′ from the last 20 steps to derive a goal g = PROJ(s′), with a distribution\nproportional to their basic value Vbasic(s′). For step (2), we train G with s, ∆t, Vbasic(s) and Vbasic(s′) as in-\nop\nput to generate goals, where ∆t the time slot of goal completion, Vbasic(s′) and Vbasic(s) the basic state value.\n18"
  },
  {
    "id": 430,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\nCoFT SFT EFT\nHI SI AI RI\nState\nStates HI SI AI RI\nStates\nCoT LoRA-FT\nQuestions ... ... ... ... Instructions ChatGLM + LoRA..-.CK1\nCoT Goals LoRA-CK2\nResponses ... ... ... ... generating\ntuning ...\ntuning Response 1 Response 2 Response N\nChatGLM + LoRA-FT ChatGLM + LoRA-FT\nEnsemble Response\nSFT\nInstruction\nvoting\nCo-Training\nReward\nGoal Completion Reward\nENV Complete status Examination Reward Exam Set selecting\nFormat recognition\nFormat Reward\nRewards\nInteraction State PPO Tuning\nRL Model Response ChatGLM LoRA-FT + LoRA-CT Instruction State\nFigure8:OverviewofthetrainingframeworkwithLLM.ThistrainingframeworkhasthreekindsofLLMtuningapproaches:\nCoFT(ChainofThoughtsassistedFine-Tuning),SFT(SupervisedFine-Tuning),EFT(EnsembleFine-Tuning);andone\nLLM-RLco-trainingapproach.\nPPOclipeps 0.2\nOptimizer Adam\nLearningrate 0.0001\nValidation. TovalidatetheefficiencyofG op , weconductacom- Batchsize 20480"
  },
  {
    "id": 431,
    "text": "LLM-RLco-trainingapproach.\nPPOclipeps 0.2\nOptimizer Adam\nLearningrate 0.0001\nValidation. TovalidatetheefficiencyofG op , weconductacom- Batchsize 20480\nparison between generated goals and oracles w.r.t distribution, as NumberofCPUs 5120(AMDEPYC7H1264-Core)\nillustratedinFigure10. Empirically,wefirstconstructatestdataset NumberofGPUs 2(A100)\nγ (basic) 0.995\nofgoals(welabelthemasOracles)withtrajectoriesthataresampled\nγ (oa) 0.92\nwithmultiplecheckpointsofnon-goalpolicy,toensurethediversity. γ (goal) 0.993\nThen,thetuplesof(s,∆t,∆V)thatcorrespondingtotheoraclesare λ 0.95\nEntropycoefficient 0.025\nusedtogenerategoalpredictions(welabelthemasPrediction)with\nUnrolllength 20\nG op . Tovisualizethegoaldistribution,weleverageTSNE(Vander Samplemaxusetimes 3\nMaaten&Hinton,2008)toshrinkthegoaldimensionfrom68to1. Gradientclipthreshold 10\nTheresultsinFigure10showthatthedistributionofPredictionwell\nTable11: ParametersettingsforRL.\nmatchesthedistributionofOracles.\nJ.AblationStudyonLLMs"
  },
  {
    "id": 432,
    "text": "TheresultsinFigure10showthatthedistributionofPredictionwell\nTable11: ParametersettingsforRL.\nmatchesthedistributionofOracles.\nJ.AblationStudyonLLMs\nTheImpactofLoraRank. WeevaluatetheimpactoftherankparameteronperformanceduringLoRAfine-tuningof\nlargelanguagemodelneuralnetworks. Generallyspeaking,thelargertherankparameter,themorecomprehensiveand\nthorough the fine-tuning of the neural network, but the corresponding training time and model footprint will be larger.\nThe experimental results are shown in Table 12. The\nsizeoflorarankhaslittleimpactonmodelperformance\nPrecision Recall F1 Accurate\nRank Precision Recall F1 Accurate\nindicators,butalargerankwillcausethemodeltraining (Choice) (Choice) (Choice) (Choice)\ntimeandthesizeofthesavedparameterfiletoincrease 8 0.544 0.672 0.482 0.608 0.502 0.629 0.060 0.124\n16 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.124\ndramatically. 32 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159\n64 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124"
  },
  {
    "id": 433,
    "text": "16 0.550 0.673 0.487 0.601 0.507 0.626 0.070 0.124\ndramatically. 32 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159\n64 0.547 0.675 0.501 0.616 0.519 0.635 0.070 0.124\n128 0.552 0.684 0.507 0.626 0.524 0.645 0.075 0.134\nTheImpactofLoraTarget. Wenextverifiedwhich\nneural networks in fine-tuning the ChatGLM-6B large Table12: Evaluationonlorarank.\nlanguagemodelcanachievethebestperformance. The\n19"
  },
  {
    "id": 434,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\n(a) Altitude map (0.8m) (b) Altitude map (4m) (c) Aerial view map (0.4m)\nFigure9: IllustrationofBEVfeaturesinobservationspace. (a)and(b)arethealtitudemapswherebrightareasarehigher\nthandarkareas. (c)istheaerialviewmapwherethedisconnectedareasarewindowsordoors. Onepixelin(a),(b)and(c)\ndenotes0.8meter,4metersand0.4meterrespectively. Thesmallyellowblocksrepresentplayerpositionsandsmallblue\nblocksrepresentenemypositions.\nOracles\nPrediction 100\n100 50\n5\n5\n0\n0\n0\nGoal\nProjection\n0\n100\n50\n250\n225\n200\n20 15 10 V 5 0 5 10 15 50 75 100 125 150 175 T 100 50 75 100 125 15T0 175 200 225 250\n(a) Correspondingto(∆T,∆V)\nnoitcejorP\nlaoG\n100\n50\n0\n50\nOracles 100 Prediction 20 15 10 5V 0 5 10 15\n(b) Correspondingto∆T\nnoitcejorP\nlaoG\nOracles Prediction\n(c) Correspondingto∆V\nFigure10: Distributioncomparisonbetweenrealgoals(Oracles)andgoalsgeneratedbyG (Prediction). Theillustration\nop"
  },
  {
    "id": 435,
    "text": "(b) Correspondingto∆T\nnoitcejorP\nlaoG\nOracles Prediction\n(c) Correspondingto∆V\nFigure10: Distributioncomparisonbetweenrealgoals(Oracles)andgoalsgeneratedbyG (Prediction). Theillustration\nop\nshows that G generates goals that follows the real distribution, indicating good generalization on open-ended goal\nop\ngeneration.\nexperimentalresultsareshowninTable13. Itisworthnotingthatonlyfine-tuningtheMLPnetworkwithoutfine-tuningthe\nattentionnetworkcanachievethebesttrainingresults. Althoughgenerallyspeaking,themainstreamfine-tuningtaskof\nlargelanguagemodelsistofine-tunetheattentionlayernetwork,butthattaskusuallyfocusesmoreonanswersemantics. In\nourtask,wepaymoreattentiontotheformattomeetthemetastateparsingrequirements,sofine-tuningtheMLPnetwork\ncanachievebetterresults.\nTheImpactofDatasetScale. Weconductexperimentsofvariousmodelswithfourpercentagesoffine-tuningtrainset,\ni.e.,100%,30%,10%,3%,onthegoalgenerationtask. TheresultsareshowninTable15. Itcanbeseenthatastheamount"
  },
  {
    "id": 436,
    "text": "i.e.,100%,30%,10%,3%,onthegoalgenerationtask. TheresultsareshowninTable15. Itcanbeseenthatastheamount\nofdatagraduallydecreases,theperformanceofvarioustrainingindicatorsgraduallydeterioratesundervarioussettings.\nHowever,thesmallertheamountofdata,thegreatertheimprovementbroughtbypre-trainingofourproposedCoFTmethod.\nTheresultsshowthattheCoFTmethodweproposedcaneffectivelycollectandexpandthechainofthoughtdatarelatedto\nthefinalgoal,therebyavoidingoverfittingofthetrainingsetinthecaseofsmalldata.\nK.RewardFunctionsforCo-Training\nAgentFeedbackRewards. Thecalculationoftheagentfeedbackrewardismultifaceted,aimingtoreflectthedegreeof\ncompletionasfeedbackforthetrainingoftheLLM.Specifically,threeaspectsareconsideredtosatisfytherequirements,\nandthetotalagentfeedbackrewardisgivenbythesumofthem:\n• rf -MinimalDistancetoaGoalWhenSatisfyingEnvironmentTermination. AsdepictedbyEquation(11),the\ng\n20"
  },
  {
    "id": 437,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\n0.225\n0.200\n0.175\n0.150\n0.125\n0.100\n0.075\ng 4\nS u b g g 8 o a l g 1 2 g 16 g20 L1 L2 L3 T i L m 4 e i n L l 5 o o p s L6 L7 0\n0\n.\n.\n0\n0\n2\n5\n5\n0\nytilibissoP\n(a) (b)\nFigure11: (a)Sub-goal distributionduringco-training. The 20mostfrequentlyoccurringgoalmeta statesarefiltered\noutanddisplayed. Theverticalaxisrepresentstheprobabilityofthestatebeingoutputbythelanguagemodel;(b)Fora\ncollectedtrajectorysegmentwithlengthk = 200,wefirstlyestimatethebasicvalueforthelastk−j +1states(here\nj =20)andselectonestateasthegoalwiththeprobabilityproportionaltotheirvalues.\nagentprogressivelyreducesthedistancebetweentheinitialstateandthegoal,scalingitbythemagnitudeoftheinitial\nstate-goaldifference:\nT\nRf = (cid:88) ∥ |g−Proj(s t−1 )|−|g−Proj(s t )| ∥ , whereϵ=1e-6 (11)\ng |g−Proj(s )|+ϵ 1\n0\nt=1\n• rf -RewardIndicatingHowLongtheGoalCanBeKept. AsdepictedbyEquation(12),uponaccomplishingthe\nkeep"
  },
  {
    "id": 438,
    "text": "T\nRf = (cid:88) ∥ |g−Proj(s t−1 )|−|g−Proj(s t )| ∥ , whereϵ=1e-6 (11)\ng |g−Proj(s )|+ϵ 1\n0\nt=1\n• rf -RewardIndicatingHowLongtheGoalCanBeKept. AsdepictedbyEquation(12),uponaccomplishingthe\nkeep\ngoal,theagentreceivesarewardproportionaltothecumulativenumberofstepstakentosustainthegoalstate,scaled\nbythecountofdistinctsub-goalsbetweentheinitialstates\n0\nandthegoalg,i.e. n(g∩PROJ(s\n0\n)):\nT\n(cid:88)\nR\nk\nf\neep\n=n(g∩PROJ(s\n0\n))· 1\ng∩PROJ(st)̸=∅\n(12)\nt=0\n• rf -RewardIndicatingWhethertheGeneratedGoalisReachablefortheCurrentState. RND(Burdaetal.,\nrnd\n2018;Nikulinetal.,2023;Zhangetal.,2021;Duetal.,2023)isaneffectivemethodtomeasurethevisitingfrequency\nofstatesortransitionsinRL,wherehigheraRNDscore(reward),themorefrequentastateisvisited. Thus,wecan\nleveragesuchamethodtoquantifyhownovelastateis:\nT\n(cid:88)\nRf =− ∥φ(E(s ,g))−φ⋆(E(s ,g))∥, (13)\nrnd t t\nt=0\nwhereφ⋆atargetnetworkwhichsharesthesamearchitectureastheRNDpredictorbutthenetworkisnon-trainable.\nThus,weexpressRf asRf =Rf +Rf +Rf .\ng keep rnd"
  },
  {
    "id": 439,
    "text": "Rf =− ∥φ(E(s ,g))−φ⋆(E(s ,g))∥, (13)\nrnd t t\nt=0\nwhereφ⋆atargetnetworkwhichsharesthesamearchitectureastheRNDpredictorbutthenetworkisnon-trainable.\nThus,weexpressRf asRf =Rf +Rf +Rf .\ng keep rnd\nExaminationRewardFunction. Theexaminationrewardfunctionisintroducedasanintrinsicsignaltoencouragethe\nLLMtogenerategoalswithessentialsub-goals. WeusetheSIdatasetastheexaminationsetI . Foreachtrainingiteration,\nS\na batch of instructions I is randomly sampled from the full instruction dataset I, and corresponding goals g are\ntrain\ngenerated. Aftertheagentfinishesitsrollout,theexaminationrewardforeachbatchiscomputedbasedontheintersection\nI =I ∩I . Fornon-emptyI ,anexaminationrewardforeachinstructioninI iscomputedas:\n∩ S train ∩ ∩\n21"
  },
  {
    "id": 440,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\n(cid:40)\n+2 g ∈g\nre(ι,g,g )= sub ,∀ι∈I (14)\nsub ∩\n−2 otherwise\nThen,ReiscalculatedasRe = 1 (cid:80) re(ι,g,g |g =G (s,ι)).\n|I∩| ι∈I∩ sub llm\nFormattingRewardFunction. TheformattingrewardRmforeachgeneratedgoaliscalculatedbycomputinganedit\ndistance,utilizingtheWagner-Fischeralgorithm(Wagner&Fischer,1974).\nWiththedefinedrewardfunctions, RLAFisappliedwitharewardfunctionR = Rf +Re +Rm andProximalPolicy\nOptimization(PPO)foreachdatapointinabatch.\nFigure12: ImplementationoftheRNDpredictornetwork.\n22"
  },
  {
    "id": 441,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\n1. Analyzetheverbalordersofteammatesandplayers,whatdoyouwanttodo? Accordingtothecommand,\nalsoanalysistherelevantstatesofteammatesandenemiesthatneedattention.\nTheverbalcommandoftheteammateplayeris‘Youshouldlieinwait’,whichmeansteammateplayerwants\ntheagenttosetupanambushortakeahidingposition.\n2. Analyzewhichstatesoftheagentsaremostrelevanttotheverbalcommandsofteammateplayer. Theagents\nintheunselectedstateswilladjustthemselvestocompleteyourplan.\nAccordingtotheteammate‘Scommand:\n2.1. Needtohide: ‘Whetherproneposition’,‘Whethercrouchposition’\n2.2. Reducemovementtostayunnoticed: ‘Averagevelocity’,‘Lengthofdistancemoved’\n2.3. Ensurereadinessforpotentialattack: ‘Whetherholdagun’\n3. Planhowthesekeystatesneedtobeadjusted.\nAccordingtotheteammate‘Scommand:\n3.1. ‘Whetherproneposition’: Needtohide: ‘False’->‘True’\n3.2. ‘Whethercrouchposition’: Alternativehidingpostureifnotprone: ‘False’->‘True’"
  },
  {
    "id": 442,
    "text": "Accordingtotheteammate‘Scommand:\n3.1. ‘Whetherproneposition’: Needtohide: ‘False’->‘True’\n3.2. ‘Whethercrouchposition’: Alternativehidingpostureifnotprone: ‘False’->‘True’\n3.3. ‘Averagevelocity’: Reducemovement: ‘Fast’->‘Static’\n3.4. ‘Lengthofdistancemoved’: Reducemovement: ‘Short’->‘Nomovement’\n3.5. ‘Whetherholdagun’: Ensurereadinessforpotentialattack: ‘True’->‘True’\n4. Modifytheadjustmentthatmaybewrong,andrefertotheRulestoanalyzewhichstateadjustmentsmay\nconflict,repeatorbeunnecessary,andoutputthemodifiedadjustmentplan:\nAccordingtothestatesadjustmentsin3:\n4.1. ‘Whetherproneposition’: ‘False’->‘True’\n4.2. ‘Averagevelocity’: ‘Fast’->‘Static’\n4.3. ‘Lengthofdistancemoved’: ‘Short’->‘Nomovement’\n4.4. ‘Whetherholdagun’: ‘True’->‘True’\n(Removed‘Whethercrouchposition’asitconflictswith‘Whetherproneposition’andpronepositiongives\nbettercoverforanambush.)\n5. Accordingtotheanalyzeandtheplaningoftheverbalcommand,furtheranalyzethebehaviortendency\nrequiredintheadjustmentprocess."
  },
  {
    "id": 443,
    "text": "bettercoverforanambush.)\n5. Accordingtotheanalyzeandtheplaningoftheverbalcommand,furtheranalyzethebehaviortendency\nrequiredintheadjustmentprocess.\nMobile: Needtostopallmovement: 0%\nOffense: Mustbereadytoattackifnecessary: 20%\nWaiting: Themaingoalistosetupanambushandwait: 70%\nSupplies: Noneedforsuppliesduringthisphase: 0%\nScouting: Needtoobservethesurroundings,butnotactivelyscout: 10%\n6. Analyzehowlongthecurrentcommandneedstobekept.\nAccordingtothecommandoftheteammate,takingahidingpositionorsettingupanambushisusuallydone\nwiththeintentionofwaitingforanopportunemomenttostrikeorobservetheenemy‘Smovement: Thecurrent\ncommandneedstobekeptby‘longterm’.\nTable9: ChainofthoughtresponsefromGPT4.\n23"
  },
  {
    "id": 444,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\n1.Onlyselectthemostrelevantandnecessarystatesforplanning,andtheunplannedstateswillbeadjustedby\ntheagentitself\n2.[Choose1,Choose2,...] indicatesthevaluesthatcanbeselectedforthestate. Whenyouplan,youcanonly\nchoosethevalueofthestatefromit,anddonotinventnewvaluenotlistedin[Choice1,Choice2,...].\n3.Theselectedstatecanchangethecurrentvalueormaintainthecurrentvalue. Theagentwilltrytoachieve\nandmaintainthevalueofthestateyouchooseafteryougivetheplan.\n4.Agentsdon’tvoluntarilydiscarditems(forexampleguns,bullets,medicalkits)unlessitemsarereducedor\nsetasFalseinyourplan,sothereisnoneedtokeepthem,onlytochoosewhenmakingchanges.\n5.Donotplanandadjustthestatesofteammatesandenemies,theycanmovefreelyandcannotbecontrolled.\n6.Avoidconflictsofstatesplaning. Forexample,agentunabletomovequicklywhenlyingdown,andunableto\nseeenemieswhenlengthofdistancefromagenttoenemyisfaraway."
  },
  {
    "id": 445,
    "text": "6.Avoidconflictsofstatesplaning. Forexample,agentunabletomovequicklywhenlyingdown,andunableto\nseeenemieswhenlengthofdistancefromagenttoenemyisfaraway.\n7.Avoidtherepetitionofstatesplaning. Forexample,iftheAveragevelocityhasbeenadjustedtobeFast,there\nisnoneedtoadjusttheWhetherpronepositiontoFalse,becausetheagentcanautomaticallyadjuststatetofit\noverlappingmeanings.\n8.Whenitisnecessarytorefertoenemyorteammateinformationforplaning,describethespecificstatevalue\nduringanalysis.\nTable10: RulepromptforGPT4.\nPrecision Recall F1 Accurate\nDataset Precision Recall F1 Accurate\n(Choice) (Choice) (Choice) (Choice)\nAttention 0.555 0.685 0.505 0.621 0.529 0.652 0.065 0.159\nMlp 0.549 0.664 0.482 0.587 0.514 0.620 0.065 0.134\nAll 0.529 0.642 0.471 0.581 0.485 0.596 0.069 0.119\nTable13: EvaluationonLoRAtarget.\nSymbol Sub-goalClass\ng1 Averagevelocity\ng2 Horizontaldirectionofmovement\ng3 Whetherseenenemy\ng4 Whetherholdagun\ng5 Whetherproneposition\ng6 Lengthofdistancemoved"
  },
  {
    "id": 446,
    "text": "Table13: EvaluationonLoRAtarget.\nSymbol Sub-goalClass\ng1 Averagevelocity\ng2 Horizontaldirectionofmovement\ng3 Whetherseenenemy\ng4 Whetherholdagun\ng5 Whetherproneposition\ng6 Lengthofdistancemoved\ng7 Lengthofdistancefromagenttoteammate\ng8 Distancewithnearestenemy\ng9 Whetherseenbyenemy\ng10 Damagetoenemy\ng11 Whetherhavebullets\ng12 Horizontaldirectionofview\ng13 Whetherfollowwiththemovementdirectionofteammate\ng14 Whethercrouchposition\ng15 Whetherhaveagun\ng16 Whetherhavemedicalkits\ng17 Whethertorestorehealth\ng18 Healthlevel\ng19 Whetherknockdownenemy\ng20 Whethertargetthesameenemyasteammate\nTable14: Top20sub-goalsrankedbyfrequency.\n24"
  },
  {
    "id": 447,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\nTraining Precision Recall F1\nDatasetSize Precision Recall F1\nMethod (Choice) (Choice) (Choice)\nCoFT 51.85% 64.84% 46.68% 57.91% 49.13% 61.18%\nCoFT→SFT 55.48% 68.52% 50.48% 62.10% 52.86% 65.15%\n100%\nSFT 54.70% 65.20% 49.00% 60.20% 51.70% 63.20%\nImproveRate 1.42% 5.09% 3.02% 3.16% 2.25% 3.09%\nCoFT 49.43% 62.66% 44.45% 56.29% 46.81% 59.30%\nCoFT→SFT 49.92% 62.59% 45.51% 57.65% 47.61% 60.02%\n30%\nSFT 46.12% 60.96% 33.68% 45.39% 38.93% 52.03%\nImproveRate 8.25% 2.68% 35.11% 27.01% 22.30% 15.34%\nCoFT 45.58% 60.84% 41.77% 53.92% 43.59% 57.17%\nCoFT→SFT 48.06% 61.01% 43.15% 54.31% 45.47% 57.47%\n10%\nSFT 42.08% 55.31% 30.86% 41.45% 35.61% 47.39%\nImproveRate 14.20% 10.32% 39.80% 31.03% 27.69% 21.28%\nCoFT 39.42% 58.35% 34.28% 50.10% 36.67% 53.91%\nCoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%\n3%\nSFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%\nImproveRate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%"
  },
  {
    "id": 448,
    "text": "CoFT→SFT 41.61% 60.40% 34.55% 50.32% 37.75% 54.90%\n3%\nSFT 17.66% 38.28% 13.33% 29.47% 15.20% 33.30%\nImproveRate 135.52% 57.78% 159.15% 70.79% 148.43% 64.88%\nTable15: Languagemodelperformanceevaluationwithdifferentsizesoffine-tuningtrainingset. Theunderlined“Improve\nRate”valuesrepresenttheimprovementpercentageofthe“CoFT→SFT”methodrelativeto“SFT”method.\n25"
  },
  {
    "id": 449,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\nInordertocompletethecommand‘Youshouldlieinwait’,letusplanthestatesoftheagentstepbystepusing\nthefollowingtemplate:\n1. Analyzetheverbalordersofteammatesandplayers,whatdoyouwanttodo? Accordingtothecommand,\nalsoanalysistherelevantstatesofteammatesandenemiesthatneedattention.\nTheverbalcommandoftheteammateplayeris[Command],whichmeansteammateplayerwantstheagent...\n2. Analyzewhichstatesoftheagentsaremostrelevanttotheverbalcommandsofteammateplayer. Theagents\nintheunselectedstateswilladjustthemselvestocompleteyourplan(analyzethereasonfirst,thenselectkey\nstatesonebyoneasfewaspossibleandasimportantaspossibleaccordingtothedegreeofimportance)?\nAccordingtotheteammate’scommand:\n2.1. [Reason1]: [State1]\n2.2. [Reason2]: [State2]\n...\n3. Planhowthesekeystatesneedtobeadjusted(analyzethereasonfirst,andthenmakeadjustmentsonestate\nbyonestate,thestatecanbechangedorremainthesame,andmustbeselectedfromthevaluerangeofthe"
  },
  {
    "id": 450,
    "text": "...\n3. Planhowthesekeystatesneedtobeadjusted(analyzethereasonfirst,andthenmakeadjustmentsonestate\nbyonestate,thestatecanbechangedorremainthesame,andmustbeselectedfromthevaluerangeofthe\ngamestate[Choice1,Choice2,...])?\nAccordingtotheteammate’scommand:\n3.1. [State1]: [Reason1]: [Current_value1]->[Target_value2]\n3.2. [State2]: [Reason2]: [Current_value1]->[Target_value2]\n...\n4. Modifytheadjustmentthatmaybewrong,andrefertotheRulestoanalyzewhichstateadjustmentsmay\nconflict,repeatorbeunnecessary,andoutputthemodifiedadjustmentplan: Accordingtothestatesadjustments\nin3...\n4.1. [State1]: [Current_value1]->[Target_value2]\n4.2. [State2]: [Current_value1]->[Target_value2]\n...\n5. Accordingtotheanalyzeandtheplaningoftheverbalcommand,furtheranalyzethebehaviortendency\nrequiredintheadjustmentprocess(theproportionofMobile,Offense,Waiting,Supplies,Scouting,firstanalyze\nthereason,andthencalculatethepercentage)\nMobile: [Reason1]: [Percent1]\nOffense: [Reason2]: [Percent2]\nWaiting: [Reason3]: [Percent3]"
  },
  {
    "id": 451,
    "text": "thereason,andthencalculatethepercentage)\nMobile: [Reason1]: [Percent1]\nOffense: [Reason2]: [Percent2]\nWaiting: [Reason3]: [Percent3]\nSupplies: [Reason4]: [Percent4]\nScouting: [Reason5]: [Percent5]\n6. Analyzehowlongthecurrentcommandneedstobekept(forexample,thecommandof‘killingtheenemy’\nneedstobekeptfora‘shortterm’,andthecommandof‘payattentiontoreconnaissance’needstobekeptfora\n‘longterm’. Firstanalyzethereasonandthenmakeajudgment).\nAccordingtothecommandoftheteammate,[Analysis]:Thecurrentcommandneedstobekeptby‘[XXterm]’.\nIfyouseephraseslike[Context]inanswertemplate,replacetheentirephraseaccordingtothemeaningofthe\nContext,donotrepeatthecontent;makeanalogyexpansionfor‘...’;keep‘:’;absolutelydonotmodifyothersin\ntemplate.\nTable16: ChainofthoughtpromptforGPT4.\n26"
  },
  {
    "id": 452,
    "text": "BuildingOpen-EndedEmbodiedAgentviaLanguage-PolicyBidirectionalAdaptation\nWehaveanagentandaplayerworkingtogetherasateammateina\nsystem\nPUBGgame. Wehopeyoucanhelptheagentplanhowtheagent’s\nbackground\ngamestateshouldchange,soastocompletetheplayer’scommandand\nprompt\nhelptheplayerwinthegame.\nprompt Thestateoftheagent’steammatescanbedescribedasfollows:{‘\nLengthofdistancemoved’: ‘Nomovement’,‘Averagevelocity’: ‘Slow’,\n‘Horizontaldirectionofmovement’: ‘Southeast’,‘Horizontal\ndirectionofview’: ‘South’,‘Pitchdirectionofview’: ‘Medium’,\n‘Healthlevel’: ‘Empty’,‘Whethertorestorehealth’: ‘False’,\n‘Whetherthehealthisdamaged’: ‘False’,‘Whetherrescuedteammate’:\nteammate\n‘False’,‘Whetherproneposition’: ‘False’,‘Whethercrouch\nstate\nposition’: ‘False’,‘Whetherhaveagun’: ‘True’,‘Whetherholda\nprompt\ngun’: ‘False’,‘Whetherhavebullets’: ‘True’,‘Whetherhavemedical\nkits’: ‘True’,‘Whetherbeknockeddown’: ‘False’,‘Damagetoenemy’:\n‘Zero’,‘Whetherknockdownenemy’: ‘False’,‘Whetherseenenemy’:"
  },
  {
    "id": 453,
    "text": "prompt\ngun’: ‘False’,‘Whetherhavebullets’: ‘True’,‘Whetherhavemedical\nkits’: ‘True’,‘Whetherbeknockeddown’: ‘False’,‘Damagetoenemy’:\n‘Zero’,‘Whetherknockdownenemy’: ‘False’,‘Whetherseenenemy’:\n‘True’,‘Numberofenemieshaveeverseen’: 5,‘Whetherseenby\nenemy’: ‘True’,‘Distancewithnearestenemy’: ‘Nearby’,‘Whether\ncloserwithnearestenemy’: ‘False’,‘IDofteammateplayer’: 2}\nenemy Thestateoftheenemycanbedescribedasfollows:{‘Horizontal\nstate directionofmovementofenemy’: ‘Southwest’,‘Velocityofenemy’:\nprompt ‘Slow’,‘Enemy’spositionrelativetoagent’: ‘West’}\nThestateoftheagentcanbedescribedasfollows:{‘Damageto\nenemy’: ‘Zero’,‘Whetherknockdownenemy’: ‘False’,‘Whetherkill\nenemy’: ‘False’,‘Whetherseenenemy’: ‘True’,‘Whetherseenby\nenemy’: ‘True’,‘Numberofenemieshaveeverseen’: 3,‘Lengthof\ndistancemoved’: ‘Short’,‘Averagevelocity’: ‘Fast’,‘Horizontal\ndirectionofmovement’: ‘West’,‘Horizontaldirectionofview’:\n‘NorthEast’,‘Pitchdirectionofview’: ‘Medium’,‘Healthlevel’:"
  },
  {
    "id": 454,
    "text": "distancemoved’: ‘Short’,‘Averagevelocity’: ‘Fast’,‘Horizontal\ndirectionofmovement’: ‘West’,‘Horizontaldirectionofview’:\n‘NorthEast’,‘Pitchdirectionofview’: ‘Medium’,‘Healthlevel’:\n‘Full’,‘Whethertorestorehealth’: ‘False’,‘Whetherthehealthis\nself damaged’: ‘False’,‘Whetherrescuedteammate’: ‘False’,‘Whetherbe\nstate knockeddown’: ‘False’,‘Whetherproneposition’: ‘False’,‘Whether\nprompt haveagun’: ‘True’,‘Whetherhavebullets’: ‘True’,‘Whetherhave\nmedicalkits’: ‘True’,‘Distancewithnearestenemy’: ‘Nearby’,\n‘Whethercloserwithnearestenemy’: ‘True’,‘Whethercrouch\nposition’: ‘False’,‘Whetherholdagun’: ‘True’,‘Whetherseenby\nteammate’: ‘True’,‘Lengthofdistancefromagenttoteammate’:\n‘Touch’,‘Teammate’spositionrelativetoagent’: ‘Southwest’,\n‘Whetherfollowwiththeviewsofteammate’: ‘False’,‘Whether\ntargetthesameenemyasteammate’: ‘False’,‘Whetherfollowwith\nthemovementdirectionofteammate’: ‘False’}\ncommand Teammateplayergivesyouaverbalcommandbasedonthecurrent\nprompt gamestates:‘Youshouldlieinwait.’"
  },
  {
    "id": 455,
    "text": "themovementdirectionofteammate’: ‘False’}\ncommand Teammateplayergivesyouaverbalcommandbasedonthecurrent\nprompt gamestates:‘Youshouldlieinwait.’\nquestion Inordertocompletethecommand‘Youshouldlieinwait.’,how\nprompt theagent’sgamestateshouldchange?\nWhetherproneposition:True\ngoal\nAveragevelocity:Static\nresponse meta-state\nLengthofdistancemoved:Nomovement\nprompt\nWhetherholdagun:True\nTable17: Exampleofpromptandresponse.\n27"
  },
  {
    "id": 456,
    "text": "Yanagisawa, H. & Honda, S.\nModeling arousal potential of epistemic emotions using Bayesian\ninformation gain: Inquiry cycle driven by free energy fluctuations\nHideyoshi Yanagisawa*a, Shimon Hondab\na The University of Tokyo, 7-3-1 Bunkyo, Hongo, Tokyo, 113-8656, JAPAN. hide@mech.t.u-tokyo.ac.jp\nb The University of Tokyo, 7-3-1 Bunkyo, Hongo, Tokyo, 113-8656, JAPAN. hondar8@g.ecc.u-tokyo.ac.jp\n*Corresponding author\nAbstract\nEpistemic emotions, such as curiosity and interest, drive the inquiry process. This study proposes a novel\nformulation of epistemic emotions such as curiosity and interest using two types of information gain\ngenerated by the principle of free energy minimization: Kullback–Leibler divergence (KLD) from\nBayesian posterior to prior, which represents free energy reduction in recognition, and Bayesian surprise\n(BS), which represents the expected information gain by Bayesian prior update. By applying a Gaussian"
  },
  {
    "id": 457,
    "text": "(BS), which represents the expected information gain by Bayesian prior update. By applying a Gaussian\ngenerative model with an additional uniform likelihood, we found that KLD and BS form an upward-\nconvex function of surprise (minimized free energy and prediction error), similar to Berlyne’s arousal\npotential functions, or the Wundt curve. We consider that the alternate maximization of BS and KLD\ngenerates an ideal inquiry cycle to approach the optimal arousal level with fluctuations in surprise, and\nthat curiosity and interest drive to facilitate the cyclic process. We exhaustively analyzed the effects of\nprediction uncertainty (prior variance) and observation uncertainty (likelihood variance) on the peaks of\nthe information gain function as optimal surprises. The results show that greater prediction uncertainty,\nmeaning an open-minded attitude, and less observational uncertainty, meaning precise observation with"
  },
  {
    "id": 458,
    "text": "meaning an open-minded attitude, and less observational uncertainty, meaning precise observation with\nattention, are expected to provide greater information gains through a greater range of exploration. The\nproposed mathematical framework unifies the free energy principle of the brain and the arousal potential\ntheory to explain the Wundt curve as an information gain function and suggests an ideal inquiry process\ndriven by epistemic emotions.\nKeywords: Emotion, free energy, Bayes, arousal, curiosity, inquiry.\n1. Introduction\nInquiry is an essential cognitive process in human activities such as scientific research, creation, and\neducation. American philosopher Charles Sanders Peirce defines inquiry as a cycle of three inferences:\nabduction, deduction, and induction (Peirce, 1974). In the observation of surprising phenomena,\nabduction infers a possible cause of the observation, deduction predicts unknown effects based on the"
  },
  {
    "id": 459,
    "text": "abduction, deduction, and induction (Peirce, 1974). In the observation of surprising phenomena,\nabduction infers a possible cause of the observation, deduction predicts unknown effects based on the\ninferred cause, and induction tests the prediction and updates the causal knowledge. A voluntary inquiry\n1"
  },
  {
    "id": 460,
    "text": "Yanagisawa, H. & Honda, S.\nprocess is facilitated by epistemic emotions such as surprise, curiosity, interest, and confusion (Kashdan\n& Silvia, 2009; Vogl, Pekrun, Murayama, & Loderer, 2020). Psychologist Berlyne defined two types of\nepistemic curiosity: diversive and specific (Berlyne, 1966; Silvia, 2012). Diversive curiosity seeks\nnovelty, and thus, in this type of curiosity, surprise triggers abductive reasoning. On the other hand,\nspecific curiosity drives induction, which seeks evidence of deductive reasoning to resolve confusion.\nEmotions are generally mapped to a dimensional space (Lang, 1995; Russell, 1980). The most\ncommonly used dimensions are arousal and valence, termed the core affect (Russell, 2003). Arousal is the\nintensity of emotions, whereas valence is the dimension of the positive and negative poles. A recent\nfunctional magnetic resonance imaging (fMRI) study showed that arousal and valence are correlated with"
  },
  {
    "id": 461,
    "text": "functional magnetic resonance imaging (fMRI) study showed that arousal and valence are correlated with\nneural activity in the orbitofrontal cortex and amygdala, respectively (Wilson-Mendenhall, Barrett, &\nBarsalou, 2013). The emotional dimensions are not independent, and arousal affects valence. Berlyne’s\narousal potential theory suggests that an appropriate level of arousal potential induces a positive hedonic\nresponse, whereas extreme arousal induces a negative response (Berlyne, 1960). Thus, valence forms an\ninverse-U-shaped function of the arousal potential, termed the Wundt curve (Fig. 1). Berlyne suggests that\nepistemic curiosity approaches the optimal arousal potential, where the hedonic response (or positive\nvalence) is maximized (Berlyne, 1960, 1966; Silvia, 2012).\nBerlyne also illustrated a number of arousal potential factors such as novelty, complexity, and\nuncertainty (Berlyne, 1960). Yanagisawa mathematically explains that the free energy, which is"
  },
  {
    "id": 462,
    "text": "Berlyne also illustrated a number of arousal potential factors such as novelty, complexity, and\nuncertainty (Berlyne, 1960). Yanagisawa mathematically explains that the free energy, which is\ninformation on the brain’s prediction error or surprise (Friston, Kilner, & Harrison, 2006), represents the\narousal potential because free energy is decomposed into information quantity terms representing\nperceived novelty, complexity, and uncertainty (Yanagisawa, 2021). This free-energy arousal model\nsuggests that an appropriate level of free energy or surprise induces a positive emotional valence based on\nBerlyne’s Wundt curve, which is supported by experimental evidence (Honda, Yanagisawa, & Kato,\n2022; Sasaki, Kato, & Yanagisawa, 2023).\nBy contrast, the free energy principle (FEP) (Friston et al., 2006), known as the unified brain\ntheory (Friston, 2010), suggests that the brain must minimize its free energy during perception and action."
  },
  {
    "id": 463,
    "text": "theory (Friston, 2010), suggests that the brain must minimize its free energy during perception and action.\nPrevious studies have proposed that decreasing and increasing free energy (or expected free energy)\ncorrespond to positive and negative valence, respectively (Clark, Watson, & Friston, 2018; Hesp et al.,\n2021; Joffily & Coricelli, 2013; Seth & Friston, 2016; Wager et al., 2015; Yanagisawa, Wu, Ueda, &\nKato, 2023), and that high and low free energies indicate uncertain and certain states, respectively.\nReducing free energy resolves uncertainty and produces positive emotions.\nThe FEP argument that minimizing free energy corresponds to a positive valence seems to\ncontradict the argument of arousal potential theory that an appropriate level of arousal potential\n(represented by free energy (Yanagisawa, 2021)) maximizes positive valence. To resolve this\ncontradiction and integrate the FEP-based valence and arousal potential theories, we propose a novel"
  },
  {
    "id": 464,
    "text": "(represented by free energy (Yanagisawa, 2021)) maximizes positive valence. To resolve this\ncontradiction and integrate the FEP-based valence and arousal potential theories, we propose a novel\nvalence framework based on the theory that a decrement in free energy and its expectation explain the\n2"
  },
  {
    "id": 465,
    "text": "Yanagisawa, H. & Honda, S.\nvalence of epistemic emotions. A decrease in free energy represents information gain and an epistemic\nvalue (Friston et al., 2017; Parr, Pezzulo, & Friston, 2022). The more information gain (epistemic value)\none obtains or expects, the more positive the valence one experiences.\nBased on this framework, we formulated emotion valence functions of the arousal potential\nusing decrements in free energy (or information gains). By applying a Gaussian generative model with an\nadditional uniform likelihood, we demonstrated that the epistemic valence function forms an inverse-U\nshape and analyzed the effects of prediction error and uncertainties on the peaks of the valence functions.\nWe associated epistemic emotions such as curiosity and interest with the free-energy-based valence\nmodel. Furthermore, we proposed an inquiry cycle model based on free-energy-based epistemic emotions.\nPositive\nArousal\n(Surprise)\nNegative"
  },
  {
    "id": 466,
    "text": "model. Furthermore, we proposed an inquiry cycle model based on free-energy-based epistemic emotions.\nPositive\nArousal\n(Surprise)\nNegative\nFig. 1 Arousal potential function, or Wundt curve. Appropriate level of arousal maximizes positive\nemotion valence (optimal arousal level).\n2. Method\n2.1 Free energy formulations\nFEP suggests that the brain must minimize its free energy through recognition, action, and learning\n(Friston et al., 2006). Assume an agent recognizes a hidden state 𝑠 as a cause of an observation 𝑜 given\nby an action based on a policy 𝜋. We assume that the agent has a generative model 𝑝(cid:4666)𝑠,𝑜|𝜋(cid:4667) as its\nknowledge about the probabilistic relationship between hidden states and observation and a recognition\ndensity 𝑞(cid:4666)𝑠|𝜋(cid:4667) of hidden states for a given policy. The free energy of a policy 𝜋 is defined as a function\nof an observation representing the difference between a recognition density and a generative model"
  },
  {
    "id": 467,
    "text": "of an observation representing the difference between a recognition density and a generative model\naveraged by the recognition density in terms of their energies (negative log probability).\n𝐹 (cid:3095) (cid:3404)⟨ln𝑞(cid:4666)𝑠|𝜋(cid:4667)(cid:3398)ln𝑝(cid:4666)𝑠,𝑜|𝜋(cid:4667)⟩ (cid:3044)(cid:4666)𝑠|𝜋(cid:4667) (1)\nThe free energy represents the prediction error of recognition from the knowledge, i.e., the generative\nmodel. It refers to uncertainty and the prediction error of signals in a Bayesian brain theory (Knill &\nPouget, 2004). The first and second terms on the right-hand side denote the negative-state entropy and\n3\necnelaV\nOptimal arousal level\nWundt curve\nFamiliar, simple Novel, complex"
  },
  {
    "id": 468,
    "text": "Yanagisawa, H. & Honda, S.\ninternal energy, respectively. Thus, the definition corresponds to the Helmholtz free energy when the\ntemperature is one.\nWith the definition of conditional probability, the generative model is factorized into true\nposterior and evidence: 𝑝(cid:4666)𝑠,𝑜|𝜋(cid:4667)(cid:3404)𝑝(cid:4666)𝑠|𝑜,𝜋(cid:4667)𝑝(cid:4666)𝑜|𝜋(cid:4667). With this factorization, the free energy is expanded\nto the summation of a Kullback–Leibler (KL) divergence and Shannon surprise (hereafter referred to as\nsurprise).\n𝐹 (cid:3404)𝐷 (cid:4670)𝑞(cid:4666)𝑠|𝜋(cid:4667)||𝑝(cid:4666)𝑠|𝑜,𝜋(cid:4667)(cid:4671)(cid:3398)ln𝑝(cid:4666)𝑜|𝜋(cid:4667) (2)\n(cid:3095) (cid:3012)(cid:3013)\nThe first-term KL divergence forms the true posterior to the recognition density, which represents a\nstatistical difference between the two distributions: 𝐷 (cid:4670)𝑞(cid:4666)𝑠|𝜋(cid:4667)||𝑝(cid:4666)𝑠|𝑜,𝜋(cid:4667)(cid:4671)(cid:3404)〈ln𝑞(cid:4666)𝑠|𝜋(cid:4667)(cid:3398)\n(cid:3012)(cid:3013)"
  },
  {
    "id": 469,
    "text": "statistical difference between the two distributions: 𝐷 (cid:4670)𝑞(cid:4666)𝑠|𝜋(cid:4667)||𝑝(cid:4666)𝑠|𝑜,𝜋(cid:4667)(cid:4671)(cid:3404)〈ln𝑞(cid:4666)𝑠|𝜋(cid:4667)(cid:3398)\n(cid:3012)(cid:3013)\nln𝑝(cid:4666)𝑠|𝑜,𝜋(cid:4667)〉 (cid:3044)(cid:4666)𝑠|𝜋(cid:4667) . When the recognition approximates the true posterior to minimize free energy, the KL\ndivergence becomes zero, and the free energy is approximated to the second term, i.e., surprise. Thus, the\nlower bound of free energy is surprise. Surprise is a negative log of the model evidence, 𝑝(cid:4666)𝑜|𝜋(cid:4667), and\nrefers to the information content used to process given observations, representing cognitive load\n(Yanagisawa, 2021).\nThe generative model is decomposed to a state prior 𝑝(cid:4666)𝑠|𝜋(cid:4667) for a given policy and a likelihood\nfunction 𝑝(cid:4666)𝑜|𝑠(cid:4667).\n𝑝(cid:4666)𝑠,𝑜|𝜋(cid:4667)(cid:3404)𝑝(cid:4666)𝑠|𝜋(cid:4667)𝑝(cid:4666)𝑜|𝑠(cid:4667) (2)"
  },
  {
    "id": 470,
    "text": "function 𝑝(cid:4666)𝑜|𝑠(cid:4667).\n𝑝(cid:4666)𝑠,𝑜|𝜋(cid:4667)(cid:3404)𝑝(cid:4666)𝑠|𝜋(cid:4667)𝑝(cid:4666)𝑜|𝑠(cid:4667) (2)\nWith this decomposition, the free energy is expanded to another two terms.\n𝐹 (cid:3095) (cid:3404)𝐷 (cid:3012)(cid:3013) (cid:4670)𝑞(cid:4666)𝑠|𝜋(cid:4667)||𝑝(cid:4666)𝑠|𝜋(cid:4667)(cid:4671)(cid:3398)〈ln𝑝(cid:4666)𝑜|𝑠(cid:4667)〉 (cid:3044)(cid:4666)𝑠|𝜋(cid:4667) (3)\nThe first term is a KL divergence of state prior from recognition. This term represents the complexity of\nthe generative model. The second term is the difference between likelihood and recognition. This term\nindicates negative model accuracy. Thus, minimizing the free energy signifies minimizing the complexity\nand maximizing the accuracy of the model.\n2.2 Information gain in recognition\nAssume that an initial recognition density before an action based on a policy 𝜋 is approximated to the\nstate prior. The initial free energy 𝐹 is a summation of KL divergence and surprise.\n(cid:3095)(cid:2868)"
  },
  {
    "id": 471,
    "text": "state prior. The initial free energy 𝐹 is a summation of KL divergence and surprise.\n(cid:3095)(cid:2868)\n𝐹 (cid:3095)(cid:2868) (cid:3404)〈ln𝑝(cid:4666)𝑠|𝜋(cid:4667)(cid:3398)𝑝(cid:4666)𝑠,𝑜|𝜋(cid:4667)〉 (cid:3043)(cid:4666)𝑠|𝜋(cid:4667) (cid:3404)𝐷 (cid:3012)(cid:3013) (cid:4670)𝑝(cid:4666)𝑠|𝜋(cid:4667)||𝑝(cid:4666)𝑠|𝑜,𝜋(cid:4667)(cid:4671)(cid:3398)ln𝑝(cid:4666)𝑜|𝜋(cid:4667) (4)\nAn agent receives an observation 𝑜 by the action based on the policy 𝜋. The recognition density\napproximates the true posterior by minimizing the free energy. The KL divergence becomes zero, and the\nfree energy decreases to the lower bound 𝐹 , corresponding to surprise.\n(cid:3095)(cid:3019)\n𝑞(cid:4666)𝑠|𝜋(cid:4667):𝑝(cid:4666)𝑠|𝜋(cid:4667)→𝑝(cid:4666)𝑠|𝑜,𝜋(cid:4667), 𝐹 (cid:3404)(cid:3398)ln𝑝(cid:4666)𝑜|𝜋(cid:4667) (5)\n(cid:3095)(cid:3019)\nThe decrease in free energy in the recognition process is equivalent to the KL divergence from the true"
  },
  {
    "id": 472,
    "text": "(cid:3095)(cid:3019)\nThe decrease in free energy in the recognition process is equivalent to the KL divergence from the true\nposterior to the initial recognition, 𝐾𝐿𝐷 . Herein, 𝐾𝐿𝐷 denotes the information gain from recognizing\n(cid:3095) (cid:3095)\nthe causal state of observations given by an action based on a policy 𝜋.\n𝛥𝐹 (cid:3404)𝐹 (cid:3398)𝐹 (cid:3404)𝐾𝐿𝐷 (cid:3404)𝐷 (cid:4670)𝑝(cid:4666)𝑠|𝜋(cid:4667)||𝑝(cid:4666)𝑠|𝑜,𝜋(cid:4667)(cid:4671) (6)\n(cid:3019) (cid:3095)(cid:2868) (cid:3095)(cid:3019) (cid:3095) (cid:3012)(cid:3013)\n4"
  },
  {
    "id": 473,
    "text": "Yanagisawa, H. & Honda, S.\nA greater KLD indicates that the recognition of an observation under a policy provides greater\ninformation gain. Thus, KLD represents the epistemic value of recognizing an observation under a policy.\nThis suggests that an agent prefers to recognize observations with a greater KLD and is motivated to act\nbased on a policy that likely obtains such observations. Therefore, we infer that KLD increases positive\nvalence by increasing information gain (epistemic value) in recognition.\n2.3 Information gain expected from Bayesian updating prior belief: Bayesian surprise\nThe free energy minimized by a recognition, 𝐹 , approximates surprise. The minimized free energy\n(cid:3095)(cid:3019)\nequals a summation of complexity and inverse accuracy with a recognition approximated to the true\nposterior, 𝑞(cid:4666)𝑠|𝜋(cid:4667)(cid:3406)𝑝(cid:4666)𝑠|𝑜,𝜋(cid:4667).\n𝐹 (cid:3404)(cid:3398)ln𝑝(cid:4666)𝑜|𝜋(cid:4667)(cid:3404)𝐵𝑆 (cid:3397)𝑈 (7)"
  },
  {
    "id": 474,
    "text": "posterior, 𝑞(cid:4666)𝑠|𝜋(cid:4667)(cid:3406)𝑝(cid:4666)𝑠|𝑜,𝜋(cid:4667).\n𝐹 (cid:3404)(cid:3398)ln𝑝(cid:4666)𝑜|𝜋(cid:4667)(cid:3404)𝐵𝑆 (cid:3397)𝑈 (7)\n(cid:3095)(cid:3019) (cid:3095) (cid:3095)\n𝐵𝑆 (cid:3404)𝐷 (cid:4670)𝑝(cid:4666)𝑠|𝑜,𝜋(cid:4667)||𝑝(cid:4666)𝑠|𝜋(cid:4667)(cid:4671) (8)\n(cid:3095) (cid:3012)(cid:3013)\n𝑈 (cid:3095) (cid:3404)(cid:3398)〈ln𝑝(cid:4666)𝑜|𝑠(cid:4667)〉 (cid:3043)(cid:4666)𝑠|𝑜,𝜋(cid:4667) (9)\nThe complexity and inverse accuracy terms represent the Bayesian surprise 𝐵𝑆 and perceived\n(cid:3095)\nuncertainty U, respectively, and their summation (surprise) denotes the arousal potential (Yanagisawa,\n2021). The Bayesian surprise, 𝐵𝑆 , is a KL divergence from posterior to prior, i.e., the deviation of\n(cid:3095)\nrecognition from prior expectation. It represents the novelty of the recognized observation and is\ncorrelated with the surprise response to novel stimuli (Yanagisawa, Kawamata, & Ueda, 2019). The"
  },
  {
    "id": 475,
    "text": "recognition from prior expectation. It represents the novelty of the recognized observation and is\ncorrelated with the surprise response to novel stimuli (Yanagisawa, Kawamata, & Ueda, 2019). The\nsurprise response decreases with repeated exposure to the same novel stimuli. Such habituation is\nformulated as a decrease in BS in the Bayesian update of the prior (Ueda, Sekoguchi, & Yanagisawa,\n2021).\nBy repeatedly observing the same observation 𝑜 by an action under the same policy, the prior\nis updated by Bayesian updating such that the prior comes close to the true posterior, i.e.,\n𝑝(cid:4666)𝑠|𝜋(cid:4667):𝑝(cid:4666)𝑠|𝜋(cid:4667)→𝑝(cid:4666)𝑠|𝑜,𝜋(cid:4667). When the prior is updated to the posterior, 𝐵𝑆 is zero, and the free energy\n(cid:3095)\ndecreases to the inverse accuracy term. We refer to this term as uncertainty because it refers to the\nperceived uncertainty (Yanagisawa, 2021). Thus, the lower bound of free energy after the prior updating"
  },
  {
    "id": 476,
    "text": "perceived uncertainty (Yanagisawa, 2021). Thus, the lower bound of free energy after the prior updating\nis the uncertainty, 𝐹 (cid:3409)𝑈 , whereas the upper bound of the free energy decrease is the Bayesian\n(cid:3095)(cid:3013) (cid:3095)\nsurprise, 𝐵𝑆 .\n(cid:3095)\n𝛥𝐹 (cid:3404)𝐹 (cid:3398)𝐹 (cid:3409)𝐵𝑆 (10)\n(cid:3013) (cid:3095)(cid:3019) (cid:3095)(cid:3013) (cid:3095)\nHerein, 𝐵𝑆 is equivalent to the maximum information gain expected from the prior update based on\n(cid:3095)\nobserving a sufficient number of the same observations given under the same policy. A greater 𝐵𝑆\n(cid:3095)\ndenotes a greater information gain expected from the update with the action under the policy 𝜋. Thus, BS\nrepresents the expected epistemic value given by the model (prior) update or learning. This suggests that\nan agent prefers novel observations with a greater BS, which is expected to provide a chance to learn new"
  },
  {
    "id": 477,
    "text": "an agent prefers novel observations with a greater BS, which is expected to provide a chance to learn new\ninformation (update its own generative model), and that the agent is motivated to approach such novel\nobservations. Therefore, we infer that BS increases emotional valence in anticipation of information gain\n5"
  },
  {
    "id": 478,
    "text": "Yanagisawa, H. & Honda, S.\nfrom updating prior beliefs.\n2.4 Linking free energy reduction, information gain, and arousal potential\nFig. 2 summarizes the two-step free energy reduction and information gain. The free energy given an\nobservation 𝑜 decreases by 𝐾𝐿𝐷 as the first information gain when one succeeds in recognizing the state\nas a cause of the observation. The minimized free energy approximates surprise. The surprise is a\nsummation of 𝐵𝑆 and 𝑈. When one’s prior is updated to approximate the true posterior, the free energy\nis decreased by 𝐵𝑆, which is the expected second information gain.\nRecognition\nKL divergence\nBelief update\nBayesian surprise\nSurprise\nUncertainty\ntime\nFig. 2 Two-step free energy reduction and information gain. Decreases in free energy in recognition and\nbelief update correspond to KL divergence (KLD) and Bayesian surprise, respectively.\nThe upper bound of the total free energy reduction (or information gain) from recognizing and updating"
  },
  {
    "id": 479,
    "text": "belief update correspond to KL divergence (KLD) and Bayesian surprise, respectively.\nThe upper bound of the total free energy reduction (or information gain) from recognizing and updating\nstate beliefs, given an observation, is a summation of the two KL divergences, i.e., information gain.\n𝛥𝐹 (cid:3397)𝛥𝐹 (cid:3409) 𝐾𝐿𝐷 (cid:3397)𝐵𝑆 (cid:3404)𝐷 (cid:4670)𝑝(cid:4666)𝑠|𝜋(cid:4667)||𝑝(cid:4666)𝑠|𝑜,𝜋(cid:4667)(cid:4671)(cid:3397)𝐷 (cid:4670)𝑝(cid:4666)𝑠|𝑜,𝜋(cid:4667)||𝑝(cid:4666)𝑠|𝜋(cid:4667)(cid:4671)≡𝐼𝐺\n(cid:3019) (cid:3013) (cid:3095) (cid:3095) (cid:3012)(cid:3013) (cid:3012)(cid:3013)\n(11)\nWe consider that the total information gain represents the epistemic values that explain the emotional\nvalence of the arousal potential.\nThe two types of KL divergence denote the difference between the prior and posterior. When\nthe posterior given an observation is the same as the prior, the KL divergences are zero, and the"
  },
  {
    "id": 480,
    "text": "The two types of KL divergence denote the difference between the prior and posterior. When\nthe posterior given an observation is the same as the prior, the KL divergences are zero, and the\nobservation provides minimum free energy and minimum surprise (or maximum evidence). Hence, an\nobservation that provides minimal free energy does not provide any KL divergence or information gain.\nTo provide epistemic value with an emotional valence, given information gain, a certain level of surprise\nrepresenting arousal potential (Yanagisawa, 2021) is required by observing unexpected outcomes that\ngive certain KL divergences. However, if the likelihood of an observation is far from the prior\n6\nygrene\neerF\nTotal information gain"
  },
  {
    "id": 481,
    "text": "Yanagisawa, H. & Honda, S.\ndistribution, where the likelihood does not provide any information, the posterior is not updated from the\nprior. In this case, the KL divergences are zero, and the observation does not provide any information.\nTherefore, we consider that an appropriate level of surprise maximizes the KL divergences (information\ngains) and that an appropriate level represents the optimal arousal potential that maximizes the positive\nvalence for its epistemic value.\nKL divergence is an asymmetric operation. Hence, although both KL divergences, 𝐾𝐿𝐷 and\n(cid:3095)\n𝐵𝑆 , denote differences between the prior and posterior, they are different from each other. This suggests\n(cid:3095)\nthat the two KL divergences as functions of surprise are different. KLD signifies information gain due to\nrecognition, whereas BS signifies information gain expected from updating prior beliefs. Namely, KLD is"
  },
  {
    "id": 482,
    "text": "recognition, whereas BS signifies information gain expected from updating prior beliefs. Namely, KLD is\nthe current, whereas BS is the future, information gain. This suggests that maximizing KLD and BS are\ndifferent strategies for approaching the optimal arousal level that maximizes the total epistemic value\nwith a positive valence.\n2.5 Analytical methodology\nWe modeled the two information gains, KLD and BS, as functions of surprise using a Gaussian-like\ngenerative model with a flat likelihood of uniform noise and demonstrated that the two functions, KLD\nand BS, form an inverse-U shape and have different peaks. Using the function model, we analyzed the\neffect of Gaussian parameters, the difference between the prior mean and likelihood peak as prediction\nerror (Yanagisawa, 2016), variance of prior as prediction uncertainty, and variance of likelihood as\nobservation uncertainty on the peaks of the information gain functions. From the analysis, we elucidated"
  },
  {
    "id": 483,
    "text": "observation uncertainty on the peaks of the information gain functions. From the analysis, we elucidated\nthe conditions for optimal prediction errors and uncertainties of prediction and observation to maximize\nthe information gains in an ideal inquiry process.\n3. Results\n3.1 Gaussian model of information gains\nThe Gaussian Bayesian model has been used in past research studies to analyze the characteristics of free\nenergy and Bayesian surprise (Buckley, Kim, McGregor, & Seth, 2017; Yanagisawa et al., 2023;\nYanagisawa, 2021). The Laplace approximation suggests that a Gaussian distribution is applied around\nthe mode of unknown distributions. The Gaussian form is useful for analyzing the effect of interpretable\nand independently manipulatable parameters on free energy and KL divergence. The distance between the\nprior mean and likelihood peak, 𝛿, represents prediction error; the variance of prior, 𝑠 , represents prior\n(cid:3043)"
  },
  {
    "id": 484,
    "text": "prior mean and likelihood peak, 𝛿, represents prediction error; the variance of prior, 𝑠 , represents prior\n(cid:3043)\nuncertainty; and the variance of likelihood, 𝑠 , represents observation uncertainty. The likelihood\n(cid:3013)\nfunction of n data randomly sampled from a source following a Gaussian distribution is\n𝑝(cid:4666)𝑜(cid:3041)|𝑠(cid:4667)(cid:3404)(cid:3436) (cid:2869) (cid:3440) (cid:3041) exp(cid:4674) (cid:2879)(cid:4666)(cid:3046)(cid:2879)(cid:3042)(cid:3364)(cid:4667)(cid:3118)(cid:2878)(cid:3041)(cid:3023) (cid:4675), (12)\n(cid:3493)(cid:2870)(cid:3095)(cid:3046)(cid:3261) (cid:2870)(cid:3046)(cid:3261)\nwhere 𝑜̅ and 𝑉 denote the mean and variance of the observed data, respectively. With a Gaussian prior\ndistribution 𝑝(cid:4666)𝑠(cid:4667)(cid:3404)𝑁(cid:3435)𝜂,𝑠 (cid:3439)≡𝑁 , the posterior distribution is also of a Gaussian form.\n(cid:3043) (cid:3043)(cid:3045)(cid:3036)\n7"
  },
  {
    "id": 485,
    "text": "Yanagisawa, H. & Honda, S.\n𝑝(cid:4666)𝑠|𝑜(cid:3041)(cid:4667)(cid:3404)\n(cid:3043)(cid:3435)𝑜(cid:3041) (cid:3627)𝑠(cid:3439)(cid:3043)(cid:4666)(cid:3046)(cid:4667)\n(cid:3404)𝑁(cid:3435)𝜂 ,𝑠 (cid:3439)≡𝑁 , (13)\n(cid:3043)(cid:4666)(cid:3042)(cid:3289)(cid:4667) (cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3043)(cid:3042)(cid:3046)(cid:3047)\nwhere 𝜂 (cid:3404)\n(cid:3041)(cid:3046)(cid:3291)(cid:3042)(cid:3364)(cid:2878)(cid:3046)(cid:3261) (cid:3086)\nand 𝜂 (cid:3404)\n(cid:3041)(cid:3046)(cid:3291)(cid:3042)(cid:3364)(cid:2878)(cid:3046)(cid:3261) (cid:3086)\n. The evidence 𝑝(cid:4666)𝑜(cid:4667) is a marginal likelihood:\n(cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3041)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261) (cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3041)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)\n(cid:3041)"
  },
  {
    "id": 486,
    "text": "(cid:3041)\n𝑝(cid:4666)𝑜(cid:3041)(cid:4667)(cid:3404)(cid:1516) (cid:2998) 𝑝(cid:4666)𝑜(cid:3041)|𝑠(cid:4667)𝑝(cid:4666)𝑠(cid:4667)𝑑𝑠 (cid:3404)(cid:3495) (cid:3046)(cid:3261) (cid:3436) (cid:2869) (cid:3440) exp(cid:3428)(cid:3398) (cid:3041) 𝛿(cid:2870)(cid:3398) (cid:3041)(cid:3435)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)(cid:3439) 𝑉(cid:3432) ≡𝑒(cid:4666)𝛿(cid:4667)\n(cid:2879)(cid:2998) (cid:3041)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261) (cid:3493)(cid:2870)(cid:3095)(cid:3046)(cid:3261) (cid:2870)(cid:3435)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)(cid:3439) (cid:2870)(cid:3046)(cid:3261)(cid:3435)(cid:3041)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)(cid:3439)\n(14)\nwhere 𝛿 (cid:3404) 𝜂(cid:3398)𝑜̅ is a prediction error. The evidence is an inverse exponential function of the square of\nthe prediction error. Hence, we describe the Gaussian evidence as 𝑒(cid:4666)𝛿(cid:4667). The evidence exponentially"
  },
  {
    "id": 487,
    "text": "the prediction error. Hence, we describe the Gaussian evidence as 𝑒(cid:4666)𝛿(cid:4667). The evidence exponentially\ndecreases as the prediction error increases. Surprise, the lower bound of free energy, is a negative log\nfunction of evidence, i.e., (cid:3398)log𝑝(cid:4666)𝑜(cid:3041)(cid:4667)(cid:3404)(cid:3398)log𝑒(cid:4666)𝛿(cid:4667). Thus, the surprise is a quadratic function of a\nprediction error.\nThe free energy of n observations randomly obtained from a stimulus source following a\nGaussian distribution of variance 𝑠 is formed as a quadratic function of the prediction error with\n(cid:3013)\ncoefficients of variance (for the derivation, see (Yanagisawa, 2021)):\n𝐹 (cid:3404)𝐴 𝛿(cid:2870)(cid:3397)𝐵 , (15)\n(cid:3007) (cid:3007)\n(cid:2869) (cid:3041) (cid:2869)\nwhere the coefficients are functions of uncertainties, i.e., 𝐴 (cid:3404) and 𝐵 (cid:3404) (cid:3419)ln(cid:3435)𝑛𝑠 (cid:3397)𝑠(cid:3439) (cid:3397)"
  },
  {
    "id": 488,
    "text": "(cid:2869) (cid:3041) (cid:2869)\nwhere the coefficients are functions of uncertainties, i.e., 𝐴 (cid:3404) and 𝐵 (cid:3404) (cid:3419)ln(cid:3435)𝑛𝑠 (cid:3397)𝑠(cid:3439) (cid:3397)\n(cid:3007) (cid:2870)(cid:3041)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3287) (cid:3007) (cid:2870) (cid:3043) (cid:3039)\n(cid:4666)𝑛(cid:3398)1(cid:4667)ln𝑠 (cid:3397)𝑛ln2𝜋(cid:3397)𝑛𝑉/𝑠(cid:3423). To simplify further analysis, we consider the case of a single data\n(cid:3039) (cid:3039)\n(cid:2869) (cid:2869)\nobservation, n=1. When n=1, the coefficients A and B are simplified as 𝐴 (cid:3404) and 𝐵 (cid:3404)\nF F (cid:3007) (cid:2870)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3287) (cid:3007)\n(cid:2869)\n(cid:3419)ln(cid:3435)𝑠 (cid:3397)𝑠(cid:3439) (cid:3397)ln2𝜋(cid:3423).\n(cid:3043) (cid:3039)\n(cid:2870)\nThe gradient 𝐴 is the inverse of the sum of two variances. Thus, both uncertainties increase\n(cid:3007)\nthe sensitivity of the free energy to prediction error."
  },
  {
    "id": 489,
    "text": "(cid:3043) (cid:3039)\n(cid:2870)\nThe gradient 𝐴 is the inverse of the sum of two variances. Thus, both uncertainties increase\n(cid:3007)\nthe sensitivity of the free energy to prediction error.\nUsing the same Gaussian model with a single data observation, we derive the information\ngains, KLD and BS, as quadratic functions of the prediction error with the coefficients of variance:\n𝐾𝐿𝐷 (cid:3404)𝐷 (cid:4666)𝑝(cid:4666)𝑠(cid:4667)||𝑝(cid:4666)𝑠|𝑜(cid:4667)(cid:4667)(cid:3404)𝐴 𝛿(cid:2870)(cid:3397)𝐵 , (16)\n(cid:3012)(cid:3013) (cid:3012)(cid:3013)(cid:3005) (cid:3012)(cid:3013)(cid:3005)\nwhere the coefficients are 𝐴 (cid:3404)\n(cid:3046)(cid:3291)\nand 𝐵 (cid:3404)(cid:3398)ln\n(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3287)(cid:3397) (cid:3046)(cid:3291)\n; and\n(cid:3012)(cid:3013)(cid:3005) (cid:2870)(cid:3046)(cid:3261)(cid:3435)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)(cid:3439) (cid:3012)(cid:3013)(cid:3005) (cid:3046)(cid:3261) (cid:3046)(cid:3261)"
  },
  {
    "id": 490,
    "text": "𝐵𝑆(cid:3404)𝐷 (cid:4666)𝑝(cid:4666)𝑠|𝑜(cid:4667)||𝑝(cid:4666)𝑠(cid:4667)(cid:4667)(cid:3404)𝐴 𝛿(cid:2870)(cid:3397)𝐵 , (17)\n(cid:3012)(cid:3013) (cid:3003)(cid:3020) (cid:3003)(cid:3020)\nwhere the coefficients are 𝐴 (cid:3404)\n(cid:3046)(cid:3291)\nand 𝐵 (cid:3404)ln\n(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3287)(cid:3398) (cid:3046)(cid:3291)\n.\n(cid:3003)(cid:3020) (cid:2870)(cid:3435)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)(cid:3439) (cid:3118) (cid:3003)(cid:3020) (cid:3046)(cid:3261) (cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3287)\nThe prediction error always increases both KLD and BS. We found that the observation variation 𝑠\n(cid:3013)\nalways increases the gradient of both KLD and BS because the partial derivatives of the gradients are\nalways negative, i.e.,\n(cid:3105)(cid:3002)(cid:3260)(cid:3261)(cid:3253)\n(cid:3407)0 and\n(cid:3105)(cid:3002)(cid:3251)(cid:3268)\n(cid:3407)0. This signifies that the lower the observation uncertainty"
  },
  {
    "id": 491,
    "text": "(cid:3105)(cid:3002)(cid:3260)(cid:3261)(cid:3253)\n(cid:3407)0 and\n(cid:3105)(cid:3002)(cid:3251)(cid:3268)\n(cid:3407)0. This signifies that the lower the observation uncertainty\n(cid:3105)(cid:3046)(cid:3261) (cid:3105)(cid:3046)(cid:3261)\n8"
  },
  {
    "id": 492,
    "text": "Yanagisawa, H. & Honda, S.\n(i.e., the more precise the observation), the more susceptible the information gains (KLD and BS) are to\nprediction errors.\nHowever, the effects of prediction uncertainty on the sensitivity of prediction errors are\ninversed between KLD and BS. We found that the prediction uncertainty increases sensitivity for KLD\nbut decreases it for BS because the partial derivatives are\n(cid:3105)(cid:3002)(cid:3260)(cid:3261)(cid:3253)\n(cid:3408)0 and\n(cid:3105)(cid:3002)(cid:3251)(cid:3268)\n(cid:3407)0. Thus, the lower the\n(cid:3105)(cid:3046)(cid:3291) (cid:3105)(cid:3046)(cid:3291)\nprediction uncertainty, the more susceptible the information gains in recognition, i.e., KLD, and the more\nsusceptible the information gains expected from Bayesian updating prior beliefs, i.e., BS.\nTo compare KLD and BS in the gradient of functions of prediction error, we derived the\ndifference and found that KLD is always greater than BS because the coefficients are always positive."
  },
  {
    "id": 493,
    "text": "To compare KLD and BS in the gradient of functions of prediction error, we derived the\ndifference and found that KLD is always greater than BS because the coefficients are always positive.\n𝐾𝐿𝐷(cid:3398)𝐵𝑆(cid:3404)𝐴 𝛿(cid:2870)(cid:3397)𝐵 (cid:3408)0, (18)\n(cid:3012)(cid:3013)(cid:3005)(cid:2879)(cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005)(cid:2879)(cid:3003)(cid:3020)\nwhere 𝐴 (cid:3404)\n(cid:3046)(cid:3291) (cid:3118)\n(cid:3408)0 and 𝐵 (cid:3404)\n(cid:3046)(cid:3291) (cid:3118)\n(cid:3408)0. Therefore, for any prediction error, the\n(cid:3012)(cid:3013)(cid:3005)(cid:2879)(cid:3003)(cid:3020) (cid:2870)(cid:3046)(cid:3261)(cid:3435)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)(cid:3439) (cid:3012)(cid:3013)(cid:3005)(cid:2879)(cid:3003)(cid:3020) (cid:3046)(cid:3261)(cid:3435)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)(cid:3439)\ninformation gain in recognition is greater than that expected from updating the prior."
  },
  {
    "id": 494,
    "text": "information gain in recognition is greater than that expected from updating the prior.\n3.2 Convexity of information gain function by considering uniform noise\nThe Gaussian model suggests that prediction error always increases information gain. This is because the\nlikelihood function is distributed over an infinite band and deviates from the prior distribution as the\nprediction error increases. However, the Laplace approximation is valid only around the mode, and there\nis no guarantee that the tail of the likelihood is infinitely distributed following a Gaussian distribution.\nThe rate-coding hypothesis suggests that the likelihood function is coded by the distribution of the firing\nrates of neurons. A single stimulation fires specific neural populations but not all neurons. Neurons that\ndo not receive external stimulation fire spontaneously (Raichle, 2006). The frequency of the spontaneous"
  },
  {
    "id": 495,
    "text": "do not receive external stimulation fire spontaneously (Raichle, 2006). The frequency of the spontaneous\nfiring activity of neurons is lower than the millisecond-order frequency of stimuli-driven neural activity\n(Destexhe, Rudolph, & Paré, 2003). We infer that such spontaneous firing activity is independent of the\nneural activity evoked by sensory observations and provides no information about the cause of sensory\nstimuli (observation). To represent the influence of such independent and spontaneous neural activity, we\nadded an independent uniformed likelihood with very small constant probability 𝜀 to the observation-\nbased likelihood (Jones, 2016).\n𝑝 (cid:4666)𝑜|𝑠(cid:4667)(cid:3404)𝑝(cid:4666)𝑜|𝑠(cid:4667)(cid:3397)𝜀 (19)\n(cid:3084)\n9"
  },
  {
    "id": 496,
    "text": "Yanagisawa, H. & Honda, S.\nPrior Posterior likelihood\n𝑝(cid:4666)𝑠(cid:4667) 𝑞(cid:4666)𝑠(cid:4667) (cid:3406) 𝑝(cid:4666)𝑠|𝑜(cid:4667) 𝑝(cid:4666)𝑜|𝑠(cid:4667)\n𝑠\n(cid:3039)\n𝑠\n𝜀 (cid:3043)\n𝑠\nPrediction error 𝛿\nFig. 3 Gaussian Bayesian model with uniform likelihood. 𝑠 : prior variance, 𝑠: Gaussian likelihood\n(cid:3043) (cid:3039)\nvariance, 𝛿: prediction error, and 𝜀: probability of uniform likelihood.\nThis uniform likelihood addition flattens the tail of the Gaussian likelihood function, as shown in Fig. 3.\nThe effect of the Gaussian tail becomes negligible as the prediction error increases. Therefore, we infer\nthat adding a uniform likelihood is the simplest modeling method to represent the likelihood of\nspontaneous neural activity and to ignore the effect of the Gaussian likelihood tail.\nThe evidence with the likelihood function is the Gaussian evidence and the constant\nprobability.\n(cid:2998)"
  },
  {
    "id": 497,
    "text": "spontaneous neural activity and to ignore the effect of the Gaussian likelihood tail.\nThe evidence with the likelihood function is the Gaussian evidence and the constant\nprobability.\n(cid:2998)\n𝑝 (cid:4666)𝑜(cid:4667)(cid:3404)(cid:1516) 𝑝 (cid:4666)𝑜|𝑠(cid:4667)𝑝(cid:4666)𝑠(cid:4667)𝑑𝑠(cid:3404)𝑒(cid:4666)𝛿(cid:4667)(cid:3397)𝜀 (20)\n(cid:3084) (cid:2879)(cid:2998) (cid:3084)\nNote that surprise increases monotonically with respect to the prediction error. We find that the posterior\ndistributions with the likelihood function form a weighted linear model of the Gaussian posterior and\nprior.\n𝑝 (cid:4666)𝑠|𝑜(cid:4667)(cid:3404)\n(cid:3043)(cid:4666)(cid:3046)(cid:4667)(cid:3043)(cid:3332)(cid:4666)𝑜|𝑠(cid:4667)\n(cid:3404)\n(cid:3032)(cid:4666)(cid:3083)(cid:4667)(cid:3015)(cid:3291)(cid:3290)(cid:3294)(cid:3295)(cid:2878)(cid:3084)(cid:3015)(cid:3291)(cid:3293)(cid:3284)\n(cid:3404)𝑤 𝑁 (cid:3397)𝑤 𝑁 , (21)"
  },
  {
    "id": 498,
    "text": "(cid:3404)\n(cid:3032)(cid:4666)(cid:3083)(cid:4667)(cid:3015)(cid:3291)(cid:3290)(cid:3294)(cid:3295)(cid:2878)(cid:3084)(cid:3015)(cid:3291)(cid:3293)(cid:3284)\n(cid:3404)𝑤 𝑁 (cid:3397)𝑤 𝑁 , (21)\n(cid:3084) (cid:3043)(cid:3332)(cid:4666)(cid:3042)(cid:4667) (cid:3032)(cid:4666)(cid:3083)(cid:4667)(cid:2878)(cid:3084) (cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3043)(cid:3045)(cid:3036) (cid:3043)(cid:3045)(cid:3036)\n(cid:3032)(cid:4666)(cid:3083)(cid:4667) (cid:3084)\nwhere 𝑤 (cid:3404) and 𝑤 (cid:3404) are the standardized linear weights. When the prediction error is\n(cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3032)(cid:4666)(cid:3083)(cid:4667)(cid:2878)(cid:3084) (cid:3043)(cid:3045)(cid:3036) (cid:3032)(cid:4666)(cid:3083)(cid:4667)(cid:2878)(cid:3084)\nsmall, the term 𝜀𝑁 is negligible because 𝜀 is very small compared to 𝑒(cid:4666)𝛿(cid:4667). In this case, the posterior\n(cid:3043)(cid:3045)(cid:3036)"
  },
  {
    "id": 499,
    "text": "small, the term 𝜀𝑁 is negligible because 𝜀 is very small compared to 𝑒(cid:4666)𝛿(cid:4667). In this case, the posterior\n(cid:3043)(cid:3045)(cid:3036)\nis approximated to the Gaussian posterior, 𝑝 (cid:4666)𝑠|𝑜(cid:4667)(cid:3406)𝑁 . Thus, the prediction error increases both\n(cid:3084) (cid:3043)(cid:3042)(cid:3046)(cid:3047)\ninformation gains, KLD and BS. By contrast, when the prediction error increases toward infinity, the\nevidence converges to zero, lim 𝑒(cid:4666)𝛿(cid:4667)(cid:3404)0, where 𝑉 (cid:3404)0, because the evidence is the inverse exponential\n(cid:3083)→(cid:2998)\nfunction of the prediction error. In this case, the Gaussian posterior is negligible, and thus, the posterior is\napproximated to the Gaussian prior, lim 𝑝 (cid:4666)𝑠|𝑜(cid:4667)(cid:3404)𝑁 . When the posterior is equal to the prior, both\n(cid:3084) (cid:3043)(cid:3045)(cid:3036)\n(cid:3083)→(cid:2998)"
  },
  {
    "id": 500,
    "text": "approximated to the Gaussian prior, lim 𝑝 (cid:4666)𝑠|𝑜(cid:4667)(cid:3404)𝑁 . When the posterior is equal to the prior, both\n(cid:3084) (cid:3043)(cid:3045)(cid:3036)\n(cid:3083)→(cid:2998)\ninformation gains, KLD and BS, are zero. Thus, in the case of a large prediction error, where 𝑒(cid:4666)𝛿(cid:4667) is\nvery small compared to 𝜀, and 𝜀𝑁 is dominant in the posterior, the information gains decrease to zero\n(cid:3043)(cid:3045)(cid:3036)\nas prediction error increases toward infinity. We use 𝜀 (cid:3404)10(cid:2879)(cid:2871) for the following analysis.\nThe standardized linear weights 𝑤 and 𝑤 represent the dominances of the Gaussian\n(cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3043)(cid:3045)(cid:3036)\n10"
  },
  {
    "id": 501,
    "text": "Yanagisawa, H. & Honda, S.\nposterior and prior, respectively, in the mixed posterior distribution. Fig. 4 shows the dominances as\nfunctions of prediction error 𝛿. When the prediction error is zero or small, the Gaussian posterior is\ndominant. For a certain prediction error, the prior becomes dominant as the prediction error increases.\nFig. 5 shows an example of posterior distributions switching over from Gaussian posterior dominance to\nprior dominance. In the switching over area of prediction errors, the Gaussian posterior and prior are\nmixed with certain weights, 𝑤 and 𝑤 .\n(cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3043)(cid:3045)(cid:3036)\nUsing the posterior function, we derived KLD and BS:\n(cid:3084)\n𝐾𝐿𝐷 (cid:3404)𝐷 (cid:4670)𝑝(cid:4666)𝑠(cid:4667)||𝑝(cid:4666)𝑠|𝑜(cid:4667)(cid:4671)(cid:3404)𝐾𝐿𝐷 (cid:3397)ln(cid:4672)1(cid:3397) (cid:4673)(cid:3398)𝐼, (22)\n(cid:3012)(cid:3013) (cid:3015) (cid:3032)(cid:4666)(cid:3083)(cid:4667)"
  },
  {
    "id": 502,
    "text": "(cid:3012)(cid:3013) (cid:3015) (cid:3032)(cid:4666)(cid:3083)(cid:4667)\nwhere 𝐾𝐿𝐷 is a KLD using only the Gaussian likelihood, and 𝐼 is an improper integral:\n(cid:3015)\n𝐼 (cid:3404)(cid:1516)\n(cid:2879)\n(cid:2998)\n(cid:2998)\n𝑁\n(cid:3043)(cid:3045)(cid:3036)\nln(cid:3436)1(cid:3397)\n(cid:3032)(cid:4666)(cid:3083)\n(cid:3084)(cid:3015)\n(cid:4667)(cid:3015)\n(cid:3291)\n(cid:3291)\n(cid:3293)\n(cid:3290)\n(cid:3284)\n(cid:3294)(cid:3295)\n(cid:3440)𝑑𝑠(cid:3404)(cid:1516)\n(cid:2879)\n(cid:2998)\n(cid:2998)\n𝑁\n(cid:3017)(cid:3045)(cid:3036)\nln(cid:4674)1(cid:3397)𝜀(cid:4666)2𝜋𝑠\n(cid:3039)\n(cid:4667) (cid:3289) (cid:3118)exp(cid:4676) (cid:3041)(cid:4666)(cid:3046)(cid:2879)\n(cid:2870)\n(cid:3042)(cid:3364)\n(cid:3046)\n(cid:4667)\n(cid:3287)\n(cid:3118)(cid:2878)(cid:3041)(cid:3023) (cid:4677)(cid:4675)𝑑𝑠. (23)\nUsing the KLD, we derived BS as\n(cid:2869) (cid:3084)"
  },
  {
    "id": 503,
    "text": "(cid:2870)\n(cid:3042)(cid:3364)\n(cid:3046)\n(cid:4667)\n(cid:3287)\n(cid:3118)(cid:2878)(cid:3041)(cid:3023) (cid:4677)(cid:4675)𝑑𝑠. (23)\nUsing the KLD, we derived BS as\n(cid:2869) (cid:3084)\n𝐵𝑆(cid:3404)𝐷 (cid:4670)𝑝(cid:4666)𝑠|𝑜(cid:4667)||𝑝(cid:4666)𝑠(cid:4667)(cid:4671)(cid:3404) (cid:4674)𝑒(cid:4666)𝛿(cid:4667)(cid:4676)𝐵𝑆 (cid:3398)ln(cid:4672)1(cid:3397) (cid:4673)(cid:3397)𝐽(cid:4677)(cid:3398)𝜀𝐾𝐿𝐷(cid:4675), (24)\n(cid:3012)(cid:3013) (cid:3032)(cid:4666)(cid:3083)(cid:4667)(cid:2878)(cid:3084) (cid:3015) (cid:3032)(cid:4666)(cid:3083)(cid:4667)\nwhere 𝐵𝑆 is the 𝐵𝑆 of using only the Gaussian likelihood, and 𝐽 is an improper integral:\n(cid:3015)\n𝐽 (cid:3404)(cid:1516)\n(cid:2879)\n(cid:2998)\n(cid:2998)\n𝑁\n(cid:3043)(cid:3042)(cid:3046)(cid:3047)\nln(cid:3436)1(cid:3397)\n(cid:3032)(cid:4666)(cid:3083)\n(cid:3084)(cid:3015)\n(cid:4667)(cid:3015)\n(cid:3291)\n(cid:3291)\n(cid:3293)\n(cid:3290)\n(cid:3284)\n(cid:3294)(cid:3295)\n(cid:3440)𝑑𝑠(cid:3404)(cid:1516)\n(cid:2879)\n(cid:2998)\n(cid:2998)\n𝑁"
  },
  {
    "id": 504,
    "text": "(cid:3084)(cid:3015)\n(cid:4667)(cid:3015)\n(cid:3291)\n(cid:3291)\n(cid:3293)\n(cid:3290)\n(cid:3284)\n(cid:3294)(cid:3295)\n(cid:3440)𝑑𝑠(cid:3404)(cid:1516)\n(cid:2879)\n(cid:2998)\n(cid:2998)\n𝑁\n(cid:3043)(cid:3042)(cid:3046)(cid:3047)\nln(cid:4674)1(cid:3397)𝜀(cid:4666)2𝜋𝑠\n(cid:3039)\n(cid:4667) (cid:3289) (cid:3118)exp(cid:4676) (cid:3041)(cid:4666)(cid:3046)(cid:2879)\n(cid:2870)\n(cid:3042)(cid:3364)\n(cid:3046)\n(cid:4667)\n(cid:3287)\n(cid:3118)(cid:2878)(cid:3041)(cid:3023) (cid:4677)(cid:4675)𝑑𝑠. (25)\nBecause improper integrals 𝐼 and 𝐽 could not be solved analytically, we used a computational approach\nfor further analysis.\nFig. 4 Dominances of Gaussian posterior and prior in posterior distribution as functions of prediction\nerror. The dominances swatch over at a certain prediction error level. (Variances: 𝑠 (cid:3404)10.0, 𝑠=1.0.)\n(cid:3043) (cid:3039)\n11\nroiretsop\nni\necnanimoD"
  },
  {
    "id": 505,
    "text": "Yanagisawa, H. & Honda, S.\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n-10 -5 0 5 10 15\nFig. 5 Posterior distributions for different prediction errors (𝛿=2.0, 4.0, 5.0, and 6.0). The dominances in\nthe posterior distributions switch over from Gaussian posterior to prior. (Variances: 𝑠 (cid:3404)10.0, 𝑠=1.0.)\n(cid:3043) (cid:3039)\nFig. 6 (a) shows the information gains and their total value, 𝐼𝐺 (cid:3404)𝐾𝐿𝐷(cid:3397)𝐵𝑆, as functions of the\nprediction errors. All information gains are upward-convex functions of the prediction errors. This\nconvexity is general because when the prediction error is small, the Gaussian posterior is dominant in the\nposterior, and information gains increase as the prediction error increases; whereas when the prediction\nerror is larger than a certain level, the prior becomes dominant, and the information gains decrease to zero\nas the prediction error increases.\nFig. 7 shows surprise as a function of the prediction error. Surprise increases monotonically"
  },
  {
    "id": 506,
    "text": "as the prediction error increases.\nFig. 7 shows surprise as a function of the prediction error. Surprise increases monotonically\nwith respect to the prediction error. Thus, the information gains are also upward-convex functions of\nsurprise, and the total information gain 𝐼𝐺 that induces positive emotions by reducing free energy is an\nupward-convex function of surprise (and prediction error). We infer that the upward-convex function of\nthe total information gain represents the arousal potential function (i.e., the Wundt curve). Fig. 6 (b)\nshows an example of information gain as a function of surprise.\n12\nytilibaborP\nδ=2.0\n0.6\nPrior\nPosterior 0.5\nLikelihood\n0.4\n0.3\n0.2\n0.1\n0\n-10 -5 0 5 10 15\nytilibaborP\nδ=4.0\nPrior\nPosterior\nLikelihood\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n-10 -5 0 5 10 15\nytilibaborP\nδ=5.0\nPrior 0.6\nPosterior\nLikelihood 0.5\n0.4\n0.3\n0.2\n0.1\n0\n-10 -5 0 5 10 15\nytilibaborP\nδ=6.0\nPrior\nPosterior\nLikelihood"
  },
  {
    "id": 507,
    "text": "Yanagisawa, H. & Honda, S.\nFig. 6 Example of information gain functions of (a) prediction error and (b) surprise using Gaussian\nmodel with uniform noise. KLD and BS represent free energy reduction in recognition and prior updating\n(learning), respectively. Total information gain 𝐼𝐺 is a summation of KLD and BS. (Uncertainties: 𝑠 (cid:3404)\n(cid:3043)\n10.0, 𝑠=1.0.)\n(cid:3039)\nFig. 7 Surprise as a function of prediction error. (variances: 𝑠 (cid:3404)10.0, 𝑠=1.0.)\n(cid:3043) (cid:3039)\nInformation gain functions are upward-convex and have a peak. We define the prediction errors that\nmaximize information gains 𝐾𝐿𝐷, 𝐵𝑆, and 𝐼𝐺 as optimal prediction errors 𝛿 , 𝛿 and 𝛿 ,\n(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3010)(cid:3008)\nrespectively. Similarly, we define the surprises that maximize information gains 𝐾𝐿𝐷, 𝐵𝑆, and 𝐼𝐺 as\noptimal surprises 𝑆 , 𝑆 , and 𝑆 , respectively. We use the term “optimal” because it represents the"
  },
  {
    "id": 508,
    "text": "respectively. Similarly, we define the surprises that maximize information gains 𝐾𝐿𝐷, 𝐵𝑆, and 𝐼𝐺 as\noptimal surprises 𝑆 , 𝑆 , and 𝑆 , respectively. We use the term “optimal” because it represents the\n(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3010)(cid:3008)\noptimal arousal level that maximizes information gain (epistemic value) that evokes emotional valence.\nWhen the prediction errors are greater than 𝛿 and smaller than 𝛿 , 𝐾𝐿𝐷 and 𝐵𝑆 have a negative\n(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)\nrelationship, where 𝐾𝐿𝐷 decreases as 𝐵𝑆 increases, and vice versa. The prediction error that maximizes\nthe total information gain 𝛿 always falls into this area. Alternate maximizations of 𝐾𝐿𝐷 and 𝐵𝑆 by\n(cid:3010)(cid:3008)\ndecreasing and increasing the prediction error and surprise in this area iteratively reach the optimal\n13\nniaG\nnoitamrofnI\nniaG\nnoitamrofnI\n7\n6\n5\n4\n3\n2\n1\n0 2 4 6 8\nPrediction Error\nesirpruS"
  },
  {
    "id": 509,
    "text": "Yanagisawa, H. & Honda, S.\nsurprise 𝑆 . This alternation generates fluctuations of surprise. The magnitude of fluctuation is\n(cid:3010)(cid:3008)\ndetermined by the difference between KLD and BS in the optimal prediction error 𝐷 (cid:3404)𝛿 (cid:3398)𝛿 and\n(cid:3083) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005)\nsurprise 𝐷 (cid:3404)𝑆 (cid:3398)𝑆 . In the next section, we analyze the effects of uncertainties on the optimal\n(cid:3020) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005)\nprediction errors and surprise, together with their differences.\n3.3 Effects of uncertainties on information gains\nThe optimal prediction error and surprise change depending on uncertainties. We found the optimal\nprediction error and optimal surprise for all combinations of likelihood variances 𝑠 [1.0, 50] and prior\n(cid:3039)\nvariance 𝑠 [1.0, 50] in steps of 0.1 using the MATLAB fminbnd.m function, which is based on golden\n(cid:3043)\nsection search and parabolic interpolation."
  },
  {
    "id": 510,
    "text": "(cid:3039)\nvariance 𝑠 [1.0, 50] in steps of 0.1 using the MATLAB fminbnd.m function, which is based on golden\n(cid:3043)\nsection search and parabolic interpolation.\nFig. 8 shows the maximum information gain as a function of the two uncertainties, 𝑠 and 𝑠 .\n(cid:3039) (cid:3043)\nAll maximum information gains decrease as 𝑠 increases, and increase as 𝑠 decreases. While 𝑠\n(cid:3039) (cid:3043) (cid:3043)\napproaches zero, the sensitivity of 𝑠 to the maximum information gains are low. The sensitivity of 𝑠\n(cid:3039) (cid:3039)\nincreases as 𝑠 increases. The peak of the maximum information gain is observed when 𝑠 is small, and\n(cid:3043) (cid:3039)\n𝑠 is large. The maximum information gains of a large 𝑠 and large 𝑠 are greater than those of a small\n(cid:3043) (cid:3039) (cid:3043)\n𝑠 and small 𝑠 . Fig. 9 shows examples of the maximum information gain as a function of 𝑠 and 𝑠 . The\n(cid:3039) (cid:3043) (cid:3039) (cid:3043)"
  },
  {
    "id": 511,
    "text": "(cid:3043) (cid:3039) (cid:3043)\n𝑠 and small 𝑠 . Fig. 9 shows examples of the maximum information gain as a function of 𝑠 and 𝑠 . The\n(cid:3039) (cid:3043) (cid:3039) (cid:3043)\nmaximum information gains increase exponentially as 𝑠 decreases. Thus, the sensitivity of 𝑠 to the\n(cid:3039) (cid:3039)\nmaximum information gain increases as 𝑠 decreases. By contrast, the sensitivity of 𝑠 to information\n(cid:3039) (cid:3043)\ngain is significant when 𝑠 is small (e.g., from 1.0 to 10.0 in this example).\n(cid:3043)\nFig. 8 Maximum information gains as function of uncertainties 𝑠 and 𝑠 . (a) Max KLD, (b) Max BS,\n(cid:3039) (cid:3043)\nand (c) Max 𝐼𝐺.\n14"
  },
  {
    "id": 512,
    "text": "Yanagisawa, H. & Honda, S.\nFig. 9 Maximum information gains as functions of (a) likelihood variance when 𝑠 (cid:3404)10 and (b) prior\n(cid:3043)\nvariance when 𝑠 (cid:3404)1.0.\n(cid:3039)\nFig. 10 shows the optimal prediction errors, 𝛿 , 𝛿 and 𝛿 , as functions of likelihood variance 𝑠\n(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3010)(cid:3008) (cid:3039)\nand prediction variance 𝑠 . These two variances increase the optimal prediction errors. The sensitivity of\n(cid:3043)\n𝑠 is greater than that of 𝑠 in KLD. Fig. 11 shows an example of the optimal prediction error as a\n(cid:3039) (cid:3043)\nfunction of each uncertainty. All functions are monotonically increasing convex. 𝛿 is more sensitive\n(cid:3012)(cid:3013)(cid:3005)\nto 𝑠 than 𝛿 . Thus, the difference 𝛿 and 𝛿 decreases as 𝑠 increases. By contrast, 𝛿 is less\n(cid:3039) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3039) (cid:3012)(cid:3013)(cid:3005)"
  },
  {
    "id": 513,
    "text": "(cid:3039) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3039) (cid:3012)(cid:3013)(cid:3005)\nsensitive to 𝑠 than 𝛿 . Thus, the difference increases as 𝑠 increases.\n(cid:3043) (cid:3003)(cid:3020) (cid:3043)\nFig. 10 Optimal prediction errors as functions of observation and prediction uncertainties for (a) 𝐾𝐿𝐷, (b)\n𝐵𝑆, and (c) 𝐼𝐺.\n15\nniaG\nnoitamrofnI\nmumixaM\nniaG\nnoitamrofnI\nmumixaM"
  },
  {
    "id": 514,
    "text": "Yanagisawa, H. & Honda, S.\nFig. 11 Optimal prediction errors as functions of uncertainties, (a) likelihood variance when 𝑠 =10.0 and\n(cid:3043)\n(b) prediction variance when 𝑠=1.0.\n(cid:3039)\nFig. 12 Optimal surprises as functions of observation and prediction uncertainties for (a) KLD, (b) BS,\nand (c) 𝐼𝐺.\nFig. 12 shows the optimal surprises 𝑆 , 𝑆 , and 𝑆 as functions of the two uncertainties. 𝑠\n(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3010)(cid:3008) (cid:3039)\nmonotonically increases all optimal surprises. However, the effects of 𝑠 are different. 𝑠 decreases\n(cid:3043) (cid:3043)\n𝑆 and increases 𝑆 . Fig. 13 shows examples of optimal surprises as functions of each uncertainty.\n(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)\n𝑆 is more sensitive to 𝑠 than 𝑆 , and thus, 𝑆 approaches 𝑆 as 𝑠 increases. Consequently, the\n(cid:3012)(cid:3013)(cid:3005) (cid:3039) (cid:3003)(cid:3020) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3039)"
  },
  {
    "id": 515,
    "text": "(cid:3012)(cid:3013)(cid:3005) (cid:3039) (cid:3003)(cid:3020) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3039)\ndifference between 𝑆 and 𝑆 decreases as 𝑠 increases. By contrast, 𝑠 decreases 𝑆 and\n(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3039) (cid:3043) (cid:3012)(cid:3013)(cid:3005)\nincreases 𝑆 . Thus, the difference between 𝑆 and 𝑆 increases as 𝑠 increases.\n(cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3043)\n16\nrorrE\nnoitciderP\nlamitpO\nrorrE\nnoitciderP\nlamitpO"
  },
  {
    "id": 516,
    "text": "Yanagisawa, H. & Honda, S.\nFig. 13 Optimal surprises as functions of (a) likelihood variance when 𝑠 (cid:3404)10.0 and (b) prediction\n(cid:3043)\nvariance when 𝑠 (cid:3404)1.0.\n(cid:3039)\nFig. 14 shows the differences in the optimal prediction error and optimal surprise. Both differences are\nalways positive, and thus, 𝛿 (cid:3408)𝛿 and 𝑆 (cid:3408)𝑆 . Both differences increase as 𝑠 decreases and 𝑠\n(cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3039) (cid:3043)\nincreases. Thus, the larger the 𝑠 , and the smaller the 𝑠, the larger the differences in both the optimal\n(cid:3043) (cid:3039)\nprediction errors and surprises. 𝑠 has the greatest sensitivity to increase the difference when 𝑠 is large.\n(cid:3039) (cid:3043)\nFor the optimal prediction errors, 𝑠 has the greatest sensitivity to increase the difference when\n(cid:3043)\n𝑠 is small. The difference in the optimal prediction error is larger when both 𝑠 and 𝑠 are large than"
  },
  {
    "id": 517,
    "text": "(cid:3043)\n𝑠 is small. The difference in the optimal prediction error is larger when both 𝑠 and 𝑠 are large than\n(cid:3039) (cid:3039) (cid:3043)\nwhen both 𝑠 and 𝑠 are small. By contrast, the difference in the optimal surprise is larger when both 𝑠\n(cid:3039) (cid:3043) (cid:3039)\nand 𝑠 are small than when both 𝑠 and 𝑠 are large.\n(cid:3043) (cid:3039) (cid:3043)\nFig. 14 Difference in optimal prediction error 𝐷 and difference in optimal surprise 𝐷 .\n(cid:3083) (cid:3020)\n4. Discussions\n4.1 Arousal potential functions and curiosities\nThe results of the analysis using a Gaussian generative model with an additional uniform likelihood\nsuggest that the two information gains, KLD and BS, form upward-convex functions of surprise and\n17\nesirpruS\nlamitpO\nesirpruS\nlamitpO"
  },
  {
    "id": 518,
    "text": "Yanagisawa, H. & Honda, S.\nprediction errors (i.e., the distance between the prior mean and likelihood peak). The prediction error\nmonotonically increases surprise, as shown in Fig. 7. Fig. 15 shows a schematic of the information gain\nfunctions that conceptualize the analytical results, as shown in Fig. 6 and the related emotions. Surprise,\n(cid:3398)ln𝑝(cid:4666)𝑜(cid:4667), corresponds to free energy minimized in recognition. A previous study argued that surprise\nrepresents arousal potential because minimized free energy consists of the summation of information\ncontent provided by novelty and perceived complexity, which are collative variables as dominant factors\nof arousal potential (Yanagisawa, 2021).\nBerlyne suggested that an appropriate level of arousal potential induces a positive hedonic\nresponse, termed the optimal arousal level (Berlyne, 1960). Extreme arousal level caused by novel and"
  },
  {
    "id": 519,
    "text": "Berlyne suggested that an appropriate level of arousal potential induces a positive hedonic\nresponse, termed the optimal arousal level (Berlyne, 1960). Extreme arousal level caused by novel and\ncomplex stimuli may cause confusion. By contrast, a low arousal level with familiar and simple stimuli\nresults in boredom. Thus, emotional valence shapes the upward-convex function of the arousal potential,\ntermed the Wundt curve.\nBerlyne also suggested that two epistemic curiosities, diversive and specific, drive the\napproach to the optimal arousal level (Berlyne, 1966). Diversive curiosity drives the pursuit of novelty,\nwhereas specific curiosity drives the search for evidence of one’s model predictions. Consequently,\ndiversive curiosity increases the arousal potential to climb the Wundt curve on the left, from a low level\nof arousal (boredom). By contrast, specific curiosity motivates a decrease in the arousal potential to climb"
  },
  {
    "id": 520,
    "text": "of arousal (boredom). By contrast, specific curiosity motivates a decrease in the arousal potential to climb\nthe Wundt curve on the right side from a high arousal level (confusion). The alternation between the two\ncuriosity-driven activities approaches the optimal arousal level.\n𝐾𝐿𝐷 is a free energy reduction in recognition of a state 𝑠 given an observation 𝑜 that\nincreases model evidence, 𝑝(cid:4666)𝑜(cid:4667)(cid:3404)〈𝑝(cid:4666)𝑜|𝑠(cid:4667)〉 , where recognition 𝑞(cid:4666)𝑠(cid:4667) is updated from a prior 𝑝(cid:4666)𝑠(cid:4667) to\n(cid:3044)(cid:4666)(cid:3046)(cid:4667)\ntrue posterior 𝑝(cid:4666)𝑠|𝑜(cid:4667). 𝐵𝑆 is the expected information gain given by novel stimuli that corresponds to\nhuman surprise response to novelty (Itti & Baldi, 2009; Sekoguchi, Sakai, & Yanagisawa, 2019; Ueda et\nal., 2021; Yanagisawa et al., 2019). Therefore, we consider that specific curiosity drives an increase in\nKLD, whereas diversive curiosity drives an increase in BS.\n18"
  },
  {
    "id": 521,
    "text": "Yanagisawa, H. & Honda, S.\nIG\nKLD\nBS\nBoredom Pleasure Interest Confusion\nArousalpotential: Surprise F\nFig. 15 Schematic of arousal potential functions and related emotions. Valence of epistemic emotions\nrepresented by information gains forms upward-function of arousal potential represented by free energy\nor surprise. Diversive and specific curiosity drive to maximize KLD and BS, respectively. These alternate\nmaximizations achieve optimal arousal level with fluctuation of surprise. Emotions such as boredom,\npleasure, interest, and confusion are induced by free energy and its fluctuations (see main text for detailed\ndiscussion)\n4.2 Inquiry process and epistemic emotions\nThe analytical result shown in Fig. 14 demonstrate that the optimal surprise and optimal prediction error\nof BS is always greater than that of KLD, i.e., 𝑆 (cid:3408)𝑆 and 𝛿 (cid:3408)𝛿 , respectively. This result\n(cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005)"
  },
  {
    "id": 522,
    "text": "(cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005)\nsuggests that maximizing information gain through novelty seeking (driven by diversive curiosity)\nrequires a greater prediction error, causing greater surprise than that from maximizing information gain\nthrough evidence seeking (driven by specific curiosity).\nWhen surprise is less than 𝑆 , both KLD and BS monotonically increase as surprise\n(cid:3012)(cid:3013)(cid:3005)\nincreases. By contrast, when surprise is greater than 𝑆 , both KLD and BS monotonically decrease as\n(cid:3003)(cid:3020)\nsurprise increases. Thus, the two curiosities increase and decrease prediction errors in the former and\nlatter areas of surprise, respectively. However, when surprise is greater than 𝑆 and less than 𝑆 , KLD\n(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)\ndecreases, and BS increases as surprise increases. Thus, in this area of surprise, maximizing both the"
  },
  {
    "id": 523,
    "text": "(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)\ndecreases, and BS increases as surprise increases. Thus, in this area of surprise, maximizing both the\nKLD and BS at same time is impossible. We infer that the two types of curiosity alternately maximize\nKLD and BS. This alternating maximization of information gains generates fluctuations of surprise. The\noptimal arousal level, as a maximum summation of KLD and BS, falls into this area. Therefore, the\noptimum arousal level, 𝑆 , involves fluctuations in surprise by alternately seeking novelty and evidence,\n(cid:3010)(cid:3008)\ndriven by the two types of curiosity.\n19\nFΔ\nniag\nnoitamrofnI\n:ecnelav\ncimetsipE\nOptimal\narousal"
  },
  {
    "id": 524,
    "text": "Yanagisawa, H. & Honda, S.\nWe consider that alternating the two kinds of curiosity by increasing and decreasing prediction\nerrors represents an ideal inquiry process that achieves optimal arousal. This process provides continuous\npositive emotions through the continuous acquisition of maximum information gain (i.e., epistemic\nvalue). For example, “interest” is defined as disfluency reduction in fluency–disfluency theory (Graf &\nLandwehr, 2015). We previously formalized disfluency reduction as free energy reduction in recognition\n(i.e., KLD) from increased free energy (Yanagisawa et al., 2023). This corresponds to an increase in KLD\nfrom the high-surprise state shown in Fig. 15. Thus, “interest” is achieved by specific curiosity (i.e.,\nclimbing a hill of KLD from the right side in Fig. 15). By contrast, increasing KLD from the low-surprise\nstate (i.e., climbing a hill of KLD from the left side in Fig. 15) may explain “pleasure” defined as increase"
  },
  {
    "id": 525,
    "text": "state (i.e., climbing a hill of KLD from the left side in Fig. 15) may explain “pleasure” defined as increase\nin fluency (Graf & Landwehr, 2015). We have previously formalized fluency as KLD in recognition\n(Yanagisawa et al., 2023).\nBS denotes the expected information gain, as discussed in the Methods section. Active\ninference suggests that an agent infers an optimal policy of action that minimizes expected free energy.\nThe expected free energy includes the negative expected information gain as an epistemic value. This\nepistemic value drives curious behavior (Friston et al., 2017). Thus, diversive curiosity, formalized as\nmaximizing the BS, corresponds to curiosity in active inference. We discuss the mathematical\ninterpretations of KLD and BS in terms of the expected free energy in a later section.\n4.3 Effect of uncertainties on optimal arousal level and epistemic values\nWe analyzed the effects of prediction and observation uncertainties, manipulated using prior and"
  },
  {
    "id": 526,
    "text": "4.3 Effect of uncertainties on optimal arousal level and epistemic values\nWe analyzed the effects of prediction and observation uncertainties, manipulated using prior and\nlikelihood variances, on optimal information gains. Table 1 summarizes the effects of the two\nuncertainties in four quadrants for combinations of small and large uncertainties. A small prediction\nuncertainty 𝑠 indicates that the prior belief is certain because of, for example, prior experience and\n(cid:3043)\nknowledge. However, prior beliefs are not always correct. The prediction error represents the error of\nprior belief from reality. Thus, a case with small 𝑠 and large prediction error indicates a preconceived\n(cid:3043)\nnotion. By contrast, a large 𝑠 denotes that the prior belief is uncertain, owing to, for example, a lack of\n(cid:3043)\nprior knowledge and experience. Thus, observation uncertainty 𝑠 indicates precision of observations.\n(cid:3039)"
  },
  {
    "id": 527,
    "text": "(cid:3043)\nprior knowledge and experience. Thus, observation uncertainty 𝑠 indicates precision of observations.\n(cid:3039)\nWe evaluate the condition of uncertainties using four indices: maximum information gain\n(max𝐼𝐺), optimal prediction errors (𝛿 , 𝛿 ), optimal surprises (𝑆 , 𝑆 ), difference in optimal\n(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)\nprediction errors (𝐷 ), and difference in optimal surprises (𝐷 ). As shown in Fig. 8, the condition\n(cid:3083) (cid:3046)\ncombining a small 𝑠 and large 𝑠 provides the largest max𝐼𝐺 with the largest 𝐷 between small 𝛿\n(cid:3039) (cid:3043) (cid:3083) (cid:3012)(cid:3013)(cid:3005)\nand moderate 𝛿 . A larger 𝐷 signifies a wider exploration range through alternations of diversive and\n(cid:3003)(cid:3020) (cid:3083)\nspecific curiosities. Smaller 𝑆 and 𝑆 indicate less surprise as a cognitive load in the inquiry\n(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)"
  },
  {
    "id": 528,
    "text": "(cid:3003)(cid:3020) (cid:3083)\nspecific curiosities. Smaller 𝑆 and 𝑆 indicate less surprise as a cognitive load in the inquiry\n(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)\nprocess. Therefore, the condition combining a small 𝑠 and large 𝑠 is the best solution to achieve the\n(cid:3039) (cid:3043)\nideal inquiry process with the largest epistemic value (information gain; max𝐼𝐺) and the largest range of\nexploration (𝐷 ) under less cognitive load (𝑆 and 𝑆 ).\n(cid:3083) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)\n20"
  },
  {
    "id": 529,
    "text": "Yanagisawa, H. & Honda, S.\nThe condition combining a small 𝑠 and small 𝑠 is expected to yield the second largest\n(cid:3039) (cid:3043)\nepistemic value (information gain) under less cognitive load (𝑆 , 𝑆 ); however, the range of\n(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)\nexploration (𝐷 ) is small. The condition combining a large 𝑠 and large 𝑠 is expected to result in a small\n(cid:3083) (cid:3039) (cid:3043)\ninformation gain with a moderate range of exploration at the largest prediction error level. The condition\ncombining a large 𝑠 and small 𝑠 is the worst case, corresponding to the smallest information gain and\n(cid:3039) (cid:3043)\nthe smallest exploration range.\nAs overall trends, prediction uncertainty 𝑠 increases the range of exploration (𝐷 ). This\n(cid:3043) (cid:3083)\nsuggests that an extremely certain prior brief, such as a preconceived notion and strong assumption,"
  },
  {
    "id": 530,
    "text": "(cid:3043) (cid:3083)\nsuggests that an extremely certain prior brief, such as a preconceived notion and strong assumption,\nsuppresses the range of exploration, whereas an open mind involving a flat prior belief widens the range\nof exploration. The observation uncertainty 𝑠 decreases the expected maximum information gain (max\n(cid:3039)\n𝐼𝐺). This suggests that precise observation increases expected information gains (epistemic value) with\npositive emotions. 𝑠 can be decreased in different ways; for example, by increasing the precision of\n(cid:3039)\nstimuli, paying attention to stimuli, and improving the accuracy of the observation models.\nTable 1 Summary of the effects of likelihood variance (observation uncertainty) 𝑠 and prior variance\n(cid:3039)\n(prediction uncertainty) 𝑠 on maximum information gain, max𝐼𝐺, optimal prediction errors, 𝛿 , 𝛿 ,\n(cid:3043) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)"
  },
  {
    "id": 531,
    "text": "(cid:3039)\n(prediction uncertainty) 𝑠 on maximum information gain, max𝐼𝐺, optimal prediction errors, 𝛿 , 𝛿 ,\n(cid:3043) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)\noptimal surprises, 𝑆 , 𝑆 , difference in optimal prediction errors, 𝐷 , and difference in optimal\n(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3083)\nsurprises, 𝐷 . 𝑋:⇒𝑌 signifies that X dominantly affects Y. Solid and broken underlines denote positive\n(cid:3046)\nand negative effects on epistemic emotions, respectively.\n𝑠 :⇒ 𝐷 Small 𝑠 : Large 𝑠 :\n(cid:3043) (cid:3083) (cid:3043) (cid:3043)\n𝑠:⇒max𝐼𝐺, 𝐷 small 𝐷\n(cid:3039) (cid:3046) (cid:3083)\nSmall 𝑠: Small 𝑠 and small 𝑠 : Small 𝑠 and large 𝑠 :\n(cid:3039) (cid:3039) (cid:3043) (cid:3039) (cid:3043)\nlarge max𝐼𝐺 Large max𝐼𝐺, Largest max𝐼𝐺,\nSmall 𝛿 smallest 𝛿 , 𝛿 , small 𝛿 , moderate 𝛿 ,\n(cid:3012)(cid:3013)(cid:3005) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)"
  },
  {
    "id": 532,
    "text": "Small 𝛿 smallest 𝛿 , 𝛿 , small 𝛿 , moderate 𝛿 ,\n(cid:3012)(cid:3013)(cid:3005) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)\nSmall 𝑆 , 𝑆 smallest 𝑆 ,𝑆 , small 𝑆 , 𝑆 ,\n(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)\nlarge 𝐷 . small 𝐷 , large 𝐷 . largest 𝐷 , 𝐷 .\n(cid:3046) (cid:3083) (cid:3046) (cid:3083) (cid:3046)\nLarge 𝑠: Large 𝑠 and small 𝑠 : Large 𝑠 and large 𝑠 :\n(cid:3039) (cid:3039) (cid:3043) (cid:3039) (cid:3043)\nsmall max𝐼𝐺 smallest max𝐼𝐺, small max𝐼𝐺,\nmoderate 𝛿 ,𝛿 , largest 𝛿 ,𝛿 ,\n(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)\nlarge 𝑆 largest 𝑆 , moderate 𝑆 , large 𝑆 , largest 𝑆 ,\n(cid:3012)(cid:3013)(cid:3005) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)\nsmallest 𝐷 , 𝐷 . moderate 𝐷 , 𝐷 ."
  },
  {
    "id": 533,
    "text": "(cid:3012)(cid:3013)(cid:3005) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)\nsmallest 𝐷 , 𝐷 . moderate 𝐷 , 𝐷 .\n(cid:3083) (cid:3046) (cid:3083) (cid:3046)\n4.4 Expected free energy and information gains\nAn active inference framework suggests that an agent’s action policy is selected to minimize expected\nfree energy (Friston et al., 2017; Parr et al., 2022; Smith, Friston, & Whyte, 2022). Here, we discuss the\nrelationship between the expected free energy and the two types of information gains, KLD and BS, as\n21"
  },
  {
    "id": 534,
    "text": "Yanagisawa, H. & Honda, S.\ndrivers of specific and diversive curiosity, respectively.\nBefore giving observations by action, an agent calculates expected free energy under a policy 𝜋.\n𝐺 (cid:3095) (cid:3404)〈ln𝑞(cid:4666)𝑠|𝜋(cid:4667)(cid:3398)𝑝(cid:4666)𝑠,𝑜|𝜋(cid:4667)〉 (cid:3044)(cid:4666)𝑠,𝑜|𝜋(cid:4667) , (26)\nwhere 𝑞(cid:4666)𝑠,𝑜|𝜋(cid:4667)(cid:3404)𝑞(cid:4666)𝑠|𝜋(cid:4667)𝑝(cid:4666)𝑜|𝑠(cid:4667). This definition implies that the expected free energy is the free energy\naveraged by likelihood 𝑝(cid:4666)𝑜|𝑠(cid:4667) of observations expected by future action under a policy 𝜋. The expected\nfree energy forms the prior distribution of the policies. A policy is randomly selected based on the policy\nprior 𝑝(cid:4666)𝜋(cid:4667)(cid:3404)𝜎(cid:4666)(cid:3398)𝛾𝐺 (cid:4667) such that the expected free energy is minimized, where 𝜎(cid:4666)𝐸(cid:4667) is a softmax\n(cid:3095)\nfunction that transforms from energy 𝐸 to probability, and 𝛾 is the precision of policy representing the"
  },
  {
    "id": 535,
    "text": "(cid:3095)\nfunction that transforms from energy 𝐸 to probability, and 𝛾 is the precision of policy representing the\nconfidence of policy selection.\nThe expected free energy is expanded in two terms using the decomposition of a generative\nmodel 𝑝(cid:4666)𝑠,𝑜|𝜋(cid:4667)(cid:3404)𝑝(cid:4666)𝑠|𝜋(cid:4667)𝑝(cid:4666)𝑜|𝑠(cid:4667).\n𝐺 (cid:3095) (cid:3404)〈ln𝑞(cid:4666)𝑠|𝜋(cid:4667)(cid:3398)𝑝(cid:4666)𝑠|𝜋(cid:4667)〉 (cid:3044)(cid:4666)𝑠,𝑜|𝜋(cid:4667) (cid:3398)〈ln𝑝(cid:4666)𝑜|𝑠(cid:4667)〉 (cid:3044)(cid:4666)𝑠,𝑜|𝜋(cid:4667)\n(cid:3404)𝐷 (cid:3012)(cid:3013) (cid:4670)𝑞(cid:4666)𝑠|𝜋(cid:4667)||𝑝(cid:4666)𝑠|𝜋(cid:4667)(cid:4671)(cid:3398)𝔼 (cid:3044)(cid:4666)𝑜|𝜋(cid:4667) (cid:4670)〈ln𝑝(cid:4666)𝑜|𝑠(cid:4667)〉 (cid:3044)(cid:4666)𝑠|𝑜,𝜋(cid:4667) (cid:4671)\n(cid:3404)𝐷 (cid:3012)(cid:3013) (cid:4670)𝑞(cid:4666)𝑠|𝜋(cid:4667)||𝑝(cid:4666)𝑠|𝐶(cid:4667)(cid:4671)(cid:3397)𝔼 (cid:3044)(cid:4666)𝑜|𝜋(cid:4667) (cid:4670)𝑈 (cid:3095) (cid:4671) (27)"
  },
  {
    "id": 536,
    "text": "(cid:3404)𝐷 (cid:3012)(cid:3013) (cid:4670)𝑞(cid:4666)𝑠|𝜋(cid:4667)||𝑝(cid:4666)𝑠|𝐶(cid:4667)(cid:4671)(cid:3397)𝔼 (cid:3044)(cid:4666)𝑜|𝜋(cid:4667) (cid:4670)𝑈 (cid:3095) (cid:4671) (27)\nThe first term is a KL divergence from a state prior to a recognition density under a policy, 𝑞(cid:4666)𝑠|𝜋(cid:4667). We\nassume that the state prior is given from the agent’s preference 𝐶, 𝑝(cid:4666)𝑠|𝜋(cid:4667)(cid:3404)𝑝(cid:4666)𝑠|𝐶(cid:4667). A preference refers\nto a desired state expected to be achieved through actions based on policy selection. The KL divergence,\ntermed risk in state, refers to the difference between the preferred state and the state expected by acting\nwith a policy. A lower KL divergence indicates that the desired state is more likely to be achieved. The\nsecond term is expected uncertainty, which represents uncertainty averaged over expected observations.\nThis term is called ambiguity because it is equivalent to the entropy of likelihood,"
  },
  {
    "id": 537,
    "text": "second term is expected uncertainty, which represents uncertainty averaged over expected observations.\nThis term is called ambiguity because it is equivalent to the entropy of likelihood,\n(cid:3398)〈ln𝑝(cid:4666)𝑜|𝑠(cid:4667)〉 (cid:3044)(cid:4666)𝑠,𝑜|𝜋(cid:4667) (cid:3404)〈(cid:3398)ln𝑝(cid:4666)𝑜|𝑠(cid:4667)〉 (cid:3043)(cid:4666)𝑜|𝑠(cid:4667)(cid:3044)(cid:4666)𝑠|𝜋(cid:4667) (cid:3404)〈𝐻(cid:4666)𝑜|𝑠(cid:4667)〉 (cid:3044)(cid:4666)𝑠|𝜋(cid:4667) .\nThe expected uncertainty is decomposed into two terms using a conditional probability definition:\n(cid:3044)(cid:4666)𝑜|𝜋(cid:4667)(cid:3044)(cid:4666)𝑠|𝑜,𝜋(cid:4667)\n𝑝(cid:4666)𝑜|𝑠(cid:4667)(cid:3404) .\n(cid:3044)(cid:4666)𝑠|𝜋(cid:4667)\n𝔼 (cid:3044)(cid:4666)𝑜|𝜋(cid:4667) (cid:4670)𝑈 (cid:3095) (cid:4671)(cid:3404)(cid:3398)𝔼 (cid:3044)(cid:4666)𝑜|𝜋(cid:4667) (cid:3427)〈ln𝑝(cid:4666)𝑜|𝑠(cid:4667)〉 (cid:3044)(cid:4666)𝑠|𝑜,𝜋(cid:4667) (cid:3431)"
  },
  {
    "id": 538,
    "text": "(cid:3404)(cid:3398)𝔼 (cid:3044)(cid:4666)𝑜|𝜋(cid:4667) (cid:3427)〈ln𝑞(cid:4666)𝑠|𝑜,𝜋(cid:4667)(cid:3398)ln𝑞(cid:4666)𝑠|𝜋(cid:4667)(cid:3397)ln𝑞(cid:4666)𝑜|𝜋(cid:4667)〉 (cid:3044)(cid:4666)𝑠|𝑜,𝜋(cid:4667) (cid:3431)\n(cid:3404)(cid:3398)𝔼 (cid:3044)(cid:4666)𝑜|𝜋(cid:4667) (cid:4670)𝐷 (cid:3012)(cid:3013) (cid:4670)𝑞(cid:4666)𝑠|𝑜,𝜋(cid:4667)||𝑞(cid:4666)𝑠|𝜋(cid:4667)(cid:4671)(cid:4671)(cid:3398)𝔼 (cid:3044)(cid:4666)𝑜|𝜋(cid:4667) (cid:4670)ln𝑞(cid:4666)𝑜|𝜋(cid:4667)(cid:4671) (28)\nThe first term of the expected uncertainty is a negative KL divergence from approximate posterior to prior\naveraged by expected observations with a policy 𝜋. This KL divergence corresponds to the Bayesian\nsurprise, 𝐵𝑆 . Thus, this term signifies the expected information gain by prior updating using predicted\n(cid:3095)\nobservations under policy 𝜋. Note that the observation is not yet given, and 𝐵𝑆 is averaged based on\n(cid:3095)\nthe predicted distribution under a policy, 𝑞(cid:4666)𝑜|𝜋(cid:4667)."
  },
  {
    "id": 539,
    "text": "(cid:3095)\nobservations under policy 𝜋. Note that the observation is not yet given, and 𝐵𝑆 is averaged based on\n(cid:3095)\nthe predicted distribution under a policy, 𝑞(cid:4666)𝑜|𝜋(cid:4667).\nThe second term is entropy under a policy. By definition, surprise is the sum of the negative\nKLD and free energy.\n(cid:3398)ln𝑞(cid:4666)𝑜|𝜋(cid:4667)(cid:3404)(cid:3398)𝐷 (cid:3012)(cid:3013) (cid:4670)𝑞(cid:4666)𝑠|𝜋(cid:4667)||𝑞(cid:4666)𝑠|𝑜,𝜋(cid:4667)(cid:4671)(cid:3397)〈ln𝑞(cid:4666)𝑠|𝜋(cid:4667)(cid:3398)ln𝑞(cid:4666)𝑠,𝑜|𝜋(cid:4667)〉 (cid:3044)(cid:4666)𝑠|𝜋(cid:4667) (29)\n22"
  },
  {
    "id": 540,
    "text": "Yanagisawa, H. & Honda, S.\nThus, the entropy is a summation of the negative predictive KLD and the predicted free energy.\n(cid:3398)𝔼 (cid:3044)(cid:4666)𝑜|𝜋(cid:4667) (cid:4670)ln𝑞(cid:4666)𝑜|𝜋(cid:4667)(cid:4671)(cid:3404)(cid:3398)𝔼 (cid:3044)(cid:4666)𝑜|𝜋(cid:4667) (cid:4670)𝐷 (cid:3012)(cid:3013) (cid:3427)𝑞(cid:4666)𝑠|𝜋(cid:4667)(cid:3627)|𝑞(cid:4666)𝑠|𝑜,𝜋(cid:4667)(cid:4671)(cid:3431)(cid:3397) 𝔼 (cid:3044)(cid:4666)𝑜|𝜋(cid:4667) (cid:4670)𝐹 (cid:3095) (cid:4671) (30)\nIn summary, the expected free energy under a policy is the sum of the risk, predicted free energy, and\nnegative predicted information gains.\n𝐺 (cid:3404)𝑅𝑖𝑠𝑘(cid:3397)𝑝𝐹 (cid:3398)(cid:4666)𝑝𝐾𝐿𝐷 (cid:3397)𝑝𝐵𝑆 (cid:4667) (31)\n(cid:3095) (cid:3095) (cid:3095) (cid:3095)\nwhere\nrisk in state: 𝑅𝑖𝑠𝑘 (cid:3404)𝐷 (cid:4670)𝑞(cid:4666)𝑠|𝜋(cid:4667)||𝑝(cid:4666)𝑠|𝐶(cid:4667)(cid:4671), (32)\n(cid:3012)(cid:3013)"
  },
  {
    "id": 541,
    "text": "(cid:3095) (cid:3095) (cid:3095) (cid:3095)\nwhere\nrisk in state: 𝑅𝑖𝑠𝑘 (cid:3404)𝐷 (cid:4670)𝑞(cid:4666)𝑠|𝜋(cid:4667)||𝑝(cid:4666)𝑠|𝐶(cid:4667)(cid:4671), (32)\n(cid:3012)(cid:3013)\npredicted free energy: 𝑝𝐹 (cid:3095) (cid:3404)𝔼 (cid:3044)(cid:4666)𝑜|𝜋(cid:4667) (cid:3427)〈ln𝑞(cid:4666)𝑠|𝜋(cid:4667)(cid:3398)ln𝑞(cid:4666)𝑠,𝑜|𝜋(cid:4667)〉 (cid:3044)(cid:4666)𝑠|𝜋(cid:4667) (cid:3431), (33)\npredicted KLD: 𝑝𝐵𝑆 (cid:3095) (cid:3404)𝔼 (cid:3044)(cid:4666)𝑜|𝜋(cid:4667) (cid:4670)𝐷 (cid:3012)(cid:3013) (cid:4670)𝑞(cid:4666)𝑠|𝑜,𝜋(cid:4667)||𝑞(cid:4666)𝑠|𝜋(cid:4667)(cid:4671)(cid:4671), (34)\nand predicted Bayesian surprise: 𝑝𝐾𝐿𝐷 (cid:3095) (cid:3404)𝔼 (cid:3044)(cid:4666)𝑜|𝜋(cid:4667) (cid:4670)𝐷 (cid:3012)(cid:3013) (cid:4670)𝑞(cid:4666)𝑠|𝜋(cid:4667)||𝑞(cid:4666)𝑠|𝑜,𝜋(cid:4667)(cid:4671)(cid:4671). (35)\nWhen preference is expected to be fully satisfied by a policy, the predicted state equals the state"
  },
  {
    "id": 542,
    "text": "When preference is expected to be fully satisfied by a policy, the predicted state equals the state\nprior, 𝑞(cid:4666)𝑠|𝜋(cid:4667)(cid:3406)𝑝(cid:4666)𝑠|𝐶(cid:4667). In this case, the risk term becomes zero. However, the expected free energy still\nremains. The remaining expected free energy is the predicted free energy minus the two predicted\ninformation gains.\n𝐺 (cid:3406)𝑝𝐹 (cid:3398)(cid:4666)𝑝𝐾𝐿𝐷 (cid:3397)𝑝𝐵𝑆 (cid:4667) (36)\n(cid:3095) (cid:3095) (cid:3095) (cid:3095)\nThe remaining expected free energy is minimized by maximizing the predicted information gains,\n𝑝𝐾𝐿𝐷 (cid:3397)𝑝𝐵𝑆 , and minimizing the predicted free energy. Therefore, the two types of expected\n(cid:3095) (cid:3095)\ninformation gains, 𝑝𝐾𝐿𝐷 ,𝑝𝐵𝑆 , drive the agent’s action based on the active inference framework. This\n(cid:3095) (cid:3095)\ncorresponds to the expected drives of the two types of curiosity.\n4.5 Limitations and further discussions"
  },
  {
    "id": 543,
    "text": "(cid:3095) (cid:3095)\ncorresponds to the expected drives of the two types of curiosity.\n4.5 Limitations and further discussions\nThe analytical results are based on a Gaussian generative model. A Gaussian model was used to\nindependently manipulate the prediction errors and uncertainties and analyze their effects on information\ngains. Although Laplace approximation and the principle of maximum entropy reasonably support the\nGaussian assumption, true distributions can be more complex than Gaussian distributions. For specific\napplications with complex distributions, further analysis based on the method proposed in this study\npaper for specific applications with complex distributions.\nThis study focusses on emotions induced by epistemic values (epistemic emotions) such as curiosity and\ninterest. However, emotions are affected by individual preference and appraisal of the situation against\nobjectives(Ellsworth & Scherer, 2003). We may expand the emotion model to include such preference-"
  },
  {
    "id": 544,
    "text": "objectives(Ellsworth & Scherer, 2003). We may expand the emotion model to include such preference-\nbased emotions by introducing the pragmatic value formalized as risk term in expected free energy(Parr et\nal., 2022). The model does not consider individual capacity to process information. Surprise (free energy)\nexceeding the capacity may affect negative emotions.\nThis study was limited to analyzing two types of information gain linked to epistemic emotions as\nfunctions of surprise in a context-independent manner. Epistemic emotions based on epistemic values,\nsuch as curiosity, can be observed based on the agent’s behavior. Active inference, where an action policy\n23"
  },
  {
    "id": 545,
    "text": "Yanagisawa, H. & Honda, S.\nis inferred to minimize the expected free energy, can be used to simulate agent behavior based on\nepistemic emotions in a specific context (Friston et al., 2017). As discussed, the expected free energy\nincludes two types of information gain. In future studies, it will be necessary to accumulate evidence of\nthe model predictions based on correspondence between agent simulations and actual human behavior in\na variety of specific contexts.\n5. Conclusion\nThis study mathematically formulated arousal potential functions of epistemic emotions, such as curiosity\nand interest, that drive inquiry processes, based on information gains. Decrements in free energy in\nBayesian recognition and prior belief update correspond to two types of information gain, i.e., KLD and\nBS, respectively. Free energy reduction induces positive emotions by reducing surprise caused by\nprediction errors and uncertainty, which provide information gains (i.e., epistemic value). We"
  },
  {
    "id": 546,
    "text": "BS, respectively. Free energy reduction induces positive emotions by reducing surprise caused by\nprediction errors and uncertainty, which provide information gains (i.e., epistemic value). We\ndemonstrated that the two types of information gain form upward-convex curve functions of surprise\nusing a Gaussian generative model with a uniform noise likelihood, and defined epistemic emotions as\ninformation gains (or decrements of free energy). An analysis using the model exhaustively revealed the\neffects of prediction and observation uncertainties on the peak of information gain functions as the\noptimal arousal level. Specifically, the analytical results suggest that the greater the prediction uncertainty\nand the lower the observation uncertainty, the greater the information gained through a larger exploration\nrange.\nThese results provide general and fundamental knowledge to increase the valence of epistemic"
  },
  {
    "id": 547,
    "text": "range.\nThese results provide general and fundamental knowledge to increase the valence of epistemic\nemotions that facilitate the inquiry process because the model is deduced from the synthesis of free\nenergy minimization as the first principle of the brain and the well-established arousal potential theory.\nTherefore, this model framework is applicable to diverse areas that deal with epistemic emotions and\nmotivations, such as education, creativity, aesthetics, affective computing, and related cognitive sciences.\nFurther studies are needed to accumulate empirical evidence for the principle-based model and\nunderstand the relationship between the inquiry process and emotions in diverse complex situations.\nAcknowledgements\nThis research was supported by Japan Society for the Promotion of Science (KAKENHI Grant Number\n21H03528, Mathematical model development of emotion dimensions based on variation of uncertainty and\nits application to inverse problems).\n24"
  },
  {
    "id": 548,
    "text": "Yanagisawa, H. & Honda, S.\nReferences\nBerlyne, D. E. (1960), Conflict, arousal, and curiosity, 350. New York: McGraw-Hill Book Company.\nBerlyne, D. E. (1966). Curiosity and exploration. Science, 153(3731), 25–33.\ndoi:10.1126/science.153.3731.25.\nBuckley, C. L., Kim, C. S., McGregor, S., & Seth, A. K. (2017). The free energy principle for action and\nperception: A mathematical review. Journal of Mathematical Psychology, 81, 55–79.\ndoi:10.1016/j.jmp.2017.09.004.\nClark, J. E., Watson, S., & Friston, K. J. (2018). What is mood? A computational perspective.\nPsychological Medicine, 48(14), 2277–2284. doi:10.1017/S0033291718000430.\nDestexhe, A., Rudolph, M., & Paré, D. (2003). The high-conductance state of neocortical neurons in vivo.\nNature Reviews. Neuroscience, 4(9), 739–751. doi:10.1038/nrn1198.\nEllsworth, P. C., & Scherer, K. R. (2003). Appraisal processes in emotion. Handbook of affective sciences,\n1199, 572–595."
  },
  {
    "id": 549,
    "text": "Nature Reviews. Neuroscience, 4(9), 739–751. doi:10.1038/nrn1198.\nEllsworth, P. C., & Scherer, K. R. (2003). Appraisal processes in emotion. Handbook of affective sciences,\n1199, 572–595.\nFriston, K. (2010). The free-energy principle: A unified brain theory? Nature Reviews. Neuroscience,\n11(2), 127–138. doi:10.1038/nrn2787.\nFriston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., & Ondobaka, S. (2017). Active inference,\ncuriosity and insight. Neural Computation, 29(10), 2633–2683. doi:10.1162/neco_a_00999.\nFriston, K., Kilner, J., & Harrison, L. (2006). A free energy principle for the brain. Journal of Physiology,\nParis, 100(1–3), 70–87. doi:10.1016/j.jphysparis.2006.10.001.\nGraf, L. K. M., & Landwehr, J. R. (2015). A dual-process perspective on fluency-based aesthetics: The\npleasure-interest model of aesthetic liking. Personality and Social Psychology Review, 19(4),\n395–410. doi:10.1177/1088868315574978."
  },
  {
    "id": 550,
    "text": "pleasure-interest model of aesthetic liking. Personality and Social Psychology Review, 19(4),\n395–410. doi:10.1177/1088868315574978.\nHesp, C., Smith, R., Parr, T., Allen, M., Friston, K. J., & Ramstead, M. J. D. (2021). Deeply felt affect:\nThe emergence of valence in deep active inference. Neural Computation, 33(2), 398–446.\ndoi:10.1162/neco_a_01341.\nHonda, S., Yanagisawa, H., & Kato, T. (2022). Aesthetic shape generation system based on novelty and\ncomplexity. Journal of Engineering Design, 33(12), 1016–1035.\ndoi:10.1080/09544828.2022.2155343.\nItti, L., & Baldi, P. (2009). Bayesian surprise attracts human attention. Vision Research, 49(10), 1295–\n1306. doi:10.1016/j.visres.2008.09.007.\nJoffily, M., & Coricelli, G. (2013). Emotional valence and the free-energy principle. PLOS Computational\nBiology, 9(6), e1003094. doi:10.1371/journal.pcbi.1003094.\nJones, P. R. (2016). A tutorial on cue combination and Signal Detection Theory: Using changes in"
  },
  {
    "id": 551,
    "text": "Biology, 9(6), e1003094. doi:10.1371/journal.pcbi.1003094.\nJones, P. R. (2016). A tutorial on cue combination and Signal Detection Theory: Using changes in\nsensitivity to evaluate how observers integrate sensory information. Journal of Mathematical\nPsychology, 73, 117–139. doi:10.1016/j.jmp.2016.04.006.\nKashdan, T. B., & Silvia, P. J. (2009). Curiosity and interest: The benefits of thriving on novelty and\n25"
  },
  {
    "id": 552,
    "text": "Yanagisawa, H. & Honda, S.\nchallenge. Oxford handbook of positive psychology, 2, 367–374.\nKnill, D. C., & Pouget, A. (2004). The Bayesian brain: The role of uncertainty in neural coding and\ncomputation. Trends in Neurosciences, 27(12), 712–719. doi:10.1016/j.tins.2004.10.007.\nLang, P. J. (1995). The emotion probe. Studies of motivation and attention. American Psychologist, 50(5),\n372–385. doi:10.1037//0003-066x.50.5.372.\nParr, T., Pezzulo, G., & Friston, K. J. (2022), Active inference: The free energy principle in mind, brain,\nand behavior. Cambridge, MA: MIT Press.\nPeirce, C. S. (1974), Collected papers of Charles Sanders Peirce. Cambridge, MA: Harvard University\nPress.\nRaichle, M. E. (2006). Neuroscience. The brain’s dark energy. Science, 314(5803), 1249–1250.\ndoi:10.1126/science. 1134405.\nRussell, J. A. (1980). A circumplex model of affect. Journal of Personality and Social Psychology, 39(6),\n1161–1178. doi:10.1037/h0077714."
  },
  {
    "id": 553,
    "text": "doi:10.1126/science. 1134405.\nRussell, J. A. (1980). A circumplex model of affect. Journal of Personality and Social Psychology, 39(6),\n1161–1178. doi:10.1037/h0077714.\nRussell, J. A. (2003). Core affect and the psychological construction of emotion. Psychological Review,\n110(1), 145–172. doi:10.1037/0033-295x.110.1.145.\nSasaki, H., Kato, T., & Yanagisawa, H. (2023). Quantification of “novelty” based on free-energy principle\nand its application for “aesthetic liking” for industrial products. Research in Engineering Design.\ndoi:10.1007/s00163-023-00422-6.\nSekoguchi, T., Sakai, Y., & Yanagisawa, H. (2019). Mathematical model of emotional habituation to\nnovelty: Modeling with Bayesian update and information theory IEEE International Conference\non Systems, Man and Cybernetics (SMC), 2019 (pp. 1115–1120).\ndoi:10.1109/SMC.2019.8914626.\nSeth, A. K., & Friston, K. J. (2016). Active interoceptive inference and the emotional brain. Philosophical"
  },
  {
    "id": 554,
    "text": "on Systems, Man and Cybernetics (SMC), 2019 (pp. 1115–1120).\ndoi:10.1109/SMC.2019.8914626.\nSeth, A. K., & Friston, K. J. (2016). Active interoceptive inference and the emotional brain. Philosophical\nTransactions of the Royal Society of London. Series B, Biological Sciences, 371(1708),\n20160007. doi:10.1098/rstb.2016.0007.\nSilvia, P. J. (2012). Curiosity and motivation. The Oxford handbook of human motivation, 157–166.\nSmith, R., Friston, K. J., & Whyte, C. J. (2022). A step-by-step tutorial on active inference and its\napplication to empirical data. Journal of Mathematical Psychology, 107.\ndoi:10.1016/j.jmp.2021.102632.\nUeda, K., Sekoguchi, T., & Yanagisawa, H. (2021). How predictability affects habituation to novelty.\nPLOS ONE, 16(6), e0237278. doi:10.1371/journal.pone.0237278.\nVogl, E., Pekrun, R., Murayama, K., & Loderer, K. (2020). Surprised–curious–confused: Epistemic\nemotions and knowledge exploration. Emotion, 20(4), 625–641. doi:10.1037/emo0000578."
  },
  {
    "id": 555,
    "text": "Vogl, E., Pekrun, R., Murayama, K., & Loderer, K. (2020). Surprised–curious–confused: Epistemic\nemotions and knowledge exploration. Emotion, 20(4), 625–641. doi:10.1037/emo0000578.\nWager, T. D., Kang, J., Johnson, T. D., Nichols, T. E., Satpute, A. B., & Barrett, L. F. (2015). A Bayesian\nmodel of category-specific emotional brain responses. PLOS Computational Biology, 11(4),\ne1004066. doi:10.1371/journal.pcbi.1004066.\n26"
  },
  {
    "id": 556,
    "text": "Yanagisawa, H. & Honda, S.\nWilson-Mendenhall, C. D., Barrett, L. F., & Barsalou, L. W. (2013). Neural evidence that human\nemotions share core affective properties. Psychological Science, 24(6), 947–956.\ndoi:10.1177/0956797612464242.\nYanagisawa, H. (2016). A computational model of perceptual expectation effect based on neural coding\nprinciples. Journal of Sensory Studies, 31(5), 430–439. doi:10.1111/joss.12233.\nYanagisawa, H. (2021). Free-energy model of emotion potential: Modeling arousal potential as\ninformation content induced by complexity and novelty. Frontiers in Computational\nNeuroscience, 15, 698252. doi:10.3389/fncom.2021.698252.\nYanagisawa, H., Kawamata, O., & Ueda, K. (2019). Modeling emotions associated with novelty at\nvariable uncertainty levels: A Bayesian approach. Frontiers in Computational Neuroscience,\n13(2), 2. doi:10.3389/fncom.2019.00002.\nYanagisawa, H., Wu, X., Ueda, K., & Kato, T. (2023). Free energy model of emotional valence in dual-"
  },
  {
    "id": 557,
    "text": "13(2), 2. doi:10.3389/fncom.2019.00002.\nYanagisawa, H., Wu, X., Ueda, K., & Kato, T. (2023). Free energy model of emotional valence in dual-\nprocess perceptions. Neural Networks: The Official Journal of the International Neural Network\nSociety, 157, 422–436. doi:10.1016/j.neunet.2022.10.027.\n27"
  },
  {
    "id": 558,
    "text": "Multispectral palmprint recognition based on three\ndescriptors: LBP, Shift LBP, and Multi Shift LBP\nwith LDA classifier\nSalwua Aqreerah Alhaam Alariyibi Wafa El-Tarhouni\nFaculty of Information Technology Faculty of Information Technology Faculty of Information Technology\nUniversity of Benghazi University of Benghazi University of Benghazi\nBenghazi, Libya Benghazi, Libya Libya, Benghazi\nsalwua.khalifa@uob.edu.ly Alhaam.Alariyibi@uob.edu.ly wafa.eltarhouni@uob.edu.ly\nAbstract— Local Binary Patterns (LBP) are extensively dimensionality reduction, and classification process for the\nused to analyze local texture features of an image. Several new proposed approach. Section IV describes the experimental\nextensions to LBP-based texture descriptors have been setup and results, followed by the discussion in the section\nproposed, focusing on improving noise robustness by using V. The paper ends with conclusion in Section VI.\ndifferent coding or thresholding schemes. In this paper we"
  },
  {
    "id": 559,
    "text": "proposed, focusing on improving noise robustness by using V. The paper ends with conclusion in Section VI.\ndifferent coding or thresholding schemes. In this paper we\npropose three algorithms (LBP), Shift Local Binary Pattern II. RELATED WORKS\n(SLBP), and Multi Shift Local Binary Pattern (MSLBP),to\nThe human identification system has become a prominent\nextract features for palmprint images that help to obtain the\nplace in the field of research. Where it has led many\nbest unique and characteristic values of an image for\nresearchers to focus on proposing different techniques that\nidentification. The Principal Component Analysis (PCA)\ncontribute to further improving the rate of recognition\nalgorithm has been applied to reduce the size of the extracted\naccuracy. Specially that associated with the palm recognition\nfeature matrix in random space and in the matching process;\nthe Linear Discriminant Analysis (LDA) algorithm is used. system. In this section, we will provide a brief overview of"
  },
  {
    "id": 560,
    "text": "feature matrix in random space and in the matching process;\nthe Linear Discriminant Analysis (LDA) algorithm is used. system. In this section, we will provide a brief overview of\nSeveral experiments were conducted on the large multispectral some of the previous research presented by some researchers\ndatabase (blue, green, red, and infrared) of the University of on some innovative techniques in the field of human\nHong Kong. As result, distinguished and high results were identification using the palm print.\nobtained where it was proved that, the blue spectrum is\nGumaei et al. [1] suggest an algorithm based on directed\nsuperior to all spectra perfectly.\ngradients (Histogram of Oriented Gradients (HOG)). The\nextracted features are reduced using Auto-Encoder (AE). The\nKeywords— Palmprint, Local Binary Pattern (LBP), Shift\nLocal Binary Pattern (SLBP), Multi Shift Local Binary Pattern matching process is done using Machine Learning (RELM)"
  },
  {
    "id": 561,
    "text": "Keywords— Palmprint, Local Binary Pattern (LBP), Shift\nLocal Binary Pattern (SLBP), Multi Shift Local Binary Pattern matching process is done using Machine Learning (RELM)\n(MSLBP), Linear Discriminant Analysis (LDA) classifier. Extreme Regularized technology. Experiments were\nconducted on three palmprint databases, namely MS-PolyU,\nI. INTRODUCTION which holds multispectral palm images, CASIA and Tongji.\nBiometric identification systems are considered a fertile\nAn effective classifier for palmprint recognition has been\narea for research to enhance security systems scientific.\nproposed by Rida et al. [2] where it is based on the method\nPalmprint is a relatively new biometric physiological trait\nof Subspace Random (RSM) technology. The authors used\nthat offers stability and uniqueness as part of its traits. The\nthe (2DPCA) Two Dimension Analysis Component Principle\nmain advantage of a palmprint is the ability to access a large\nalgorithm, which is a linear dimension reduction technique,"
  },
  {
    "id": 562,
    "text": "the (2DPCA) Two Dimension Analysis Component Principle\nmain advantage of a palmprint is the ability to access a large\nalgorithm, which is a linear dimension reduction technique,\narea to extract biometric features. It is worth noting that palm\nand in the classification process the Neighbors Nearest (NN)\nimages are often affected by problems when capturing the\nalgorithm was applied.\nimage, such as noise, contrast, and rotation which making the\ntask of recognition critical. A number of studies have been In [3], researchers used images with \"Bmp\" format, being\npresented to overcome these problems and provide reliable suitable for extracting the main lines and wrinkle lines of the\nvisibility and fast computations. To date, researchers have palm of the hand. Then, they are used Contrast Limited\ndeveloped a number of approaches to extract features and Adaptive Histogram Equalization (CLAHE), to improve the"
  },
  {
    "id": 563,
    "text": "developed a number of approaches to extract features and Adaptive Histogram Equalization (CLAHE), to improve the\nclassification with palms. In this research, three algorithms, contrast of the image. In order to increase the accuracy, a\nLBP, SLBP, and MSLBP were used to extract the distinctive technique is used segmentation, where the \"Global Scheme\nand unique features. In order to make the matching process Thresolding\" process is applied. After that, the \"Gabor Log\"\neasier, the PCA algorithm was used to reduce the size of the technology is used to reduce the unwanted noise from the\nmatrix. In the classification stage, the LDA linear required information. The experiments of this research were\ndiscrimination function was applied, which gives the least conducted on CASIA database.\nprobability of error. Several experiments have been\nGumaei et al [4], proposed a ROI region segmentation\nconducted on the global database PolyU by many researches"
  },
  {
    "id": 564,
    "text": "probability of error. Several experiments have been\nGumaei et al [4], proposed a ROI region segmentation\nconducted on the global database PolyU by many researches\nsystem for images in all spectral bands using Zhang's\nwhere in this paper two experiments were conducted. In the\nmethod, and they used NGist algorithm in the process of\nfirst experiment, 3 samples for training and 9 samples for\nimage feature extraction, which is an extended version of the\ntesting were identified. In the second experiment, 6 samples\nGist algorithm. An automatic encryption algorithm has been\nfor testing and 6 samples for training were divided; The\nused to reduce the properties, and the classification process is\nsamples for training were selected randomly.\ndone by using the Machine Learning Extreme Regularized\nThis paper is organized as follows : Section II is (RELM) algorithm. Using the MS-PolyU database for"
  },
  {
    "id": 565,
    "text": "done by using the Machine Learning Extreme Regularized\nThis paper is organized as follows : Section II is (RELM) algorithm. Using the MS-PolyU database for\ndedicated to presenting some of the research studies multispectral palm images, several experiments were\nconcerned with palmprint recognition systems , while section performed.\nIII provides a brief description of the feature extraction,\nXXX-X-XXXX-XXXX-X/XX/$XX.00 ©20XX IEEE"
  },
  {
    "id": 566,
    "text": "Guo et al. [5], a study in which useful information was A. Feature Extraction\nextracted from a palmprint (non-uniform patterned) pattern\nThis stage is one of the important steps for palmprint\nto create a bins histogram called multi-hierarchical local\nrecognition because it helps to extract some useful features\nbinary patterns (HM-LBP). This approach focuses on LBP-\nfrom the palm ROI. The ROI images obtained in the pre-\nbased descriptors.\nprocessing stage.\nThe method for identifying the palm of the hand was\n1) Local Binary Patterns(LBP) : The original version of\nproposed by Rida et al. [6], in which the classifier was based\nthe LBP descriptor considers only eight neighbors of a pixel\non the RSM method, and the 2DPCA algorithm was relied\nand labels the pixels in the image with the neighborhood\non to build and reduce random subspaces. To extract the\nthreshold of each. Then, it treats the result as a binary\nfeatures, an algorithm was applied 2DLDA In the"
  },
  {
    "id": 567,
    "text": "on to build and reduce random subspaces. To extract the\nthreshold of each. Then, it treats the result as a binary\nfeatures, an algorithm was applied 2DLDA In the\nclassification stage, the NN algorithm was relied on, and the number, as originally suggested by Ojala et al. [7]. This\nstudy was based on three general groups, including the demonstrated the great differentiation capacity of this\nPOlyU database. operator to classify textures. Where, the image is divided\ninto (Pixels) cells and the center field and the cells adjacent\nIII. THE PROPOSED APPROACH\nto the center cell are selected, which in turn will be a square\nIn this section, we will present the proposed palmprint matrix. The adjacent cells are selected by specifying the\nrecognition system. The structure for palmprint identification radius to be dealt with, specifying the number of adjacent\nshould be based on the best spectrum with the highest\ncells with a different radius. Each value in the surrounding"
  },
  {
    "id": 568,
    "text": "should be based on the best spectrum with the highest\ncells with a different radius. Each value in the surrounding\nclassification rate. Biometric recognition systems in general,\ncell is compared with the value in the center cell. Where the\nalthough they differ in techniques, still share five main steps\nnumber 1 places for the surrounding cell that has a value\nthat they must pass through, which are image capture, initial\ngreater than or equal to the value carried by the center cell\nprocessing, feature extraction, dimensionality reduction and\nfinally classification or matching. The main innovation lies and put the value zero if it is otherwise, as shown in (2). As\nin the introduction of LBP, SLBP and MSLBP descriptors result, it will be formed a series of binary numbers obtained\nfor characteristic extraction followed by PCA to decrease the from these positions and will be converted to a decimal"
  },
  {
    "id": 569,
    "text": "for characteristic extraction followed by PCA to decrease the from these positions and will be converted to a decimal\ndimensionality of the extracted palmprint characteristic number, and all this may be done based on (1).\nvectors and the employment of the LDA classifier. Fig. 1\nillustrates the steps of the proposed system to identify the\n(1)\npalm of the hand. The main components of the proposed\npalmprint system are described below.\n(2)\nWhere Q is a set of sample points regularly spaced on a\ncircle of radius r. The value in the central cell c is denoted by\n, and the value in the adjacent cell by .\n2) Shift Local Binary Patterns (SLBP): In the LBP\nalgorithm, one binary code is generated for each pixel in the\ncell, while in the SLBP algorithm, a number of binary codes\nis generated for each pixel in the cell and the number of these\nbinary is determined by the variable K, that is determined by\n(3).\n(3)\nWhere\n(4)\n(5)\nFor each change in k a new binary code is generated and"
  },
  {
    "id": 570,
    "text": "binary is determined by the variable K, that is determined by\n(3).\n(3)\nWhere\n(4)\n(5)\nFor each change in k a new binary code is generated and\nadded to the histogram of patterns [8].\nConsidering the difference between the LBP and SLBP\nalgorithms lies in the shift value k, the SLBP value is\ncalculated using (5).\nAnd the value obtained from the equation S( - -k) has\nan impact on choosing the binary number, and S and k can\nbe represented as follows:\nWe notice that when the number 3 is assumed as the\nvalue of the determinant l, we will get the number 7 as a\nFig. 1. The process of the proposed system."
  },
  {
    "id": 571,
    "text": "product of the variable k, which takes the values{ −3, −2, −1, Step 6: Calculate the projection of the eigenvectors. We\n0, 1, 2, 3 }. chose the projection as a palm feature.\nThat is, for each pixel, we find that k contributes seven C. Classification\nbinary codes to the histogram pattern. The final histogram\nImage matching stage, is the process of comparing the\nwill be divided by k, making the sum of the histogram equal\ntest sample and the training set previously stored in the\nto the number of pixel positions as mentioned in LBP [8, 9].\ndatabase to identify the person's identity; If a match is found\n3) Multi Shift Local Binary Patterns (MSLBP): The\nwith one of the training samples, the identification process is\ncompleted. The proposed approach used the LDA Linear\nLocal Multi Pattern Binary (MLBP) algorithm has achieved\nDiscriminant Analysis algorithm to perform this stage. It\ngreat success on a large scale in distinguishing between"
  },
  {
    "id": 572,
    "text": "Local Multi Pattern Binary (MLBP) algorithm has achieved\nDiscriminant Analysis algorithm to perform this stage. It\ngreat success on a large scale in distinguishing between\nderived the linear discriminant function (LDA) from (Fisher)\ndifferent patterns, which has contributed to the interest of\nand it is used when the relationship between the variables is\nmany researchers in exploring solutions to problems of\nlinear [11].\ncontrast (changes) in lighting and rotation that make the task\nThe main concept of LDA is that classes are separated by\nof recognition more complex. Studies have suggested some\nfinding a suitable boundary between them and then\ndifferent algorithms derived from LBP to overcome these\nclassification is performed on the transformed space\ndifficult problems.\naccording to a metric such as Euclidean distance. From a\nThe idea used in MSLBP is to change the radius in the\nmathematical point of view, the LDA criteria can be satisfied"
  },
  {
    "id": 573,
    "text": "difficult problems.\naccording to a metric such as Euclidean distance. From a\nThe idea used in MSLBP is to change the radius in the\nmathematical point of view, the LDA criteria can be satisfied\nSLBP algorithm, where all non-local information is\nby maximizing the ratio between the intra-class variance\nextracted. Variation in radius depends on the distance of the\ndeterminant and the inter-class variance determinant.\nadjacent cell from the middle cell and allows the formation\nof a multivariate representation by SLBP binary sequencing. The classification method can be divided into two steps:\nThe histogram contains information about the distribution of (1) Calculate the posterior values for each class. And (2) the\nmultivariate features on the entire palm image. Equation (6) index of the class to which the test sample belongs in the\nwas applied at a set of radii (r) as shown in (7) to extract class with the maximum score is determined by solving the"
  },
  {
    "id": 574,
    "text": "was applied at a set of radii (r) as shown in (7) to extract class with the maximum score is determined by solving the\nstrong patterns from the palm of the hand. [8, 12].\nIV. EXPERIMENTAL RESULTS\nThe POlyU multispectral palm image database [13] was\nused to test the performance of the proposed system. The\nWhere the histogram is formed as shown in (8) based on database contains 6000 (12*500) images obtained from 500\nthe bin patterns obtained from the above equation [10]. different palms. So it contains (4*6000) 24000 different palm\nimages. The database contains four types of palm images\ntaken using flexible and infrared light under red, green, blue\nThe resulting multi-scale histogram are used as vectors to and NIR illumination. These images were collected in two\nbe used for the classification stage for the palm of the hand. separate sessions, nine days apart, in each session, 6 images"
  },
  {
    "id": 575,
    "text": "be used for the classification stage for the palm of the hand. separate sessions, nine days apart, in each session, 6 images\nThe algorithm for this method is as follows:- were taken for each of the right and left palms for each of the\nStep 1: Read the image. blue, red, green , and NIR spectral. Two different\nStep 2: Read the radius (r). experiments were applied to evaluation the proposed\nStep 3: Extract the palm pattern features by using equations approach. Fig. 2. Illustrates sample palmprint images of\n6, 7, 8. different spectra (red, green, blue and NIR) [15].\nStep 4: Store palm pattern features in the histogram.\nStep 5: If r is less than 8, go to step 2.\nB. Features Dimensionality Reduction\nIn palmprint recognition process, there is a problem with\nthe large amounts of data. Dimensional reduction has\nbecome a necessity to select the most distinct features for\nusing in the classification stage. The process of reducing\nstatistical dimensional can be used to identify characteristics"
  },
  {
    "id": 576,
    "text": "become a necessity to select the most distinct features for\nusing in the classification stage. The process of reducing\nstatistical dimensional can be used to identify characteristics\nthat are most helpful in the classification process. Principal\nComponent Analysis (PCA) algorithm was used.\n1) Principal Component Analysis (PCA): This algorithm\nis a powerful tool for data analysis, and it is used to reduce\nthe dimensions of the data while maintaining the strength of\nthese features to be useful in the classification process. PCA\nsteps can be illustrated as follows:\nStep 1: Convert an image to the x, y dimension of the matrix.\nStep 2: Calculate the average of each column.\nStep 3: Center and standardize the source matrix by\nsubtracting the mean from each column. Fig. 2. Sample of ROI palmprint images from PolyU database [15]:\nStep 4: Calculate the Covariance Matrix. (a) NIR, (b) red, (c) green, and (d) blue.\nStep 5: Calculate the eigenvectors and eigenvalues of this\ncovariance matrix."
  },
  {
    "id": 577,
    "text": "A. Experiment I\nIn this experiment, 3 images were selected for each\nspectrum of palms images (blue, green, red, infrared (NIR)).\nThese images were randomly selected for the training set,\nand the rest of the images were used as samples for testing\nset. Where the number of training samples was 1500 images\n(3×500=1500) while the number of test samples was 4500\n(9×500) for each of these spectral (blue, green, red, infrared)\nto extract the characteristics using the MSLBP algorithm by\nusing different radii (from 1 to 8) and 8 neighbours, the size\nof the feature matrix extracted from this algorithm was 2048\n(8×256), and other algorithms (LBP, SLBP) were also\napplied in this experiment. The results obtained are presented\nin Table I, which shows that the MSLBP algorithm, which\nshowed that the MSLBP algorithm had distinguished results\nwith all spectral, as it had the highest rate of identification\nwith the blue spectrum by 99.91% in a time of 24.26 s. As"
  },
  {
    "id": 578,
    "text": "showed that the MSLBP algorithm had distinguished results\nwith all spectral, as it had the highest rate of identification\nwith the blue spectrum by 99.91% in a time of 24.26 s. As\nillustrated in Fig. 3, we also obtained good recognition rates\nwith the other spectral, and the less one was with the green\nspectrum by 99.44% in an average time of 22.17 s. And we\nalso note that it is much higher than the best percentage that\nwe obtained when using the SLBP algorithm, which was\nwith the blue spectrum and is equivalent to 81.36% in a time\nof 35.47 s.\nTABLE I. THE RESULTS OF EXPEREMENT I.\n3 Training and 9 Testing\nRecognition Rate (%) ygolodohteM\nBlue Green Red NIR\nycaruccA\nemiT\nycaruccA\nemiT\nycaruccA\nemiT\nycaruccA\nemiT\n(blue, green, infrared, red). Here, three algorithms (LBP,\nSLBP, MSLB) were applied to extract the image features,\nand after conducting this experiment, the results shown in\nTable II were obtained, which showed that when applying"
  },
  {
    "id": 579,
    "text": "SLBP, MSLB) were applied to extract the image features,\nand after conducting this experiment, the results shown in\nTable II were obtained, which showed that when applying\nthe LBP algorithm in its simple concept, the results ranged\nbetween 64.33% at a time 10.86 seconds with the infrared\nspectrum and 92.3% in a time of 8.60 seconds with the blue\nspectrum. As for the SLBP algorithm, these percentages\nincreased to 96,40, 78.83, 67.7,88 μ with the blue, red and\ngreen spectral, respectively. However, as shown in Fig. 4,\nthe experiment showed that the MSLBP algorithm had the\nstrongest results, as it reached a good recognition accuracy\nwith the red and infrared spectrum by up to 99.93%, with a\ndifference in time where it was in the red spectrum up to\n23.48. While it improved with the infrared spectrum to\n20.90, and the obtaining with the blue spectrum a somewhat\nbetter recognition accuracy reached 99.96% in a time of 19\nseconds, and this algorithm reached its best results with the"
  },
  {
    "id": 580,
    "text": "20.90, and the obtaining with the blue spectrum a somewhat\nbetter recognition accuracy reached 99.96% in a time of 19\nseconds, and this algorithm reached its best results with the\ngreen spectrum at a rate of 99.9% in 14.81 seconds. By\ncomparing the results of this algorithm with the recognition\nrate obtained when applying the HOG-SGF+ AE+RELM\nalgorithm[14] and the RSM algorithm[6], the MSLBP\nalgorithm still maintains its strength as the lowest\nrecognition rate still exceeds it with the most powerful access\nmodifier the RSM algorithm, which was with the infrared\nspectrum by 99.54%, and the percentages were less than that\nwith the rest of the spectral, and the recognition rates were\n99.23%, 99.02%, 98.95% with the blue, red and green\nspectra, respectively.\nTABLE II. THE RESULTS OF EXPEREMENT II\n6 Training and 6 Testing\nLBP 79.44 10.67 50.42 10.32 57.44 12.86 44.69 15.12\nSLBP 81.36 35.47 55.89 28.87 66.64 37.66 52.04 32.21\nMSLBP 99.91 24.26 99.44 22.17 99.87 35.11 99.69 26.42"
  },
  {
    "id": 581,
    "text": "6 Training and 6 Testing\nLBP 79.44 10.67 50.42 10.32 57.44 12.86 44.69 15.12\nSLBP 81.36 35.47 55.89 28.87 66.64 37.66 52.04 32.21\nMSLBP 99.91 24.26 99.44 22.17 99.87 35.11 99.69 26.42\nFig. 3. A comparison of the recognition accuracy of algorithms (LBP,\nSLBP, MSLBP) obtained in the first experiment using (3 training set and 9\ntesting set).\nB. Experiment II\nIn this experiment, 6 images were chosen to be the samples\nof the training set, and 6 to be the samples for the test set,\nwhich was chosen randomly for each spectrum of spectral\nygolodohteM\nBlue Green Red NIR\nycaruccA\nemiT\nycaruccA\nemiT\nycaruccA\nemiT\nycaruccA\nemiT\nLBP 92.3 8.60 67.7 5.81 74.8 10.46 64.33 10.86\nSLBP 96.4 22.68 78.83 21.79 88.53 28.33 78.83 26.53\nMSLBP 99.96 19.00 99.9 14.81 99.93 23.48 99.93 20.90\nHOG-SGF +\n99.47 - 99.40 - 99.70 - 99.47 -\nAE+ RELM[14]\nRSM[6] 99.23 - 98.95 - 99.02 - 99.54 -"
  },
  {
    "id": 582,
    "text": "classification stage, the LDA linear discrimination function\nwas applied, which gives the least probability of error.\nSeveral experiments have been conducted on the global\ndatabase POlyU; Where 3 samples were identified for\ntraining and 9 samples for testing the first experiment, while\nthe second experiment was divided into 6 samples for testing\nand 6 samples for training; the training samples were\nrandomly selected for two experiments. It was concluded\nthat the proposed approach performed better when compared\nwith previous studies. The MSLBP algorithm proved to be\nhighly efficient and accurate in discrimination compared to\nthe LBP and SLBP algorithms.\nREFERENCES\n[1] A. Gumaei, R. Sammouda, A. M. Al-Salman and A. Alsanad, \"An\neffective palmprint recognition approach for visible and multispectral\nFig. 4. A comparison of the recognition accuracy of algorithms (LBP, sensor images\", Sensors, vol. 18, no. 5, pp. 1575, 2018.\nSLBP, MSLBP) obtained in the second experiment using (6 training"
  },
  {
    "id": 583,
    "text": "Fig. 4. A comparison of the recognition accuracy of algorithms (LBP, sensor images\", Sensors, vol. 18, no. 5, pp. 1575, 2018.\nSLBP, MSLBP) obtained in the second experiment using (6 training\n[2] I. Rida, R. Herault, G. L. Marcialis and G. Gasso, \"Palmprint\nset and 6 testing set).\nrecognition with an efficient data driven ensemble classifier\", Pattern\nRecognit. Letters, 126, pp.21-30, 2019.\nV. DISCUSSION\n[3] S. Bala,\"Comparative analysis of palm print recognition system with\nThe proposed method successfully captures discriminant Repeated Line Tracking method\". Procedia Computer Science, 92,\npp. 578-582, Jan 2016.\ninformation to provide better recognition performance using\nLDA classification. By looking at the results, it can be seen [4] A. Gumaei, R. Sammouda, A. Malik S. Al-Salman and A. Alsanad,\n\"An improved multispectral palmprint recognition system using\nthat the applied MSLBP achieves excellent recognition rates\nautoencoder with regularized extreme learning machine\", Comput."
  },
  {
    "id": 584,
    "text": "\"An improved multispectral palmprint recognition system using\nthat the applied MSLBP achieves excellent recognition rates\nautoencoder with regularized extreme learning machine\", Comput.\ncompared to previous studies[6, 14] . In addition, the best Intell. Neurosci., vol. 2018, May 2018.\nresult obtained was by applying the MSLBP algorithm with\n[5] X. Guo, W. Zhou and Y. Zhang, \"Collaborative representation with\nthe blue spectrum with a recognition rate of 99.91%, 99.96% HM-LBP features for palmprint recognition\", Mach. Vis. Appl., vol.\nin the first and second experiments, respectively, followed by 28, no. 3, pp. 283-291, May 2017.\nthe red spectrum characterized in the two experiments on the [6] I. Rida, R. Herault, G. L. Marcialis and G. Gasso, \"Palmprint\nrecognition rate of the green spectrum as shown in Fig. 5. recognition with an efficient data driven ensemble classifier\", Pattern\nRecognit. Letters.126, pp. 21-30, 2019.\nThe important information that must be referred to are the"
  },
  {
    "id": 585,
    "text": "Recognit. Letters.126, pp. 21-30, 2019.\nThe important information that must be referred to are the\n[7] T. Ojala, M. Pietikinen and T. Maenpaa, \"Gray scale and rotation\nresults of the proposed methods for the second experiment (6\ninvariant texture classification with local binary patterns\", Proc.\nTraining, 6 Testing) that exceed the results obtained in the\nECCV, pp. 404-420, Springer, Berlin, Heidelberg, June 2000.\nfirst experiment (3 Training, 9 Testing). We can confirm that\n[8] W. El-Tarhouni, \"Finger Knuckle Print and Palmprint for efficient\nincreasing the number of training samples can improve the person recognition\", (Doctoral dissertation, Northumbria University),\nperformance of the recognition system, also as the Fig. 4 2017.\nshows the higher the number of features selected, the more [9] G. Kylberg and I. M. Sintorn, \"Evaluation of noise robustness for\naccurate the discrimination will be. local binary pattern descriptors in texture classification\", EURASIP J."
  },
  {
    "id": 586,
    "text": "accurate the discrimination will be. local binary pattern descriptors in texture classification\", EURASIP J.\nImage Video Process., vol. 2013, Dec. 2013.\n[10] W. EI-Tarhouni, M. K. Shaikh, L. Boubchir and A. Bouridane,\n\"Multi-Scale Shift Local Binary Pattern Based-Descriptor for Finger-\nKnuckle-Print Recognition\", International Conference on\nMicroelectronics (ICM), pp. 184-187, 2014.\n[11] T. Duong, \"Bandwidth selectors for multivariate kernel density\nestimation\", 2004.\nBlue\n[12] S. Verma and P. Mishra, \" A Survey Paper on Palm Prints Based\nRed\nBiometric Authentication System\" , International Journal of Electrical\nGreen and Electronics Engineering (IJEEE), ISSN: 2231-5, Vol-1, 284Iss-3.,\nNir 2012.\n[13] D. Zhang, Z. Guo, G. Lu, L. Zhang and W. Zuo, \"An online system of\nmultispectral palmprint verification\", IEEE Trans. Instrum. Meas.,\nvol. 59, no. 2, pp. 480-490, Feb. 2010.\n[14] I. Rida, R. Herault, G. L. Marcialis and G. Gasso, \"Palmprint"
  },
  {
    "id": 587,
    "text": "multispectral palmprint verification\", IEEE Trans. Instrum. Meas.,\nvol. 59, no. 2, pp. 480-490, Feb. 2010.\n[14] I. Rida, R. Herault, G. L. Marcialis and G. Gasso, \"Palmprint\nrecognition with an efficient data driven ensemble classifier\", Pattern\nRecognit. Letters, 126, pp. 21-30, 2019.\n[15] W. El-Tarhouni, L. Boubchir, M. Elbendak and A. Bouridane, \"\nFig. 5. A digram illustrated the accurancy of the proposed method for the Multispectral palmprint recognition using Pascal coefficients-based\nsecond experiment (6 Training, 6 Testing). LBP and PHOG descriptors with random sampling\", Neural\nComputing and Applications, vol. 31, no. 2, pp. 593-603, 2017.\nVI. CONCLUSION\nIn this paper, the stage of extracting the characteristics of\nthe palm image is highlighted. Where three algorithms LBP,\nSLBP, MSLBP were used to extract the distinctive and\nunique features. Subsequently, all the extracted features are\nstored in a vector to complete the identification process, and"
  },
  {
    "id": 588,
    "text": "SLBP, MSLBP were used to extract the distinctive and\nunique features. Subsequently, all the extracted features are\nstored in a vector to complete the identification process, and\nthe PCA algorithm is used to reduce the size of the matrix in\norder to facilitate and succeed in the matching process. In the"
  },
  {
    "id": 589,
    "text": "Turing’s Test, a Beautiful Thought Experiment\nBernardoGonçalves , UniversityofSãoPaulo,SãoPaulo,Brazil,&King’sCollege,Cambridge,UK\nAbstract—Inthewakeofthelatesttrendsofartificialintelligence(AI),therehasbeen\naresurgenceofclaimsandquestionsabouttheTuringtestanditsvalue,whichare\nreminiscent of decades of practical “Turing” tests. If AI were quantum physics, by\nnowseveral“Schrödinger’s”catswouldhavebeenkilled.Itistimeforahistoricalre-\nconstruction of Turing’s beautiful thought experiment. This paper presents a wealth\nof evidence, including new archival sources, and gives original answers to several\nopen questions about Turing’s 1950 paper, including its relation with early AI.\nSince the early 1990s, Turing’s test has been used\nfor publicity purposes as a practical experiment, and\nhas been aptly criticized [30], [31], [39]. It has been\nthe whipping boy of AI [13], cognitive sciences and\nanalyticphilosophy(cf.[32]),andincreasingly,withthe"
  },
  {
    "id": 590,
    "text": "has been aptly criticized [30], [31], [39]. It has been\nthe whipping boy of AI [13], cognitive sciences and\nanalyticphilosophy(cf.[32]),andincreasingly,withthe\nrise of AI, the humanities and social sciences [2]. It\nis not uncommon for critics from all these fields to\ntake Turing’s test literally, while ignoring parts of his\n1950 text. It is often assumed that he was promoting\ndeceptionasacriterionforintelligenceand/orpropos-\ning a crucial experiment to establish the existence of\nmachine intelligence. Now, with the latest trends of AI\ntechnology, publications ask whether the Turing test\ncan be a “benchmark” for AI [1], and whether it is\n“dead”[41].Basedonrecentprimaryresearch[7]–[11],\nthispaperpresentsanewperspectiveonTuring’stest.\nA structural reading of Turing’s 1950 paper is pre-\nsented. Newly discovered archival sources are intro-\nduced and Turing’s concept of imitation is examined.\nIt is argued that his discursive presentation of the"
  },
  {
    "id": 591,
    "text": "sented. Newly discovered archival sources are intro-\nduced and Turing’s concept of imitation is examined.\nIt is argued that his discursive presentation of the\nimitation game in various versions, as opposed to a\nFIGURE 1. Alan Turing (1912-1954). Photographs of Alan\nwell-defined, controlled experiment, conforms to what\nTuring, copyright The Provost and Scholars of King’s Col-\nthephysicistandhistorianErnstMachcalled“thebasic\nlegeCambridge2023.ArchivesCentre,King’sCollege,Cam-\nmethod of thought experiments” [19]. The historical\nbridge,AMT/K/7/12.Reproducedwithpermission.\nconditions of Turing’s proposal are reconstructed from\na controversy about mind and computing machine in\nEngland, contextualizing his requirement of gender threeseminalpapers.1 Thetexthas28pages,divided\nimitation and conversational performance as success in seven sections, §1-§7. Three main logical steps\ncriteria for machine intelligence. Turing’s responses to can be identified in his argument: the proposal (§1-"
  },
  {
    "id": 592,
    "text": "criteria for machine intelligence. Turing’s responses to can be identified in his argument: the proposal (§1-\ncritics and propaganda for machine intelligence are §3, in 3` pp.), the science (§4-§5, in 6 pp.), and the\nhighlighted.Finally,thequestionofthevalueofTuring’s discussion (§6-§7, in 18+ pp.). This structure, with its\ntest and its relation to the history of AI is revisited. thematic order and size distribution, can be revealing\nabout the nature of Turing’s paper and argument.\nThe proposal sought to replace with the imitation\nWHAT IS THE TURING TEST?\ngame the question “Can machines think?,” which he\nIn 1950, Alan Turing (Fig. 1) published “Computing considered “too meaningless to deserve discussion”\nMachinery and Intelligence” [36], the second of his (p.442;Turingwascomingfromunstructuredmultidis-\n1058-6180©2024IEEE 1The othertwo being “On Computable Numbers” [35] and\nDigitalObjectIdentifier10.1109/MAHC.2024.3432278 “TheChemicalBasisofMorphogenesis”[37]."
  },
  {
    "id": 593,
    "text": "1058-6180©2024IEEE 1The othertwo being “On Computable Numbers” [35] and\nDigitalObjectIdentifier10.1109/MAHC.2024.3432278 “TheChemicalBasisofMorphogenesis”[37].\nIEEEAnnalsoftheHistoryofComputing Author’sacceptedmanuscript 1\n4202\nluJ\n22\n]IA.sc[\n3v90000.1042:viXra"
  },
  {
    "id": 594,
    "text": "ciplinary debates in two editions of a seminar, “Mind pretation. A close reading of the text identifies four\nand Machine,” held at the Philosophy Department differentconditionsofthegamewithrespecttoplayers\nof Manchester University in October and December, A-B, namely, man-woman (p. 433), machine-woman\n1949.)2Thepurposeoftheproposalwastochangethe (p. 434), machine-machine (pp. 441, 451-452), and\ncommonmeaningoftheword“machine”(e.g.,asteam machine-man (p. 442). These different conditions re-\nengine, a bulldozer) in light of the new mathematical late to four variants of the “new” question that Turing\nscience of “universal” digital computing. The imitation posedtoreplacehis“original”question(seeBox1).In\ngame would allow for a grounded discussion of “ma- addition to varying the species (types) of the players,\nchine”and“thinking,”seekingtoexpandthemeaningof he also increased the storage and speed of the ma-"
  },
  {
    "id": 595,
    "text": "chine”and“thinking,”seekingtoexpandthemeaningof he also increased the storage and speed of the ma-\n“thinking”anddetachitfromthehumanspecies,much chineandprovideditwithahypotheticallyappropriate\nasthemeaningof“universe”wasoncedetachedfrom program (Q3), and suggested a base time for the\nthe Earth, but also as a critique of anthropocentrism. interrogation session (Q4). Other seemingly relevant\nIn 1950, one of the OED definitions of “machine” parameters were not mentioned, such as the number\nwas:3 “acombinationofpartsmovingmechanicallyas of interrogators used to arrive at a statistically sound\ncontrastedwithabeinghavinglife,consciousnessand conclusion, although their profile is mentioned — they\nwill...Henceappliedtoapersonwhoactsmerelyfrom should be “average” —, and later reiterated — they\nhabitorobediencetoarule,withoutintelligence,orto “should not be expert about machines.”5\nonewhoseactionshavetheundeviatingprecisionand Version Q4 of the test appears at the beginning"
  },
  {
    "id": 596,
    "text": "habitorobediencetoarule,withoutintelligence,orto “should not be expert about machines.”5\nonewhoseactionshavetheundeviatingprecisionand Version Q4 of the test appears at the beginning\nuniformity of a machine.” Thus, by definition, common of §6 of the 1950 paper. As we will see shortly, this is\nsense did not allow the meanings of “machine” and theversionthatTuringmostdirectlyassociateswiththe\n“thinking” to overlap. Despite Turing’s emphasis in his ideaofafutureexperiment.Astheleastunderspecified\nopening paragraph that he did not intend to discuss version of the test (cf. Box 1), it has been the one\nhow these words were “commonly used” (p. 433), picked up by promoters of practical “Turing” tests. In\nthe hostility to his proposal can be seen from one thatpassage,Turingexpresseshisbeliefthatin“about\nof the first reactions, from a participant in the 1949 fifty years” an “average interrogator” would miss the"
  },
  {
    "id": 597,
    "text": "of the first reactions, from a participant in the 1949 fifty years” an “average interrogator” would miss the\nManchester seminars, who quoted the above OED identification in at least 30% of the test sessions.\ndefinition to appeal to common sense [20, p. 149]. Twosentenceslater,Turingstatesasecondbelief:\nThenewquestion,whichTuringconsideredtohave Ibelievethatattheendofthecenturytheuse\na “more accurate form” [36, p. 442], would be based of words and general educated opinion will\non a vivid image, his “criterion for ‘thinking’” (p. 436), havealteredsomuchthatonewillbeableto\nwhich he called interchangeably the “imitation game” speakofmachinesthinkingwithoutexpecting\nand his “test.”4 The new question was whether a ma- to be contradicted. [36, p. 442]\nchine playing A, the deceiver, could imitate a woman,\nOnce again we are brought to a crucial moment at\na man, a human being, or a different machine playing\nthe end of the century. But this second stated belief"
  },
  {
    "id": 598,
    "text": "Once again we are brought to a crucial moment at\na man, a human being, or a different machine playing\nthe end of the century. But this second stated belief\nB, the assistant, in a remotely played conversation\nneatly reformulates the first, to which it is almost\ngame to pass as B in the eyes of an average human\njuxtaposed,andseemstorevealincommonlanguage\ninterrogator playing C, the judge.\nwhat is meant by the rhetoric of achieving 30% of\nHowever, the details and exact conditions of the\nmisidentification in the imitation game: it expresses\nimitation game as an experiment slipped through Tur-\nin rough round numbers the experience of living in\ning’s text in a series of variations that defies inter-\nthe culture he envisions, where “one will be able to\nspeak of machines thinking without expecting to be\ncontradicted.”Couldsuchaculturalshiftcomewiththe\n2OftheDecemberedition,aparticipantwrote:“Iwishyou\nfutureofdigitalcomputing?Howremotewasit?These\nhadbeenwithusafewdaysagowehadanamusingevening"
  },
  {
    "id": 599,
    "text": "contradicted.”Couldsuchaculturalshiftcomewiththe\n2OftheDecemberedition,aparticipantwrote:“Iwishyou\nfutureofdigitalcomputing?Howremotewasit?These\nhadbeenwithusafewdaysagowehadanamusingevening\ndiscussionwithThuring[sic],Williams,MaxNewman,Polyani discursivequestionsarearguablytherealquestionshe\n[sic],Jefferson,JZYoung&myself...Anelectronicanalyser addresses. What about running his test?\nandadigitalcomputer(universaltype)mighthavesortedthe Havinggonethroughnineobjectionstomachinein-\nargumentsoutabit.”ChristmaspostcardfromJulesY.Bogue\ntelligencediscussedonthebasisoftheimitationgame,\nto Warren McCulloch, c. December, 1949. American Philo-\nwecometothecrux:Turing’sreferenceto“experiment”\nsophicalSociety,W.S.McCullochPapers,Mss.B.M139_005.\nThankstoJonathanSwintonforthisarchivalfinding.\n3NewEnglishDictionary.Oxford,Vol.VI,PartII,M-N,p.7.\n4Turing referred to his “test” four times — in pp. 446–447, 5“Can automatic calculating machines be said to think?,”"
  },
  {
    "id": 600,
    "text": "3NewEnglishDictionary.Oxford,Vol.VI,PartII,M-N,p.7.\n4Turing referred to his “test” four times — in pp. 446–447, 5“Can automatic calculating machines be said to think?,”\n454. He also referred to it as an “experiment” — once on p. Broadcast on BBC Third Programme, 14 and 23 Jan. 1952.\n436,twiceonp.455,andtwiceagainonp.457. ArchivesCentre,King’sCollege,Cambridge,AMT/B/6.\n2"
  },
  {
    "id": 601,
    "text": "BOX 1\nThe various questions and conditions of Turing’s test\nQ: “I propose to consider the question, ‘Can ma- which would do well.” (p. 436)\nchines think?’” (p. 433) Q3: “It was suggested tentatively that the question\nQ1: “We now ask the question, ‘What will happen [Q] should be replaced by [Q2] ... But in view of\nwhen a machine takes the part of A in this game?’ theuniversalitypropertyweseethateitherofthese\nWill the interrogator decide wrongly as often when questions is equivalent to this, ‘Let us fix our atten-\nthegameisplayedlikethis[machine-woman]ashe tion on one particular digital computer C. Is it true\ndoeswhenthegameisplayedbetweenamananda thatbymodifyingthiscomputertohaveanadequate\nwoman?Thesequestionsreplaceouroriginal,‘Can storage,suitablyincreasingitsspeedofaction,and\nmachines think?’” (pp. 433-434) providing it with an appropriate programme, C can\nQ2: “There are already a number of digital comput- be made to play satisfactorily the part of A in the"
  },
  {
    "id": 602,
    "text": "machines think?’” (pp. 433-434) providing it with an appropriate programme, C can\nQ2: “There are already a number of digital comput- be made to play satisfactorily the part of A in the\nimitation game, the part of B being taken by a\nersinworkingorder,anditmaybeasked,‘Whynot\nman?’” (p. 442)\ntry the experiment straight away? It would be easy\nto satisfy the conditions of the game. A number of Q4: “I believe that in about fifty years’ time it will\ninterrogatorscouldbeused,andstatisticscompiled be possible to programme computers, with a stor-\ntoshowhowoftentherightidentificationwasgiven.’ age capacity of about 109, to make them play the\nTheshortansweristhatwearenotaskingwhether imitation game so well that an average interrogator\nall digital computers would do well in the game nor will not have more than 70 per cent. chance of\nwhether the computers at present available would making the right identification after five minutes of"
  },
  {
    "id": 603,
    "text": "whether the computers at present available would making the right identification after five minutes of\ndowell,butwhetherthereareimaginablecomputers questioning.” (p. 442)\nat the beginning of his §7, “Learning Machines.” This changeheexpectstoseeintalkof“machinesthinking”\npassage, which comes after his revisiting of Lady — could be successful.\nLovelace’s Objection, has received little attention: The contrast between what he proposes for the\nfuture and for the present is revealing. If the research\nTheselasttwoparagraphsdonotclaimtobe\nis done as he suggests, by the time “the experiment”\nconvincingarguments.Theyshouldratherbe\nis to be conducted, he expects the machines to be\ndescribed as ‘recitations tending to produce\nso advanced that talk of “machines thinking” will be\nbelief.’\ncommonplace. In his major 1948 “Intelligent Machin-\n[Continues] ery” report,6 the rhetoric of a crucial experiment does"
  },
  {
    "id": 604,
    "text": "so advanced that talk of “machines thinking” will be\nbelief.’\ncommonplace. In his major 1948 “Intelligent Machin-\n[Continues] ery” report,6 the rhetoric of a crucial experiment does\nThe only really satisfactory support that can not appear. It was rather “the actual production of the\nbegivenfortheviewexpressedatthebegin- machines” that “would probably have some effect” in\nning of §6 [his two stated beliefs] will be that convincingcriticsandopponents,because“theideaof\nprovidedbywaitingfortheendofthecentury ‘intelligence’ is itself emotional rather than mathemat-\nand then doing the experiment described. ical” (p. 3). But in the crucial year of 1949, as we will\nseelater,Turingfacedthestrongestwaveofopposition\n[Concludes]\nfrom contemporaries, leading to his test.\nButwhatcanwesay inthemeantime?What\nsteps should be taken now if the experiment IMITATION: FROM 1936 TO 1950\nis to be successful? [36, p. 455]\nBecausethemachinemustimitatestereotypesofwhat"
  },
  {
    "id": 605,
    "text": "Butwhatcanwesay inthemeantime?What\nsteps should be taken now if the experiment IMITATION: FROM 1936 TO 1950\nis to be successful? [36, p. 455]\nBecausethemachinemustimitatestereotypesofwhat\nThis passage, in three sentences followed by two itisnot,Turing’sproposalhasoftenbeencriticizedfor\nrhetorical questions, sums up the sophistication of encouragingfakesandtricks.Butthisviewisrelatedto\nTuring’srhetoric.Thesuggestiontoruntheexperiment thereadingthatTuringwouldhavemeanthistestasa\ncomesjuxtaposedwithhisindicationthatheisactively practical experiment. Such a literal reading of Turing’s\nengaged in propaganda (“recitations tending to pro- testmissesthepointofhisuseofirony[8],andmisses\nducebelief”).Again,hepushes“theexperiment”to“the thefactthathisnotionofimitationin1950waslargely\nend of the century,” but now lands in the present, in a\ncall to arms for research into “learning machines” so\nthat the experiment — an iconic representation of the 6ArchivesCentre,King’sCollege,Cambridge,AMT/C/11.\n3"
  },
  {
    "id": 606,
    "text": "BOX 2\nTuring’s mathematical concept of imitation\nDear Miss Worsley, and therefore any sequence computed by them is\nIwasinterestedinyourworkontherelationbetween ultimately periodic. It is easy therefore in theory to\ncomputers and Turing machines. I think it would be make one imitate the other, though the size of the\nbetterthoughifyoucouldtryandfindarealtion[sic] imitating machine will (if this technique is adopted)\nbetween T machines and infinite computers, rahter have to be of the order of the exponential of the\n[sic]thanbetweenfiniteTmachinesandcomputers. sizeoftheimitated machine.Probablyyourmethods\nThe relation that you suggest is rather too trivial. could prove that this exponential relation could be\nThe fact is that the motions of either a finite T reduced to a multiplicative factor.\nmachineorafinitecomputerareultimatelyperiodic, Yours sincerely, A. M. Turinga\naTuringtoB.H.Worsley,June11,1951,typeset;emphasisadded.UnpublishedwritingsofAlanTuring,copyrightThe"
  },
  {
    "id": 607,
    "text": "machineorafinitecomputerareultimatelyperiodic, Yours sincerely, A. M. Turinga\naTuringtoB.H.Worsley,June11,1951,typeset;emphasisadded.UnpublishedwritingsofAlanTuring,copyrightThe\nProvostandScholarsofKing’sCollegeCambridge2023.B.H.WorsleyCollection,ArchivesCenter,NationalMuseumof\nAmericanHistory,SmithsonianInstitution.Quotedwithpermission.ThankstoMarkPriestleyforthisarchivalfinding.\nincontinuitywithhis1936paper[35].Thiswashinted will behave like a brain.”10\natinthewordsofthedirectoroftheNationalPhysical Even if the human brain can only be compared\nLaboratoryinaBBCbroadcastinlate1946;7 andboth to an infinite computer, could it not be simulated by\ninTuring’slectureinearly1947andinhisNPLreportin a digital computer equipped with a sufficiently large\nmid-1948;8 and as newly discovered correspondence memory? An excerpt of another newly discovered\nwiththeMexican-CanadiancomputerpioneerBeatrice Turing letter to Worsley from mid-1951 can give more"
  },
  {
    "id": 608,
    "text": "mid-1948;8 and as newly discovered correspondence memory? An excerpt of another newly discovered\nwiththeMexican-CanadiancomputerpioneerBeatrice Turing letter to Worsley from mid-1951 can give more\nWorsley (1921-1972) helps to clarify (see Box 2). contour and provide further insight into Turing’s views\nIn his letter to Worsley, Turing seems to be more (seeBox3).AhighlightinthisexcerptisTuring’sview\ninterested in the relations between “the motions” of that to the extent that the behavior of a neuron can\nTuringmachinesandinfinitecomputers,whosebehav- be described as a stochastic process, it would be\nior can be non-periodic. Perhaps he thought of the possible to “calculate random samples” of the mecha-\nlivinghumanbrainasaninfinitecomputer,inthesense nism that embodies the brain and then imitate it. An\nthatithasacontinuousinterfacewithitsenvironment, effective imitation of the brain by a machine would"
  },
  {
    "id": 609,
    "text": "thatithasacontinuousinterfacewithitsenvironment, effective imitation of the brain by a machine would\nwhich constantly intervenes and changes its logical require knowledge of the anatomy and physiology of\nstructure.9 Now,theimitationgameputsintoempirical thebraintoinspireanappropriateprogram,aswellas\nform the relation between digital computers, whose much more storage and speed than was available to\nbehaviorisultimatelyperiodic,andthebehaviorofthe the Ferranti Mark I at the time (see Fig. 2). Another\nhuman players. Can the behavior of their brains be important element in the excerpt is Turing’s point that,\napproximated by a digital computer? Turing pursued even if a thinking machine is possible, the relation he\nthis question. For his May 1951 broadcast, he wrote: has in mind is not one of identity but one of analogy:\n“the view which I hold myself, that it is not altogether “It’ll just be another species of the thinking genus.”"
  },
  {
    "id": 610,
    "text": "“the view which I hold myself, that it is not altogether “It’ll just be another species of the thinking genus.”\nunreasonable to describe digital computers as brains Anoriginalanswertothequestionofwhydesigna\n...Ifitisacceptedthatrealbrains,asfoundinanimals, testbasedonimitation,whichcanbeseenasencour-\nand in particular in men, are a sort of machine it will aging deception, is that imitation was actually Turing’s\nfollow that our digital computer, suitably programmed, fundamental principle of the new science of universal\ndigitalcomputing.Heconceivedhis1950paperpartly\nin continuity with his 1936 paper. Both were based on\n7“...abouttwelveyearsago,ayoungCambridgeMathemati- hiscoreconceptsofmachineandimitation,i.e.,whatit\ncianbythenameofTuring,wroteapaperwhichappearedin takesforamachinetoimitateanothermachine.Akey\noneofthemathematicaljournals,inwhichheworkedoutby differencethatbreaksthecontinuityisthat,by1950,he\nstrictlogicalprinciples,howfaramachinecouldbeimagined"
  },
  {
    "id": 611,
    "text": "oneofthemathematicaljournals,inwhichheworkedoutby differencethatbreaksthecontinuityisthat,by1950,he\nstrictlogicalprinciples,howfaramachinecouldbeimagined\nhadgeneralizedthelogicalarchitectureofhisuniversal\nwhich would imitate the processes of thought” (emphasis\nadded).TheListener,Nov.14,1946,p.663.\n8“LecturetoL.M.S.Feb.201947”and“IntelligentMachin-\nery.” Archives Centre, King’s College, Cambridge, AMT/B/1 10“Can digital computers think?,” broadcast on BBC Third\nandAMT/C/11. Programme, 15 May 1951. Archives Centre, King’s College,\n9“IntelligentMachinery”(op.cit.). Cambridge,AMT/B/5.\n4"
  },
  {
    "id": 612,
    "text": "BOX 3\n“...any sharp line between what machine and brain can do will fail”\nDear Miss Worsley, ... think any attempt to draw any sharp line between\nI do not think you will be able to find any clue to what machine and brain can do will fail. I think it\nessentialdifferencesbetweenbrainsandcomputing islargelyaquantitativematter.Probablyoneneeds\nmachines (if there are any), in neuron behaviour. immenselymorestoragecapacitythen[sic]wehave\nSo long as what we know about a neuron can be got, and possibly more than we shall ever have.\nembodiedinthedescriptionofstochasticprocesses, Perhaps we may have enough capacity, but just\nthe behaviour of any mechanism embodying such won’tfindanappropriateprogramme.Naturallyone\nneurons can, in principle, be calculated by a suit- won’tmakeamanthatwayever.It’lljustbeanother\nable enlarged and speeded up Ferranti [Mark II] species of the thinking genus.\nmachine.a More accurately I should say that one Yours sincerely, A. M. Turingb"
  },
  {
    "id": 613,
    "text": "able enlarged and speeded up Ferranti [Mark II] species of the thinking genus.\nmachine.a More accurately I should say that one Yours sincerely, A. M. Turingb\ncan calculate random samples of its behaviour. I\na“Ferranti”istypedanderased,and‘MarkII’(aversionoftheManchesterelectroniccomputer)isaddedinpencil.\nbTuringtoB.H.Worsley,circaJune,1951,Turing’semphasis.Creditforthissourceisexactlythesameasforthatof\nBox2.Quotedwithpermission.\nchangingitsneuroncircuitsbythegrowthofaxonsand\ndendrites.”12\nTheuseofpsychologicaltricksinpractical“Turing”\ntests has little to do with Turing’s 1950 proposal. In\n1951, he warned: “It would be quite easy to arrange\nthe [machine’s] experiences in such a way that they\nautomatically caused the structure of the machine to\nbuildupintoapreviouslyintendedform,andthiswould\nobviouslybeagrossformofcheating,almostonapar\nwith having a man inside the machine.”13 The “human\nfallibility” that Turing encouraged the machine to show"
  },
  {
    "id": 614,
    "text": "obviouslybeagrossformofcheating,almostonapar\nwith having a man inside the machine.”13 The “human\nfallibility” that Turing encouraged the machine to show\nwas meant as a by-product of learning by experience\n[36,p.459]:“Anotherimportantresultofpreparingour\nmachineforitspartintheimitationgamebyaprocess\nof teaching and learning is that ‘human fallibility’ is\nFIGURE 2. ConsoleofFerrantiMarkIandagroupwithTur- likely to be omitted [from the teaching] in a rather\ning’ssecretaryattheComputingMachineLaboratory,Sylvia natural way, i.e., [learned] without special ‘coaching’.”\nRobinson (née Wagstaff), pretending to play chess with the That is, for a machine to be a valid player of Turing’s\nmachine,c.1955.CourtesyofTheUniversityofManchester. test,itcannotbespeciallypreparedforit.Thismeans\nthat we have never seen a practical Turing test.\nmachine.Itwouldnotonlyfollowinstructions,butwould\nTHE METHOD OF THOUGHT\nalso be able to acquire new cognitive skills by learn-"
  },
  {
    "id": 615,
    "text": "that we have never seen a practical Turing test.\nmachine.Itwouldnotonlyfollowinstructions,butwould\nTHE METHOD OF THOUGHT\nalso be able to acquire new cognitive skills by learn-\ning. Using Turing’s 1948 language,11 universality can EXPERIMENTS\nbe achieved by starting with an “organized” machine The various rhetorical questions Turing posed,\n(1936),orwithan“unorganized”machine(1948/1950). Q1...Q4, to replace the original question, Q, can be\nWhereas in 1936 the machine would be given an a generalized as follows [11]:\npriori, well-defined and fixed table of instructions for\neach task, in 1950 it would also be able to perform a\nnew task by changing its logical structure as a result 12Turing to Ross Ashby, circa November 19, 1946. British\noflearningbyexperience,muchasthebraindoes“by Library, Collection “W. Ross Ashby: correspondence of W.\nRossAshby,”AddMS89153/26.\n13“Intelligentmachinery,ahereticaltheory,”alecturegiven\nto “51 Society” at Manchester, c. 1951. Archives Centre,"
  },
  {
    "id": 616,
    "text": "RossAshby,”AddMS89153/26.\n13“Intelligentmachinery,ahereticaltheory,”alecturegiven\nto “51 Society” at Manchester, c. 1951. Archives Centre,\n11“IntelligentMachinery”(op.cit.). King’sCollege,Cambridge,AMT/B/4.\n5"
  },
  {
    "id": 617,
    "text": "Question Q‹: could player A imitate intellectual itself by some experience. [19, pp. 138-139]\nstereotypes associated with player B’s type success- Theconditions,i.e.,thedistanceofthefallandthesize\nfully (well enough to deceive player C), despite A and ofthestones,arecontinuouslyvariedinthephysicist’s\nB’s physical differences? mind and eventually stretched to the celestial scale.\nIthasbeenlargelyunnoticedthatthevariousques- Reciprocally, the concept of a celestial body, such as\ntions instantiating Q‹ can be viewed as following a theEarthortheMoon,becomesinterchangeablewith\ncase-control methodology, applied in two stages. the concept of a stone, and quite unequal stones can\nAt the more obvious intra-game level, A plays the then become mutually attracted. The cases continu-\ncase, and B plays the control. However, at the inter- ously merge into one another, and a conceptual inte-\ngamelevel,twovariantssetthecase(machine-woman gration is established that connects near-earth bodies"
  },
  {
    "id": 618,
    "text": "gamelevel,twovariantssetthecase(machine-woman gration is established that connects near-earth bodies\nandthemachine-man)andtheothertwosetthecontrol to celestial bodies under a unified concept.\n(man-woman and the machine-machine). While the Turing’s imitation game extended the scope of\nfirst two are open, creating suspense around the test, ideas and expectations established earlier in his 1936\nthe latter two are resolved as follows. It is known that paper, moving from machine-machine and restricted\na man (A) can imitate gender stereotypes associated human-machine imitation in 193614 to more general\nwith a woman (B) to deceive an interrogator (C) de- human-machine imitation in 1950.\nspite their physical differences, as assumed in parlor Tounderstandthisbetter,letustakeabrieflookat\ngames and as Turing illustrates on p. 434: “My hair Turing’s 1948 report “Intelligent Machinery” (op. cit.)."
  },
  {
    "id": 619,
    "text": "games and as Turing illustrates on p. 434: “My hair Turing’s 1948 report “Intelligent Machinery” (op. cit.).\nisshingled...”Further,regardingthemachine-machine In section (§3) ‘Varieties of machinery,’ he noted: “All\nvariant, it is also known that a digital computer (A), machinerycanberegardedascontinuous,butwhenit\nbecause of its universality property [36, §§4, 5], can is possible to regard it as discrete it is usually best to\nimitateanydiscrete-statemachine(B)andevenacon- do so.” A brain, he wrote, “is probably” a ‘continuous\ntinuousone(p.451),despitetheirphysicaldifferences. controlling’ machine, but in light of the digital nature\nWe can now explore how Turing’s presentation of of neural impulses, it “is very similar to much discrete\nhistestfitsMach’sdescriptionof“thebasicmethodof machinery.” In section (§6) “Man as Machine,” he re-\nthoughtexperiments,”whichisvariation,continuouslyif ferred to the imitation of “any small part of a man” by"
  },
  {
    "id": 620,
    "text": "thoughtexperiments,”whichisvariation,continuouslyif ferred to the imitation of “any small part of a man” by\npossible.Machistheauthorofperhapsthemostclas- machines:“Agreatpositivereasonforbelievinginthe\nsictextonthoughtexperimentsinthemodernscientific possibilityofmakingthinkingmachineryisthefactthat\ntradition[19],inwhichhedevelopedobservationsand it is possible to make machinery to imitate any small\ninsightsbasedonexamplesfromthehistoryofmodern partofaman”(p.420).Inlightofthis,heargued:“One\nphysics,mathematics,andcommonsenseexperience. way of setting about our task of building a ‘thinking\nHe wrote: “By varying the conditions (continuously machine’wouldbetotakeamanasawholeandtotry\nif possible), the scope of ideas (expectations) tied toreplaceallthepartsofhimbymachinery.”ButTuring\nto them is extended: by modifying and specializing dismissed such a method as “altogether too slow and\nthe conditions we modify and specialize the ideas, impracticable,”andlateralludedtomoralandaesthetic"
  },
  {
    "id": 621,
    "text": "the conditions we modify and specialize the ideas, impracticable,”andlateralludedtomoralandaesthetic\nmakingthemmoredeterminate,andthetwoprocesses reasons as well.15\nalternate” (p. 139). Mach illustrated his point with the We can now follow Turing’s use of the method\nprocess of discovery of universal gravitation: of continuous variation in the design of his imita-\nA stone falls to the ground. Increase the tion tests. The central question (Q‹) Turing asks is\nstone’s distance from the earth, and it would whethertheintellectualandculturalperformances(the\ngo against the grain to expect that this con- stereotypes)16 associated with woman, man, machine\ntinuousincreasewouldleadtosomedisconti-\nnuity.Evenatlunardistancethestonewillnot\nsuddenly lose its tendency to fall. Moreover,\n14“Wemaycompareamanintheprocessofcomputinga\nreal number to a machine which is only capable of a finite\nbigstonesfalllikesmallones:themoontends\nnumberofconditions”[35,p.231]."
  },
  {
    "id": 622,
    "text": "14“Wemaycompareamanintheprocessofcomputinga\nreal number to a machine which is only capable of a finite\nbigstonesfalllikesmallones:themoontends\nnumberofconditions”[35,p.231].\nto fall to the earth. Our ideas would lose 15For the May 1951 broadcast (op. cit.), he wrote: “I cer-\nthe requisite determination if one body were tainly hope and believe that no great efforts will be put into\nattractedtotheotherbutnotthereverse,thus makingmachineswiththemostdistinctivelyhuman,butnon-\nthe attraction is mutual and remains so with intellectual characteristics such as the shape of the human\nbody;itappearstometobequitefutiletomakesuchattempts\nunequalbodies,forthecasesmergeintoone\nand their results would have something like the unpleasant\nanother continuously ... discontinuities are\nqualityofartificialflowers.”\nquite conceivable, but it is highly improbable 16SusanSterrettfirstemphasizedtheimportanceofstereo-\nthat their existence would not have betrayed typesintheimitationgame[34].\n6"
  },
  {
    "id": 623,
    "text": "(the types) could be imitated, and thus softly trans- and computer pioneer, Fellow of the Royal Society\nposed.Notethatforanyarbitrarilychosentype,say,a (FRS), Douglas Hartree (1897-1958), over the mean-\nwoman, further specific subtypes can be continuously ing of the newly existing digital computers, which had\nconceived and considered as varied conditions of the started in 1946 [9]. Now, in mid-1949, new opponents\nimitation game: women having a certain property, a had arrived, most notably the neurosurgeon Geoffrey\nsubproperty,andsoon.Foranytwoarbitrarilychosen Jefferson (1886-1961), and the chemist and philoso-\ntypes, a new type can be conceived, whether as a pherMichaelPolanyi(1891-1976),bothalsoFRSand\nspecialization or a modification (“any small part of a based at the same institution as Turing, the University\nman”). Because concepts are fluid abstractions, there of Manchester, where Turing had spent a year as a"
  },
  {
    "id": 624,
    "text": "man”). Because concepts are fluid abstractions, there of Manchester, where Turing had spent a year as a\nis an evolving continuum of levels and types. ReaderintheDepartmentofMathematics[15].These\nThe question across the various versions of the three thinkers challenged Turing’s claims about the\ngamecanbeposedthisway:howdoesC’sperception futurepossibilitiesandlimitationsofdigitalcomputers.\nofA’sperformanceagainstB’sperformancechangeas InJune1949,HartreepublishedhisCalculatingIn-\nthe game’s conditions are (continuously) varied? Will strumentsandMachines[12],inwhichAdaLovelace’s\nit change if gendered verbal behavior is required as work was acknowledged seemingly for the first time\na subtype of human verbal behavior? Will it change if by a twentieth-century computer pioneer [9]. Since\nthemachine’shardwareisincreasedand/oritslearning November1946,Hartreehadbeenopposingtheuseof\nprogramismodified?ForTuring,thereisnoconceptual the term “electronic brain.” He wrote in a letter to the"
  },
  {
    "id": 625,
    "text": "themachine’shardwareisincreasedand/oritslearning November1946,Hartreehadbeenopposingtheuseof\nprogramismodified?ForTuring,thereisnoconceptual the term “electronic brain.” He wrote in a letter to the\ndiscontinuitybetweenthevariousconditionsinstantiat- Times: “These machines can only do precisely what\ning his thought experiment. It is often asked whether they are instructed to do by the operators who set\nthe imitation goal changes from the machine-woman them up.”18 Now in 1949, Hartree added strength to\ntest(p.434)tothemachine-mantest(p.442).Notethat his argument by quoting the words of Ada Lovelace\nthisopenquestionvanishesunderthisinterpretationof from the 1840s about Charles Babbage’s machine:\nthetest,whichobservesmaterialandstructuralfactsof “TheAnalyticalEnginehasnopretensionstooriginate\nTuring’stext:hedescribessuchgoalsonlyveryloosely, anything...Itcandowhateverweknowhowtoorderit\nand, more importantly, continuously varies the design toperform”(heremphasis)[12,p.70].NotingHartree’s"
  },
  {
    "id": 626,
    "text": "Turing’stext:hedescribessuchgoalsonlyveryloosely, anything...Itcandowhateverweknowhowtoorderit\nand, more importantly, continuously varies the design toperform”(heremphasis)[12,p.70].NotingHartree’s\nof the game. His focus is on the power of universal anachronism in taking Lovelace’s words out of their\ndigital computing to imitate and deconstruct arbitrary time and place, Turing further developed his earlier,\ntypes, not on setting this or that type for the sake of 1947 response to Hartree’s challenge,19 now calling\npromoting a particular test. it “(6) Lady Lovelace’s objection” [36, p. 450]. Turing\nFrom 1948 to 1952, Turing presented various im- arguedthatintelligentbehavioristheresultoflearning,\nitation tests based on both the game of chess and a capability he had no problem attributing to future\nconversation, even bringing back chess at the end of digital computers. He also questioned the implicit as-"
  },
  {
    "id": 627,
    "text": "conversation, even bringing back chess at the end of digital computers. He also questioned the implicit as-\nhis1950paper(p.460),andreferringtohis(“my”)var- sumption of Hartree’s challenge: “Who can be certain\nious“imitationtests”in1952.17 Whywouldhepresent that ‘original work’ that he has done was not simply\nvarioustests,asopposedtoawell-defined,controlled the growth of the seed planted in him by teaching, or\nexperiment? This is a historically sound question, the effect of following well-known general principles”\nbecause it does not struggle with the materiality of (p. 450). In the imitation game, Turing suggested, the\nTuring’s texts and their chronological coherence. Nor interrogator would be able to evaluate the machine’s\ndoes it erase some of his tests in favor of others, or ability to learn: “The game (with the player B omitted)\noverlook the historical conditions of his proposal. This is frequently used in practice under the name of viva"
  },
  {
    "id": 628,
    "text": "overlook the historical conditions of his proposal. This is frequently used in practice under the name of viva\npaper has provided an answer by reconstructing what vocetodiscoverwhethersomeonereallyunderstands\ncan be called Turing’s use of the method of thought somethingorhas‘learntitparrotfashion’”(p.446).But\nexperiments, whose context will now be explored. then, what is player B doing in the imitation game?\nFollowing the 1949 events will suggest an answer.\n1949, THE CRUCIAL YEAR OnJune9,inLondon,Jeffersondeliveredhispres-\nAs is often the case with thought experiments, Turing tigiousListerOrationon“TheMindofMechanicalMan,”\nproposed his test out of a controversy [10]. He was which was published in the debuting British Medical\ncoming from his continuing disputes with the physicist\n18“The‘ElectronicBrain’:AMisleadingTerm;NoSubstitute\n17‘Can automatic calculating machines be said to think?’, forThought,”Times,November7,1946."
  },
  {
    "id": 629,
    "text": "coming from his continuing disputes with the physicist\n18“The‘ElectronicBrain’:AMisleadingTerm;NoSubstitute\n17‘Can automatic calculating machines be said to think?’, forThought,”Times,November7,1946.\nJanuary1952(op.cit.). 19‘LecturetoL.M.S.Feb.201947’(op.cit.),p.22.\n7"
  },
  {
    "id": 630,
    "text": "Journal on June 25 [16]. His lecture was headlined in ing mate.\nthe Times on June 10,20 emphasizing his claim that In reaction to Grey Walter and his transgressive tor-\n“Notuntilamachinecanwriteasonnetorcomposea toises[14],24 Jeffersonofferedtheimageofagenuine\nconcerto because of thoughts and emotions felt, and individual of a species placed side by side with an\nnot by the chance fall of symbols, could we agree artificial one to emphasize the latter’s artificiality. The\nthat machine equals brain” (p. 1110). This rendered function of the genuine individual is to expose the\nTuring’sfamousresponse:“Idonotthinkyoucaneven artificiality of the impostor. The means of exposure\ndraw the line about sonnets, though the comparison is the failure to demonstrate the authentic (sexual)\nis perhaps a little bit unfair because a sonnet written behavior of the original species. This can explain\nby a machine will be better appreciated by another Turing’s introduction of a (gendered) control player"
  },
  {
    "id": 631,
    "text": "by a machine will be better appreciated by another Turing’s introduction of a (gendered) control player\nmachine.”21 In October and December 1949, the two B, who appears in Turing’s 1950 test, whose design\nseminars on “Mind and Machine” were organized by was prompted by his reading of Jefferson, but not in\nPolanyi et al., and attended by Jefferson, Turing et Turing’s1948,1951,and1952tests.Indiscussing“(4)\nal., at the Philosophy Department in Manchester [28, TheArgumentfromConsciousness,”Turingaddressed\np. 275]. These seminar discussions, followed by Jef- Jefferson directly and quoted in full his conditions for\nferson giving Turing an offprint of his Lister Oration,22 agreeing “that machine equals brain,” including “be\nwhichTuringreadandmarkedwithapencil,23 ledhim warmed by flattery” and “be charmed by sex” [36,\nto write his 1950 paper and propose his test [10]. pp. 445-446].25 In discussing the “(5) Argument from"
  },
  {
    "id": 632,
    "text": "whichTuringreadandmarkedwithapencil,23 ledhim warmed by flattery” and “be charmed by sex” [36,\nto write his 1950 paper and propose his test [10]. pp. 445-446].25 In discussing the “(5) Argument from\nJefferson had characterized intelligence as an Various Disabilities” (p. 447), Turing again addressed\nemergent property of the animal nervous system. He Jefferson (p. 450) and argued that to say that a ma-\nemphasizedthat“sexhormonesintroducepeculiarities chine could never “fall in love” or “make someone fall\nof behaviour often as inexplicable as they are impres- in love with it” was a flawed scientific induction from\nsive” (p. 1107). Because “modern automata” are not the capabilities of present machines.\nmoved by male and female sex hormones, they could Turing’s test design may have been an ironic re-\nnot exhibit such peculiarities to imitate the actions of sponse to Jefferson’s suggestion that gendered be-"
  },
  {
    "id": 633,
    "text": "not exhibit such peculiarities to imitate the actions of sponse to Jefferson’s suggestion that gendered be-\nanimalsor“men.”Specifically,heusedathoughtexper- havior is causally related to the physiology of male\niment to criticize Grey Walter’s mechanical turtles by and female sex hormones. As a repressed homo-\nsuggesting that gendered behavior is causally related sexual [15] and non-conformist in postwar England\nto the physiology of sex hormones (ibid.): [8], Turing might have been prepared to refer to a\n[...It]shouldbepossibletoconstructasimple gender test. However, we have just seen that a basic\nanimal such as a tortoise (as Grey Walter version of this idea was actually available to him in\ningeniously proposed) thatwould show by its the form of Jefferson’s reaction to Walter’s tortoises.\nmovements that it disliked bright lights, cold, ApartfromTuring’s1950paper,inwhichheisindirect\nand damp, and be apparently frightened by dialoguewithJefferson,helinkshisviewsonmachine"
  },
  {
    "id": 634,
    "text": "movements that it disliked bright lights, cold, ApartfromTuring’s1950paper,inwhichheisindirect\nand damp, and be apparently frightened by dialoguewithJefferson,helinkshisviewsonmachine\nloud noises, moving towards or away from intelligence to sex and gender in one other known\nsuch stimuli as its receptors were capable source,againwithJeffersoninthebackground.Atthe\nof responding to. In a favourable situation end of a letter to a friend written after the Wilmslow\nthe behaviour of such a toy could appear to police challenged his testimony,26 he comments on\nbe very lifelike — so much so that a good the BBC broadcast of January 1952 (op. cit.): “Glad\ndemonstrator might cause the credulous to youenjoyedthebroadcast.J.[Jefferson]certainlywas\nexclaim ‘This is indeed a tortoise.’ I imagine, rather disappointing though. I’m rather afraid that the\nhowever, that another tortoise would quickly following syllogism may be used by some in the fu-"
  },
  {
    "id": 635,
    "text": "exclaim ‘This is indeed a tortoise.’ I imagine, rather disappointing though. I’m rather afraid that the\nhowever, that another tortoise would quickly following syllogism may be used by some in the fu-\nfinditapuzzlingcompanionandadisappoint- ture[:] Turing believes machines think. Turing lies with\n20“NoMindForMechanicalMan.”Times,10June1949,p.2. 24Jefferson would attack Walter’s automata again in\n21“TheMechanicalBrain.”Times,11June1949,p.4. speechestolearnedsocietiesinManchesterin1953[17]and\n22ThismayhavehappenedintheeveningoftheDecember 1956[18]inthewakeofWalter’sTheLivingBrain[40].\nmeetingoftheManchesterseminar(op.cit.),when,according 25Jefferson’sresponsewas[17,p.73]:“Buttherearethose\nto a later letter from Jefferson to Ethel S. Turing, Turing and likeDr.Turingwhobelievethatwehavenorighttodenyself-\nJ.Z.YoungwenttodinneratJefferson’shouse[38,p.xx]. consciousnesstothemachinessincetheyfulfillthedefinition"
  },
  {
    "id": 636,
    "text": "J.Z.YoungwenttodinneratJefferson’shouse[38,p.xx]. consciousnesstothemachinessincetheyfulfillthedefinition\n23Off-print, “The mind of mechanical man” by Geof- ofmindasgivenabove—theabilitytomakechoices.”\nfrey Jefferson. Archives Centre, King’s College, Cambridge, 26Turing to Norman Routledge, circa mid-Feb., 1952.\nAMT/B/44. ArchivesCentre,King’sCollege,Cambridge,AMT/D/14a.\n8"
  },
  {
    "id": 637,
    "text": "men. Therefore machines do not think.” It remains to Machinery’ (op. cit., pp. 21-22), Turing had discussed\nbeexploredTuring’schoiceofconversationforhistest. a tradeoff between convenient and impressive intel-\nSurvivingminutesofthe“MindandMachine”sem- lectual fields for exploring machine intelligence. After\ninarheldonOctober27,1949,werepublishedin2000 discussing “various games e.g. chess,” he wrote: “Of\nby a participant, Wolfe Mays [21]. In the first session, the above possible fields the learning of languages\nPolanyi presented a statement, “Can the mind be wouldbethemostimpressive,sinceitisthemosthu-\nrepresented by a machine?,”27 which was a Gödelian manoftheseactivities.”However,heavoidedlanguage\nargument that humans can do things that machines learning because it seemed “to depend rather too\ncannot. Although Turing had already addressed this muchonsenseorgansandlocomotiontobefeasible,”"
  },
  {
    "id": 638,
    "text": "argument that humans can do things that machines learning because it seemed “to depend rather too\ncannot. Although Turing had already addressed this muchonsenseorgansandlocomotiontobefeasible,”\nargument in his 1947 lecture (op. cit.), Polanyi’s in- stuck with chess, and ended up describing a chess-\nsistence may help explain Turing’s inclusion of “(3) based imitation game. Now, in October 1949, he saw\nTheMathematical Objection”[36, p.444].Further, the chess being dismissed as unimpressive to make the\nminutes of the Manchester seminar show that Polanyi case for machine intelligence because its rules could\ntried to distinguish the formal “rules of the logical sys- be specified.\ntem”fromtheinformal“ruleswhichdetermineourown Sometimelater,probablyaroundChristmas1949,\nbehaviour,” and this helps explain Turing’s inclusion TuringreadJefferson’sListerOration[16]andmarked\nof “(8) The Argument from Informality of Behaviour” thepassagequotingRenéDescartes(p.1106),which"
  },
  {
    "id": 639,
    "text": "behaviour,” and this helps explain Turing’s inclusion TuringreadJefferson’sListerOration[16]andmarked\nof “(8) The Argument from Informality of Behaviour” thepassagequotingRenéDescartes(p.1106),which\n(p. 452). Polanyi’s argument came too late, as Turing starts: “Descartes made the point, and a basic one\nhad long been involved in such debates in the Moral it is, that a parrot repeated only what it had been\nSciences Club at Cambridge University, both before taught and only a fragment of that; it never used\nand after World War II.28 words to express its own thoughts.” Overall, Jefferson\nYears later [28, p. 275], Polanyi remembered “a suggested ‘speech’ to be the distinguishing feature of\ncommunication to a Symposium held on ‘Mind and human intelligence compared to other kinds of animal\nMachine’ at Manchester University in October, 1949,” intelligence: “Granted that much that goes on in our"
  },
  {
    "id": 640,
    "text": "Machine’ at Manchester University in October, 1949,” intelligence: “Granted that much that goes on in our\nin which “A.M. Turing has shown that it is possi- heads is wordless ... we certainly require words for\nble to devise a machine which will both construct conceptual thinking as well as for expression ... It is\nand assert as new axioms an indefinite sequence of herethatthereisthesuddenandmysteriousleapfrom\nGödeliansentences.”29 Polanyiresumed,showingthat thehighestanimaltoman,anditisinthespeechareas\nhe assimilated the punch: “Any heuristic process of a of the dominant hemisphere ... that Descartes should\nroutinecharacter—forwhichinthedeductivesciences have put the soul, the highest intellectual faculties”\nthe Gödelian process is an example—could likewise (p. 1109).\nbe carried out automatically.” However, Polanyi used Unlike chess, which is governed by definite rules,\nthe same argument to dismiss the game of chess good performance in conversation cannot be easily"
  },
  {
    "id": 641,
    "text": "the same argument to dismiss the game of chess good performance in conversation cannot be easily\nas a testbed for machine intelligence, noting: “A rou- specified. Turing’s 1950 choice for “the learning of\ntine game of chess can be played automatically by languages” as the intellectual field addressed in his\na machine, and indeed, all arts can be performed testcanbebestunderstoodasyetanotherconcession\nautomatically to the extent to which the rules of the to Jefferson and, in this case, to Polanyi as well.\nart can be specified.” Finally, Jefferson also argued that the nervous\nChess,notconversation,hadbeenTuring’schosen impulse is not a purely electrical phenomenon but\nfieldtoillustrate,develop,andtestmachineintelligence also a chemical one that depends on the continuity of\nsinceatleastFebruary1946.30 Inhis1948‘Intelligent specific physical quantities (p. 1108). It would thus be\nincommensurablewiththeactivityofadigitalcomputer.\nInresponse,Turingformulated“(7)TheArgumentfrom"
  },
  {
    "id": 642,
    "text": "incommensurablewiththeactivityofadigitalcomputer.\nInresponse,Turingformulated“(7)TheArgumentfrom\n27Polanyi, Michael. Papers, Box 32, Folder 6, Hanna Hol-\nContinuityintheNervousSystem”[36,p.451],inwhich\nbornGraySpecialCollectionsResearchCenter,Universityof\nhe used the imitation game in its machine-machine\nChicagoLibrary.\n28Minutes and other papers of the Moral Sciences version to neutralize that physical difference. A digital\nClub, 1878–2018, Cambridge University Library, computer (a discrete-state machine) could imitate a\nGBR/0265/UA/Min.IX.39-48˚,56-6˚ etc. differential analyzer (a continuous-state machine) to\n29Polanyiaddedthat“thisisforeshadowed”inTuring’s1938 compute a transcendental number such as π up to a\npaperbasedonhisPh.D.thesis,“SystemsofLogicBasedon\ncertaindigit.Turinggavethisasaproofofconceptthat\nOrdinals,”J.LondonMath.Soc.s2-45(1),161-228.\nsuch a difference in nature disappears with the power\n30“Proposedelectroniccalculator,”February1946.Archives"
  },
  {
    "id": 643,
    "text": "Ordinals,”J.LondonMath.Soc.s2-45(1),161-228.\nsuch a difference in nature disappears with the power\n30“Proposedelectroniccalculator,”February1946.Archives\nofuniversaldigitalcomputing,givensufficientstorage.\nCentre, King’s College, Cambridge, AMT/C/32. On p. 16,\nTuringasks:“Canthemachineplaychess?” In summary, there is enough evidence to suggest\n9"
  },
  {
    "id": 644,
    "text": "that Turing varied the design of his imitation tests in (p.vii),andtowhichtheyinvitedTuringtocontribute.33\nresponsetothechallengesposedbyHartree,Polanyi, Turingdeclinedtheinvitation,sayingthathehadbeen\nand Jefferson. Turing’s test was a response to critics. working for the last two years on “the mathematics\nBut was it also intended as a positive proposition? ofmorphogenesis,”althoughheexpected“togetback\nIn the 1990s, Turing’s former student, close friend, to cybernetics very shortly.”34 One year and four days\nandliteraryexecutor,RobinGandy,wrotethatTuring’s later,Turingwasdead,andearlyAIwouldnotnotehis\n1950 paper “was intended not so much as a pene- biological turn. Commenting on “the Turing definition\ntrating contribution to philosophy but as propaganda.” of thinking” (p. vi), McCarthy and Shannon found it\nGandy added: “Turing thought the time had come “interesting” because it “has the advantages of being"
  },
  {
    "id": 645,
    "text": "Gandy added: “Turing thought the time had come “interesting” because it “has the advantages of being\nfor philosophers and mathematicians and scientists to operationalor,inthepsychologists’term,behavioristic\ntakeseriouslythefactthatcomputerswerenotmerely ...Nometaphysicalnotionsofconsciousness,egoand\ncalculating engines but were capable of behaviour the like are involved.” They also thought that this very\nwhich must be accounted as intelligent; he sought strength could be a weakness, because it has “the\nto persuade people that this was so. He wrote this disadvantage” of being susceptible to a memorizing\npaperunlikehismathematicalpapersquicklyandwith machine playing the imitation game by looking up “a\nenjoyment. I can remember him reading aloud to me suitable dictionary.”\nsomeofthepassagesalwayswithasmile,sometimes McCarthyandShannonreferredinterchangeablyto\nwithagiggle”[6,p.125].Wecannowexploretheeffect “definition” and to a word that Turing actually used,"
  },
  {
    "id": 646,
    "text": "someofthepassagesalwayswithasmile,sometimes McCarthyandShannonreferredinterchangeablyto\nwithagiggle”[6,p.125].Wecannowexploretheeffect “definition” and to a word that Turing actually used,\nofTuring’spropagandaformachineintelligenceonthe “criterion:”“Whilecertainlynomachinesatthepresent\nother side of the North Atlantic. time can even make a start at satisfying this rather\nstrongcriterion,Turinghasspeculatedthatwithinafew\nTURING’S TEST AND EARLY AI decadesitwillbepossibletoprogramgeneralpurpose\ncomputersinsuchawayastosatisfythistest”[25,p.v,\nClaude Shannon visited Turing in Manchester in Oc-\ntober 1950,31 and may have returned to the United emphasis added].\nStates with an offprint of Turing’s 1950 paper,32 which In 1955, before the publication of Automata Stud-\nhe would cite in his “Computers and Automata” [29] ies,McCarthyandShannon,togetherwithMarvinMin-\npublished in Proc. IRE in October 1953. “Reread- sky and Nathaniel Rochester, co-authored their well-"
  },
  {
    "id": 647,
    "text": "he would cite in his “Computers and Automata” [29] ies,McCarthyandShannon,togetherwithMarvinMin-\npublished in Proc. IRE in October 1953. “Reread- sky and Nathaniel Rochester, co-authored their well-\ning Samuel Butler’s Erewhon today,” Shannon wrote, known “Proposal” for AI research [24]. Unlike Turing\n“one finds ‘The Book of the Machines’ disturbingly himself, they seem to have thought of machine intel-\nprophetic” (p. 1235). Butler’s novel [3] envisioned a ligence in complete continuity with Turing machines,\nrevolutionofthemachinesagainstasatireoftheVicto- as their opening paragraph suggests: “The study is to\nrians representing the human species. It was invoked proceed on the basis of the conjecture that every as-\nas a dystopia in June 1949, first by a Catholic priest pectoflearningoranyotherfeatureofintelligencecan\nin a letter to The Times published on the 14th (p. 5), in principle be so precisely described that a machine"
  },
  {
    "id": 648,
    "text": "in a letter to The Times published on the 14th (p. 5), in principle be so precisely described that a machine\nand then in an editorial, “Mind, Machine, and Man,” can be made to simulate it.” Turing’s postwar view,\nof the British Medical Journal introducing Jefferson’s which we have seen in connection with his concepts\narticle on the 25th (pp. 1129-1130). Both urged scien- of machine and imitation above, seems instead to be\ntiststodisassociatethemselvesfromTuring’sresearch thatmachineswouldlearntheirbehaviorprimarilyfrom\nprogram. Turing responded with irony, not without a experience,growinginintelligencelikeahumanchild,\npoint. In his writings of 1950 and 1951 he referred to notalwaysbybeinggivenpreciseinstructionsonhow\nButler’sworkforanimageofhisvisionofthefutureof to do things. In any case, the proponents of the Dart-\nmachines in nature and society [8]. mouthseminardidfollowTuringcloselyinwriting:“For\nthe present purpose the artificial intelligence problem"
  },
  {
    "id": 649,
    "text": "machines in nature and society [8]. mouthseminardidfollowTuringcloselyinwriting:“For\nthe present purpose the artificial intelligence problem\nBy May 1953, Shannon was working with John\nis taken to be that of making a machine behave in\nMcCarthy on their collection Automata Studies [25],\nwaysthatwouldbecalledintelligentifahumanwereso\nwhichrevolvedaround“thetheoryofTuringmachines”\nbehaving”(p.7).Thisdefinition—compareitwith“the\nTuring definition of thinking” — would stay. Intelligent\n31ClaudeE.Shannon,anoralhistoryconductedin1982by\nRobertPrice.IEEEHistoryCenter,Piscataway,NJ,USA.\n32ThefirstreprintofTuring’s‘ComputingMachineryandIn- 33Shannon and McCarthy to Turing, May 18, 1953. Alan\ntelligence’intheUSappearstobethatofJamesR.Newman Turing Papers (Additional), University of Manchester Library,\n(Ed.),TheWorldofMathematics,vol.4(NewYork:Simonand GB133TUR/Add/123.\nSchuster),firstpublishedJanuary1,1956. 34TuringtoShannon,June3,1953(ibid.).\n10"
  },
  {
    "id": 650,
    "text": "machines would be machines designed to match or heuristic techniques, which relied on the expertise of\nexceed human performance in a range of cognitive human players to prune the search space of possible\ntasks.Theimplication,emphasizedbybothTuringand moves,withbruteforcecomputing.“[I]tisameasureof\nNorbertWienerbutnotbythefoundersofAI,wasthat our limited understanding of the principles of artificial\nhumans could no longer be needed for most white- intelligence,” McCarthy wrote, “that this level of play\ncollar jobs in the labor market. requiresmanymillionsoftimesasmuchcomputingas\nIntheearly1960s,E.FeigenbaumandJ.Feldman a human chess player does.” It may be, but that the\nnotedinComputersandThought [5]thatTuring’s1950 problemwas“largelyaquantitativematter”washinted\npaper “appeared five years before concrete develop- at by Turing in his letter of c. June 1951 (Box 3).\nments in intelligent behavior by machine began to TenyearsafterDeepBluevs.Kasparov,McCarthy"
  },
  {
    "id": 651,
    "text": "paper “appeared five years before concrete develop- at by Turing in his letter of c. June 1951 (Box 3).\nments in intelligent behavior by machine began to TenyearsafterDeepBluevs.Kasparov,McCarthy\noccur;” and “yet,” they continued, “it remains today referred to Turing’s 1947 lecture (op. cit.) as “the first\none of the most cogent and thorough discussions in scientific discussion of human level machine intelli-\nthe literature on the general question “Can a machine gence,”andtoTuring’s1950paperas“amplifying”that\nthink?” (pp. 9-10). They observed Turing’s “behavior- discussion into a “goal” [23, p. 1174].\nistic posture relative to the question,” which “is to be In 1992, Minsky co-authored a work of fiction, The\ndecidedbyanunprejudicedcomparisonofthealleged Turing Option (Warner, New York), in which Turing’s\n‘thinkingbehavior’ofthemachinewithnormal‘thinking test is featured in the preface. In 1995, Minsky took"
  },
  {
    "id": 652,
    "text": "‘thinkingbehavior’ofthemachinewithnormal‘thinking test is featured in the preface. In 1995, Minsky took\nbehavior’ in human beings” (emphasis added). They a stand against Loebner’s Weizenbaum experiments,\nconcluded:“Heproposesanexperiment—commonly pleadingto“revokehisstupidprize,savehimselfsome\ncalled‘Turing’stest’—inwhichtheunprejudicedcom- money, and spare us the horror of this obnoxious and\nparison could be made ... Though the test has flaws, unproductive annual publicity campaign.”35 In 2013,\nit is the best that has been proposed to date.” whenaskedabouttheTuringtestinatapedinterview,\nMinsky, in the preface to his 1967 collection [27], Minsky said: “The Turing test is a joke, sort of, about\nreiteratesthedefinitionofAIas“thescienceofmaking saying‘Amachinewouldbeintelligentifitdoesthings\nmachines do things that would require intelligence if that an observer would say must be being done by"
  },
  {
    "id": 653,
    "text": "reiteratesthedefinitionofAIas“thescienceofmaking saying‘Amachinewouldbeintelligentifitdoesthings\nmachines do things that would require intelligence if that an observer would say must be being done by\ndone by men” (p. v). Around the same time, Minsky a human’ ... it was suggested by Alan Turing as one\ncollaborated with Stanley Kubrick and Arthur Clarke way to evaluate a machine but he had never intended\non their 1968 screenplay, also written as a novel, it as being the way to decide whether a machine was\n2001:ASpaceOdyssey [4],whichfeaturedafuturistic really intelligent.”36 This materially connects McCarthy\ncomputer named HAL: et al.’s definition of “the AI problem” with Turing’s test,\nWhetherHALcouldactuallythinkwasaques- if material evidence were still needed.\ntion which had been settled by the British Overall, it seems that all of these AI pioneers\nmathematicianAlanTuringbackinthe1940s. understood and were inspired by Turing’s test at the"
  },
  {
    "id": 654,
    "text": "tion which had been settled by the British Overall, it seems that all of these AI pioneers\nmathematicianAlanTuringbackinthe1940s. understood and were inspired by Turing’s test at the\nTuringhadpointedoutthat,ifonecouldcarry level of conceptual foundations. Even if some of them\nout aprolonged conversation witha machine also used the term “experiment,” none of them took it\n— whether by typewriter or microphone was literally as a practical experiment, which would indeed\nimmaterial—withoutbeingabletodistinguish imply an astonishing lack of imagination on their part.\nbetween its replies and those that a man TheTuringtesthelpedmovetheburgeoningfieldofAI\nmightgive,thenthemachinewasthinking,by awayfromunproductivedebatesaboutthemeaningof\nanysensibledefinitionoftheword.HALcould words, allowing Minsky, for example, to write in 1967\npass the Turing test with ease. [26]: “Turing discusses some of these issues in his"
  },
  {
    "id": 655,
    "text": "anysensibledefinitionoftheword.HALcould words, allowing Minsky, for example, to write in 1967\npass the Turing test with ease. [26]: “Turing discusses some of these issues in his\nThe “Turing definition of thinking\" was to become leg- brilliant article, ‘Computing Machines and Intelligence’\nendary. [sic], and I will not recapitulate his arguments ... They\nMcCarthyandShannon’smemorizingmachineob- amount,inmyview,toasatisfactoryrefutationofmany\njection was studied in depth by Stuart Shieber, who such objections” (p. 107).\nelaborated on its assumptions and concluded that it\nis invalid [33]. But McCarthy’s concept of memorizing\nmay have been more elastic, as his later comment 35‘Annual Minsky Loebner Prize Revocation Prize 1995\non Deep Blue’s defeat of Gary Kasparov suggests Announcement,’ 2 March 1995. Available at: ’https://groups.\ngoogle.com/g/comp.ai/c/dZtU8vDD_bk/m/QYaYB18qAToJ.\n[22]. He expressed disappointment that it was mostly\nAccessed25Nov2023."
  },
  {
    "id": 656,
    "text": "google.com/g/comp.ai/c/dZtU8vDD_bk/m/QYaYB18qAToJ.\n[22]. He expressed disappointment that it was mostly\nAccessed25Nov2023.\nan achievement of computational power rather than\n36‘MarvinMinskyonAI:theTuringtestisajoke!’,from23’\nthinking, and gave a clear argument why he thought 35”to24’45”.Availableat’https://www.singularityweblog.com/\nso.Essentially,computerchessadvancedbyreplacing marvin-minsky/.AccessedJun.10,2024.\n11"
  },
  {
    "id": 657,
    "text": "The value of Turing’s test is that it has long been earlier version of this article; Mark Priestley for the\nand still is a unifying “definition,” a “criterion,” a “goal” gift of the Turing letters to Worsley; Fabio Cozman\nfor, in the words of McCarthy et al., the science and and Murray Shanahan for their support; The author\nengineering of “making a machine behave in ways is solely responsible for the accuracy of this work.\nthat would be called intelligent if a human were so The author thanks the Center for Artificial Intelligence\nbehaving.”Forbetterorworse,everytimeAIsucceeds (C4AI-USP) and the support from the São Paulo Re-\nin automating a new task that was once reserved for search Foundation (FAPESP grants nos. 2019/07665-\nhumans because it requires intelligence, “the Turing 4,2019/21489-4,and2022/16793-9)andfromtheIBM\ndefinition of thinking” conquers new territory, and the Corporation. This article is a result of the project “The"
  },
  {
    "id": 658,
    "text": "definition of thinking” conquers new territory, and the Corporation. This article is a result of the project “The\nsignificance of Turing’s early message to his contem- Future of Artificial Intelligence: The Logical Structure\nporaries becomes clearer. of Alan Turing’s Argument”).\nCONCLUSION BIBLIOGRAPHY\nThis paper presented a new perspective on Turing’s [1] C. Biever, “ChatGPT broke the Turing test\ntest. New light has been shed on Turing’s concept of — the race is on for new ways to assess\nimitation, suggesting that it does not give a license AI,” Nature, vol. 619, pp. 686–689, 2023,\nfor deception in AI. Rather, imitation was for Turing doi:10.1038/d41586-023-02361-7.\na mathematical concept, largely in continuity with his [2] E. Brynjolfsson, “The Turing trap: The promise\n1936paper,althoughhelatergeneralizedhowitcould & peril of human-like artificial intelligence,”\nbe achieved. It was also suggested that Turing’s pre- Daedalus, vol. 151, no. 2, pp. 272–287, 2022,"
  },
  {
    "id": 659,
    "text": "1936paper,althoughhelatergeneralizedhowitcould & peril of human-like artificial intelligence,”\nbe achieved. It was also suggested that Turing’s pre- Daedalus, vol. 151, no. 2, pp. 272–287, 2022,\nsentation of the various versions of his test fits what doi:10.1162/DAED_a_01915.\nMachcalled“thebasicmethodofthoughtexperiments” [3] S. Butler, Erewhon or over the range. London:\nin the history of science. The historical conditions of Trubner & Co., 1872.\nTuring’sproposalwerereconstructed,showingthatthe [4] A.C.Clarke,2001:ASpaceOdyssey.NewYork:\nbasic idea of a gender test had been raised originally Dutton, 1968.\nby Jefferson, and Turing’s imitation game comes out [5] E. A. Feigenbaum and J. Feldman, Eds., Com-\nof that context. Conversational performance was also puters and Thought. New York: McGraw-Hill,\na concession to his opponents, and overall Turing’s 1963.\ntest was a response to critics. But Turing also took [6] R. Gandy, “Human versus mechanical intelli-"
  },
  {
    "id": 660,
    "text": "a concession to his opponents, and overall Turing’s 1963.\ntest was a response to critics. But Turing also took [6] R. Gandy, “Human versus mechanical intelli-\nthe opportunity to promote his positive views. The gence,”inMachinesandThought:TheLegacyof\nknown primary and secondary sources indicate that AlanTuring,P.MillicanandA.Clark,Eds.,vol.1,\nhe became actively engaged in machine intelligence Oxford: University Press, 1996, pp. 125–136.\npropaganda, and it was also in this spirit that he [7] B. Gonçalves, “Galilean resonances: The\nproposed his test, hoping to influence contemporaries role of experiment in Turing’s construction\nand future generations of scientists. The question of of machine intelligence,” Ann. Sci., 2023,\nthevalueofTuring’stestanditsrelationtoearlyAIwas doi:10.1080/00033790.2023.2234912.\nrevisited,arguingthat“theTuringdefinitionofthinking” [8] B.Gonçalves,“Ironywithapoint:AlanTuringand"
  },
  {
    "id": 661,
    "text": "thevalueofTuring’stestanditsrelationtoearlyAIwas doi:10.1080/00033790.2023.2234912.\nrevisited,arguingthat“theTuringdefinitionofthinking” [8] B.Gonçalves,“Ironywithapoint:AlanTuringand\nprovidedMcCarthy,Minsky,andotherswithadefinition his intelligent machine utopia,” Philos. Technol.,\noftheAIproblematthelevelofconceptualfoundations vol. 36, no. 3, 2023, doi:10.1007/s13347-023-\nthat arguably still drives AI research today. 00650-7.\nButwhateveritsutility,wecannowappreciatethat [9] B. Gonçalves, “Lady Lovelace’s objection:\nthere is more to the imitation game. With its structural The Turing-Hartree disputes over the\nelementsneatlydesignedaslightheartedconcessions meaning of digital computers, 1946-\nto opponents, and at the same time able to demon- 1951,” IEEE Ann. Hist. Comput., 2023,\nstratethepowerofdigitalcomputingasearlyas1950, doi:10.1109/MAHC.2023.3326607.\nTuring’s test has secured its place as one of the most [10] B. Gonçalves, The Turing Test Argument"
  },
  {
    "id": 662,
    "text": "stratethepowerofdigitalcomputingasearlyas1950, doi:10.1109/MAHC.2023.3326607.\nTuring’s test has secured its place as one of the most [10] B. Gonçalves, The Turing Test Argument\nbeautifulthoughtexperimentsinthehistoryofscience. (Routledge Studies in Twentieth-Century\nPhilosophy). New York: Routledge, 2023,\ndoi:10.4324/9781003300267.\nACKNOWLEDGMENTS\n[11] B. Gonçalves, “The Turing test is a thought ex-\nThe author thanks Andrew Hodges, Jim Miles, and periment,”MindsMach.,vol.33,no.1,pp.1–31,\nH. V. Jagadish for their valuable comments on an 2023, doi:10.1007/s11023-022-09616-8.\n12"
  },
  {
    "id": 663,
    "text": "[12] D. R. Hartree, Calculating Instruments and Ma- [28] M. Polanyi, Personal Knowledge: Towards a\nchines.Urbana:UniversityofIllinoisPress,1949. Post-Critical Philosophy, Second. Chicago: Uni-\n[13] P. Hayes and K. Ford, “Turing test considered versity Press, 1962 [1958].\nharmful,” in Proc. IJCAI, 1995, pp. 972–977. [29] C. Shannon, “Computers and automata,” Proc.\n[14] R. Hayward, “The tortoise and the love- IRE, vol. 41, no. 10, pp. 1234–1241, 1953,\nmachine: Grey Walter and the politics doi:10.1109/JRPROC.1953.274273.\nof electroencephalography,” Sci. Context, [30] S.M.Shieber,“LessonsfromarestrictedTuring\nvol. 14, no. 4, pp. 615–641, 2001, test,” Comm. ACM, vol. 37, no. 6, pp. 70–78,\ndoi:10.1017/S0269889701000278. 1994, doi:10.1145/175208.175217.\n[15] A. Hodges, Alan Turing: The Enigma. London: [31] S. M. Shieber, “On Loebner’s lessons,” Comm.\nBurnett, 1983. ACM,vol.37,no.6,pp.83–84,1994.DOI:https:\n[16] G.Jefferson,“Themindofmechanicalman,”Brit. //doi.org/10.1145/175208.175604."
  },
  {
    "id": 664,
    "text": "Burnett, 1983. ACM,vol.37,no.6,pp.83–84,1994.DOI:https:\n[16] G.Jefferson,“Themindofmechanicalman,”Brit. //doi.org/10.1145/175208.175604.\nMed. J., vol. 1, no. 4616, pp. 1105–1110, 1949, [32] S. M. Shieber, Ed., The Turing Test: Verbal\ndoi:10.1136/bmj.1.4616.1105. Behavior as the Hallmark of Intelligence. Cam-\n[17] G. Jefferson, “The search for the mechanisms bridge, MA: MIT Press, 2004.\ninvolved in thinking and talking,” Manchester [33] S. M. Shieber, “There can be no Turing-\nMemoirs, vol. XCV, no. 6, pp. 69–84, 1953-54, test-passing memorizing machines,” Philos.\nPresidential address of the Manchester Literary Impr., vol. 14, no. 16, pp. 1–13, 2014,\nand Philosophical Society. doi:2027/spo.3521354.0014.016.\n[18] G. Jefferson, “The search for the mechanisms [34] S. G. Sterrett, “Turing’s two tests for intelli-\ninvolved in thinking,” Scientia Ululae, vol. 2, gence,”MindsMach.,vol.10,pp.541–559,2000,\nno. Jan. Pp. 3–7, 1956, Published jointly by the doi:10.1023/A:1011242120015."
  },
  {
    "id": 665,
    "text": "involved in thinking,” Scientia Ululae, vol. 2, gence,”MindsMach.,vol.10,pp.541–559,2000,\nno. Jan. Pp. 3–7, 1956, Published jointly by the doi:10.1023/A:1011242120015.\nPhilosophical and Natural History Societies of [35] A. M. Turing, “On computable numbers, with an\nManchester Grammar School. application to the Entscheidungsproblem,” Proc.\n[19] E. Mach, “On thought experiments,” in Knowl- Lond.Math.Soc.,vol.s2-42,no.1,pp.230–265,\nedge and Error: Sketches on the Psychology 1936, doi: 10.1112/plms/s2-42.1.230.\nof Enquiry, ser. Vienna Circle Collection, E. N. [36] A. M. Turing, “Computing machinery and intel-\nHiebert, Ed., Dordrecht: Springer, 1976 [1897], ligence,” Mind, vol. 59, no. 236, pp. 433–460,\nch. 11, pp. 134–147. 1950, doi:10.1093/mind/LIX.236.433.\n[20] W. Mays, “Can machines think?” Philoso- [37] A.M.Turing,“Thechemicalbasisofmorphogen-\nphy, vol. 27, no. 101, pp. 148–162, 1952, esis,”Philos.Trans.R.Soc.B,vol.237,no.641,"
  },
  {
    "id": 666,
    "text": "[20] W. Mays, “Can machines think?” Philoso- [37] A.M.Turing,“Thechemicalbasisofmorphogen-\nphy, vol. 27, no. 101, pp. 148–162, 1952, esis,”Philos.Trans.R.Soc.B,vol.237,no.641,\ndoi:10.1017/S003181910002266X. pp. 37–72, 1952, doi:10.1098/rstb.1952.0012.\n[21] W. Mays, “Turing and Polanyi on minds and [38] E. S. Turing, Alan M. Turing: Centenary Edition.\nmachines,” Appraisal, vol. 3, no. 2, pp. 55–62, Cambridge: University Press, 2012 [1959].\n2000. [39] M. Y. Vardi, “Would Turing have passed the\n[22] J. McCarthy, “Al as sport,” Science, Turing test?” Comm. ACM, vol. 57, no. 9, p. 5,\nvol. 276, no. 5318, pp. 1518–1519, 1997, 2014, doi:10.1145/2643596.\ndoi:10.1126/science.276.5318.1518. [40] W. G. Walter, The Living Brain. London: Duck-\n[23] J.McCarthy,“Fromheretohuman-levelAI,”Artif. worth, 1953.\nIntell., vol. 171, no. 18, pp. 1174–1182, 2007, [41] S.Wells,“IstheTuringtestdead?”IEEESpectr.,\ndoi:10.1016/j.artint.2007.10.009. 2023, doi:https://spectrum.ieee.org/turing-test."
  },
  {
    "id": 667,
    "text": "Intell., vol. 171, no. 18, pp. 1174–1182, 2007, [41] S.Wells,“IstheTuringtestdead?”IEEESpectr.,\ndoi:10.1016/j.artint.2007.10.009. 2023, doi:https://spectrum.ieee.org/turing-test.\n[24] J.McCarthy,M.L.Minsky,N.Rochester,andC.\nShannon,“AproposalfortheDartmouthsummer\nresearch project on artificial intelligence, august\n31,1955,”AIMag.,vol.27,no.4,pp.1–12,2006\n[1955], doi:10.1609/aimag.v27i4.1904. Bernardo Gonçalves is currently a researcher at the\n[25] J. McCarthy and C. Shannon, “Preface,” in Au- Center for Artificial Intelligence (C4AI), University of\ntomata Studies, C. Shannon and J. McCarthy, São Paulo, Brazil, and a Visiting Fellow at King’s\nEds., Princeton: University Press, 1956. College, University of Cambridge, UK. He works on\n[26] M. Minsky, Computation: Finite and Infinite Ma- Alan Turing, AI and computer science. He received\nchines. Hoboken, NJ: Prentice-Hall, 1967. Ph.D. degrees in Philosophy from the University of"
  },
  {
    "id": 668,
    "text": "[26] M. Minsky, Computation: Finite and Infinite Ma- Alan Turing, AI and computer science. He received\nchines. Hoboken, NJ: Prentice-Hall, 1967. Ph.D. degrees in Philosophy from the University of\n[27] M. Minsky, Semantic Information Processing. São Paulo and in Computational Modeling from the\nCambridge, MA: MIT Press, 1968. National Laboratory for Scientific Computing, Brazil.\n13"
  },
  {
    "id": 669,
    "text": "Professional Network Matters:\nConnections Empower Person-Job Fit\nHaoChen∗ LunDu† YuxuanLu∗\nhaochen@mail.bnu.edu.cn lun.du@microsoft.com lu.yuxuan@northeastern.edu\nBeijingNormalUniversity MicrosoftResearch NortheasternUniversity\nBeijing,China Beijing,China Boston,USA\nQiangFu XuChen ShiHan\nqifu@microsoft.com xu.chen@microsoft.com shihan@microsoft.com\nMicrosoftResearch MicrosoftResearch MicrosoftResearch\nBeijing,China Beijing,China Beijing,China\nYanbinKang GuangmingLu ZiLi\nybkang@linkedin.com glu@linkedin.com zili@linkedin.com\nLinkedInCorporation LinkedInCorporation LinkedInCorporation\nBeijing,China Beijing,China Beijing,China\nABSTRACT of talent profiles and job descriptions are posted on these plat-\nOnlinerecruitmentplatformstypicallyemployPerson-JobFitmod- forms.TakingLinkedInasanexample,morethan900millionmem-\nelsinthecoreservicethatautomaticallymatchsuitablejobseekers bershaveregistered,and90jobswerepostedeverysecondbythe\nwithappropriatejobpositions.Whileexistingworksleveragehis-"
  },
  {
    "id": 670,
    "text": "elsinthecoreservicethatautomaticallymatchsuitablejobseekers bershaveregistered,and90jobswerepostedeverysecondbythe\nwithappropriatejobpositions.Whileexistingworksleveragehis-\nfirstquarterof20234.Consideringsuchalargenumberofoptions,\ntoricalorcontextualinformation,theyoftendisregardacrucial Person-JobFit(PJF)[26]hasbecomeacriticalresearchtopicfor\naspect:jobseekers’socialrelationshipsinprofessionalnetworks. improvingtheefficiencyofrecruitmentandjobseeking.Person-\nThis paper emphasizes the importance of incorporating profes- JobFitaimstoautomaticallylinktherighttalentstotherightjob\nsional networks into the Person-Job Fit model. Our innovative positionsaccordingtotalentcompetenciesandjobrequirements.\napproachconsistsoftwostages:(1)definingaWorkplaceHetero- PreviousworksonPerson-JobFitmainlyfocusonleveraging\ngeneousInformationNetwork(WHIN)tocaptureheterogeneous two types of information, namely, (1) historical job application"
  },
  {
    "id": 671,
    "text": "geneousInformationNetwork(WHIN)tocaptureheterogeneous two types of information, namely, (1) historical job application\nknowledge,includingprofessionalconnectionsandpre-training informationand(2)textualinformationinprofilesandjobdescrip-\nrepresentationsofvariousentitiesusingaheterogeneousgraph tions. Collaborative filtering-based methods [3, 30] are applied\nneuralnetwork;(2)designingaContextualSocialAttentionGraph tocaptureco-applyrelationsbetweenjobseekersandco-applied\nNeuralNetwork(CSAGNN)thatsupplementsusers’missinginfor- relationsbetweenjobpositionsinthehistoricalapplicationinfor-\nmationwithprofessionalconnections’contextualinformation.We mation.Manually-engineeredtextualfeaturesanddeeplanguage\nintroduceajob-specificattentionmechanisminCSAGNNtohandle modelshavebeenwidelyadoptedtoleveragethetextualinforma-\nnoisyprofessionalnetworks,leveragingpre-trainedentityrepre- tion[4,25,28,36]."
  },
  {
    "id": 672,
    "text": "introduceajob-specificattentionmechanisminCSAGNNtohandle modelshavebeenwidelyadoptedtoleveragethetextualinforma-\nnoisyprofessionalnetworks,leveragingpre-trainedentityrepre- tion[4,25,28,36].\nsentationsfromWHIN.Wedemonstratetheeffectivenessofour However,workplacesocialconnectionsamongmembers,com-\napproachthroughexperimentalevaluationsconductedacrossthree monlycalledprofessionalnetworks,havebeenoverlookedasa\nreal-worldrecruitmentdatasetsfromLinkedIn,showingsuperior pivotalsourceofinformation.AsurveyreportedontheOfficial\nperformancecomparedtobaselinemodels.\nLinkedInBlog5showedthatoutofmorethan15,000LinkedInmem-\nbers,80%believedthatprofessionalnetworkingcouldhelpfind\nnewjobopportunities,anevenmorestriking70%gainedjoboppor-\nKEYWORDS\ntunitiesdirectlythroughtheirconnections.Asanimportantsource\nPerson-JobFit,HeterogeneousInformationNetwork,GraphNeural\nfor achieving employment relationships, professional networks\nNetwork\nhavegreatpotentialforhelpingwithPerson-JobFit.Incorporating"
  },
  {
    "id": 673,
    "text": "Person-JobFit,HeterogeneousInformationNetwork,GraphNeural\nfor achieving employment relationships, professional networks\nNetwork\nhavegreatpotentialforhelpingwithPerson-JobFit.Incorporating\nprofessionalnetworksintothemodelscanoffertwomainbene-\n1 INTRODUCTION fits:(1)ProfessionalnetworksdirectlyimprovePerson-JobFitby\nbridgingthegapbetweenjobseekersandpotentialjobopportuni-\nWiththerapiddevelopmentoftheInternet,onlinerecruitment\nplatforms(e.g.,LinkedIn1,Indeed2,andZipRecruiter3)arebecom- ties.APerson-JobFitmodelenrichedwithprofessionalnetwork\ninformationcansuggestjobopportunitiestousersbasedontheir\ningessentialforrecruitingandjobseeking.Aconsiderablenumber\nprofessionalconnections.(2)Professionalnetworkscanalsohelp\ncompletejobseekers’profiles.Oftenjobseekers’onlineprofiles\n∗ThisworkwasdonewhentheywereinternsatLinkedIn.\n†Correspondingauthor\n1https://www.linkedin.com 4https://news.linkedin.com/about-us#Statistics"
  },
  {
    "id": 674,
    "text": "completejobseekers’profiles.Oftenjobseekers’onlineprofiles\n∗ThisworkwasdonewhentheywereinternsatLinkedIn.\n†Correspondingauthor\n1https://www.linkedin.com 4https://news.linkedin.com/about-us#Statistics\n2https://www.indeed.com 5https://blog.linkedin.com/2017/june/22/the-best-way-to-land-your-next-job-\n3https://www.ziprecruiter.com opportunity\n3202\nceD\n91\n]IS.sc[\n1v01000.1042:viXra"
  },
  {
    "id": 675,
    "text": "HaoChenetal.\nlackcomprehensivedetails,yetone’sprofessionalexperienceand direct path\nmeta path\nskillscanbe,tosomeextent,discerniblethroughtheirprofessional\nrecommended entities\nconnections. auxiliaryentity\nconnect\nWhileprofessionalnetworksofferadvantages,theyoftencontain\nalotofnoise.Thisnoiseincludesconnectionsthataren’trelevant M apply J\nandinformationfromrelevantconnectionsthatdon’thelpimprove educate post\nco-apply co-applied\nPerson-JobFit.Toillustrate,irrelevantconnectionsmightinvolve\nwork(ed)\njobseekers’formerclassmatesorrecruitersfromunrelatedindus-\ntries.Evenwhentheconnectionsarerelevanttothejobseeker,they C master require H\ncouldstillhaveinformationunrelatedtothejobseeker’saspirations S\nfortherole.Totacklethesenoises,apromisingstrategyinvolves\nusingheterogeneousknowledge,suchasjobseekers’skills,work\nFigure1:ThemetagraphofWorkplaceHeterogeneousInfor-\nexperience,andeducationalbackground,togaugeprofessionalcon-\nmationNetwork.Itencompassesnotonlymembers(M)and"
  },
  {
    "id": 676,
    "text": "Figure1:ThemetagraphofWorkplaceHeterogeneousInfor-\nexperience,andeducationalbackground,togaugeprofessionalcon-\nmationNetwork.Itencompassesnotonlymembers(M)and\nnections’relevanceandextractpertinentinformationtoelevate\njobs(J),whicharecrucialforPerson-JobFit,butalsoentities\nPerson-JobFit.\nsuchasskills(S),companies(C),andschools(H).\nInthispaper,weproposeagraphneuralnetwork-basedframe-\nworkthatutilizesheterogeneousknowledgetointegrateprofes-\nsionalnetworksintoPerson-JobFit.Weaddressthechallengeof\nrichcontextualknowledge.Forexample,JLMIA[31]isalatentvari-\nsocialnoiseinprofessionalnetworksbydesigningajob-specific\nablemodeltomodeljobdescriptionsandmemberprofilesjointly.\nattentionmechanism.Initially,wedefineaWorkplaceHeteroge-\nPJFNN[41]encodesmemberprofilesandjobdescriptionsbyhier-\nneousInformationNetwork(WHIN)thatcapturesheterogeneous\narchicalCNN.BPJFNN[28]leverageBiLSTMtogetthesemantic\nknowledge,includingprofessionalconnections.Weemployahet-"
  },
  {
    "id": 677,
    "text": "neousInformationNetwork(WHIN)thatcapturesheterogeneous\narchicalCNN.BPJFNN[28]leverageBiLSTMtogetthesemantic\nknowledge,includingprofessionalconnections.Weemployahet-\nrepresentationofeachwordinmemberprofilesandjobdescriptions.\nerogeneousgraphpre-trainingtechniquetolearntherepresenta-\nAPJFNN[28]automaticallyweighsabilitiesmentionedintextual\ntionsofvariousentitiesintheWHIN.Subsequently,weintroduce\ninformationbasedonhistoricalrecruitmentresults.Atransferable\ntheContextualSocialAttentionGraphNeuralNetwork(CSAGNN),\ndeepglobalmatchnetwork[4]isproposedtosolvethedomain\ndesignedtosupplementusers’lackinginformationwithcontextual\nadaptationprobleminthreelevelsforPerson-JobFit.JRMPM[36]\ninformationfromtheirprofessionalconnections.Totackletheso-\nproposesamatchingnetworkwithpreferencemodeledtoexplore\ncialnoiseintheworkplacesocialnetwork,weinfuseajob-specific\nthelatentpreferencegiventhehistoryofthematchingprocess.\nattentionmechanismintoCSAGNN,capitalizingonthepre-trained"
  },
  {
    "id": 678,
    "text": "cialnoiseintheworkplacesocialnetwork,weinfuseajob-specific\nthelatentpreferencegiventhehistoryofthematchingprocess.\nattentionmechanismintoCSAGNN,capitalizingonthepre-trained\nSomeworkstakestructureknowledgeintoconsideration.For\nentityrepresentationsfromWHIN.\nexample,MV-CoN[3]adoptsaco-teachingmechanismtocapture\nThemaincontributionsofthisstudyareasfollows:\nsemanticandstructureknowledgeatthesametime.DPGNN[37]\nToourknowledge,wearethefirsttodefineaheteroge- explicitlymodelsthetwo-wayselectionpreferenceforPJFusing\n•\nneous information network that incorporates heteroge- GCN.KGRM[39]modelmembersandjobsastwographsandfuse\nneousknowledgeinthePerson-JobFitscenario. priorexternalknowledge,e.g.,skillknowledgegraph,intothegraph\nWesystematicallyutilizeprofessionalnetworksinatwo- representations.\n•\nstageapproach,WHINpre-trainingandCSAGNN,toad- Previous studies have made good use of textual information,\ndressthePerson-JobFittask. skillentity,anddirectinteractionbetweenmembersandjobsto"
  },
  {
    "id": 679,
    "text": "•\nstageapproach,WHINpre-trainingandCSAGNN,toad- Previous studies have made good use of textual information,\ndressthePerson-JobFittask. skillentity,anddirectinteractionbetweenmembersandjobsto\nWepresentanovelContextualSocialAttentionGraphNeu- helpwithPerson-JobFit.However,thesestudiesoftenoverlookthe\n•\nral Network (CSAGNN) specifically designed to handle importanceofworkplacesocialrelationsandtheincorporationof\nnoisyprofessionalnetworks,effectivelymitigatingtheim- diverse,heterogeneousknowledge.\npactofirrelevantinformationwhilefocusingonrelated\nprofessionalconnectionsandcontextsforPerson-JobFit. 2.2 HeterogeneousInformationNetwork-based\nWe evaluate our approach on three real-world datasets\n• Recommendation\nacrossdiverseindustries.Experimentalresultsshowthat\nRecommendersystemshavebeenwidelydeployedontheInternet\nourmodeloutperformsbaselinemodels.\ntoalleviateinformationoverload[15].Duetotheexcellentabilityto\nmodelcomplexauxiliaryinformation,HeterogeneousInformation\n2 RELATEDWORKS"
  },
  {
    "id": 680,
    "text": "ourmodeloutperformsbaselinemodels.\ntoalleviateinformationoverload[15].Duetotheexcellentabilityto\nmodelcomplexauxiliaryinformation,HeterogeneousInformation\n2 RELATEDWORKS\nNetwork(i.e.,HIN)hasbecomeoneofthemainstreamapproaches\nThe related work of our study can be grouped into two main\ninrecommendationtasks.Dependingonthetrainingmethod,HIN-\ncategories,namely,Person-JobFitandHeterogeneousInformation\nbasedmodelscanbedividedintotwo-stagetraining-basedmodels\nNetwork-basedRecommendation.\nandend-to-endtraining-basedmodels[24].\nTwo-stagetraining-basedmodelslearnlow-dimensionalrepre-\n2.1 Person-JobFit\nsentationsofnodesandgraphstructureusingunsupervisedtasks\nAsacorefunctionoftheonlinerecruitmentplatform,Person-Job andusetheserepresentationsonvariousdownstreamtasks[1,2,7,\nFit[26]hasreceivedwidespreadattention. 14].InspiredbyDeepWalk[27]andnode2Vec[11],metapath2vec\nMainstreamworksviewPerson-JobFitasatext-matchingprob- [6]leveragesmetapath-basedrandomwalkstoconstructahetero-"
  },
  {
    "id": 681,
    "text": "MainstreamworksviewPerson-JobFitasatext-matchingprob- [6]leveragesmetapath-basedrandomwalkstoconstructahetero-\nlembetweenmemberprofilesandjobdescriptionstofullyusethe geneousneighborhoodofnodesandleveragesskip-grammodelto"
  },
  {
    "id": 682,
    "text": "ProfessionalNetworkMatters:ConnectionsEmpowerPerson-JobFit\nmember\njob …\nskill … RGCN la M y L e P rs loss\nschool pos path\ncompany pos metapath\nneg path\nneg metapath\n(a) (b) (c)\nFigure2:StepsofWHINpre-training.(a)Workplaceheterogeneousgraphwithmetapath.(b)Subgraphsamplingformini-batch\npre-training.(c)Apre-trainingmodelwithencoder-decoderarchitectureusingLink-levelpre-trainingtask.\ngeneratenoderepresentations.Similarly,HIN2Vec[9]definesan introducethepre-trainingapproachonWHINinSection3.3.To\nunsupervisedmetapathpredictiontaskandjointlylearnsmetapath overcomethenoiseinprofessionalnetworks,weproposethecon-\npredictorsandnoderepresentations.Manymodelsareproposed textualsocialattentiongraphneuralnetwork(CSAGNN)inthe\nbasedonthegraphrepresentationmethodssuchasthoseabove.For secondstage,asdescribedinSection3.4.\nexample,HERecgeneratesnodesequenceswithmetapath-based\nrandomwalks,usesnode2vectolearnnoderepresentations,and 3.1 FormalTaskDescriptionandNotations"
  },
  {
    "id": 683,
    "text": "example,HERecgeneratesnodesequenceswithmetapath-based\nrandomwalks,usesnode2vectolearnnoderepresentations,and 3.1 FormalTaskDescriptionandNotations\ncompletesrecommendationsbyrepresentationsimilarity.MAGNN Person-Job Fit aims to match a set of job seekers with a set of\n[10]leveragesnodecontentfeaturesandinformationofintermedi- job opportunities. Assume that there is a set of members =\natenodesalongthemetapathbynodecontenttransformation,intra- 𝑚1,𝑚2,...,𝑚𝑛 whomaybeseekingjobopportunitiesanda M setof\nandinter-metapathaggregation.Besides,therearesomemethods j { obs = 𝑗1,𝑗2, } ...,𝑗𝑚 .Formally,givenacandidatepair 𝑚𝑗,𝑗𝑘 ,\nnotrelyingonmanualmetapath,suchasHRLHG[17],NREP[38], them J odel { isrequiredto } learnafunction topredictthepro ( babilit ) y\nandECHCDR[20]. ofwhetherthememberappliedforthe F job.\nComparedwithtwo-stagetraining-basedmodels,end-to-end-\n(cid:40)\nbasedmodelscanusesupervisionsignalsdirectlywhiletraining. 1 𝑚𝑖 appliedfor𝑗𝑘\n𝑚𝑖,𝑗𝑘 ="
  },
  {
    "id": 684,
    "text": "Comparedwithtwo-stagetraining-basedmodels,end-to-end-\n(cid:40)\nbasedmodelscanusesupervisionsignalsdirectlywhiletraining. 1 𝑚𝑖 appliedfor𝑗𝑘\n𝑚𝑖,𝑗𝑘 =\nThus,theyaremorecustomizedtospecificdownstreamtasks.For F( ) 0 𝑚𝑖 didn’tapplyfor𝑗𝑘\nHINswithrichrelations,relation-awaregraphneuralnetworks\nThesetofcandidatepairscanbedefinedas\ncanachievegreatresults.Forexample,RGCN[29]assignsaweight\nmatrixtoeachtypeofrelation,thusextendingGCN[35]tomulti-\nrelationgraphs.DisenHAN[34]projectsnodesintodifferentsub- 𝑇 = 𝑚𝑖,𝑗𝑘 𝑚𝑖 ,𝑗𝑘\n{( )| ∈M ∈ J}\nspacesusingtype-specifictransformationandusesintra-andinter- .\naggregationtolearnnoderepresentations.HAN[33]usesadual Inadditiontothesetsofmembers( )andjobs( ),thereexist\nattentionmechanismtoaggregateinformationfromdifferentmeta- threeothersetsofentitiesthatarestro M nglyrelatedto J membersand\npaths.HPN[16]designsasemanticfusionmechanismforlearning jobs:skills( ),companies( ),andschools( ).Theseentitiesare"
  },
  {
    "id": 685,
    "text": "paths.HPN[16]designsasemanticfusionmechanismforlearning jobs:skills( ),companies( ),andschools( ).Theseentitiesare\ntheimportanceofmetapathandfusingthemjudiciously.Byusing referredasa S uxiliaryinform C ationandcanbe H leveragedtoimprove\nonlyitsimmediateconnectionsasinput,HGT[13]canlearnandex- themodel’saccuracy.Givenanentity𝑒andarelation𝑟,aninterac-\ntractrelevantmetapathsforvarioustasksthroughitsautomaticand tionmap 𝑒 𝑟 isusedtospecifythedestinationentitiesrelatedto𝑒\nimplicitattentionmechanismwithoutrequiringmanualmetapath A 𝑎𝑝𝑝𝑙𝑦\ndesign.\nthrough𝑟.Forexample,\nA\n𝑚𝑖 represents\n𝑎\nth\n𝑝𝑝\ne\n𝑙\ns\n𝑦\netofjobsapplied\nInthispaper,weadoptthetwo-stagetraining-basedparadigm. by𝑚𝑖.If𝑚𝑖 hasnotappliedforanyjobs, A 𝑚𝑖 = ∅ .\nDuringthepre-trainingstage,weintegrateheterogeneousknowl-\n3.2 TheConstructionofWorkplace\nedgespecifictothePerson-JobFitscenariotoobtaindiverseentity\nrepresentationsthatencompassbothstructuralandtextualinfor- HeterogeneousInformationNetwork"
  },
  {
    "id": 686,
    "text": "3.2 TheConstructionofWorkplace\nedgespecifictothePerson-JobFitscenariotoobtaindiverseentity\nrepresentationsthatencompassbothstructuralandtextualinfor- HeterogeneousInformationNetwork\nmation.Inthedownstreamstage,wedesignanovelmodelthat ThePerson-JobFitscenariocontainsawiderangeofheterogeneous\nleveragestheinformationfromthepre-trainingstagetofilterout knowledge,includingvariousentitiessuchasskills,companies,and\nnoisewithinprofessionalnetworks.Asapointofcomparison,we schools,aswellasrelationshipssuchasprofessionalconnections\nhavealsoselectedtwoend-to-end-basedmodelsasbaselines. andskillrequirements.WedefineaWorkplaceHeterogeneousIn-\nformationNetwork(WHIN)tomodelthisknowledge.Asshownin\nFigure1,WHINisaheterogeneousinformationnetworkconsisting\n3 METHODLOGY\noffivetypesofentities,namelyMember,Job,Skill,Company,and\nHere we introduce our two-stage approach to improve the per- School.Italsocontainsninetypesofrelations,whichareintroduced"
  },
  {
    "id": 687,
    "text": "3 METHODLOGY\noffivetypesofentities,namelyMember,Job,Skill,Company,and\nHere we introduce our two-stage approach to improve the per- School.Italsocontainsninetypesofrelations,whichareintroduced\nformanceofthePerson-JobFittaskbyincorporatingprofessional tocapturethedifferenttypesofconnectionsbetweentheentities.\nnetworks.ThetaskandnotationareformallydefinedinSection3.1. TheWorkplaceHeterogeneousInformationNetworkprimarily\nInthefirststage,wepre-trainontheWorkplaceHeterogeneous consistsofnaturalrelationsamongdiverseentities.Besides,two\nInformationNetwork(WHIN),whichisdetailedinSection3.2.We additionalmetapathshavebeenartificiallyconstructed.Thefirst"
  },
  {
    "id": 688,
    "text": "HaoChenetal.\n𝑦!!,#!\nlayer\nSkill based\ncross\nattention\nNeighbor\nSampler\nAggregation\n…\n…\n…\nNeighbor\nSampler\nAggregation\n×(𝑙−1)\n𝑚# E P m re B b - e E tr d R a d i T n in e g d s ℎ! (% \" ) ℎ! (# # ) … ℎ! (%) # Grou 𝑦 n # d ! -t ! ru ,# t ! h value\n𝑠# 𝑚\" … ℎ! (% # ) ℎ! (# ! ) … ℎ! (%) ! loss\n𝑠\" ℎ! (% # )\n𝑠! 𝑚! … ℎ! (% ! ) ℎ! (% ! ) ℎ! (# \" ) … ℎ! (%) \" la M y L e P rs Predict value\n𝑗! … ℎ!! ℎ!!\nℎ!!\nAverage\nFigure3:ArchitectureofCSAGNN.Whendeterminingthematchbetweenjob𝑗0andmember𝑚0,theinformationfrom𝑚0’s\nprofessionalconnections,𝑚1 and𝑚2,issimultaneouslyacquired.Theinitialrepresentationsofboththememberandjob\nareformedbyconcatenatingtheWHINpre-trainingembeddingwiththerepresentationobtainedafterprocessingthetext\ninformationthroughBERT.Allmembertextinformationisre-weightedthroughanattentionmechanismbasedontheskills𝑠𝑖\nrequiredby𝑗0.Aneighborsamplermodule,operatingbasedonthesimilaritybetweendifferentmembersandtherequired\nskills,canfilteroutprofessionalconnectionsthatarenotrelevanttothejob."
  },
  {
    "id": 689,
    "text": "requiredby𝑗0.Aneighborsamplermodule,operatingbasedonthesimilaritybetweendifferentmembersandtherequired\nskills,canfilteroutprofessionalconnectionsthatarenotrelevanttothejob.\nmetapath,co-apply( - - ),connectsmemberswhohaveap- names,andothertextualdescriptionstoinitializeentityrepresen-\nM J M\npliedforthesamejob.Thesecondmetapath,co-applied( - - ), tations,therebyenhancingtheinitialembeddingsforeachentity\nJ M J\nconnectsmultiplejobsthathavebeenappliedforbythesamemem- type.\nber.Thesemetapathswereconstructedbasedonpriorknowledge Afterobtaininganinitializedrepresentationwithsemanticin-\nandhaveproventobeusefulinenhancingtheperformanceofthe formation,weutilizeanencoder,RGCN[29],tointegratethein-\nWHINpre-trainingprocess[10]. formationbetweenentities.Themessage-passingprocessinthe\nRGCNencoderisrepresentedbythefollowingequation:"
  },
  {
    "id": 690,
    "text": "WHINpre-trainingprocess[10]. formationbetweenentities.Themessage-passingprocessinthe\nRGCNencoderisrepresentedbythefollowingequation:\n3.3 P In r f e o -t r r m ai a n t i i n on go N n et W w o o r r k k placeHeterogeneous 𝑧𝑖( 𝑙 + 1 ) =𝜎(cid:169) (cid:173) 𝑟 ∑︁ 𝑅𝑗 ∑︁ 𝑟 𝑐𝑖 1 ,𝑟 𝑊𝑟( 𝑙 )𝑧𝑗( 𝑙 ) + 𝑊 0( 𝑙 )𝑧𝑖( 𝑙 ) (cid:170) (cid:174) , (1)\n3.3.1 link-levelpre-trainingtask. Weutilizeaheterogeneousgraph (cid:171) ∈ ∈A 𝑒𝑖 (cid:172)\n𝑙\nneuralnetworkwithalink-levelpre-trainingtasktoacquirenode where𝑊𝑟( ) isaparametermatrixatthe𝑙-thlayerofrelation\nrepresentationswithrichstructuralknowledge.Toimprovethe 𝑟,𝑊 0( 𝑙 ) isusedforself-loopatthe𝑙-thlayerand𝑐𝑖,𝑟 = (cid:12) (cid:12) A 𝑒 𝑟 𝑖 (cid:12) (cid:12)isa\nscalabilityofthepre-trainingprocess,wecreatemini-batchesby normalizationconstant.\nsampling subgraphs from the entire Workplace Heterogeneous Weemploy𝐿encoderlayersforLinkPrediction,utilizingthe"
  },
  {
    "id": 691,
    "text": "scalabilityofthepre-trainingprocess,wecreatemini-batchesby normalizationconstant.\nsampling subgraphs from the entire Workplace Heterogeneous Weemploy𝐿encoderlayersforLinkPrediction,utilizingthe\nInformationNetwork(WHIN),asillustratedinFigure2(a)and(b). finalrepresentation𝑧𝑖( 𝐿 ) forentity𝑒𝑖.Weapplyamulti-layerper-\nSpecifically,abatchofcandidatepairsisselectedasthesource ceptron(MLP)toscoreeachrelationormetapath𝑟𝑖,represented\nn\nto\nod\nco\nes\nn\n.\ns\nF\ntr\nr\nu\nom\nct\nt\na\nh\ns\ne\nu\ns\nb\ne\ng\ns\nr\no\na\nu\np\nr\nh\nce\nb\nn\nas\no\ne\nd\nd\nes\no\n,\nn\nth\ne\ne\na\ni\nc\nr\nh\nk-\nd\nh\ni\no\nre\np\nc\nn\nt\ne\nre\nig\nla\nh\nt\nb\nio\no\nn\nrs\no\na\nr\nre\nm\ns\ne\na\nt\nm\nap\np\na\nle\nth\nd\n.\nb\nas\ny\nf\nv\no\ne\nl\nc\nlo\nto\nw\nr\ns\n𝑀\n:\n𝑟𝑖 .Thescoreforaparticularlink\n(\n𝑠,𝑟,𝑑\n)\nisdetermined\nTheseidentifiedrelationsandmetapathsserveaspositivesamples.\nInaddition,tointroducenegativesamples,werandomlyselectpairs\n𝑦 𝑠,𝑟,𝑑 =𝑀𝐿𝑃 𝑧𝑠 𝑀𝑟 𝑧𝑑 (2)\nofentitiesthatdonotexhibittheexpectedrelationships.Thelink- ( ) ( || || )"
  },
  {
    "id": 692,
    "text": "Inaddition,tointroducenegativesamples,werandomlyselectpairs\n𝑦 𝑠,𝑟,𝑑 =𝑀𝐿𝑃 𝑧𝑠 𝑀𝑟 𝑧𝑑 (2)\nofentitiesthatdonotexhibittheexpectedrelationships.Thelink- ( ) ( || || )\nlevelpre-trainingtaskrevolvesarounddeterminingtheexistence Weemploycrossentropyasthelossfunctionforoptimizing\nofthegivensamples.Weemployanencoder-decoderstructure,as thepre-trainingmodel.Notably,thedescribedpre-trainingprocess\nillustratedinFigure2(c).Theencoderlearnsnoderepresentations incorporatesprofessionalnetworkinformationintoentityembed-\nwithineachsubgraph,whilethedecoderpredictslinkexistence. dings.\n(cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)\n3.3.2 pre-trainingmodelstructure. Duringthepre-trainingphase, L 𝑝𝑟𝑒𝑡𝑟𝑎𝑖𝑛 = − 𝑦 ( 𝑠,𝑟,𝑑 ) log𝑦′ ( 𝑠,𝑟,𝑑 ) + 1 − 𝑦 ( 𝑠,𝑟,𝑑 ) log 1 − 𝑦′ ( 𝑠,𝑟,𝑑 )\n(3)\nweselectafixednumberoftokensfromthetextualcontentofeach\nentitytype.Theseselectedtokensarethenprocessedthroughthe\n3.4 ContextualSocialAttentionGraphNeural\nBERTmodel[5].TheresultingBERToutputsareaveragedtocreate"
  },
  {
    "id": 693,
    "text": "entitytype.Theseselectedtokensarethenprocessedthroughthe\n3.4 ContextualSocialAttentionGraphNeural\nBERTmodel[5].TheresultingBERToutputsareaveragedtocreate\ninitialrepresentationsforeachentity. Network\nInaformalcontext,whenworkingwithatextualrepresentation Asdiscussedearlier,professionalnetworksareinherentlynoisy.\nassociatedwithanentitydenotedas𝑒𝑖 ,weconstructacorre- Tocapturejob-specificinsightsfromtheprofilesof𝑚𝑖’sprofes-\n∈E\n0\nspondinginitialembeddingreferredtoas𝑧𝑖( ) byutilizingBERT. sionalconnections,amulti-headattention(MHA)mechanism[32]\nSpecifically,Weleveragememberprofiles,jobdescriptions,skill isemployed.AsshowninEq.4,𝑐𝑖 istheprofilerepresentationof"
  },
  {
    "id": 694,
    "text": "ProfessionalNetworkMatters:ConnectionsEmpowerPerson-JobFit\nmember𝑚𝑖 obtainedfromapre-trainedBERTmodel.TheMHA featuresandWHINpre-trainedfeaturesasthefinalrepresentation\nmechanismuseseachrequiredskillembedding𝑧𝑠𝑘 asthequery, foralljobentities.\nand𝑐𝑖 asboththekeyandvalue.Itcomputesattentionweights\nb\nav\ne\ne\ntw\nra\ne\ng\ne\ne\nn\ns\n𝑐\nth\n𝑖\ne\na\ns\nn\ne\nd\na\nt\nt\nh\nte\ne\nn\nr\nt\ne\nio\nq\nn\nui\no\nr\nu\ned\ntp\ns\nu\nk\nt\ni\ns\nll\nfo\ne\nr\nm\na\nb\nll\ne\nr\nd\ne\nd\nq\nin\nui\ng\nr\ns\ne\n.\nd\nT\ns\nh\nk\ne\nill\nw\ns.\ne\nT\nig\nh\nh\nis\nte\nr\nd\nes\ns\nu\nu\nl\nm\nts\nℎ𝑗𝑘 =\n(\n𝐹 𝑐 𝑗𝑘|| 𝐹 𝑠 𝑗𝑘) (11)\ninthejob-specificcontextualfeature𝐹𝑚 𝑐 𝑖( 𝑗𝑘 ) thatcapturesrelevant av ℎ an 𝑗𝑘 i ( ll 𝑚 a 𝑖 m ) u an lt d i-l ℎ a 𝑚 y 𝑖 e ( r 𝑗 P 𝑘 ) er w ce il p l t b ro e n co (M nc L a P te ) n to at g e e d t a t n h d e p p a re s d se ic d te th d r v o a u l g u h e\ninformationfrom𝑚𝑖’sconnectionsforjob𝑗𝑘.\n𝑦𝑚𝑖,𝑗𝑘 ,andthecrossentropylosscouldbeformulatedasEq.13,\n𝑐 1 ∑︁ where𝑦𝑚′ 𝑖,𝑗𝑘 isthegroundtruth:"
  },
  {
    "id": 695,
    "text": "informationfrom𝑚𝑖’sconnectionsforjob𝑗𝑘.\n𝑦𝑚𝑖,𝑗𝑘 ,andthecrossentropylosscouldbeformulatedasEq.13,\n𝑐 1 ∑︁ where𝑦𝑚′ 𝑖,𝑗𝑘 isthegroundtruth:\n𝐹𝑚𝑖( 𝑗𝑘 ) = (cid:12) (cid:12) (cid:12)A 𝑟 𝑗𝑘 𝑒𝑞𝑢𝑖𝑟𝑒(cid:12) (cid:12) (cid:12)𝑠𝑘\n∈A\n𝑟\n𝑗𝑘\n𝑒𝑞𝑢𝑖𝑟𝑒 𝑀𝐻𝐴 ( 𝑄 =𝑧𝑠𝑘 ,𝐾 =𝑐𝑖,𝑉 =𝑐𝑖 ) 𝑦𝑚𝑖,𝑗𝑘 =𝑀𝐿𝑃\n(\nℎ𝑚𝑖( 𝑗𝑘\n)||\nℎ𝑗𝑘) (12)\n(4)\n𝑐\nwit A h s th s e h W ow H n IN in p E re q - . t 5 ra , i c n o e n d t f e e x a t t u u a r l e f 𝐹 e 𝑚 𝑠 at 𝑖 u t r o e fo 𝐹 r 𝑚 m 𝑖( m 𝑗𝑘 e ) m i b s e c r o ’s n 𝑗 c 𝑘 a - t s e p n e a c t i e fi d c L 𝑐𝑠𝑎 = − (cid:16) 𝑦𝑚𝑖,𝑗𝑘 log𝑦𝑚′ 𝑖,𝑗𝑘 + (cid:0) 1 − 𝑦𝑚𝑖,𝑗𝑘 (cid:1) log (cid:16) 1 − 𝑦𝑚′ 𝑖,𝑗𝑘 (cid:17)(cid:17) (13)\ninitialfeature.\nℎ𝑚( 0 𝑖) ( 𝑗𝑘 ) = ( 𝐹𝑚 𝑐 𝑖( 𝑗𝑘 )|| 𝐹𝑚 𝑠 𝑖) (5) 4 EXPERIMENTS\nTherelevancedegreeofprofessionalconnectionstothejob𝑗𝑘 Inthissection,wewillvalidatetheperformanceoftheCSAGNN\nneeds to be considered in the messaging process. Here, we use modelonthreerealdatasetsandanswerthefollowingresearch\nthedistancebetweenWHINpre-trainedrepresentationsofskills questions:"
  },
  {
    "id": 696,
    "text": "needs to be considered in the messaging process. Here, we use modelonthreerealdatasetsandanswerthefollowingresearch\nthedistancebetweenWHINpre-trainedrepresentationsofskills questions:\nmasteredbymembersandtheskillsrequiredbyjobsastheweights\nRQ1:CanCSAGNNoutperformexistingmodelsonreal-\nduringmessagepassing. •\nworlddatasets?\n∑︁ RQ2:Whataretheeffectsofdifferentcomponentsinour\n𝑧′ 𝑆ˆ = 𝑧𝑠 𝑆ˆ (6) •\nmodel?\n( ) /| |\n𝑠 𝑆ˆ RQ3:AreprofessionalnetworkshelpfulforthePerson-Job\n∈ •\n𝑚𝑎𝑠𝑡𝑒𝑟 𝑟𝑒𝑞𝑢𝑖𝑟𝑒 Fittask?\n𝑑𝑚𝑖( 𝑗𝑘\n)\n=\n√︃ ( 𝑧\n𝑧\n′(\n′\nA\n(A\n𝑚 𝑚\n𝑚\n𝑖 𝑎\n𝑖\n𝑠𝑡𝑒𝑟 ))\n)\n2\n·\n√︃\n𝑧 ′\n(\n(\n𝑧\nA\n′(A\n𝑗𝑘\n𝑟 𝑗𝑘 𝑒𝑞𝑢𝑖\n)\n𝑟𝑒 )) 2\n(7)\n•\nR\nre\nQ\nse\n4\nn\n:\nt\nD\nat\no\nio\nes\nns\nth\no\ne\nf\nW\nsk\nH\nill\nI\ns\nN\nfo\np\nr\nre\nC\n-t\nS\nr\nA\nai\nG\nni\nN\nn\nN\ng\n?\nprovidemeaningfulrep-\nGivenasetofskills𝑆ˆ,averageembeddingcanbedescribedinEq. 4.1 ExperimentSetup\n6anddistancebetweenmembers’andjobs’skills𝑑𝑚𝑖( 𝑗𝑘\n)\ncanbe\n4.1.1 Datasets. Wevalidateourmodelonthreelargereal-world\ndescribedinEq.7.Further,duetosomemembersorjobsmayhave"
  },
  {
    "id": 697,
    "text": "6anddistancebetweenmembers’andjobs’skills𝑑𝑚𝑖( 𝑗𝑘\n)\ncanbe\n4.1.1 Datasets. Wevalidateourmodelonthreelargereal-world\ndescribedinEq.7.Further,duetosomemembersorjobsmayhave\n𝑚𝑎𝑠𝑡𝑒𝑟 datasetsfromLinkedIn,awell-knownworkplacesocialplatform.\nhundre 𝑟 d 𝑒𝑞 s 𝑢 o 𝑖𝑟 f 𝑒 relatedskills,wesampledtheskillsetlike A 𝑚𝑖 Datasetsaredividedbytheindustrytowhichthemembersand\nand\nA\n𝑗𝑘 inEq.4andEq.7.Thenumberofsampledskills,𝑛𝑠, jobsbelong.Thefirsttwodatasetscontainmembersandjobsfrom\nisahyperparameterofCSAGNN.Wewilldiscusstheinfluenceof thetechnologyandfinanceindustries.Duetothesingularityofthe\n𝑛𝑠 inSection4.3. industry,socialnoiseislimited.Thethirddataset,whichincludes\nmembersandjobsfromvariousindustriessuchashealthcare,educa-\ntion,andsemiconductors,isintendedtotesttheabilityofCSAGNN\nℎ𝑚( 𝑙 +𝑖 1 )\n(\n𝑗𝑘\n)\n=𝜎 (cid:169) (cid:173)\n(cid:173)\n𝑊 2( 𝑙 ) ∑︁ 𝛼𝑚𝑗( 𝑗𝑘\n)\nℎ𝑚( 𝑙 )𝑗( 𝑗𝑘\n)+\n𝑊 1( 𝑙 )ℎ𝑚( 𝑙 𝑖) (cid:170) (cid:174)\n(cid:174)\nt\nm\no\no\nfi\nr\nl\ne\nte\nc\nr\no\nn\nm\no\np\nis\nl\ne\nex\nin\ns\np\no\nr\nc\no\nia\nfe\nl\ns\nr\ns\ne\nio\nla\nn\nt\na\nio\nl"
  },
  {
    "id": 698,
    "text": "(cid:173)\n𝑊 2( 𝑙 ) ∑︁ 𝛼𝑚𝑗( 𝑗𝑘\n)\nℎ𝑚( 𝑙 )𝑗( 𝑗𝑘\n)+\n𝑊 1( 𝑙 )ℎ𝑚( 𝑙 𝑖) (cid:170) (cid:174)\n(cid:174)\nt\nm\no\no\nfi\nr\nl\ne\nte\nc\nr\no\nn\nm\no\np\nis\nl\ne\nex\nin\ns\np\no\nr\nc\no\nia\nfe\nl\ns\nr\ns\ne\nio\nla\nn\nt\na\nio\nl\nn\nn\ns\ne\nh\ntw\nip\no\ns\nrk\na\ns\nn\n.\nd\nTh\na\nis\nw\nh\nid\ny\ne\nb\nr\nrid\nra\nd\nn\na\ng\nt\ne\nas\no\ne\nf\nti\nj\nm\nob\npl\no\nie\np\ns\n-\n𝑚 𝑐𝑜𝑛𝑛𝑒𝑐𝑡\n(cid:171) ′∈A 𝑚𝑖 (cid:172) portunities.TheWHINsconstructedfromthesethreedatasetsall\n(8)\ncontainabout200,000entitiesand10millionlinks.Table2displays\nthestatisticsofthedatasets.Theskills,schools,andcompanies\n𝛼𝑚𝑗( 𝑗𝑘\n)\n=\n(cid:205) 𝑚 ′∈A\n𝑑\n𝑚 𝑐\n𝑚\n𝑜 𝑖 𝑛\n𝑗\n𝑛\n(\n𝑒𝑐\n𝑗𝑘\n𝑡\n)\n𝑑𝑚 ′( 𝑗𝑘 )\n(9) i\njo\nn\nb\nt\ns\nh\no\ne\nn\nd\nL\nat\nin\nas\nk\ne\ne\nt\nd\na\nIn\nll\n.\nc\nW\nom\nec\ne\na\nf\nn\nr\nn\no\no\nm\ntp\nex\nro\np\nv\nli\ni\nc\nd\ni\ne\nt\nf\na\nu\nn\nr\nn\nth\no\ne\nta\nr\nt\nd\nio\ne\nn\nta\ns\nil\nb\ne\ny\nd\nm\nin\ne\nfo\nm\nr\nb\nm\ne\na\nr\nt\ns\nio\no\nn\nr\nAsshowninEq.8,wecangetthemessagingfunctionofprofes- aboutthedatasetstoprotecttheusers’privacy.Eachcandidatepair"
  },
  {
    "id": 699,
    "text": "f\na\nu\nn\nr\nn\nth\no\ne\nta\nr\nt\nd\nio\ne\nn\nta\ns\nil\nb\ne\ny\nd\nm\nin\ne\nfo\nm\nr\nb\nm\ne\na\nr\nt\ns\nio\no\nn\nr\nAsshowninEq.8,wecangetthemessagingfunctionofprofes- aboutthedatasetstoprotecttheusers’privacy.Eachcandidatepair\nsionalnetworkenhancedGNNbytextattentionandskilldistance withinthedatasetsislabeledaseitherpositiveornegativebased\nattentionwhere𝛼𝑚𝑗( 𝑗𝑘\n)\nisnormalizedmessageaggregationweight onthefeedbackprovidedbymembers.\ndescribedinEq.9.Weusetheaverageofeachlayer’srepresentation\n4.1.2 BaselineModels. Wecompareourmodelwiththefollowing\nasthefinalrepresentationof𝑚𝑖:\nbaselinemodels.\n𝐿 1 LightGCN[12]isasimplifiedGraphConvolutionalNet-\n∑︁+ 𝑙 •\nℎ𝑚𝑖( 𝑗𝑘\n)\n= ℎ𝑖( )𝑗𝑘\n/\n𝐿 (10) work model for collaborative filtering with competitive\n𝑙=0 performanceandlesscomplexity.\nComparedwithmemberprofiles,jobdescriptionsprovidedby PJFNN[41]isamethodbasedonaconvolutionalneural\n•\nrecruitersoftenhaveahighquality.Thus,wedirectlyusecontextual network(CNN).HierarchicalCNNencodesResumesand"
  },
  {
    "id": 700,
    "text": "HaoChenetal.\nTable1:Performancecomparisonofallbaselinesandourmodels.Thebestandsecond-bestresultsareshowninboldand\nunderlined,respectively.Allvaluesaremultipliedby100.\nModels\nDatasetMetric\nLightGCN PJFNN BPJFNN APJFNN MV-CoN HAN HGT CSAGNN\nAUC 63.94 2.08 64.40 1.85 64.62 2.23 64.59 2.02 65.53 3.69 66.02 3.52 67.63 2.63 68.57 3.46\n± ± ± ± ± ± ± ±\nACC 63.38 3.21 63.45 1.65 63.71 2.91 63.64 3.38 64.37 4.26 64.19 2.57 66.81 2.21 68.76 1.91\nTech ± ± ± ± ± ± ± ±\nF1 61.73 1.38 62.22 3.48 62.51 3.19 62.14 1.76 63.96 3.44 63.72 1.83 64.97 1.54 65.22 2.67\n± ± ± ± ± ± ± ±\nAP 57.38 1.79 58.19 2.17 58.38 2.46 57.91 3.34 58.73 3.55 59.10 3.90 60.99 2.85 61.59 3.46\n± ± ± ± ± ± ± ±\nAUC 64.18 1.71 64.60 1.79 65.81 2.81 65.59 3.32 66.24 2.90 66.17 2.71 68.43 3.87 69.60 1.83\n± ± ± ± ± ± ± ±\nACC 62.89 2.65 63.80 2.29 64.60 3.48 64.28 3.24 64.47 3.73 67.87 3.98 67.83 2.06 69.69 3.23\nFinance ± ± ± ± ± ± ± ±\nF1 61.91 1.86 62.78 1.57 63.43 1.73 63.41 1.81 64.26 2.98 64.56 3.96 65.59 2.71 67.49 1.84"
  },
  {
    "id": 701,
    "text": "Finance ± ± ± ± ± ± ± ±\nF1 61.91 1.86 62.78 1.57 63.43 1.73 63.41 1.81 64.26 2.98 64.56 3.96 65.59 2.71 67.49 1.84\n± ± ± ± ± ± ± ±\nAP 58.96 1.25 59.37 2.23 60.37 2.32 60.14 3.27 60.58 3.17 60.38 3.31 61.06 2.65 62.70 3.35\n± ± ± ± ± ± ± ±\nAUC 66.98 1.23 67.27 2.15 68.79 1.99 68.81 1.84 69.52 2.91 69.39 3.90 70.60 1.54 72.27 1.64\n± ± ± ± ± ± ± ±\nACC 65.93 1.57 67.24 3.06 68.46 2.75 68.73 2.32 69.33 3.46 69.42 2.88 69.71 3.98 72.37 1.92\nHybrid ± ± ± ± ± ± ± ±\nF1 64.62 1.84 65.25 1.98 65.35 2.47 65.36 3.86 64.80 3.82 64.03 2.44 65.92 1.91 69.58 2.05\n± ± ± ± ± ± ± ±\nAP 60.52 1.67 61.57 1.16 62.31 1.72 62.26 1.05 63.59 3.69 63.33 3.96 64.02 3.28 64.49 2.33\n± ± ± ± ± ± ± ±\nTable2:Statisticsofdatasets.MreferstoMember,Jrefers 4.1.3 EvaluationandImplementationDetails. Weusefourwidely\ntoJob,SreferstoSkill,CPreferstoCandidatePair,andPC usedmetricstoevaluatetherankingperformance:AUC,accuracy\nreferstoProfessionalConnection. (ACC),F1,andaverageprecision(AP)."
  },
  {
    "id": 702,
    "text": "toJob,SreferstoSkill,CPreferstoCandidatePair,andPC usedmetricstoevaluatetherankingperformance:AUC,accuracy\nreferstoProfessionalConnection. (ACC),F1,andaverageprecision(AP).\nThePerson-JobFitmodels,namelyPJFNN,BPJFNN,andAPJFNN,\nIndustry #M #J #S #CP #PC areimplementedusingRecBole—anestablishedopen-sourcerecom-\nmendationlibrary[40].TheMV-CoNmodelleveragestheoriginal\nTech 33000+ 62000+ 27000+ 136000+ 1922000+\ncodeprovidedintheirrespectivepaper[3].Othermodelsareim-\nFinance 20000+ 27000+ 23000+ 36000+ 615000+\nplementedwithPyTorchGeometric[8].WehaveemployedBERT-\nHybrid 83000+ 120000+ 33000+ 200000+ 2768000+ Tiny6toreducecomputationalcosts.\nTheembeddingdimensionsforallmodelsarestandardizedat\n32.AsdiscussedinSection3.3.1,subgraphsamplingformini-batch\njobdescriptionsindependently,andthematchingdegreeis WHINpre-trainingisexecutedusingthePyTorchGeometricsub-\ncalculatedbycosinesimilarity. graphsampler[8].Subgraphconstructionstartsfromnodeswithin"
  },
  {
    "id": 703,
    "text": "jobdescriptionsindependently,andthematchingdegreeis WHINpre-trainingisexecutedusingthePyTorchGeometricsub-\ncalculatedbycosinesimilarity. graphsampler[8].Subgraphconstructionstartsfromnodeswithin\nBPJFNN[28]leveragesbidirectionalLSTMtolearnthe abatchofcandidatepairs,andeachhopsamplesfiveneighbors\n•\nrepresentationsofresumesandjobdescriptions. viarelations(ormetapaths).Reconnectingallrelations(andmetap-\nAPJFNN[28]leveragesbidirectionalLSTMandhierarchi- aths)betweenthesamplednodesfollowsthis.Wesetthenumber\n•\ncalattentionmechanismtolearntherepresentationsof ofsampledhopsto3.Thenumberofsampledhopsisfixedat3.\nresumesandjobdescriptions. InCSAGNN,thecountofsampledskills(𝑛𝑠)isdesignatedas10,\nMV-CoN[3]combinestextmatchingmodelandRGCNto andCSAGNNcomprisestwolayers.Thefirst128tokensfromall\n•\nlearnrepresentationsofresumesandjobdescriptions. textinputsarecapturedforallmodels.Allmodelsareoptimized\nHAN[33]usesadualattentionmechanismtoaggregate usingtheAdamoptimizer[19],withalearningrateadjustedfrom\n•"
  },
  {
    "id": 704,
    "text": "HAN[33]usesadualattentionmechanismtoaggregate usingtheAdamoptimizer[19],withalearningrateadjustedfrom\n•\nneighborinformationviadifferentmetapaths. 0.01to0.0001.Forthepurposeofevaluation,thedatasetutilized\nHGT[13]designsnode-andedge-typedependentparam- acrossallmodelsisrandomlypartitionedintoaratioof8:1:1for\n•\neterstocharacterizetheheterogeneousattentionovereach training,validation,andtestingrespectively.Threeindependentex-\nedge. perimentsareconducted,eachonerepeated,toascertainconsistent\nThesebaselinescanbedividedintothreegroups:(1)context- andreliableoutcomes.\nbasedmodelsthattreatPerson-JobFitasatextmatchproblem\nandusecontextualknowledgefrommembers’profilesandjobs’ 4.2 TheOverallComparison(RQ1)\ndescriptions:PJFNN,BPJFNN,APJFNN,andMV-CoN.Inparticular,\nTable1showstheperformanceofallbaselinemodelsandourmodel,\nMV-CoNadditionallyintroducesstructuralinformationtoenhance\nCSAGNN.Theresultsindicatebetterperformanceonthehybrid\nmodelperformance.(2)collaborativefiltering-basedmodelthat"
  },
  {
    "id": 705,
    "text": "MV-CoNadditionallyintroducesstructuralinformationtoenhance\nCSAGNN.Theresultsindicatebetterperformanceonthehybrid\nmodelperformance.(2)collaborativefiltering-basedmodelthat\ndatasetthanonindividualindustrydatasets,possiblybecausejobs\nusesdirectinteractionsbetweenmembersandjobs:LightGCN.(3)\nend-to-end heterogeneous graph neural network models: HAN,\nHGT. 6https://huggingface.co/prajjwal1/bert-tiny"
  },
  {
    "id": 706,
    "text": "ProfessionalNetworkMatters:ConnectionsEmpowerPerson-JobFit\nwithinthesameindustryaremoresimilarandthushardertodis-\n0.73 tinguish.End-to-endheterogeneousgraphneuralnetworkmodels\n0.72\nhaveshownsuperiorperformanceonallthreedatasetscomparedto 0.71\ncontext-basedandCF-basedmodels.HGT,utilizingheterogeneous 0.70\nknowledge,hasimprovedAUCscoresby3.20%,3.31%,and1.55% 0.69\ncomparedtothebestbaselineMV-CoN,whichemployshomoge- 0.68 0 5 10 20\n#skills\nneousknowledge.CSAGNNfurtherimprovedAUCscoresby1.39%,\n1.71%,and2.37%comparedtoHGT.\nTable3:Ablationstudiesconductedonalldatasets,withall\nvaluesmultipliedby100.\nIndustry Model AUC ACC F1 AP\nCSAGNN 68.57 68.76 65.22 61.59\nw/oS 68.26 68.42 64.41 61.34\nTech w/oA 68.23 68.40 64.38 61.29\nw/oCSA 68.06 68.22 64.26 61.21\nw/oCSA&H 62.84 62.78 61.35 57.53\nCSAGNN 69.60 69.69 67.55 62.77\nw/oS 69.39 69.48 66.68 62.21\nFinance w/oA 69.22 69.32 66.44 62.32\nw/oCSA 68.98 69.04 66.15 62.06\nw/oCSA&H 63.75 62.78 61.49 58.74\nCSAGNN 72.27 72.37 69.58 64.49"
  },
  {
    "id": 707,
    "text": "CSAGNN 69.60 69.69 67.55 62.77\nw/oS 69.39 69.48 66.68 62.21\nFinance w/oA 69.22 69.32 66.44 62.32\nw/oCSA 68.98 69.04 66.15 62.06\nw/oCSA&H 63.75 62.78 61.49 58.74\nCSAGNN 72.27 72.37 69.58 64.49\nw/oS 70.81 71.67 68.68 64.67\nHybrid w/oA 70.03 71.47 68.18 64.06\nw/oCSA 68.37 69.39 64.84 62.65\nw/oCSA&H 65.85 64.32 63.26 59.49\n4.3 AblationStudiesandParameterTuning\n(RQ2)\nOurapproachemploystwomaintechniques:WHINpre-training,\nwhichintegratesheterogeneousknowledge,includingprofessional\nnetworks,andCSAGNN,whichincorporatesprofessionalnetworks\nwithanattentionmechanism.Weconductedablationstudiestoan-\nalyzetheeffectivenessofdifferenttechniques,whereweconsidered\nthefollowingvariantsofCSAGNN:(1)CSAGNNw/oSremoves\nmessagesfromprofessionalconnectionsbutretainsthejob-specific\nattentionmechanismformembersthemselves.(2)CSAGNNw/oA\nremovesthejob-specificattentionmechanism.(3)CSAGNNw/o\nCSAremovesbothprofessionalconnectionsandjob-specificatten-\ntionmechanisms,usingonlystructuralknowledgefromtheWHIN"
  },
  {
    "id": 708,
    "text": "removesthejob-specificattentionmechanism.(3)CSAGNNw/o\nCSAremovesbothprofessionalconnectionsandjob-specificatten-\ntionmechanisms,usingonlystructuralknowledgefromtheWHIN\npre-trainapproach.(4)CSAGNNw/oCSA&HremovesWHINpre-\ntrainedembeddingsandCSAmechanism,onlyusingtextinforma-\ntionasinputandleveragesMLPtopredict.\nAsdetailedinTable3,withinsingleindustrydatasetswithlow\nsocialnoise,thejob-specificattentionmechanismandprofessional\nconnections’messagesyieldedlimitedbenefits.However,WHIN\npre-trainingembeddingswerecrucialforenhancingperformance.\nCUA\ntfhieny c bah rnicde 0 0 . . 7 7 0 1\n0.69\n0.68\n0.67\n0.66\n0.65\n0.64 0 5 10 20\n#skills\n1F\ntfhieny c bah rnicde\n(a)Modelperformancevaryingthenumberofsampledskills\nwhilefixingCSAGNNlayersto1.\n0.73\n0.72\n0.71\n0.70\n0.69\n0.68 0 1 2\n#layers\nCUA\ntfhieny c bah rnicde 0 0 . . 7 7 0 1\n0.69\n0.68\n0.67\n0.66\n0.65\n0.64 0 1 2\n#layers\n1F\ntfhieny c bah rnicde\n(b) ModelperformancevaryingthenumberofCSAGNN\nlayerswhilefixingthenumberofsampledskillsto10."
  },
  {
    "id": 709,
    "text": "tfhieny c bah rnicde 0 0 . . 7 7 0 1\n0.69\n0.68\n0.67\n0.66\n0.65\n0.64 0 1 2\n#layers\n1F\ntfhieny c bah rnicde\n(b) ModelperformancevaryingthenumberofCSAGNN\nlayerswhilefixingthenumberofsampledskillsto10.\nFigure4:Hyperparametertuningexperimentstoinvestigate\nthespecificeffectsofsocialrelationsandjob-specificatten-\ntionmechanism.\nOnhybriddatasetswithhighsocialnoise,wherecandidatepairs\nmay span different industries, the job-specific attention mecha-\nnism,alongwiththeWHINpre-trainingembedding,significantly\nimprovedthemodel.\nToinvestigatethespecificrolesofprofessionalnetworksand\njob-specificattentionmechanismsinourmodel,weperformedhy-\nperparametertuningexperimentsbyfixingonemoduleandvarying\ntheothertoobserveitsimpactonperformance.Specifically,wefirst\nfixedthenumberofCSAGNNlayersto1andtestedthemodel’sper-\nformancewithdifferentnumbersofsampledskills(𝑛𝑠),asdescribed\ninSection3.4.Wethenfixed𝑛𝑠 to10andtestedthemodelwith\ndifferentnumbersofCSAGNNlayers.Theresultsonalldatasets"
  },
  {
    "id": 710,
    "text": "formancewithdifferentnumbersofsampledskills(𝑛𝑠),asdescribed\ninSection3.4.Wethenfixed𝑛𝑠 to10andtestedthemodelwith\ndifferentnumbersofCSAGNNlayers.Theresultsonalldatasets\nshowedthat,comparedtotheneighborsamplingrange,thegain\nin model performance from the attention mechanism saturates\nwhenthenumberofsampledskillsisrelativelysmall.Thisobser-\nvationledustoexplorethebalancebetweenmodelperformance\nandefficiencybyselectingasmallervaluefor𝑛𝑠.\n4.4 CaseStudy(RQ3)\nThevalueofutilizingprofessionalnetworksisillustratedinthe\nexampleshowninFigure5.Inthiscase,wehaverephrasedthein-\nformationtopreservetheusers’privacywhileretainingitsessential\nmeaning.Eventhoughthemember’sprofilecontainsminimalin-\nformation,theCSAGNNmodeladeptlypredictstheirclassification\nbyharnessingjob-relatedinsightsfromthemember’sprofessional\nconnections.Thiscontrastssharplywiththeincorrectpredictions\nmadebythecollaborativefiltering-basedLightGCNandcontext-\nbasedPJFNNmodels.Thisspecificexampleaccentuatesthevital"
  },
  {
    "id": 711,
    "text": "connections.Thiscontrastssharplywiththeincorrectpredictions\nmadebythecollaborativefiltering-basedLightGCNandcontext-\nbasedPJFNNmodels.Thisspecificexampleaccentuatesthevital\nroleofprofessionalconnectionsintheclassificationprocessand\nunderlinestheCSAGNNmodel’sdistinctivecapabilitytoleverage\nsuchrelationshipsforprecisepredictions."
  },
  {
    "id": 712,
    "text": "HaoChenetal.\nneighbor\n• I specialize in writing and supporting CI/CDpipelines in Jenkins/Azure Profile\n• DevOps anddeploying to multi-site environment\n• ……\n#3513 …\nRich job–related connect\ndescription neighbor • I am a graduate from XX Universityin BSc Computer and mathematical science Profile\n• I have completed core computer science coursesas well as mathematics, and\nmathematical statistics.My goal upon graduation is to secure a fellowship in\n#17323 data analysis, business analysis, software development\n• ……\nconnect\nNo job–related member • I have assisted clients with: Performing computer forensics imaging, data Profile\ndescription #3241 • e … x … traction, processing,and reporting.\nMicrosoft Azure CSAGNN(ours) CF-based model(LightGCN) Context-based model(PJFNN)\nscore: 0.6115 score: 0.3613 score: 0.3215\nEducation\nrequire job • Create & Maintain Sophisticated CI/CDPipelines Description\n• Coach & mentor other engineers"
  },
  {
    "id": 713,
    "text": "score: 0.6115 score: 0.3613 score: 0.3215\nEducation\nrequire job • Create & Maintain Sophisticated CI/CDPipelines Description\n• Coach & mentor other engineers\nMathematics … • Identify technical risks and mitigate these (pre, during & post-deployment)\n#137054 • ……\nFigure5:AcasewhereprofessionalconnectionsimproveperformanceonPerson-JobFit.CSAGNNcanimprovetheperformance\nofPersonJobFitbyfilteringandaggregatinginformationfromprofessionalnetworks.Forprivacyprotectionreasons,wehave\nrewrittenthestatementintheexamplewhileensuringthatthesemanticinformationremainsunchanged.\ncompanies,andschools,withanemphasisontheprofessionalcon-\nnectionsamongmembers.Byemployingheterogeneousgraphpre-\ntrainingtechniques,theapproachacquiresrepresentationsthat\nintegrateprofessionalconnectionsandotherdiverseinformation\nfordifferententities.Theserepresentationsaresubsequentlyap-\npliedtotheCSAGNNmodel,helpingtofilteroutsocialnoise.\nExperimentalresultsshowthatprofessionalconnectionspro-"
  },
  {
    "id": 714,
    "text": "fordifferententities.Theserepresentationsaresubsequentlyap-\npliedtotheCSAGNNmodel,helpingtofilteroutsocialnoise.\nExperimentalresultsshowthatprofessionalconnectionspro-\nvidevaluablejob-specificinsights.TheWHINpre-trainingmethod\nisalsopromisingforapplicationslikeskillcompletionandpro-\nfessionalconnectionrecommendations,openingnewresearchav-\nFigure 6: Visualization of WHIN Pre-trained skill embed-\nenues.Furthermore,forlarge-scaleapplicationsofCSAGNN,akey\ndings showing clear distinction between Technology and\nareaoffutureworkinvolvesreducingthemodel’scomputational\nHealth-RelatedSkills.\noverhead[21–23].\nACKNOWLEDGMENTS\nThankstoYihanCaoandYushuDufortheirinsightfuldiscussion.\n4.5 Visualization(RQ4)\nThankstoSriramVasudevanandPeideZhongfortheircomprehen-\nTodemonstratetheeffectivenessofWHINpre-trainingincaptur- sivereviewfeedback.\ningmeaningfulskillrepresentationsforCSAGNN,weanalyzedthe\npre-trainedembeddingsofselectedskillsintheembeddingspace.\nETHICALCONSIDERATIONS"
  },
  {
    "id": 715,
    "text": "ingmeaningfulskillrepresentationsforCSAGNN,weanalyzedthe\npre-trainedembeddingsofselectedskillsintheembeddingspace.\nETHICALCONSIDERATIONS\nUtilizingPrincipalComponentAnalysis(PCA)asourmethodof\nTheintegrationofprofessionalnetworksintoPerson-JobFitmodels\ndimensionalreduction[18],wewereabletovisualizetheWHIN\nofferssignificantpotentialforenrichingrecommendations.How-\npre-trained embeddings for various skills. Figure 6 reveals that\never,wemustrecognizeandaddresstwovitalethicalconsiderations\nskillscloselyrelatedtoprogramming,suchasC++andPython,\nthatmayleadtoadversesocietalimplications:\naregroupedtogetherintheembeddingspace.Incontrast,skills\nFirstly,thecoreofourapproachinvolvesaccessingmembers’\nassociatedwithhealthcareformadistinctcluster.Itisnoteworthy\nprofessionalconnectionsandcorrespondingprofiles,whichraises\nthattheseclassificationswereachievedusingonlytheskillnames\nprivacyconcerns.Inourresearch,weensuredthatthedatacol-\nasinitialization.Yet,ourWHINpre-trainingmethodsuccessfully"
  },
  {
    "id": 716,
    "text": "thattheseclassificationswereachievedusingonlytheskillnames\nprivacyconcerns.Inourresearch,weensuredthatthedatacol-\nasinitialization.Yet,ourWHINpre-trainingmethodsuccessfully\nlectionandprocessingrespectedprivacybyadheringtoproper\ndistinguishesbetweenskillcategoriesbylearningfromheteroge-\nconsentmechanismsandlimitingaccesstopertinentinformation.\nneousknowledge,highlightingthecapabilityofWHINtorecognize\nFutureusersshouldfollowsuit,beingmindfuloftheneedforclear\nanddifferentiateskillsacrossmultipledomains.\nandvoluntary consentfrom members andcarefully controlling\naccesstoconnectioninformation.\n5 CONCLUSION\nAdditionally,connectionsoftenexhibitdemographicclustering,\nThispaperintroducesanoveltwo-stageapproachforleveraging whichcouldintroducebiasesfavoringcertaingroupswithinnet-\nprofessionalnetworksinPerson-JobFit,includingtheformation works.Themodelsmightinfersensitiveattributeslikerace,gender,"
  },
  {
    "id": 717,
    "text": "professionalnetworksinPerson-JobFit,includingtheformation works.Themodelsmightinfersensitiveattributeslikerace,gender,\noftheWorkplaceHeterogeneousInformationNetwork(WHIN). orage.Futureusersshouldconductauditstoensuredemographic\nWHINencompassesvariousentitiessuchasmembers,jobs,skills, equityandprohibitdirectutilizationofprotectedclassinformation."
  },
  {
    "id": 718,
    "text": "ProfessionalNetworkMatters:ConnectionsEmpowerPerson-JobFit\nInsummary,whiletheenhancementofPerson-JobFitmodels [20] JinLi,ZhaohuiPeng,SenzhangWang,XiaokangXu,PhilipSYu,andZhenyun\nthroughprofessionalnetworkdatabringsadvancements,italso Hao.2020. HeterogeneousGraphEmbeddingforCross-DomainRecommen-\ndationThroughAdversarialLearning.InInternationalConferenceonDatabase\nintroduces risks concerning privacy and fairness. These ethical\nSystemsforAdvancedApplications.Springer,507–522.\nchallenges call for a concerted effort from platforms to resolve, [21] TianyiLi,LuChen,ChristianSJensen,andTorbenBachPedersen.2021.TRACE:\nensuringthattheinnovationsfosteranenvironmentthatisboth Real-timecompressionofstreamingtrajectoriesinroadnetworks.Proceedings\noftheVLDBEndowment14,7(2021),1175–1187.\nrespectfulofindividualrightsandfreefromdiscriminatorybiases. [22] TianyiLi,RuikaiHuang,LuChen,ChristianSJensen,andTorbenBachPedersen.\n2020.Compressionofuncertaintrajectoriesinroadnetworks.Proceedingsofthe"
  },
  {
    "id": 719,
    "text": "2020.Compressionofuncertaintrajectoriesinroadnetworks.Proceedingsofthe\nREFERENCES VLDBEndowment13,7(2020),1050–1063.\n[23] ZhiqiLin,ChengLi,YoushanMiao,YunxinLiu,andYinlongXu.2020. Pa-\n[1] WendongBi,LunDu,QiangFu,YanlinWang,ShiHan,andDongmeiZhang. graph:Scalinggnntrainingonlargegraphsviacomputation-awarecaching.In\n2022.Makeheterophilygraphsbetterfitgnn:Agraphrewiringapproach.arXiv Proceedingsofthe11thACMSymposiumonCloudComputing.401–415.\npreprintarXiv:2209.08264(2022). [24] JiaweiLiu,ChuanShi,ChengYang,ZhiyuanLu,andSYuPhilip.2022.Asurvey\n[2] WendongBi,LunDu,QiangFu,YanlinWang,ShiHan,andDongmeiZhang. onheterogeneousinformationnetworkbasedrecommendersystems:Concepts,\n2023.MM-GNN:Mix-MomentGraphNeuralNetworktowardsModelingNeigh- methods,applicationsandresources.AIOpen(2022).\nborhoodFeatureDistribution.InProceedingsoftheSixteenthACMInternational [25] YaoLu,SandyElHelou,andDenisGillet.2013.Arecommendersystemforjob"
  },
  {
    "id": 720,
    "text": "borhoodFeatureDistribution.InProceedingsoftheSixteenthACMInternational [25] YaoLu,SandyElHelou,andDenisGillet.2013.Arecommendersystemforjob\nConferenceonWebSearchandDataMining.132–140. seekingandrecruitingwebsite.InProceedingsofthe22ndInternationalConference\n[3] ShuqingBian,XuChen,WayneXinZhao,KunZhou,YupengHou,YangSong, onWorldWideWeb.963–966.\nTaoZhang,andJi-RongWen.2020.Learningtomatchjobswithresumesfrom [26] JochenMalinowski,TobiasKeim,OliverWendt,andTimWeitzel.2006.Matching\nsparseinteractiondatausingmulti-viewco-teachingnetwork.InProceedingsof peopleandjobs:Abilateralrecommendationapproach.InProceedingsofthe39th\nthe29thACMInternationalConferenceonInformation&KnowledgeManagement. AnnualHawaiiInternationalConferenceonSystemSciences(HICSS’06),Vol.6.\n65–74. IEEE,137c–137c.\n[4] ShuqingBian,WayneXinZhao,YangSong,TaoZhang,andJi-RongWen.2019. [27] BryanPerozzi,RamiAl-Rfou,andStevenSkiena.2014.Deepwalk:Onlinelearning"
  },
  {
    "id": 721,
    "text": "65–74. IEEE,137c–137c.\n[4] ShuqingBian,WayneXinZhao,YangSong,TaoZhang,andJi-RongWen.2019. [27] BryanPerozzi,RamiAl-Rfou,andStevenSkiena.2014.Deepwalk:Onlinelearning\nDomainadaptationforperson-jobfitwithtransferabledeepglobalmatchnet- ofsocialrepresentations.InProceedingsofthe20thACMSIGKDDinternational\nwork.InProceedingsofthe2019conferenceonempiricalmethodsinnaturallan- conferenceonKnowledgediscoveryanddatamining.701–710.\nguageprocessingandthe9thinternationaljointconferenceonnaturallanguage [28] ChuanQin,HengshuZhu,TongXu,ChenZhu,LiangJiang,EnhongChen,and\nprocessing(EMNLP-IJCNLP).4810–4820. HuiXiong.2018. Enhancingperson-jobfitfortalentrecruitment:Anability-\n[5] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT: awareneuralnetworkapproach.InThe41stinternationalACMSIGIRconference\nPre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding. onresearch&developmentininformationretrieval.25–34."
  },
  {
    "id": 722,
    "text": "Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding. onresearch&developmentininformationretrieval.25–34.\nInProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAsso- [29] MichaelSchlichtkrull,ThomasNKipf,PeterBloem,RiannevandenBerg,Ivan\nciationforComputationalLinguistics:HumanLanguageTechnologies,Volume1 Titov,andMaxWelling.2018.Modelingrelationaldatawithgraphconvolutional\n(LongandShortPapers).AssociationforComputationalLinguistics,Minneapolis, networks.InEuropeansemanticwebconference.Springer,593–607.\nMinnesota,4171–4186. https://doi.org/10.18653/v1/N19-1423 [30] WalidShalaby,BahaaEddinAlAila,MohammedKorayem,LaylaPournajaf,Khal-\n[6] YuxiaoDong,NiteshVChawla,andAnanthramSwami.2017.metapath2vec: ifehAlJadda,ShannonQuinn,andWlodekZadrozny.2017.Helpmefindajob:A\nScalablerepresentationlearningforheterogeneousnetworks.InProceedingsof graph-basedapproachforjobrecommendationatscale.In2017IEEEinternational"
  },
  {
    "id": 723,
    "text": "Scalablerepresentationlearningforheterogeneousnetworks.InProceedingsof graph-basedapproachforjobrecommendationatscale.In2017IEEEinternational\nthe23rdACMSIGKDDinternationalconferenceonknowledgediscoveryanddata conferenceonbigdata(bigdata).IEEE,1544–1553.\nmining.135–144. [31] DazhongShen,HengshuZhu,ChenZhu,TongXu,ChaoMa,andHuiXiong.\n[7] LunDu,XuChen,FeiGao,QiangFu,KunqingXie,ShiHan,andDongmeiZhang. 2018.Ajointlearningapproachtointelligentjobinterviewassessment..InIJCAI,\n2022. UnderstandingandImprovementofAdversarialTrainingforNetwork Vol.18.3542–3548.\nEmbeddingfromanOptimizationPerspective.InProceedingsoftheFifteenth [32] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,\nACMInternationalConferenceonWebSearchandDataMining.230–240. AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.2017. Attentionisall\n[8] MatthiasFeyandJanE.Lenssen.2019.FastGraphRepresentationLearningwith youneed.Advancesinneuralinformationprocessingsystems30(2017)."
  },
  {
    "id": 724,
    "text": "[8] MatthiasFeyandJanE.Lenssen.2019.FastGraphRepresentationLearningwith youneed.Advancesinneuralinformationprocessingsystems30(2017).\nPyTorchGeometric.InICLRWorkshoponRepresentationLearningonGraphsand [33] XiaoWang,HouyeJi,ChuanShi,BaiWang,YanfangYe,PengCui,andPhilipSYu.\nManifolds. 2019.Heterogeneousgraphattentionnetwork.InTheworldwidewebconference.\n[9] Tao-yangFu,Wang-ChienLee,andZhenLei.2017.Hin2vec:Exploremeta-paths 2022–2032.\ninheterogeneousinformationnetworksforrepresentationlearning.InProceed- [34] YifanWang,SuyaoTang,YuntongLei,WeipingSong,ShengWang,andMing\ningsofthe2017ACMonConferenceonInformationandKnowledgeManagement. Zhang.2020.Disenhan:Disentangledheterogeneousgraphattentionnetwork\n1797–1806. forrecommendation.InProceedingsofthe29thACMInternationalConferenceon\n[10] XinyuFu,JianiZhang,ZiqiaoMeng,andIrwinKing.2020. Magnn:Metap- Information&KnowledgeManagement.1605–1614."
  },
  {
    "id": 725,
    "text": "1797–1806. forrecommendation.InProceedingsofthe29thACMInternationalConferenceon\n[10] XinyuFu,JianiZhang,ZiqiaoMeng,andIrwinKing.2020. Magnn:Metap- Information&KnowledgeManagement.1605–1614.\nathaggregatedgraphneuralnetworkforheterogeneousgraphembedding.In [35] MaxWellingandThomasNKipf.2016.Semi-supervisedclassificationwithgraph\nProceedingsofTheWebConference2020.2331–2341. convolutionalnetworks.InJ.InternationalConferenceonLearningRepresentations\n[11] AdityaGroverandJureLeskovec.2016.node2vec:Scalablefeaturelearningfor (ICLR2017).\nnetworks.InProceedingsofthe22ndACMSIGKDDinternationalconferenceon [36] RuiYan,RanLe,YangSong,TaoZhang,XiangliangZhang,andDongyanZhao.\nKnowledgediscoveryanddatamining.855–864. 2019. Interviewchoicerevealsyourpreferenceonthemarket:Toimprove\n[12] XiangnanHe,KuanDeng,XiangWang,YanLi,YongdongZhang,andMeng job-resumematchingthroughprofilingmemories.InProceedingsofthe25th"
  },
  {
    "id": 726,
    "text": "[12] XiangnanHe,KuanDeng,XiangWang,YanLi,YongdongZhang,andMeng job-resumematchingthroughprofilingmemories.InProceedingsofthe25th\nWang.2020.Lightgcn:Simplifyingandpoweringgraphconvolutionnetworkfor ACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining.\nrecommendation.InProceedingsofthe43rdInternationalACMSIGIRconference 914–922.\nonresearchanddevelopmentinInformationRetrieval.639–648. [37] ChenYang,YupengHou,YangSong,TaoZhang,Ji-RongWen,andWayneXin\n[13] ZiniuHu,YuxiaoDong,KuansanWang,andYizhouSun.2020.Heterogeneous Zhao.2022. ModelingTwo-WaySelectionPreferenceforPerson-JobFit.In\ngraphtransformer.InProceedingsofTheWebConference2020.2704–2710. Proceedingsofthe16thACMConferenceonRecommenderSystems.102–112.\n[14] JinchengHuang,LunDu,XuChen,QiangFu,ShiHan,andDongmeiZhang. [38] LibinYang,ZeqingZhang,XiaoyanCai,andLantianGuo.2019.Citationrec-\n2023.RobustMid-PassFilteringGraphConvolutionalNetworks.InProceedings ommendationasedgepredictioninheterogeneousbibliographicnetwork:a"
  },
  {
    "id": 727,
    "text": "2023.RobustMid-PassFilteringGraphConvolutionalNetworks.InProceedings ommendationasedgepredictioninheterogeneousbibliographicnetwork:a\noftheACMWebConference2023.328–338. networkrepresentationapproach.IEEEAccess7(2019),23232–23239.\n[15] JacobJacoby.1984.Perspectivesoninformationoverload.Journalofconsumer [39] KaichunYao,JingshuaiZhang,ChuanQin,PengWang,HengshuZhu,andHui\nresearch10,4(1984),432–435. Xiong.2022.KnowledgeEnhancedPerson-JobFitforTalentRecruitment.In2022\n[16] HouyeJi,XiaoWang,ChuanShi,BaiWang,andPhilipYu.2021. Heteroge- IEEE38thInternationalConferenceonDataEngineering(ICDE).IEEE,3467–3480.\nneousgraphpropagationnetwork.IEEETransactionsonKnowledgeandData [40] WayneXinZhao,ShanleiMu,YupengHou,ZihanLin,YushuoChen,XingyuPan,\nEngineering(2021). KaiyuanLi,YujieLu,HuiWang,ChangxinTian,etal.2021.Recbole:Towards\n[17] ZhuorenJiang,YueYin,LiangcaiGao,YaoLu,andXiaozhongLiu.2018.Cross- aunified,comprehensiveandefficientframeworkforrecommendationalgo-"
  },
  {
    "id": 728,
    "text": "[17] ZhuorenJiang,YueYin,LiangcaiGao,YaoLu,andXiaozhongLiu.2018.Cross- aunified,comprehensiveandefficientframeworkforrecommendationalgo-\nlanguagecitationrecommendationviahierarchicalrepresentationlearningon rithms.InProceedingsofthe30thACMInternationalConferenceonInformation&\nheterogeneousgraph.InThe41stInternationalACMSIGIRConferenceonResearch KnowledgeManagement.4653–4664.\n&DevelopmentinInformationRetrieval.635–644. [41] ChenZhu,HengshuZhu,HuiXiong,ChaoMa,FangXie,PengliangDing,and\n[18] IanTJolliffeandJorgeCadima.2016.Principalcomponentanalysis:areview PanLi.2018.Person-jobfit:Adaptingtherighttalentfortherightjobwithjoint\nandrecentdevelopments.PhilosophicaltransactionsoftheroyalsocietyA:Math- representationlearning.ACMTransactionsonManagementInformationSystems\nematical,PhysicalandEngineeringSciences374,2065(2016),20150202. (TMIS)9,3(2018),1–17.\n[19] DiederikPKingmaandJimmyBa.2014.Adam:Amethodforstochasticopti-\nmization.arXivpreprintarXiv:1412.6980(2014)."
  },
  {
    "id": 729,
    "text": "Learning of networked spreading models from noisy and incomplete data\nMateusz Wilinski and Andrey Y. Lokhov\nTheoretical Division, Los Alamos National Laboratory, Los Alamos, USA\nRecent years have seen a lot of progress in algorithms for learning parameters of spreading dy-\nnamics from both full and partial data. Some of the remaining challenges include model selection\nunderthescenariosofunknownnetworkstructure,noisydata,missingobservationsintime,aswell\nas an efficient incorporation of prior information to minimize the number of samples required for\nan accurate learning. Here, we introduce a universal learning method based on scalable dynamic\nmessage-passing technique that addresses these challenges often encountered in real data. The al-\ngorithm leverages available prior knowledge on the model and on the data, and reconstructs both\nnetworkstructureandparametersofaspreadingmodel. Weshowthatalinearcomputationalcom-"
  },
  {
    "id": 730,
    "text": "gorithm leverages available prior knowledge on the model and on the data, and reconstructs both\nnetworkstructureandparametersofaspreadingmodel. Weshowthatalinearcomputationalcom-\nplexityofthemethodwiththekeymodelparametersmakesthealgorithmscalabletolargenetwork\ninstances.\nI. INTRODUCTION (DMP) [9–28]. DMP is an inference method for spread-\ning processes on networks derived from a classical belief\npropagation (BP) algorithm [29, 30], and thus showing\nSpreading models are routinely used to generate pre- similar properties to BP [17, 21]. The key result of [8]\ndictionsforaplethoraofdiffusionprocessesonnetworks,\nstated that the model learned using the minimization of\nwhereby infectious diseases, opinions, or failures propa-\ndistance between marginal probabilities generates better\ngate in natural, social, and technological systems [1, 2].\npredictionscomparedtothemodelwithground-truthpa-\nIn these models, the nodes typically go from inactive to"
  },
  {
    "id": 731,
    "text": "gate in natural, social, and technological systems [1, 2].\npredictionscomparedtothemodelwithground-truthpa-\nIn these models, the nodes typically go from inactive to\nrameters when DMP is used as the inference algorithm.\nan active state through interactions with their neighbors\nThis has been shown through an empirical study on a\nonanetwork,similarlytohowaninfectionispassedfrom\npopular spreading model known as Independent Cas-\none person to another. When the model structure or pa-\ncade[31,32], equivalenttoapopularepidemicspreading\nrameters are unknown, it is natural to consider the in-\nsusceptible-infected-recovered (SIR) model [33, 34] with\nverse problem of learning of the spreading model from\ndeterministic recovery. Due to a prediction-centric fo-\ndata. Available data usually takes form of reported ac-\ncus, the algorithm proposed in [8] has been referred to\ntivation times for nodes in a network in several observed\nas SLICER (Scalable Learning of Independent Cascade"
  },
  {
    "id": 732,
    "text": "cus, the algorithm proposed in [8] has been referred to\ntivation times for nodes in a network in several observed\nas SLICER (Scalable Learning of Independent Cascade\nactivation cascades. Whether due to a limited observa-\nEffective Representation).\ntion budget or imperfect reporting, the accessible data\nOn many sparse network instances, SLICER produces\nis unlikely to be perfect, and may be subject to uncer-\nhigh-quality estimates of model parameters in a time\ntaintyorprovideonlypartialinformationonthesystem.\nscaling linearly with the system size, even for fraction of\nThismotivatesthedesignofrobustmethodsforselecting\nhidden nodes up to 25% of the network. However, from\na spreading model from incomplete and noisy data. In\nthe results of [8], it is not immediately obvious what the\nthis paper, we introduce a scalable learning method that\nexpected accuracy limits of the method are, i.e., what\naddresses the challenges related to imperfect data."
  },
  {
    "id": 733,
    "text": "this paper, we introduce a scalable learning method that\nexpected accuracy limits of the method are, i.e., what\naddresses the challenges related to imperfect data.\nerror can be expected when the number of unobserved\nReconstruction of networked spreading models has nodes reaches very high values. In order to get insight\nbeen addressed by a number of works in recent years. In into the performance of SLICER, in section IIIA we run\nthecontextoffullandexactobservationofnodes’activa- a systematic evaluation of the performance of SLICER\ntion times, [3–6] showed that maximum-likelihood type for learning of the spreading model parameters on a va-\napproaches succeed to learn the model structure and pa- riety of random graph classes under diverse fractions of\nrameters. The problem becomes significantly more dif- unobserved nodes.\nficult when the system is only partially observed, i.e., Partial node-observability of the system is only one of"
  },
  {
    "id": 734,
    "text": "rameters. The problem becomes significantly more dif- unobserved nodes.\nficult when the system is only partially observed, i.e., Partial node-observability of the system is only one of\nonly a fraction of nodes report information about their the several ways in which the data may be incomplete\nactivation times. In this case, the maximum likelihood or uncertain. Some of the challenges often encountered\napproachhasexponentialcomplexitywithrespecttothe in real data include model selection under the scenar-\nnumber of unobserved nodes, which warrants learning ios of unknown or partially known network structure,\nmethods based on the direct problem of predicting the noisy data, missing observations in time, or necessity to\nmodel dynamics [7]. This approach has lead to an effi- include prior information in order to minimize the re-\ncientalgorithmforlearningtheparametersofthespread- quired amount of data. Scenarios that we consider are"
  },
  {
    "id": 735,
    "text": "cientalgorithmforlearningtheparametersofthespread- quired amount of data. Scenarios that we consider are\ning model on a known network, based on minimization presentedinFigure1. Thefirstchallengethatweaddress\nof the distance between observed and model-predicted is the lack of information about the network structure.\nnode marginal probabilities [8]. This choice is motivated This task has been recently studied in [35], however, the\nby the fact that marginal probabilities of nodes’ acti- proposed method involves cubic complexity in the num-\nvation can be estimated in a computationally efficient ber of nodes, making its application to large networks\nway using a method known as dynamic message passing prohibitive. The second challenge deals with incorpo-\n3202\nceD\n02\n]IS.sc[\n1v11000.1042:viXra"
  },
  {
    "id": 736,
    "text": "2\nτ i τ i τ i τ i ∈[τ î −Δτ î,τ î] τ i ±Δτ i\nα ij α ij α ij = α α ij α ij\nτ j τ j τ j τ j ∈[ τ ĵ −Δτ ĵ,τ ĵ] τ j ±Δτ j\nunobserved nodes unknown topology simple graphs missing timestamps noisy timestamps\n(section III A) (section III B) (section III C) (section III D) (section III E)\nFIG. 1: Schematic representation of different scenarios of incomplete and uncertain data that we consider in\nlearning of networked spreading models. We focus on the general setting where a fraction of the nodes never reports\ninformation (depicted as empty nodes). In addition, we treat cases where network structure is not known or known\nonly partially; where the number of data samples is small, but prior information on the parameters is available; and\nwhere the observed node activation timestamps are missing or noisy. At the end of our experimental evaluation, we\naddress learning of real-world network instances under a combination of these scenarios."
  },
  {
    "id": 737,
    "text": "address learning of real-world network instances under a combination of these scenarios.\nration of prior information on the model parameters in rameters. Our overreaching goal consists in generalizing\nthe case where the number of available samples is small. SLICER, introducing a universal learning framework ca-\nFor instance, in epidemiological applications, at most a pable of addressing all these challenges. Here, we show\nfew realizations of the dynamics can be observed, but how to crucially define the objective function in such a\nthe spreading is often modeled by a single parameter way that it incorporates the available prior information,\n(transmissionprobability)[36]. Weshowhowtoleverage and provides a high-quality reconstruction of the model\nthispriorinformationandlearnthesimplegraphmodels parameters. As demonstrated below and contrary to the\nfrom a few observed trajectories. The third challenge is solution of these challenges on a case-by-case basis, this"
  },
  {
    "id": 738,
    "text": "from a few observed trajectories. The third challenge is solution of these challenges on a case-by-case basis, this\nmissing data in time, where nodes report their activa- universality presents an advantage in that data uncer-\ntion times only during limited time windows. This task tainties present in the data and pertaining to different\nwas analysed in [37], but with an additional assumption challengescanbetreatedatthesametime. Asademon-\nthat the full probabilistic trace for each node is avail- strationofsuchauniversality,intheendofourstudy,we\nable. A likelihood-free approach for the missing time illustrate the approach on real-world network instances\ncase was proposed in [38], together with additional goal under a combination of these challenges.\nof finding the source of spreading. A variant of the sce-\nnario of missing timestamps assumes that only the com-\nplete final state of the dynamics is available. One of the\nII. METHODS\nfirst results for this variant of the problem assumed that"
  },
  {
    "id": 739,
    "text": "nario of missing timestamps assumes that only the com-\nplete final state of the dynamics is available. One of the\nII. METHODS\nfirst results for this variant of the problem assumed that\nthe unknown network is a tree [39], but later this as-\nIn the section, we first pose the learning problem us-\nsumption has been relaxed in [40–42]. In our work, we\ningtheIndependentCascade(IC)modelasthedynamics\nshow that the spreading model can be accuractly recov-\nof choice. Then, we discuss the dynamic message pass-\nered with minimal available information in the temporal\ning (DMP) inference method that lies at the foundation\nspace,eveninthecaseofpartialnode-observability. The\nof our approach. Finally, we explain the details of our\nlast challenge that we consider, often related to the re-\nlearning method.\nporting procedure, is uncertainty in timestamps, which\nmay be modeled as an additional noise added to the ob-\nserveddata. Similarsettingwasanalysedin[43,44], but"
  },
  {
    "id": 740,
    "text": "learning method.\nporting procedure, is uncertainty in timestamps, which\nmay be modeled as an additional noise added to the ob-\nserveddata. Similarsettingwasanalysedin[43,44], but\nnot paired with any other type of missing information A. Problem formulation: Learning of the\nIndependent Cascade model\nsuch as partial node-wise reporting.\nAsweshowinthiswork,inmostofthesescenarios,the In this paper, we focus on the Independent Cascade\nobjectivefunctionusedinSLICERmethodisnotdirectly model [31, 32]. Define the spreading network as a graph\napplicable,resultinginbiasedestimatesofthemodelpa- G = (V,E), consisting of the set of vertices V = {v },\ni"
  },
  {
    "id": 741,
    "text": "3\nand the set of edges E = {(v ,v )|v ,v ∈ V}. Each B. Inference method: Dynamic Message Passing\ni j i j\nnode v in the IC model can be in one of two states:\ni\nactive and inactive. Assuming node-wise independent As discussed in the Introduction, presence of partial\ninitialconditionforthestatesofthenodesattimet=0, information warrants learning methods that feature in-\nwhich can be either deterministic or stochastic, further ference algorithms for predicting the model dynamics as\ndynamics is subject to a single stochastic rule. If node a subroutine. One of the key observables that quantify\ni gets activated at any time t, it has only one chance to thespreadisgivenbytheinfluencefunction: thenumber\nactivateanyofitsinactiveneighborsj attimet+1. This ofexpectednumberofactivatednodesatacertaintimet\nhappens independently for each of the inactive neighbor foragiveninitialcondition. Forafixedinitialconditions"
  },
  {
    "id": 742,
    "text": "happens independently for each of the inactive neighbor foragiveninitialcondition. Forafixedinitialconditions\nwith edge dependent probability α ij : in cascade c, the influence function is given by the sum\nof marginal probabilities pc(t) of activation of each node\ni\nA(i)+I(j)− α −i→j A(i)+A(j). (1) i∈V atattimet[20]. Predictionofinfluencefunctionor\nmarginal probabilities is known to be #P-hard [45, 46],\nandhenceapproximatemethodsneedtobeused. Aclas-\nRegardless of activating any of its neighbors, node i re-\nsical way of estimating the influence function consists in\nmains active forever, but it cannot activate any other\nusing Monte-Carlo simulations. However, this approach\nnode in the future beyond t + 1. In what follows, we\ntypically requires a large sampling factor to provide a\nassume that G is undirected and there are no self-loops\nreliable estimate [32, 47–51]. Aiming at an accelerated\nor multi-links (although this assumption can be relaxed"
  },
  {
    "id": 743,
    "text": "assume that G is undirected and there are no self-loops\nreliable estimate [32, 47–51]. Aiming at an accelerated\nor multi-links (although this assumption can be relaxed\napproachthatsavesthissamplingfactor,weusetheinfer-\nin general). The dynamics continues until a predefined\nencemethodknownasDynamicMessagePassing[9–28].\nnumber of steps T. A single realisation of this process is\nForICmodel[21],DMPestimatesmarginalprobabilities\ncalled a cascade. Since each node can be activated only\nofactivationinalineartimeinbothsystemsizeanddu-\nonce,everycascadecisfullydescribedbyasetofactiva-\nration of the dynamics, and has the properties of being\ntion times {τc} . Additionally, if node i does not get\ni i∈V exact on graphs without loops and asymptotically exact\nactivated before or at the specified time T in cascade c,\non random graphs, providing an upper-bound of the in-\nwe assign τc = ∗ as its activation time. It means that\ni fluence function on general networks."
  },
  {
    "id": 744,
    "text": "on random graphs, providing an upper-bound of the in-\nwe assign τc = ∗ as its activation time. It means that\ni fluence function on general networks.\n∗ summarizes all future activity of a given node. More\nFor the IC model, the equations take the following\ndetails about this notation is given in the Appendix A.\nform:\nIn all our experiments, we use a single seed as an initial\ncondition (this setting corresponds to the most popular pc(t)=1− (cid:0) 1−p¯c(cid:1) (cid:89)(cid:16) 1−α ·pc (t−1) (cid:17) , (2)\ni i ki k→i\nscenario, but can be relaxed as well). This means that\nk∈∂i\nthere is only one node activated at the beginning of the\nspreadingprocess,althoughitmaybeadifferentnodefor\neach specific cascade. The number of observed cascades pc (t)=1− (cid:0) 1−p¯c(cid:1) (cid:89) (cid:16) 1−α ·pc (t−1) (cid:17) , (3)\nis denoted by M. j→i j kj k→j\nk∈∂j\\i\nThe problem that we consider in this work is as fol-"
  },
  {
    "id": 745,
    "text": "is denoted by M. j→i j kj k→j\nk∈∂j\\i\nThe problem that we consider in this work is as fol-\nlows: givenasetofobservedactivationtimes{τ i c} i∈O,c∈C wherepc i (t)isthemarginalprobabilityofnodeibeingac-\nwhere O ⊂ V is the set of observed nodes and C is the tiveattimetunderinitialconditionsofcascadec,pc (t)\ni→j\nset of cascades, as well as using additional information is the same probability, but on an auxiliary graph where\nonthecascades(e.g.,priorknowledgethatτc arenoisy), node j was removed, ∂i denotes the set of neighbors of\ni\nlearn model parameters, i.e., estimate {α ij } (ij)∈V×V so nodeiinthegraphG,and∂j\\idenotesthesetofneigh-\nthat they are close to the parameters of the ground- bors of node j in the graph G except i. We also denote\ntruth model, denoted as {α∗} . In the case of the an initial condition for node i in cascade c as p¯c =pc(0).\nij (ij)∈E i i\nunknown network, thresholded values of {α ij } (ij)∈V×V Note that for known model parameters, the marginals"
  },
  {
    "id": 746,
    "text": "ij (ij)∈E i i\nunknown network, thresholded values of {α ij } (ij)∈V×V Note that for known model parameters, the marginals\naway from small values close to zero define the recov- given by DMP depend only on the initial condition p¯c.\ni\nered network structure E(cid:98) that can be compared to the Detailed properties of the above equations were studied\nground-truthsetofedgesE. Inwhatfollows,thefraction in [21]. In Appendix A, following the approach of [17],\nof hidden nodes |V\\O|/|O| is denoted as ξ. we provide an alternative derivation of DMP equations\nWe chose the IC model for simplicity reasons. On one for IC model that connects them and their properties to\nhand, it does capture basic properties of many known the classical belief propagation algorithm [29, 30].\nspreading processes, including a possibility of an early\ncascade termination. On the other, it simplifies the ana-\nlytical equations presented in the next subsection, mak- C. Learning Framework"
  },
  {
    "id": 747,
    "text": "spreading processes, including a possibility of an early\ncascade termination. On the other, it simplifies the ana-\nlytical equations presented in the next subsection, mak- C. Learning Framework\ning the proposed approach easier to follow and under-\nstand. Our approach can be can easily be generalized In this section, we explain the details of our method.\nto a broad class of more complex spreading models on We start by stating the approach introduced in [8] and\nnetworks for which DMP equations (explained next) are known as Scalable Learning of Independent Cascade Ef-\nknown. fective Representation (SLICER), which will be used as"
  },
  {
    "id": 748,
    "text": "4\na baseline method in all numerical experiments below. with constrains on marginal provided via DMP,\nSubsequently, we will show that the objective used in\nSLICER need to be modified to account for the prior in- L= O + C , (7)\n(cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125)\nformation on the uncertainty in the data, and enhanced objective constraints\nwith the known constraints on the model parameters –\nan approach referred to as SLICER+. where in the absence of any additional prior information\nLetusfocusonthecaseofperfectlyobservedinforma- on the parameters, C are given by DMP equations (2)-\ntionfromasubsetofvisiblenodesO[8]. SLICERisbased (3)reweightedbyLagrangemultipliersλs i (t)forallnodes\non the minimization of the Kullback–Leibler (KL) diver- i ∈ V and λs i→j (t) for all edges (ij) ∈ E of graph G for\ngence between the empirical marginal distributions com- each time t and for each cascade in the class s, see Ap-"
  },
  {
    "id": 749,
    "text": "gence between the empirical marginal distributions com- each time t and for each cascade in the class s, see Ap-\nputed from the data, and the marginals estimated from pendixBfordetails. Theiterativeequationsconstituting\nthe model using DMP, on the observed nodes. Marginal SLICERisobtainedbydifferentiatingtheLagrangian(7)\ndistributions depend on non-local model parameters, in- with respect to all variables: ps i (t) i∈V , ps i→j (t) (ij)∈V×V ,\ncludingthoseadjacenttohiddennodes,andthusallowing α ,λs(t) ,andλs (t) . Thisresults\nij(ij)∈V×V i i∈V i→j (ij)∈V×V\nfor the reconstruction of the entirety of model parame-\nin DMP equations (2)-(3) in the primal space; DMP-\nters under sufficient observations. Minimization of KL like equations on the Lagrange multipliers λs(t) , and\ndistance on marginals on the observed nodes in our case λs (t) inthedualspace; andupda i tee i q ∈ u V ations\nis equivalent to maximizing: i→j (ij)∈V×V\nfor the parameters α , see Appendix B.\nij(ij)∈V×V\n(cid:88)(cid:88)"
  },
  {
    "id": 750,
    "text": "is equivalent to maximizing: i→j (ij)∈V×V\nfor the parameters α , see Appendix B.\nij(ij)∈V×V\n(cid:88)(cid:88)\nO = logµc(τc), (4) Whenpriorinformationabouttheparametersisavail-\ni i\nable, it can be directly incorporated in the term C. As\nc∈Ci∈O\nan example, the constraint can be greatly simplified if\nwhere C is the set of available cascades, O is the set we know that for each (ij) ∈ E, α = α (the case of\nij\nof visible nodes and µc i (t) is the marginal probability of so-called simple graphs, representing a popular case in\nnodeibeingactivatedundercascadecpreciselyattimet. epidemiological models). As we show below in the sec-\nThese marginal probabilities can be calculated based on tionIIIC,thegradientoftheLagrangianwithrespectto\nthe marginal variables pc i (t) used in the DMP equations the model parameter α in this case reads for α̸=0:\nabove:\nT−1\nµc i (t)=pc i (t)·1 (t<T) −pc i (t−1)·1 (t>0) +1 (t=T) , (5) ∂ ∂ L α =− α 1 (cid:88)(cid:88) (cid:88) λs i→j (t)·ps i→j (t). (8)"
  },
  {
    "id": 751,
    "text": "above:\nT−1\nµc i (t)=pc i (t)·1 (t<T) −pc i (t−1)·1 (t>0) +1 (t=T) , (5) ∂ ∂ L α =− α 1 (cid:88)(cid:88) (cid:88) λs i→j (t)·ps i→j (t). (8)\nwhere 1 stands for the indicator function. s∈S t=0 (i,j)∈V×V\nThe cost function (4) has been first proposed in [7]\nThegradientinEq.(B6)canbeusedtolearnparameter\nshowing an asymptotic consistency of DMP-based re-\nαusinganiterativeprocedurewithα←−α+ε∂L,where\ncovery using this objective, but providing an inefficient ∂α\nεisalearningrate. Lagrangianformulationensuresthat\noptimization algorithm. The work [8] proposed an effi-\na single gradient descent step has the worst-case compu-\ncientalgorithmforminimizing(4),whichgaverisetothe\ntational complexity O(|E|T|S|), which means it is linear\nSLICER algorithm.\ninthesystemsize, cascadelengthandthenumberofini-\nFor a given set of parameters α , marginal probabili-\nij tial conditions.\nties depend only on the initial condition. Therefore, we\nIn a similar manner, under different scenarios of in-"
  },
  {
    "id": 752,
    "text": "For a given set of parameters α , marginal probabili-\nij tial conditions.\nties depend only on the initial condition. Therefore, we\nIn a similar manner, under different scenarios of in-\ncan re-write the objective in the following way:\ncomplete data considered in Figure 1, the loss function\nO = (cid:88)(cid:88)(cid:88) mτ i s logµs i (τ i s), (6) O for c m a a n ti b o e n a A pp o r n op t r h ia e t t e y ly pe m o o f d t ifi h e e d u t n o ce i r n t c a l i u n d t e y t i h n e t p h r e io d r at in a - ,\ns∈Si∈O τs\ni as we discuss in the next section below. Collectively,\nwhere S is a set of all the initial conditions occurring we refer to the algorithm based on modified objective O\nacrossallthecascadesC andmτ i s isthenumberoftimes andconstraintsC astoSLICER+. Thefullderivationof\nnodeigetsactivatedattimeτs underinitialconditions. SLICER+ for each of the challenges in Figure 1 is given\ni"
  },
  {
    "id": 753,
    "text": "nodeigetsactivatedattimeτs underinitialconditions. SLICER+ for each of the challenges in Figure 1 is given\ni\nThis equivalent reformulation allows one to reduce the in the Appendix B. Efficient implementation of the al-\ncomputing cost by evaluating DMP equations only |S| gorithm, taking advantage of the DMP-like equations on\ntimes instead of M = |C|. In our simulations, where s theevolutionoftheLagrangemultipliesinthedualspace\nis assumed to be a single node, it means that DMP will and assuring the linear complexity of the algorithm, is\nnot be run more than N times, regardless of the number available in the Appendix C.\nof available cascades. In the case of a stochastic initial\ncondition of all of the cascades, e.g., when each node is\nindependentlyinitiallyactivatedwithprobability 1, the III. RESULTS\nN\nsize of S is |S| = 1, making the whole computation far\nless costly. InthissectionwepresentdetailsontheSLICER+ap-"
  },
  {
    "id": 754,
    "text": "independentlyinitiallyactivatedwithprobability 1, the III. RESULTS\nN\nsize of S is |S| = 1, making the whole computation far\nless costly. InthissectionwepresentdetailsontheSLICER+ap-\nInordertomaximize(6)weuseaLagrangianformula- proach designed for dealing with different types of chal-\ntion, wheretheobjectivefunctionO (6)issupplemented lenges regarding uncertainty and partial observability of"
  },
  {
    "id": 755,
    "text": "5\n(cid:6)(cid:5)(cid:6)(cid:11)\n(cid:6)(cid:5)(cid:8)(cid:11)\n(cid:6)(cid:5)(cid:10)(cid:11)\n(cid:6)(cid:5)(cid:12)(cid:11)\n(cid:6)(cid:5)(cid:13)(cid:11)\n(cid:6)(cid:4)(cid:7) (cid:6)(cid:4)(cid:5)\n(cid:15)\n(cid:4)(cid:3)(cid:27)(cid:28)(cid:24)(cid:31)(cid:19)(cid:17)(cid:29)(cid:22)(cid:3)(cid:20)(cid:21)(cid:33)(cid:29)(cid:21)(cid:30)(cid:18)(cid:28)(cid:27)\n(a) (b)\n(cid:29)(cid:21)(cid:23) (cid:25)(cid:17)(cid:29)(cid:3)(cid:31)(cid:29)(cid:21)(cid:21) (cid:29)(cid:17)(cid:27)(cid:20)(cid:28)(cid:26)(cid:3)(cid:29)(cid:21)(cid:23) (cid:25)(cid:17)(cid:29) (cid:14)(cid:29)(cid:20)(cid:37)(cid:30)(cid:4)(cid:16)(cid:36)(cid:27)(cid:34)(cid:24) (cid:30)(cid:19)(cid:17)(cid:25)(cid:21)(cid:4)(cid:22)(cid:29)(cid:21)(cid:21) (cid:6)(cid:5)(cid:9)(cid:6)\n(cid:6)(cid:5)(cid:8)(cid:11)\nτ\ni\n(cid:5)(cid:4)(cid:9)\n(cid:6)(cid:5)(cid:8)(cid:6)\n𝛼\n𝑖𝑗\n(cid:5)(cid:4)(cid:8)\nτ\nj (cid:6)(cid:5)(cid:7)(cid:11)\n(cid:5)(cid:4)(cid:7)\n(cid:6)(cid:5)(cid:7)(cid:6) (cid:5)(cid:4)(cid:6)"
  },
  {
    "id": 756,
    "text": "τ\ni\n(cid:5)(cid:4)(cid:9)\n(cid:6)(cid:5)(cid:8)(cid:6)\n𝛼\n𝑖𝑗\n(cid:5)(cid:4)(cid:8)\nτ\nj (cid:6)(cid:5)(cid:7)(cid:11)\n(cid:5)(cid:4)(cid:7)\n(cid:6)(cid:5)(cid:7)(cid:6) (cid:5)(cid:4)(cid:6)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:6)(cid:11) (cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:9) (cid:6)(cid:5)(cid:10)\n(cid:11)\nobserved - fraction\n(cid:6)(cid:5)(cid:6)(cid:6)\nunobserved - fraction\n(cid:6)(cid:4)(cid:7) (cid:6)(cid:4)(cid:5) (cid:6)(cid:4)(cid:7) (cid:6)(cid:4)(cid:5) (cid:6)(cid:4)(cid:7) (cid:6)(cid:4)(cid:5)\n(cid:15) (cid:15) (cid:15)\n(cid:4)\n(cid:4)\n(cid:6)(cid:4)(cid:6)(cid:5)\n(cid:5)(cid:6)(cid:5)\n(cid:6)\n(cid:4)\n(cid:5)\n(100−ξ)%\nξ%\nFIG. 2: Heat maps of average difference between inferred α and real α∗ parameters in ℓ norm, as a function of\nij ij 1\nthe number of available cascades and the fraction of unobserved nodes. Each heat map represents a different\nnetwork type, but all of them have the same color scale. Each point is averaged over 5 different networks and 5"
  },
  {
    "id": 757,
    "text": "network type, but all of them have the same color scale. Each point is averaged over 5 different networks and 5\ndifferent sets of parameters α∗ (sampled from a uniform distribution in the range [0,1]). All networks contain\nij\nN =100 nodes and all but the tree have average degree ⟨k⟩=3. Unobserved nodes were picked at random. All\ncascades had length T =5. Note that ξ =33% corresponds to a random guess.\ndata presented in Figure 1. All subsections devoted to results of applying SLICER to all these cases are pre-\nparticular challenges are supported with simulations on sented in the form of heat maps in Fig. 2. Interestingly,\nsynthetic networks. For this purpose we choose four dif- the quality of reconstructed parameters decrease faster\nferent network models: 3-regular tree (RT), 3-regular for tree graphs than for loopy networks, because tree\nrandomgraph(RR),Erd¨os-R´enyigraphwithaveragede- graphs have significantly lower edge density than other"
  },
  {
    "id": 758,
    "text": "randomgraph(RR),Erd¨os-R´enyigraphwithaveragede- graphs have significantly lower edge density than other\ngreeequalto3(ER)[52]andascale-freegraphgenerated networks, making it not directly comparable. At the\nwithBarab´asi-Albertmodelwithaveragedegreeequalto same time, single unobserved node affects a tree struc-\n3(BA)[53]. Exceptforthetree,allmodelshavethesame ture in a more significant way – single node separates a\nnumber of edges, so that they are comparable. The tree tree into two disconnected sub-graphs. We observe an\nnetwork case is studied for the baseline benchmarking interesting behavior from a scale-free graph. Despite a\npurposes given that DMP is exact on trees. In the sec- steeperdeclineinthequalityofreconstructionforsmaller\ntion IIIF, we apply the method to real-world networks, fractionsoftheunobservedpartξ,forhighvaluesofξthe\nwhile assuming a simultaneous co-occurence of multiple reconstructionstillcorrelatewiththetruesolution. Ifwe"
  },
  {
    "id": 759,
    "text": "while assuming a simultaneous co-occurence of multiple reconstructionstillcorrelatewiththetruesolution. Ifwe\ntypes of incomplete data. look at 80−90% of unobserved nodes for both random\nregularandErd¨os-R´enyigraph,thereconstructedparam-\netersarebasicallyrandom. Dynamicsonscalefreegraph\nontheotherhand,stilldoescontaininformationthatone\nA. SLICER’s learning limits\nis able to exploit, even for unobserved fraction equal to\nξ = 90%. For lower values of the unobserved fraction ξ,\nSLICER was already shown in [8] to perform well in a\nin the interval between 30−70%, the best parameters’\nregimewheretheunobservedpartofthesystemissignif-\nreconstruction results are obtained for the Erd¨os-R´enyi\nicant. It was not shown, however, what are the limits of\ngraph. Finally, when ξ is below 30%, the best param-\nthe algorithmin terms ofthe size ofthe unobserved part\neters’ reconstruction is achieved for the regular random\nand how it depends on the sample size. Here we answer"
  },
  {
    "id": 760,
    "text": "the algorithmin terms ofthe size ofthe unobserved part\neters’ reconstruction is achieved for the regular random\nand how it depends on the sample size. Here we answer\nstructure. These results illustrate that the uncertainty\nthis question and additionally we show how these limits\nof the obtained reconstruction depends on the particular\ndepend on the network structure.\nnetwork structure.\nWetakeallfoursyntheticnetworkmodels,generateup\nuntil M = 106 cascades and vary the percentage of the\nunobserved part from 0% to 95% (with a 5% step). The"
  },
  {
    "id": 761,
    "text": "6\n(cid:6)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:8) (cid:6)(cid:5)(cid:9)\n(cid:11)\n(cid:14)(cid:13)(cid:12)(cid:15)(cid:17)(cid:19)(cid:18)(cid:3)(cid:14)(cid:20)(cid:17)(cid:19)(cid:13)(cid:3)(cid:13)(cid:16)(cid:17)\n(a) (b)\nobserved - fraction(100−ξ)%\nα\nij\nunobserved - fraction ξ%\nτ\ni\npotential “fake” edges\n(cid:4)(cid:5)(cid:4)(cid:6) (cid:4)(cid:5)(cid:7)(cid:9)(cid:6)\n(cid:4)(cid:5)(cid:8)(cid:4)(cid:6) (cid:4)(cid:5)(cid:9)(cid:4)(cid:6)\nτ true edges\nj\nFIG. 3: (a) The structure of the lattice expanded by additional fake (inactive) edges. The structure learning task is\nto find the true edges, which were used to produce the observed cascades. (b) Average ROC curve surface, as a\nfunction of the number of available cascades for square lattice with additional fake (inactive) edges in the case where"
  },
  {
    "id": 762,
    "text": "function of the number of available cascades for square lattice with additional fake (inactive) edges in the case where\na fraction ξ of nodes is unobserved. Each point is averaged over five different sets of parameters α∗ (sampled from a\nij\nuniform distribution in the range [0,1]). Network contains N =100 nodes. Unobserved nodes were picked at\nrandom. All cascades had length T =5.\n(cid:6)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:8) (cid:6)(cid:5)(cid:9)\n(cid:11)\n(cid:14)(cid:13)(cid:12)(cid:15)(cid:17)(cid:19)(cid:18)(cid:3)(cid:14)(cid:20)(cid:17)(cid:19)(cid:13)(cid:3)(cid:13)(cid:16)(cid:17)\n(cid:6)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:8) (cid:6)(cid:5)(cid:9)\n(cid:11)"
  },
  {
    "id": 763,
    "text": "(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:8) (cid:6)(cid:5)(cid:9)\n(cid:11)\n(cid:14)(cid:13)(cid:12)(cid:15)(cid:17)(cid:19)(cid:18)(cid:3)(cid:14)(cid:20)(cid:17)(cid:19)(cid:13)(cid:3)(cid:13)(cid:16)(cid:17)\n(cid:6)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:8) (cid:6)(cid:5)(cid:9)\n(cid:11)\n(cid:14)(cid:13)(cid:12)(cid:15)(cid:17)(cid:19)(cid:18)(cid:3)(cid:14)(cid:20)(cid:17)(cid:19)(cid:13)(cid:3)(cid:13)(cid:16)(cid:17)\n(cid:6)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:8) (cid:6)(cid:5)(cid:9)\n(cid:11)"
  },
  {
    "id": 764,
    "text": "(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:8) (cid:6)(cid:5)(cid:9)\n(cid:11)\n(cid:14)(cid:13)(cid:12)(cid:15)(cid:17)(cid:19)(cid:18)(cid:3)(cid:14)(cid:20)(cid:17)(cid:19)(cid:13)(cid:3)(cid:13)(cid:16)(cid:17)\n(a) (b) 3-regular tree (c) random 3-regular\nτ\ni\n(cid:4)(cid:5)(cid:4)(cid:6) (cid:4)(cid:5)(cid:7)(cid:9)(cid:6)\nα (cid:4)(cid:5)(cid:8)(cid:4)(cid:6) (cid:4)(cid:5)(cid:9)(cid:4)(cid:6)\nij\nτ\nj\n(d) Erdos-Renyi (e) scale-free\nobserved - fraction(100−ξ)%\nunobserved - fractionξ%\nFIG. 4: Structure learning starting with a super-set of edges. Average ROC curve surface, as a function of the\nnumber of available cascades for different network types in the case where a fraction ξ of nodes is unobserved and\nthere is a known set of potential edges, where the number of fake edges is equal to the true ones. Each point is"
  },
  {
    "id": 765,
    "text": "there is a known set of potential edges, where the number of fake edges is equal to the true ones. Each point is\naveraged over five different networks and five different sets of parameters α∗ (sampled from a uniform distribution\nij\nin the range [0,1]). All networks contain N =100 nodes and all but the tree have average degree ⟨k⟩=3.\nUnobserved nodes were picked at random. All cascades had length T =5.\nB. Structure learning gather. Sometimes it is possible to assess a limited\nnumberof potential connections, but detailedknowledge\nIn many real-world applications, like epidemic spread- aboutthegraphisrarelyavailable. Inpractice,structure\ning, data on contact network is extremely difficult to"
  },
  {
    "id": 766,
    "text": "7\nlearning is equivalent to learning spreading parameters, sideranadversarialscenarioofaregulartwo-dimensional\nwhere near-zero couplings signal an absence of an edge. square lattice containing a large number of short loops,\nUnder the structure recovery task, parameter learning which represents the ground-truth propagation network.\nneedstoberunonlargergraphreflectingthepriorknowl- Further,considerasuper-setofedgesbyaddingdiagonal\nedge on the network structure. The less knowledge one connections to the lattice, as shown in Fig. 3(a). Thus\nhave about the network, the larger the set of edges one expanded network containing 50% fake edges (and even\nneedstoconsider,upuntilN(N−1)/2potentialedgesin more short loops compared to the ground-truth square\nafully-connectedgraphwhennoknowledgeaboutmodel lattice)isthenusedasastartingnetworkinthestructure\nstructure is provided. When the dynamics of all nodes learning task. Ground-truth parameters α are drawn\nij"
  },
  {
    "id": 767,
    "text": "structure is provided. When the dynamics of all nodes learning task. Ground-truth parameters α are drawn\nij\nis available, edges can be recovered using maximum like- from a uniform distribution over [0,1] interval. Despite\nlihood approach and thresholding. We focus on a more this adversarial scenario, we are able to perfectly recon-\nchallenging scenario where the network structure is un- struct the correct network, see Fig. 3, even with 25% of\nknown or partially known under the presence of unob- nodes being unobserved.\nserved nodes. One of the main difficulty of this scenario In Fig. 4, we test structure recovery procedure on four\nconsists in an increased potential for the solution degen- different types of synthetic networks, where the size of\neracy. Indeed,alreadyinthecaseoffullstructuralknowl- the super-set of edges is two times larger than the num-\nedge, degeneracy may appear for specific arrangements ber of ground-truth edges in the network on which the"
  },
  {
    "id": 768,
    "text": "edge, degeneracy may appear for specific arrangements ber of ground-truth edges in the network on which the\nof unobserved nodes. The simplest example is when an data has been generated. These results show that an ac-\nunobserved node is a leaf – a node with a single connec- curate recovery of the network structure is possible even\ntion. In this case it is impossible to recover the outgo- in the presence of a large fraction of hidden nodes. In\ning spreading parameter corresponding to the leaf edge. Section IIIF, we further explore structure learning for\nThere are also more complex situations involving solu- real-world network instances. In Appendix D, we dis-\ntion degeneracy such as interconnected clusters of hid- cuss results for network structure recovery in the case\nden nodes, and increased number of unknown connec- of no available prior information on the topology, where\ntions makes these situations more likely. However, in- the super-set of edges corresponds to a fully-connected"
  },
  {
    "id": 769,
    "text": "tions makes these situations more likely. However, in- the super-set of edges corresponds to a fully-connected\nformation contained in the cascades makes it possible to graph.\ndiscover the structure of the diffusion network with high\naccuracy even on very loopy graphs, as we show below.\nC. Simple graphs\nConsider a problem of selecting the spreading graph\nfrom a set of known super-set of edges. Such a super-set\nof edges becomes a fully-connected graph in the worst In many applications, some knowledge about model\ncase of no prior information on the diffusion network. parameters is assumed, which makes it easier to esti-\nSLICER can then be used as the structure discovery al- mate them. Most often it is simply assumed that all\ngorithm as follows. A gradient descent on the parame- the spreading parameters are the same for all edges\nters {α ij } as a part of SLICER is run on a super-set of α ij =α∀ (i,j)∈E . Althoughsuchsettingsignificantlysim-"
  },
  {
    "id": 770,
    "text": "ters {α ij } as a part of SLICER is run on a super-set of α ij =α∀ (i,j)∈E . Althoughsuchsettingsignificantlysim-\nedges. If a parameter goes beyond a certain threshold plifiestheproblem,itisagoodcasestudyforunderstand-\nvalue (which we take as 10−8 in all experiments in this ing how the algorithm can take advantage of additional\nsection), it is declared as zero and the respective edge knowledge.\nis removed from the set of candidate edges, thus reduc- The constraint of equal parameters change the DMP\ning the network at the next steps of the gradient descent equations, which leads to a different form of the\nprocedure. Thisonlinepruningprocedureresultsinare- SLICER+ instantiation for this case. In this new set-\nducedoverallcomputationalcomplexityofthealgorithm, ting the constraints become a single parameter function:\ncompared to an alternative where SLICER is run on the\nL=O({τc})+ C(α) , (9)\nfullsuper-setofedgesandtheparametersarethresholded i"
  },
  {
    "id": 771,
    "text": "compared to an alternative where SLICER is run on the\nL=O({τc})+ C(α) , (9)\nfullsuper-setofedgesandtheparametersarethresholded i\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125)\nonlyintheendoftheprocedure. Inordertoevaluatethe objective constrains\neffectiveness of our method we use a receiver operating\nwhich requires recomputing the Lagrangian derivatives.\ncharacteristic (ROC) curve [54]. After assessing all po-\nFollowing the steps described in the Appendix B2 we\ntential edges, we build the ROC curve of true and false\narrive at:\npositives among them and then we compute the surface\nunder such curve. ROC curve surface equal to 1 rep- ∂L 1 (cid:88) T (cid:88) −1 (cid:88)\n=− λs (t)·ps (t). (10)\nresents a perfect reconstruction, while values oscillating ∂α α i→j i→j\naround 0.5 suggest that the edge set selection is not bet- s∈S t=0 (i,j)∈E′\nter than a random guess.\nIn the end, not only do we reduce the memory usage"
  },
  {
    "id": 772,
    "text": "around 0.5 suggest that the edge set selection is not bet- s∈S t=0 (i,j)∈E′\nter than a random guess.\nIn the end, not only do we reduce the memory usage\nSurprisingly, structure discovery with SLICER using of the algorithm, but most importantly, we reduce the\nthe procedure described above is successful even when amount of data needed to obtain desirable level of error\nthe underlying network has many short loops, which on parameters.\nstronglyaffectstheaccuracyoftheDMPapproximation, We apply the modified procedure to synthetic data\nas explained in section IIB. To illustrate this point, con- generated with different network types. As shown in"
  },
  {
    "id": 773,
    "text": "8\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:6)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:9) (cid:6)(cid:5)(cid:10)\n(cid:11)\n(cid:4)\n(cid:4)\n(cid:6)(cid:4)(cid:6)(cid:5)\n(cid:5)(cid:6)(cid:5)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:6)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:9) (cid:6)(cid:5)(cid:10)\n(cid:11)\n(cid:4)\n(cid:4)\n(cid:6)(cid:4)(cid:6)(cid:5)\n(cid:5)(cid:6)(cid:5)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:6)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:9) (cid:6)(cid:5)(cid:10)\n(cid:11)\n(cid:4)\n(cid:4)\n(cid:6)(cid:4)(cid:6)(cid:5)\n(cid:5)(cid:6)(cid:5)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:7)(cid:6)(cid:11)\n(cid:7)(cid:6)(cid:10)\n(cid:7)(cid:6)(cid:9)\n(cid:7)(cid:6)(cid:8)\n(cid:7)(cid:6)(cid:7)"
  },
  {
    "id": 774,
    "text": "(cid:4)\n(cid:6)(cid:4)(cid:6)(cid:5)\n(cid:5)(cid:6)(cid:5)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:7)(cid:6)(cid:11)\n(cid:7)(cid:6)(cid:10)\n(cid:7)(cid:6)(cid:9)\n(cid:7)(cid:6)(cid:8)\n(cid:7)(cid:6)(cid:7)\n(cid:8)(cid:7)(cid:9) (cid:8)(cid:7)(cid:11) (cid:8)(cid:7)(cid:12)\n(cid:17)\n(cid:5)\n(cid:5)\n(cid:6)(cid:4)(cid:7)(cid:6)\n(cid:5)(cid:7)(cid:6)\n(cid:6)\n(cid:4)\n(cid:5)\n3-regular tree random 3-regular\n(a) (b) (c)\n(cid:4)(cid:6)(cid:5)(cid:7)(cid:5)(cid:3)(cid:19)(cid:16)(cid:15)(cid:13)(cid:14)(cid:18)\n(cid:4)(cid:6)(cid:9)(cid:5)(cid:7)(cid:5)(cid:3)(cid:19)(cid:16)(cid:15)(cid:13)(cid:14)(cid:18)(cid:4)\n(cid:4)(cid:6)(cid:8)(cid:5)(cid:7)(cid:5)(cid:3)(cid:19)(cid:16)(cid:15)(cid:13)(cid:14)(cid:18)(cid:4)\n(cid:4)(cid:6)(cid:4)(cid:5)(cid:7)(cid:5)(cid:3)(cid:19)(cid:16)(cid:15)(cid:13)(cid:14)(cid:18)(cid:4)\nτ i\nα ij = α\nτ\nj\nErdos-Renyi scale-free\n(d) (e)\nobserved - fraction(100−ξ)%\nunobserved - fractionξ%"
  },
  {
    "id": 775,
    "text": "τ i\nα ij = α\nτ\nj\nErdos-Renyi scale-free\n(d) (e)\nobserved - fraction(100−ξ)%\nunobserved - fractionξ%\nFIG. 5: Learning of simple graphs with equal transmission probabilities on all edges. Average difference between\ninferred α and real α∗ parameters in ℓ norm, as a function of the number of available cascades for different\nij ij 1\nnetwork types in the case where a fraction ξ of nodes is unobserved. The dashed gray line denotes the benchmark\ncase where one is using standard SLICER without imposing the constraint of equal parameters on all edges. The\ntriangles corresponds to the case where SLICER includes the knowledge about equal parameters. Each point is\naveraged over 5 different networks with parameters ∀ α∗ =0.5. All networks contain N =100 nodes and all\n(i,j)∈E ij\nbut the tree have average degree equal to ⟨k⟩=3. Unobserved nodes were picked at random. All cascades had\nlength equal to T =5."
  },
  {
    "id": 776,
    "text": "(i,j)∈E ij\nbut the tree have average degree equal to ⟨k⟩=3. Unobserved nodes were picked at random. All cascades had\nlength equal to T =5.\nFig. 5 even with 90% unobserved nodes and regardless tended periods of time. Partial observability of the sys-\nof the network structure, a small number of cascades is tem, should be considered in both spatial and temporal\nneeded to obtain remarkable accuracy. Ten cascades are spaces. Hereweshowhowtomodifytheobjectivesothat\non average enough to get error below 0.1 and it does not itcorrectlyaccountsfortheunobservedtimeperiods,re-\ndepend strongly on the size of the unobserved part. As- sulting in a SLICER+ instantiation for the scenario of\nsuming different transmission probabilities for each edge missing observations in times.\ninthereconstruction,shownwithagraydashedlineasa\nbenchmark, yields results which are orders of magnitude\nworse. Moreover, as shown in Fig. 2, different trans-"
  },
  {
    "id": 777,
    "text": "inthereconstruction,shownwithagraydashedlineasa\nbenchmark, yields results which are orders of magnitude\nworse. Moreover, as shown in Fig. 2, different trans-\nmission probabilities combined with unobserved fraction Under this scenario, the state of the network is not\nabove 60% yields almost random outcome, even with observed at each time step, but only at a subset of ob-\nlarge number of cascades. servationtimes. Therefore,althoughwedonotgenerally\nobserve activation time of a node, we still know that ac-\ntivation happened somewhere in the last unobserved pe-\nriod preceding the first time we noticed a given node to\nD. Missing temporal observations\nbe active. As a result there are two cases: (i) node acti-\nvation is observed and we denote the activation time as\nNot being able to observe the whole system, as a re- τ ;(ii)nodeactivationhappensduringanunobservedin-\ni\nsult of a limited budget or other challenges related to tervaldenotedas[τˆ−∆τˆ,τˆ]. Observedsnapshotsofthe\ni i i"
  },
  {
    "id": 778,
    "text": "i\nsult of a limited budget or other challenges related to tervaldenotedas[τˆ−∆τˆ,τˆ]. Observedsnapshotsofthe\ni i i\ngatheringdata,isafairlimitationformanypracticalap- initial and of the final states of the spreading dynamics\nplications. Another realistic situation arises when, for onlyrepresentsaparticularexampleofsuchamissingin-\nsimilar reasons, one is not able to record the data for ex- formation in time. New objective will depend on known"
  },
  {
    "id": 779,
    "text": "9\n(cid:7)(cid:6)(cid:11)\n(cid:7)(cid:6)(cid:10)\n(cid:7)(cid:6)(cid:9)\n(cid:7)(cid:6)(cid:8)\n(cid:7)(cid:6)(cid:7)\n(cid:8)(cid:7)(cid:9) (cid:8)(cid:7)(cid:11) (cid:8)(cid:7)(cid:12)\n(cid:17)\n(cid:5)\n(cid:5)\n(cid:6)(cid:4)(cid:7)(cid:6)\n(cid:5)(cid:7)(cid:6)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:4)(cid:5)(cid:4)(cid:6)(cid:5)(cid:3)(cid:19)(cid:16)(cid:15)(cid:13)(cid:14)(cid:18)(cid:4)\n(cid:4)(cid:5)(cid:8)(cid:4)(cid:6)(cid:5)(cid:3)(cid:19)(cid:16)(cid:15)(cid:13)(cid:14)(cid:18)(cid:4)\n(cid:4)(cid:5)(cid:7)(cid:9)(cid:6)(cid:5)(cid:3)(cid:19)(cid:16)(cid:15)(cid:13)(cid:14)(cid:18)(cid:4)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:6)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:9) (cid:6)(cid:5)(cid:10)\n(cid:11)\n(cid:4)\n(cid:4)\n(cid:6)(cid:4)(cid:6)(cid:5)\n(cid:5)(cid:6)(cid:5)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:6)\n(cid:5)(cid:4)(cid:5)"
  },
  {
    "id": 780,
    "text": "(cid:4)\n(cid:6)(cid:4)(cid:6)(cid:5)\n(cid:5)(cid:6)(cid:5)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:6)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:9) (cid:6)(cid:5)(cid:10)\n(cid:11)\n(cid:4)\n(cid:4)\n(cid:6)(cid:4)(cid:6)(cid:5)\n(cid:5)(cid:6)(cid:5)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:6)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:9) (cid:6)(cid:5)(cid:10)\n(cid:11)\n(cid:4)\n(cid:4)\n(cid:6)(cid:4)(cid:6)(cid:5)\n(cid:5)(cid:6)(cid:5)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:7)(cid:5)(cid:6)\n(cid:6)(cid:5)(cid:13)\n(cid:6)(cid:5)(cid:11)\n(cid:6)(cid:5)(cid:9)\n(cid:6)(cid:5)(cid:8)\n(cid:6)(cid:5)(cid:6)\n(cid:6)(cid:5)(cid:6)(cid:6) (cid:6)(cid:5)(cid:8)(cid:10) (cid:6)(cid:5)(cid:10)(cid:6) (cid:6)(cid:5)(cid:12)(cid:10) (cid:7)(cid:5)(cid:6)(cid:6)\n(cid:4)\n(cid:5)(cid:6)\n(cid:4)\n(cid:4)(cid:6)(cid:5)\n(cid:6)(cid:4)(cid:5)"
  },
  {
    "id": 781,
    "text": "(cid:4)\n(cid:5)(cid:6)\n(cid:4)\n(cid:4)(cid:6)(cid:5)\n(cid:6)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:12)\n(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:8)\n(cid:19)(cid:17)(cid:16)(cid:14)(cid:15)(cid:18)(cid:4) (cid:5)(cid:4)(cid:7)\n(cid:19)(cid:17)(cid:16)(cid:14)(cid:15)(cid:18)\n(cid:5)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:5)(cid:5) (cid:5)(cid:4)(cid:7)(cid:9) (cid:5)(cid:4)(cid:9)(cid:5) (cid:5)(cid:4)(cid:11)(cid:9) (cid:6)(cid:4)(cid:5)(cid:5)\n(cid:4)\n(cid:5)(cid:6)\n(cid:4)\n(cid:4)(cid:6)(cid:5)\n(cid:6)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:12)\n(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:5)(cid:5) (cid:5)(cid:4)(cid:7)(cid:9) (cid:5)(cid:4)(cid:9)(cid:5) (cid:5)(cid:4)(cid:11)(cid:9) (cid:6)(cid:4)(cid:5)(cid:5)\n(cid:4)\n(cid:5)(cid:6)\n(cid:4)\n(cid:4)(cid:6)(cid:5)\n3-regular tree random 3-regular\n(a) (b) (c)\nτ i ∈ [ τ î −Δτ î,τ î]\nα\nij\nτ j ∈ [ τ ĵ −Δτ ĵ,τ ĵ ]\nErdos-Renyi scale-free\n(d) (e)\nobserved - fraction(100−ξ)%"
  },
  {
    "id": 782,
    "text": "(cid:4)\n(cid:4)(cid:6)(cid:5)\n3-regular tree random 3-regular\n(a) (b) (c)\nτ i ∈ [ τ î −Δτ î,τ î]\nα\nij\nτ j ∈ [ τ ĵ −Δτ ĵ,τ ĵ ]\nErdos-Renyi scale-free\n(d) (e)\nobserved - fraction(100−ξ)%\nunobserved - fractionξ%\nrandom 3-regular Erdos-Renyi scale-free\n(f) (g) (h)\nFIG. 6: (a) Reconstruction of parameters under missing temporal observations. (b-e) Average difference between\ninferred α and real α∗ parameters in ℓ norm, as a function of the number of available cascades for different\nij ij 1\nnetwork types. Only initial and final spreading states are observed and additionally, some nodes are not observed at\nall. The triangles corresponds to the case where SLICER includes the knowledge about unobserved periods of time.\nEach point is averaged over 5 different networks and 5 different sets of parameters α∗ (sampled from a uniform\nij\ndistribution in [0,1]). All networks contain N =100 nodes and all but the tree have average degree equal to ⟨k⟩=3."
  },
  {
    "id": 783,
    "text": "ij\ndistribution in [0,1]). All networks contain N =100 nodes and all but the tree have average degree equal to ⟨k⟩=3.\nUnobserved nodes were picked at random. All cascades had length equal to T =6. (f-h) Scatter plots of true\nparameters α∗ versus the estimated ones α . The results were obtained for different network structures with\nij ij\nN =100 nodes, M =106 cascades of length T =6 and ξtime =33.(3)% of unobserved times picked at random.\nExcept for the tree, all networks have average degree equal to ⟨k⟩=3.\nactivation times and activation intervals: missing temporal observations, presented in Appendix\nB3.\nL=O({τc},{τˆ,∆τˆ})+C({α }). (11)\ni i i ij\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) Results in Fig. 6(b-e) are presented in the case where\nobjective constrains onlyinitialandfinalstatesofthedynamicsareobserved,\nThis leads to a variant of the SLICER+ algorithm for similarlytothecaseconsideredin[41](wherethecaseof"
  },
  {
    "id": 784,
    "text": "10\n(cid:6)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:12)\n(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:5)(cid:5) (cid:5)(cid:4)(cid:7)(cid:9) (cid:5)(cid:4)(cid:9)(cid:5) (cid:5)(cid:4)(cid:11)(cid:9) (cid:6)(cid:4)(cid:5)(cid:5)\n(cid:4)\n(cid:5)(cid:6)\n(cid:4)\n(cid:4)(cid:6)(cid:5)\n(cid:6)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:12)\n(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:5)(cid:5) (cid:5)(cid:4)(cid:7)(cid:9) (cid:5)(cid:4)(cid:9)(cid:5) (cid:5)(cid:4)(cid:11)(cid:9) (cid:6)(cid:4)(cid:5)(cid:5)\n(cid:4)\n(cid:5)(cid:6)\n(cid:4)\n(cid:4)(cid:6)(cid:5)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:6)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:9) (cid:6)(cid:5)(cid:10)\n(cid:11)\n(cid:4)\n(cid:4)\n(cid:6)(cid:4)(cid:6)(cid:5)\n(cid:5)(cid:6)(cid:5)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)"
  },
  {
    "id": 785,
    "text": "(cid:11)\n(cid:4)\n(cid:4)\n(cid:6)(cid:4)(cid:6)(cid:5)\n(cid:5)(cid:6)(cid:5)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:6)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:9) (cid:6)(cid:5)(cid:10)\n(cid:11)\n(cid:4)\n(cid:4)\n(cid:6)(cid:4)(cid:6)(cid:5)\n(cid:5)(cid:6)(cid:5)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:6)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:9) (cid:6)(cid:5)(cid:10)\n(cid:11)\n(cid:4)\n(cid:4)\n(cid:6)(cid:4)(cid:6)(cid:5)\n(cid:5)(cid:6)(cid:5)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:7)(cid:6)(cid:11)\n(cid:7)(cid:6)(cid:10)\n(cid:7)(cid:6)(cid:9)\n(cid:7)(cid:6)(cid:8)\n(cid:7)(cid:6)(cid:7)\n(cid:8)(cid:7)(cid:9) (cid:8)(cid:7)(cid:11) (cid:8)(cid:7)(cid:12)\n(cid:17)\n(cid:5)\n(cid:5)\n(cid:6)(cid:4)(cid:7)(cid:6)\n(cid:5)(cid:7)(cid:6)\n(cid:6)\n(cid:4)\n(cid:5)"
  },
  {
    "id": 786,
    "text": "(cid:7)(cid:6)(cid:7)\n(cid:8)(cid:7)(cid:9) (cid:8)(cid:7)(cid:11) (cid:8)(cid:7)(cid:12)\n(cid:17)\n(cid:5)\n(cid:5)\n(cid:6)(cid:4)(cid:7)(cid:6)\n(cid:5)(cid:7)(cid:6)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:4)(cid:5)(cid:4)(cid:6)(cid:5)(cid:3)(cid:19)(cid:16)(cid:15)(cid:13)(cid:14)(cid:18)\n(cid:4)(cid:5)(cid:4)(cid:6)(cid:5)(cid:3)(cid:19)(cid:16)(cid:15)(cid:13)(cid:14)(cid:18)(cid:4)\n(cid:4)(cid:5)(cid:8)(cid:4)(cid:6)(cid:5)(cid:3)(cid:19)(cid:16)(cid:15)(cid:13)(cid:14)(cid:18)(cid:4)\n(cid:4)(cid:5)(cid:7)(cid:9)(cid:6)(cid:5)(cid:3)(cid:19)(cid:16)(cid:15)(cid:13)(cid:14)(cid:18)(cid:4)\n(cid:7)(cid:5)(cid:6)\n(cid:6)(cid:5)(cid:13)\n(cid:6)(cid:5)(cid:11)\n(cid:6)(cid:5)(cid:9)\n(cid:6)(cid:5)(cid:8)\n(cid:6)(cid:5)(cid:6)\n(cid:6)(cid:5)(cid:6)(cid:6) (cid:6)(cid:5)(cid:8)(cid:10) (cid:6)(cid:5)(cid:10)(cid:6) (cid:6)(cid:5)(cid:12)(cid:10) (cid:7)(cid:5)(cid:6)(cid:6)\n(cid:4)\n(cid:5)(cid:6)\n(cid:4)\n(cid:4)(cid:6)(cid:5)\n3-regular tree random 3-regular\n(a) (b) (c)\nτ\ni\n±Δτ\ni\nα\nij\nτ\nj\n±Δτ\nj"
  },
  {
    "id": 787,
    "text": "(cid:4)\n(cid:5)(cid:6)\n(cid:4)\n(cid:4)(cid:6)(cid:5)\n3-regular tree random 3-regular\n(a) (b) (c)\nτ\ni\n±Δτ\ni\nα\nij\nτ\nj\n±Δτ\nj\nErdos-Renyi scale-free\n(d) (e)\nobserved - fraction(100−ξ)%\nunobserved - fractionξ%\nrandom 3-regular Erdos-Renyi scale-free\n(f) (g) (h)\n(cid:19)(cid:17)(cid:16)(cid:14)(cid:15)(cid:18)(cid:4)\n(cid:19)(cid:17)(cid:16)(cid:14)(cid:15)(cid:18)\nFIG. 7: (a) Reconstruction of parameters under noisy observations. (b-e) Average difference between inferred α\nij\nand real α∗ parameters in ℓ norm, as a function of the number of available cascades for different network types in\nij 1\nthe case where a fraction ξ of nodes is unobserved. Additionally, observed activation times {τc} are subject to\ni i∈O\nnoise described by Eq. 14. The dashed gray line denotes the benchmark case obtained with SLICER. The triangles\ncorresponds to the case where SLICER includes the knowledge about the noise. Each point is averaged over 5"
  },
  {
    "id": 788,
    "text": "corresponds to the case where SLICER includes the knowledge about the noise. Each point is averaged over 5\ndifferent networks and 5 different sets of parameters α∗ (sampled from a uniform distribution in [0,1]). All networks\nij\ncontain N =100 nodes and all but the tree have average degree equal to ⟨k⟩=3. Unobserved nodes were picked at\nrandom. All cascades had length equal to T =5. (f-h) Scatter plots of true parameters α∗ versus the estimated\nij\nones α . The results were obtained for different network structures with N =100 nodes, M =106 cascades of\nij\nlength T =5 and ξ =10% of unobserved nodes. Additionally, observed activation times {τc} are subject to noise\ni i∈O\ndescribed by Eq. 14. Except for the tree, all networks have average degree equal to ⟨k⟩=3.\nfull spatial observations ξ = 0 has been treated). Given observed nodes, a realistic setting, which, to our best\nthe effectiveness of the method in the setting of unob- knowledge, was not addressed in the literature before."
  },
  {
    "id": 789,
    "text": "the effectiveness of the method in the setting of unob- knowledge, was not addressed in the literature before.\nserved time-periods, we explore an even more complex Results presented in Fig. 6(b-e) are of similar quality as\ncase, where we combine both unobserved times and un- the ones obtained in [8] when there are no missing ob-"
  },
  {
    "id": 790,
    "text": "11\nservations in time, despite lack of knowledge on the ex- K =1:\nact activation times, which illustrates the robustness of \n0.6, if k =0,\nSLICER+ with respect to missing observations in time. \nπ = 0.2, if |k|=1, (14)\nUnobserved time periods can be naively treated with k\n0.0, otherwise.\nSLICER, by simply assuming that if a given node was\nactivatedinsuchaperiod,itisregardedasanunobserved\nFor simplicity, in the numerical experiments in this sec-\nnode. This, however, does not take advantage of all the\ntion,weassumethatthenoisedistributionisknown,but\navailable information. As it is shown in Fig. 6(f-h), this\nthe probabilities π characterizing the noise distribution\nleads to a one-sided bias, which quantitatively depends k\ncan be treated as parameters and learned from the data.\non the network structure and the fraction of unobserved\nNotice that a naive application of SLICER to the case\ntimestamps. When the unobserved periods are treated"
  },
  {
    "id": 791,
    "text": "on the network structure and the fraction of unobserved\nNotice that a naive application of SLICER to the case\ntimestamps. When the unobserved periods are treated\nofnoisyobservationscouldleadtoinfiniteobjectivefunc-\ncorrectly, as done in SLICER+, the bias disappears.\ntion, since the realization of noisy timestamps could be\nimpossible,showinginconsistencywiththedynamicrules\nof the IC model. This could happen, for instance, if a\nE. Noisy timestamps node is observed to be activated before any of its neigh-\nbor is activated. Application of SLICER is still possible\nAnother realistic scenario in data collection relates to byneglectingtheseconflictingcasesleadingtoaninfinite\nthefactthatapartfromnotbeingabletomonitorevery- valueoftheobjectivefunction. Weusesuchanapproach\nthing, the collected data is not necessary perfect. This as a benchmark to SLICER+ that accounts for the pos-\nmay be a result of omission, deliberate actions or simply sibility of noisy observations. Fig. 7(b-e) shows that the"
  },
  {
    "id": 792,
    "text": "may be a result of omission, deliberate actions or simply sibility of noisy observations. Fig. 7(b-e) shows that the\nimperfect gathering procedure. If, for example, a popu- use of SLICER quickly leads to a significant error gap,\nlation is tested for a certain virus, the testing dates are while consideration of noise implemented in SLICER+\nmost likely not the dates when the infection occurred. brings the algorithm’s performance to a level similar to\nOn top of that, the lab results may be delayed, dates a noiseless situation. Looking more closely at the partic-\nmay be mistaken and tests can produce both false pos- ular results at Fig. 7(f-h), we see that disregarding the\nitives and false negatives. All of this builds up into an presence of noise gives results that are correlated with\nunpredictable noise, which cannot be removed in a de- the true solution, but there is a systematic bias, which"
  },
  {
    "id": 793,
    "text": "unpredictable noise, which cannot be removed in a de- the true solution, but there is a systematic bias, which\nterministic matter. It has to be accounted for in the dependsonthenetworkstructure. Thesolutiongivenby\nmodelling process. the noise-corrected algorithm, on the other hand, aligns\nIn our case of learning spreading parameters, the data perfectly with the ground-truth.\nis a set of activation times. In this context, we model\nnoisy observations as deviations around the correct acti-\nvation times. We assume that what is observed is a sum F. Study on real-world networks\nof the original activation time and a noise coming from\na certain discrete distribution. In general it is described Previously we used different network structures to\nas follows: show how they affect the results of the learning proce-\ndure for different scenarios. Real-world networks typi-\nP(τ =τ∗+k)=π k ∀|k|≤K, (12) cally combine several characteristics of different random"
  },
  {
    "id": 794,
    "text": "dure for different scenarios. Real-world networks typi-\nP(τ =τ∗+k)=π k ∀|k|≤K, (12) cally combine several characteristics of different random\nmodels [55]. In order to truly test the effectiveness of\nwhereτ istheobservedactivationtime,τ∗ istherealac- our learning framework we use two real-world social net-\ntivationtime, andK isaconstantrelatedtothesupport works together with multiple types of noisy and incom-\nof the noise distribution. The presence of noise in times- plete spreading data. The first network is the Zachary\ntamps has no effect on the DMP equations and the con- Karate Club [56] with N = 34 nodes, |E| = 78 edges\nstraints, but will affect the objective function, resulting and multiple short loops, small enough to serve as an\ninincorporationoftheprioronthenoiseintheSLICER+ explicit illustration of the quaility of structure learning\nformulation. The objective function will now depend on under partial and noisy observations. The second net-"
  },
  {
    "id": 795,
    "text": "formulation. The objective function will now depend on under partial and noisy observations. The second net-\nboth activation times and the noise distribution: work is a Facebook snapshot from [57] and previously\nanalysed in [58], with N = 2888 nodes and |E| = 2981\nL=O({τ i c},{π k })+C({α ij }). (13) edges.\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) InFigure8,wepresentresultsonlearningofthestruc-\nobjective constrains\nture of the network from a combination of missing infor-\nExact form of the objective for noisy data and detailed mation: partially reporting nodes and noisy activation\nderivation of SLICER+ for this case is given in the Ap- times. Weassumethatnopriorinformationonthetopol-\npendix B4. ogyisknowntothealgorithm. Duetomanyshortloops,\nAs in the previous sections, we test the instantiation learning spreading parameters in the Zachary Club net-"
  },
  {
    "id": 796,
    "text": "pendix B4. ogyisknowntothealgorithm. Duetomanyshortloops,\nAs in the previous sections, we test the instantiation learning spreading parameters in the Zachary Club net-\nof SLICER+ for the case of noisy observations with dif- work using SLICER+ is not an easy task. As shown in\nferent network structures. In the numerical experiments Fig.8(c),10%ofunobservednodesleadstoalowerrorin\nwe use noise described by the following distribution for parameterrecoveryof5%erroronaverage. With25%of"
  },
  {
    "id": 797,
    "text": "12\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:6)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:9) (cid:6)(cid:5)(cid:10)\n(cid:11)\n(cid:5)\n(cid:5)\n(cid:6)(cid:4)(cid:7)(cid:6)\n(cid:5)(cid:7)(cid:6)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:6)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:8) (cid:6)(cid:5)(cid:9)\n(cid:11)\n(cid:4)(cid:5)(cid:4)(cid:6)\n(cid:4)(cid:5)(cid:8)(cid:4)(cid:6)\n(cid:4)(cid:5)(cid:7)(cid:9)(cid:6)\n(cid:14)(cid:13)(cid:12)(cid:15)(cid:17)(cid:19)(cid:18)(cid:3)(cid:14)(cid:20)(cid:17)(cid:19)(cid:13)(cid:3)(cid:13)(cid:16)(cid:17)\n(a) (b) structural learning\nα\nij\n(cid:4)(cid:5)(cid:4)(cid:6) (cid:4)(cid:5)(cid:7)(cid:9)(cid:6)\n(cid:4)(cid:5)(cid:8)(cid:4)(cid:6) (cid:4)(cid:5)(cid:9)(cid:4)(cid:6)\nτ estimating parameters\ni\n(c)\nτ\nj\nfalse negative\nobserved - fraction(100−ξ)% false positive"
  },
  {
    "id": 798,
    "text": "(cid:4)(cid:5)(cid:8)(cid:4)(cid:6) (cid:4)(cid:5)(cid:9)(cid:4)(cid:6)\nτ estimating parameters\ni\n(c)\nτ\nj\nfalse negative\nobserved - fraction(100−ξ)% false positive\nunobserved - fractionξ% true positive\nFIG. 8: (a) Structure reconstruction using the Zachary Karate Club network. Solid black connections represent the\ncorrectly reconstructed edges. Dotted black connections are the ground-truth edges that were not reconstructed.\nDashed gray lines are the ones reconstructed by the algorithm, but absent in the ground-truth edge set. We fix all of\nspreading parameters to α =0.5. Network was learned based on M =1000 cascades of length T =5 with ξ =10%\nij\nof unobserved nodes. (b) Average ROC curve surface, as a function of the number of available cascades for the\nZachary Karate Club network in the case where a fraction ξ of nodes is unobserved. (c) Average difference between\ninferred α and real α∗ parameters in ℓ norm, as a function of the number of available cascades for the Zachary\nij ij 1"
  },
  {
    "id": 799,
    "text": "inferred α and real α∗ parameters in ℓ norm, as a function of the number of available cascades for the Zachary\nij ij 1\nKarate Club network in the case where a fraction ξ of nodes is unobserved. In both cases (b) and (c), each point is\naveraged over 5 different sets of parameters α∗ (sampled from a uniform distribution in [0,1]). Network contains\nij\nN =34 nodes and |E|=78 edges. Unobserved nodes were picked at random. All cascades had length T =5. We\nassume no knowledge on the topology, hence the initial set of edges is the complete graph. Additionally, observed\nactivation times are subject to known noise described in Eq. 14.\nunobservednodes,thiserrorgrowsabove10%. However, nodes. Specifically, the problem is more pronounced for\nthe structure recovery is performed with a greater accu- the two connected unobserved nodes, whereas the result\nracy,despitenoassumedknowledgeontheedgeset. Ob- is much better for an isolated unobserved node."
  },
  {
    "id": 800,
    "text": "racy,despitenoassumedknowledgeontheedgeset. Ob- is much better for an isolated unobserved node.\nviously, no algorithm can produce perfect reconstruction\nWe now test structure recovery with SLICER+ on the\nforhighfractionsofnon-reportingnodesandunobserved\nlarger Facebook network with the following setting: the\nedges due to a degeneracy in the space of solutions.\nobservedtimestampsarenoisy,andwestartwithasuper-\nAn explicit structure learning example produced for setof5962edges,twiceaslargeastheground-truthsetof\nξ = 10% unobserved nodes and M = 1000 cascades of 2981 edges. The Facebook snapshot network is a sparse\nlength T = 5 is shown in Fig. 8(a). To eliminate pos- graph with multiple hubs. Hubs tend to increase the\nsibility of bias due to a choice of ground-truth α , we number of triangles, but the clustering of this network is\nij\nuse the test case with all α =0.5. The algorithm how- onaveragelowerthanfortheZacharyClub. Thisiscon-\nij"
  },
  {
    "id": 801,
    "text": "ij\nuse the test case with all α =0.5. The algorithm how- onaveragelowerthanfortheZacharyClub. Thisiscon-\nij\never does not have access to this information during the sistent with our observation that the reconstruction er-\nlearning procedure. Although the parameter reconstruc- ror in spreading parameters is significantly smaller, even\ntion quality could be slightly improved by increasing the when 25% of nodes are unobserved, as shown in Fig. 9.\nnumber of cascades, one can already see that the recon- Furthermore,inclusionoftheknowledgeonthesuper-set\nstruction on the observed part is perfect. Incorrect clas- of 5962 potential edges allows to nearly perfectly recon-\nsification appears only in the vicinity of the unobserved struct the network structure in the partial observation"
  },
  {
    "id": 802,
    "text": "13\nregime, assuming enough data is available. Our framework can be further developed in several\nfuture directions. In the setting of noisy observations,\nwe assumed for simplicity that the noise distribution\nIV. CONCLUSIONS is known. However, the noise distribution probabilities\nπ can be treated as additional parameters and learned\nk\nfrom the data. For the structure learning task, we\nLiterature on learning of spreading models from data\nconsidered reconstruction from a super-set of edges. It\nis rich, but not much attention so far was given to the\nwould be interesting to test the impact of other graph\nproblem of learning from incomplete and uncertain in-\ntopological prior information on learning, such as details\nformation. Our work proposes a flexible method that\non density or degree distribution. Such a setting would\naddresses this problem. SLICER+ deals with multiple\nbe relevant in the setting where surveillance and data\nsettings related to incomplete and noisy data, that in-"
  },
  {
    "id": 803,
    "text": "addresses this problem. SLICER+ deals with multiple\nbe relevant in the setting where surveillance and data\nsettings related to incomplete and noisy data, that in-\ngathering procedures can be controlled. Under the\nclude partial observations in both spatial and temporal\nscenario where the observations are too sparse and the\ndimensions, noisy timestamps, unknown network struc-\ndegeneracy in the reconstruction is unavoidable and in\nture, and combinations of these settings.\napplications focused on downstream prediction tasks, it\nFor unobserved times, we show that a proper formula-\ncould be useful to study the properties of the learned\ntion of the algorithm is able to learn model parameters\nmodels as effective representations [8]. Finally, in the\nfrom final and initial states only, assuming that enough\nfuture, it would be useful to extend our framework to\ndata available. This holds even in the presence of un-\nother dynamic models, including models with reversible"
  },
  {
    "id": 804,
    "text": "future, it would be useful to extend our framework to\ndata available. This holds even in the presence of un-\nother dynamic models, including models with reversible\nobserved nodes. We further show that uncertainty in\ndynamics.\nprovided activation times leads to significant bias, when\nnot properly accounted for.\nCODE AND SUPPLEMENTARY MATERIAL\nAlthoughthestructurelearningtaskunderunobserved\nnodes may generate degenerate solutions, we empirically\nshowed that additional knowledge can significantly in- Full implementation of all algorithms studied in this\ncrease the quality of reconstruction using several repre- work is available at [59].\nsentativesyntheticnetworks,aswellastwopopularreal-\nworld networks. For real-world instances specifically, we\nused a setting where different types of incomplete and ACKNOWLEDGMENTS\nuncertain data settings are combined.\nWe found that our results are quite robust regardless Authors acknowledge support from the Laboratory"
  },
  {
    "id": 805,
    "text": "uncertain data settings are combined.\nWe found that our results are quite robust regardless Authors acknowledge support from the Laboratory\nof the synthetic network type. As an illustration, we Directed Research and Development program of Los\nconsidered quality of reconstruction as a function of the Alamos National Laboratory under project numbers\nsizeofthesetofunobservednodesonavarietyofrandom 20240245ER and 20240198ER, and from U.S. DOE/SC\ngraph families. Advanced Scientific Computing Research Program.\nAppendix A: Derivation of the DMP equations for the IC model\nThe DMP equations presented in the paper are quite intuitive. In case of simple models, such as IC model, they\ncould be derived by identifying the correct dynamical variables to use in the equations, as it was done here. They\nare also intrinsically connected to a more general framework of Belief Propagation (BP) equations, which are well"
  },
  {
    "id": 806,
    "text": "are also intrinsically connected to a more general framework of Belief Propagation (BP) equations, which are well\ndescribed in detail in the literature [29, 30]. In the dynamic setting, the BP equations on time trajectories have been\nstudied by Kanoria and Montanari in [11]. Here, for completeness, we show how DMP equations for IC model can be\nobtained directly from BP formulation.\nDenote ⃗σ = (σ0,...,σT) the time trajectory of node i variable, and w (cid:0) σt+1|σt, {σt} (cid:1) the local probability\ni i i i i i j j∈∂i\nof node i transitioning to state σt+1 at time t+1, given its previous state and the states of its neighborhood at\ni\ntime t, and P({σ0} ) is the probability of the initial state. Additionally, let us introduce new type of message\ni i∈V\nµi→j(⃗σ ||⃗σ ),whichrepresentstheprobabilityofnodeivariablehavingthetrajectory⃗σ onanauxiliarygraph,where\ni j i\nthe trajectory of node j is fixed to⃗σ . Our starting point is the dynamic belief propagation (DBP) equation on time\nj"
  },
  {
    "id": 807,
    "text": "i j i\nthe trajectory of node j is fixed to⃗σ . Our starting point is the dynamic belief propagation (DBP) equation on time\nj\ntrajectories [11]:\nt−1\nµi→j(⃗σ ||⃗σ )= (cid:88) P({σ0} ) (cid:89) w (cid:0) στ+1|στ, {στ} (cid:1)(cid:89) µk→i(⃗σ ||⃗σ ), (A1)\nt i j i i∈V i i i k k∈∂i t−1 k i\n{σ\nk\n0,...σ\nk\nt−1}k∈∂i\\j τ=0 k∈∂i\\j\nwhere µi→j(⃗σ ||⃗σ ) is a message probability for trajectories up until time t.\nt i j\nA direct use of the above equations is still impractical due to their exponential complexity over cascade length T.\nThis is where the unidirectional character of the dynamics come in handy and leads to simplifications. In the case"
  },
  {
    "id": 808,
    "text": "14\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:6)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:9) (cid:6)(cid:5)(cid:10)\n(cid:11)\n(cid:4)\n(cid:4)\n(cid:6)(cid:4)(cid:6)(cid:5)\n(cid:5)(cid:6)(cid:5)\n(cid:6)\n(cid:4)\n(cid:5)\n(cid:6)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:8) (cid:6)(cid:5)(cid:9)\n(cid:11)\n(cid:14)(cid:13)(cid:12)(cid:15)(cid:17)(cid:19)(cid:18)(cid:3)(cid:14)(cid:20)(cid:17)(cid:19)(cid:13)(cid:3)(cid:13)(cid:16)(cid:17)\n(a) (b) structural learning\n(cid:4)(cid:5)(cid:4)(cid:6)\n(cid:4)(cid:5)(cid:8)(cid:4)(cid:6)\n(cid:4)(cid:5)(cid:7)(cid:9)(cid:6)\nestimating parameters\n(c)\nobserved - fraction (100−ξ)%\nunobserved - fraction ξ%\nFIG. 9: (a) Structure of the FB network. (b) Average ROC curve surface, as a function of the number of available"
  },
  {
    "id": 809,
    "text": "estimating parameters\n(c)\nobserved - fraction (100−ξ)%\nunobserved - fraction ξ%\nFIG. 9: (a) Structure of the FB network. (b) Average ROC curve surface, as a function of the number of available\ncascades for the Facebook network in the case where a fraction ξ of nodes is unobserved. (c) Average difference\nbetween inferred α and real α∗ parameters in ℓ norm, as a function of the number of available cascades for the\nij ij 1\nFacebook network in the case where a fraction ξ of nodes is unobserved. In both cases, each point is averaged over 5\ndifferent sets of parameters α∗ (sampled from a uniform distribution in [0,1]). Network contains N =2888 nodes\nij\nand |E|=2981 edges. Unobserved nodes were picked at random. All cascades had length T =5. We assume that a\nsuper-set of 5962 edges is known, out of which we would like to infer the ground-truth set of 2981 edges.\nAdditionally, observed activation times are subject to known noise described in Eq. 14."
  },
  {
    "id": 810,
    "text": "super-set of 5962 edges is known, out of which we would like to infer the ground-truth set of 2981 edges.\nAdditionally, observed activation times are subject to known noise described in Eq. 14.\nof the IC model, vector ⃗σ can be fully described by a single number τ , which represents the activation time. This\ni i\nallows us to rewrite the DBP equations as follows:\nµi→j(τ ||τ )= (cid:88) W¯ (τ ;{τ } ) (cid:89) µk→i(τ ||τ ), (A2)\nT+1 i j i i k k∈∂i T k i\n{τk}k∈∂i\\j k∈∂i\\j\nµi (τ )= (cid:88) W¯ (τ ;{τ } ) (cid:89) µk→i(τ ||τ ), (A3)\nT+1 i i i k k∈∂i T k i\n{τk}k∈∂i k∈∂i\nwhere µi→j(τ ||τ ) and µi (τ ) correspond the same messages and marginals as before, but parameterized with acti-\nT i j T i\nvation times τ . Additionally, we denote W¯ (τ ;{τ } ) = P({σ0} )·W (τ ;{τ } ) for simplicity. Before we\ni i i k k∈∂i i i∈V i i k k∈∂i\nmove further we want to highlight some observations, which will be used later. First, all the information required to"
  },
  {
    "id": 811,
    "text": "i i i k k∈∂i i i∈V i i k k∈∂i\nmove further we want to highlight some observations, which will be used later. First, all the information required to\ncompute the probability of activation at certain time τ is available at that time.\ni\nµi (τ )=µi (τ ) ∀ , (A4)\nT i τi i T>τi\nµi→j(τ ||τ )=µi→j(τ ||τ ) ∀ . (A5)\nT i j τi i j T>τi\nSecond,iftheactivationtimeτ ofthenodeiisearlierthentheactivationtimeτ ofitsneighbork,thanthemessage\ni k\nfrom i to k does not depend on τ . If we combine this we the previous equation, we can write:\nk\nµi→k(τ ||τ )=µi (τ ||∞) ∀ , (A6)\nT i k T i τk>T"
  },
  {
    "id": 812,
    "text": "15\nwhere ∞ describes the state where a given node was activated at any time later than the cutoff time T (the time\nsubscript). Finally,theincomingmessagesinequationsaboveareindependentofnodeiactivationtimeτ andalways\ni\nbehave as τ is later than their cutoff time:\ni\nµi→j(τ ||τ )= (cid:88) W¯ (τ ;{τ } ) (cid:89) µk→i(τ ||∞), (A7)\nT i j i i k k∈∂i T−1 k\n{τk}k∈∂i\\j k∈∂i\\j\nµi (τ )= (cid:88) W¯ (τ ;{τ } ) (cid:89) µk→i(τ ||∞). (A8)\nT i i i k k∈∂i T−1 k\n{τk}k∈∂i k∈∂i\nThis is a direct consequence of the previous observations. Let us now define new type of marginals and messages:\n(cid:88)\nPi(t)= µi (τ )=µi(∗), (A9)\nS T i t\nτi>t\n(cid:88)\nPi→j(t)= µi→j(τ ||∞)=µi→j(∗||∞), (A10)\nS T i t\nτi>t\nwhere Pi(t) is the probability that node i was not activated until time t and Pi→j(t) is the same probability, but\nS S\non an auxiliary graph where node j does not exist. Symbol ∗ stands for all the trajectories with the activation"
  },
  {
    "id": 813,
    "text": "S S\non an auxiliary graph where node j does not exist. Symbol ∗ stands for all the trajectories with the activation\ntime τ being after the time horizon T. It means that when we sum over τ > b, in practice we sum over τ ∈\ni i i\n{b+1,b+2,...,T −1,T,∗}. It should be noted that in all equations below we use the convention that when τ =∗,\ni\nτ −1=T. Now,weusethesenewobjectstocomputethetermW¯ (...),whichdescribesthedynamicsoftheprocess.\ni i\nIn the case of IC model it has the following form:\nW¯ (τ ;{τ } )=(1−Pi(0))·1(τ =0)\ni i k k∈∂i S i\n(cid:34) (cid:35)\n(cid:89)\n+Pi(0)·1(τ >0) 1−α ·1(τ <τ −1)\nS i ki k i\n(A11)\nk∈∂i\n(cid:34) (cid:35)\n(cid:89)\n× 1− (1−α ·1(τ =τ −1))·1(τ ̸=∗)\nki k i i\nk∈∂i\nWe can now plug it all together and get:\n1−Pi→j(t)=1− (cid:88) W¯ (∗;{τ } ) (cid:89) µk→i(τ ||∞)\nS i k k∈∂i t−1 k\n{τk}k∈∂i\\j k∈∂i\\j\n(cid:88) (cid:89) (cid:89)\n=1−Pi(0) (1−α ·1(τ ̸=∗)) µk→i(τ ||∞)\nS ki k t−1 k\n{τk}k∈∂i\\j k∈∂i k∈∂i\\j\n(cid:88) (cid:89)\n=1−Pi(0) (1−α ·1(τ ̸=∗))µk→i(τ ||∞)\nS ki k t−1 k"
  },
  {
    "id": 814,
    "text": "{τk}k∈∂i\\j k∈∂i\\j\n(cid:88) (cid:89) (cid:89)\n=1−Pi(0) (1−α ·1(τ ̸=∗)) µk→i(τ ||∞)\nS ki k t−1 k\n{τk}k∈∂i\\j k∈∂i k∈∂i\\j\n(cid:88) (cid:89)\n=1−Pi(0) (1−α ·1(τ ̸=∗))µk→i(τ ||∞)\nS ki k t−1 k\n{τk}k∈∂i\\jk∈∂i\\j\n=1−Pi(0) (cid:89) (cid:88) (1−α ·1(τ ̸=∗))µk→i(τ ||∞) (A12)\nS ki k t−1 k\nk∈∂i\\j τk\n \n(cid:89) (cid:88)\n=1−P S i(0)  (1−α ki )µk t− → 1 i(τ k ||∞)+µk t− → 1 i(∗||∞)\nk∈∂i\\j τk≤t−1\n(cid:18) (cid:19)\n(cid:89)\n=1−Pi(0) 1−α (1−Pk→i(t−1)) ,\nS ki S\nk∈∂i\\j\nwhich is equivalent to the DMP equations (3) from the main text, where p (t) = 1−Pi→j(t) and assuming some\ni→j S\ninitialconditionp¯ =1−Pi→j(0). ReadersinterestedinderivationofDMPfromDBPfordifferentdynamicmodels\ni→j S\nshould refer to [11, 17, 23]."
  },
  {
    "id": 815,
    "text": "16\nAppendix B: Derivation of SLICER+ for different scenarios of incomplete data\nBelowwepresentthedetailsofLagrangianformulationusedintheoptimisationschemeofeachofthedifferentcase\nof missing or incomplete data. We show step-by-step how to compute all the Lagrange multipliers and the gradient\nstep used to update the parameters.\n1. SLICER\nThe SLICER algorithm [8] was developed to deal with the scenario of missing information on nodes exclusively. In\nthis setting the objective takes the following form:\nO = (cid:88)(cid:88)(cid:88) mτ i s log (cid:0) ps i (t)·1 (t≤T) −ps i (t−1)·1 (t>0) +1 (t=∗) (cid:1) . (B1)\ns∈Si∈O τs\ni\nThe constraints are the same as in the main text, but lets remind them for reader’s convenience.\nT−1 (cid:32) (cid:18) (cid:19)(cid:33)\nC = (cid:88)(cid:88)(cid:88) λs(t+1) ps(t+1)−1+ (cid:0) 1−p¯s(cid:1) (cid:89) 1−α ·ps (t)\ni i i ki k→i\ns∈S t=0 i∈V k∈∂i\n(B2)\nT−1 (cid:32) (cid:18) (cid:19)(cid:33)"
  },
  {
    "id": 816,
    "text": "C = (cid:88)(cid:88)(cid:88) λs(t+1) ps(t+1)−1+ (cid:0) 1−p¯s(cid:1) (cid:89) 1−α ·ps (t)\ni i i ki k→i\ns∈S t=0 i∈V k∈∂i\n(B2)\nT−1 (cid:32) (cid:18) (cid:19)(cid:33)\n+ (cid:88)(cid:88) (cid:88) λs (t+1) ps (t+1)−1+ (cid:0) 1−p¯s(cid:1) (cid:89) 1−α ·ps (t) .\ni→j i→j i ki k→i\ns∈S t=0 (i,j)∈E k∈∂i\\j\nThe sum of the objective and the constraints constitutes the Lagrangian. Lagrange multipliers can now be found by\ndifferentiating the Lagrangian:\n∂L =λs(t)+ (cid:88) mτ i s ·1 (t=τ i s) ·1 (τ i s≤T)\n∂ps(t) i ps(τs)−ps(τs−1)·1\ni τs i i i i (τ i s>0)\ni (B3)\n(cid:88)\nmτ\ni\ns ·1\n(t=τs−1)\n·1\n(τs>0)\n+ i i ,\nps(τs−1)−ps(τs)·1 −1\nτs i i i i (τ i s≤T) (τ i s=∗)\ni\n∂L =λs (t)−λs(t+1)α (cid:0) 1−p¯s(cid:1) (cid:89) (cid:16) 1−α ·ps (t) (cid:17)\n∂ps (t) i→j j ij j mj m→j\ni→j m∈∂j\\i\n(B4)\n− (cid:88) λs (t+1)α (cid:0) 1−p¯s(cid:1) (cid:89) (cid:16) 1−α ·ps (t) (cid:17)\nj→k ij j mj m→j\nk∈∂j\\i m∈∂j\\{i,k}\nand equating the derivatives to zero. Finally, derivatives over parameters α read\nij\nT−1"
  },
  {
    "id": 817,
    "text": "j→k ij j mj m→j\nk∈∂j\\i m∈∂j\\{i,k}\nand equating the derivatives to zero. Finally, derivatives over parameters α read\nij\nT−1\n∂L =− (cid:88)(cid:88) λs(t+1)ps (t) (cid:0) 1−p¯s(cid:1) (cid:89) (cid:16) 1−α ·ps (t) (cid:17)\n∂α i j→i i mi m→i\nij\ns∈S t=0 m∈∂i\\j\nT−1\n− (cid:88)(cid:88) λs(t+1)ps (t) (cid:0) 1−p¯s(cid:1) (cid:89) (cid:16) 1−α ·ps (t) (cid:17)\nj i→j j mj m→j\ns∈S t=0 m∈∂j\\i\n(B5)\nT−1\n− (cid:88)(cid:88) (cid:88) λs (t+1)ps (t) (cid:0) 1−p¯s(cid:1) (cid:89) (cid:16) 1−α ·ps (t) (cid:17)\ni→k j→i i mi m→i\ns∈S t=0 k∈∂i\\j m∈∂i\\{j,k}\nT−1\n− (cid:88)(cid:88) (cid:88) λs (t+1)ps (t) (cid:0) 1−p¯s(cid:1) (cid:89) (cid:16) 1−α ·ps (t) (cid:17) ,\nj→k i→j j mj m→j\ns∈S t=0 k∈∂j\\i m∈∂j\\{i,k}\nwhich can further be simplified for α ̸=0:\nij\nT−1\n∂L =− 1 (cid:88)(cid:88)(cid:0) λs (t)ps (t)+λs (t)ps (t) (cid:1) . (B6)\n∂α α i→j i→j j→i j→i\nij ij\ns∈S t=0\nThe above can be directly used in the forward–backward propagation procedure described in the main text."
  },
  {
    "id": 818,
    "text": "17\n2. Simple Graphs\nIn the case of simple graphs, where α =α∀ , the objective is the same as for the original SLICER, but the\nij (i,j)∈E\nconstraints simplify to:\nT−1 (cid:32) (cid:18) (cid:19)(cid:33)\nC = (cid:88)(cid:88)(cid:88) λs(t+1) ps(t+1)−1+ (cid:0) 1−p¯s(cid:1) (cid:89) 1−α·ps (t)\ni i i k→i\ns∈S t=0 i∈V k∈∂i\n(B7)\nT−1 (cid:32) (cid:18) (cid:19)(cid:33)\n+ (cid:88)(cid:88) (cid:88) λs (t+1) ps (t+1)−1+ (cid:0) 1−p¯s(cid:1) (cid:89) 1−α·ps (t) .\ni→j i→j i k→i\ns∈S t=0 (i,j)∈E k∈∂i\\j\nAs a result, we need to update the derivatives over messages:\n∂L =λs (t)−λs(t+1)α (cid:0) 1−p¯s(cid:1) (cid:89) (cid:16) 1−α·ps (t) (cid:17)\n∂ps (t) i→j j j m→j\ni→j m∈∂j\\i\n(B8)\n− (cid:88) λs (t+1)α (cid:0) 1−p¯s(cid:1) (cid:89) (cid:16) 1−α·ps (t) (cid:17) ,\nj→k j m→j\nk∈∂j\\i m∈∂j\\{i,k}\nwhile the derivatives over marginals remain in the same form as before. Then we can rewrite the derivatives over\nparameters, which in this case reduce to only a single equation:\nT−1"
  },
  {
    "id": 819,
    "text": "k∈∂j\\i m∈∂j\\{i,k}\nwhile the derivatives over marginals remain in the same form as before. Then we can rewrite the derivatives over\nparameters, which in this case reduce to only a single equation:\nT−1\n∂L =− (cid:88)(cid:88)(cid:88) λs(t+1) (cid:88) ps (t) (cid:0) 1−p¯s(cid:1) (cid:89) (cid:16) 1−α·ps (t) (cid:17)\n∂α i j→i i m→i\ns∈S t=0 i∈V j∈∂i m∈∂i\\j\n(B9)\nT−1\n− (cid:88)(cid:88) (cid:88) λs (t+1) (cid:88) ps (t) (cid:0) 1−p¯s(cid:1) (cid:89) (cid:16) 1−α·ps (t) (cid:17) ,\ni→k j→i i m→i\ns∈S t=0 (i,k)∈E′ j∈∂i\\k m∈∂i\\{j,k}\nwherebyE′ wedenotethesetofedgessuchthat(i,j)and(j,i)arecountedasseparateelements. Theaboveequation\ncan, assuming that α>0, be simplified to:\nT−1\n∂L 1 (cid:88)(cid:88) (cid:88)\n=− λs (t)·ps (t). (B10)\n∂α α i→j i→j\ns∈S t=0 (i,j)∈E′\nNow we can repeat the whole procedure, but the update step is in only one dimension.\n3. Missing Times\nInthissettingpartoftheactivationtimesareknownprecisely,whileothersareknowntobeincertaintimeintervals."
  },
  {
    "id": 820,
    "text": "3. Missing Times\nInthissettingpartoftheactivationtimesareknownprecisely,whileothersareknowntobeincertaintimeintervals.\nBased on that we modify the objective in a following way.\nO = (cid:88)(cid:88)(cid:88) mτ i s log (cid:16) ps i (τ i s)·1 (τs≤T) −ps i (τ i s−1)·1 (τs>0) +1 (τs=∗) (cid:17)\ni i i\ns∈Si∈O τs\ni (B11)\n+ (cid:88)(cid:88)(cid:88) mτˆ i s log (cid:16) ps i (τˆ i s)−ps i (τˆ i s−∆τˆ i s) (cid:17) .\ns∈Si∈O τˆs\ni\nThe only change is that if the node was activated during an unobserved time interval, the marginal probability of\nbeing infected precisely at τ is replaced by a marginal probability of being infected inside the interval [τˆ −∆τˆ,τˆ].\ni i i i\nThe constraints are the same as in the original SLICER. The above modification affects only the computation of the"
  },
  {
    "id": 821,
    "text": "18\nLagrange derivative over marginal probabilities.\n∂L =λs(t)+ (cid:88) mτ i s ·1 (t=τ i s) ·1 (τ i s≤T)\n∂ps(t) i ps(τs)−ps(τs−1)·1\ni τs i i i i (τ i s>0)\ni\n(cid:88)\nmτ\ni\ns ·1\n(t=τs−1)\n·1\n(τs>0)\n+ i i (B12)\nps(τs−1)−ps(τs)·1 −1\nτs i i i i (τ i s≤T) (τ i s=∗)\ni\n(cid:88)\nmτˆ\ni\ns ·1\n(t=τˆs) (cid:88)\nmτˆ\ni\ns ·1\n(t=τˆs−∆τˆs)\n+ i + i i .\nps(τˆs)−ps(τˆs−∆τˆs) ps(τˆs−∆τˆs)−ps(τˆs)\nτˆs i i i i i τˆs i i i i i\ni i\nThis requires updating the values of Lagrange multipliers λs(t). The rest of the procedure remains the same.\ni\n4. Noisy Timestamps\nTaking into account the noisy activation times results with an objective function of the following form:\n(cid:32) K (cid:20) (cid:21)(cid:33)\nO = (cid:88)(cid:88)(cid:88) mτ i s log (cid:88) π k ps i (τ i s−k)·1 (0≤τs−k≤T) −ps i (τ i s−1−k)·1 (0<τs−k≤∗) +1 (τs−k=∗) . (B13)\ni i i\ns∈Si∈O τs k=−K\ni\nUpdatingtheobjectivefunctionaffectsthelearningprocedure. However,theonlyelementoftheoptimisationscheme,\nwhich require changes is the computation of λs(t).\ni"
  },
  {
    "id": 822,
    "text": "i i i\ns∈Si∈O τs k=−K\ni\nUpdatingtheobjectivefunctionaffectsthelearningprocedure. However,theonlyelementoftheoptimisationscheme,\nwhich require changes is the computation of λs(t).\ni\n∂L =λs(t)+ (cid:88) mτ i s · (cid:80)K k=−K π k (cid:0)1 (t=τ i s−k) ·1 (0≤τ i s−k≤T) −1 (t=τ i s−1−k) ·1 (0<τ i s−k≤∗) (cid:1) . (B14)\n∂ps(t) i (cid:80)K π (cid:0) ps(τs−k)·1 −ps(τs−1−k)·1 +1 (cid:1)\ni τ i s k=−K k i i (0≤τ i s−k≤T) i i (0<τ i s−k≤∗) (τ i s−k=∗)\nAll the further computations remain the same as before. It should be noted that the described change in the learning\nprocedure does also affect its complexity. In the worst case scenario where K = T the complexity of the algorithm\nbecomes max[O(|E||S|T),O(N|S|T2)] in comparison with the complexity O(|E||S|T) of the original algorithm.\nAppendix C: Efficient Implementation\nDirect implementation of Eq. (2) and (3) suffers from evaluating the products over neighbors, in particular, this"
  },
  {
    "id": 823,
    "text": "Appendix C: Efficient Implementation\nDirect implementation of Eq. (2) and (3) suffers from evaluating the products over neighbors, in particular, this\nis repeated for every message rooted in a given node. As a result the complexity of the algorithm depends on the\nnetwork’s degree distribution, instead of the number of edges. Networks with broad degree distributions, which\nare common in real-world applications, are specifically affected by this problem. It is possible to overcome this by\nrewriting the DMP equations in the following form:\npc(t)=1− (cid:0) 1−p¯c(cid:1) (cid:89) (cid:16) 1−α ·pc (t−1) (cid:17) , (C1)\ni i ki k→i\nk∈∂i\n1−pc(t)\npc (t)=1− j , (C2)\nj→i 1−α ·pc (t−1)\nij i→j\nwhere the marginals equations are the same, butthe messages are computed usingthe pre-computed marginals. Now\nthe complexity is independent of the degree distribution and truly linear in the number of edges, assuming that"
  },
  {
    "id": 824,
    "text": "the complexity is independent of the degree distribution and truly linear in the number of edges, assuming that\nα ·pc (t−1) ̸= 1. The latter is, however, a reasonable assumption, which is unlikely to be broken, in which case\nij i→j\nthe original equation can still be used. Similar problem arises from Eq. (B4), which after equating left hand side to\nzero leads to:\nλs (t)=λs(t+1)α (cid:0) 1−p¯s(cid:1) (cid:89) (cid:16) 1−α ·ps (t) (cid:17)\ni→j j ij j mj m→j\nm∈∂j\\i\n(C3)\n+ (cid:88) λs (t+1)α (cid:0) 1−p¯s(cid:1) (cid:89) (cid:16) 1−α ·ps (t) (cid:17) .\nj→k ij j mj m→j\nk∈∂j\\i m∈∂j\\{i,k}"
  },
  {
    "id": 825,
    "text": "19\nAlthough the equation have a different form, they are dual to the DMP one. Similarly to the latter, direct imple-\nmentation leads to degree distribution dependence. Here instead of a product over neighbors we have a sum over\nneighbors and similarly as before, this can be prevented. To do so, let us introduce\nλ(cid:98) s\nj\n(t)= (cid:88) λs\nj→k\n(t+1) (cid:0) 1−p¯s\nj\n(cid:1) (cid:89) (cid:16) 1−α\nmj\n·ps\nm→j\n(t) (cid:17) , (C4)\nk∈∂j m∈∂j\\k\nwhich can be further simplified as\nλ(cid:98) s(t)= (cid:88) λs (t+1) (cid:0) 1−ps (t+1) (cid:1) . (C5)\nj j→k j→k\nk∈∂j\nNow we can write:\nλs (t)=λs(t+1)α (cid:0) 1−ps (t+1) (cid:1)\ni→j j ij j→i\nλ(cid:98)s(t)−λs (t+1) (cid:0) 1−ps (t+1) (cid:1) (C6)\n+α j j→i j→i .\nij 1−α ·ps (t)\nij i→j\nThis way, knowing that λs (T) = 0 ∀ , we reduced the complexity to being linear in the number of edges, same\ni→j i,j\nway as we did for messages. Additionally, in the directed case where (i,j) ∈ E, but (j,i) ∈/ E, one needs to rewrite\nEq. (C6) into:"
  },
  {
    "id": 826,
    "text": "i→j i,j\nway as we did for messages. Additionally, in the directed case where (i,j) ∈ E, but (j,i) ∈/ E, one needs to rewrite\nEq. (C6) into:\nλ(cid:98)s(t)+λs(t+1) (cid:0) 1−ps(t+1) (cid:1)\nλs (t)=α j j j . (C7)\ni→j ij 1−α ·ps (t)\nij i→j\nThe above equations, used in all our experiments, are implemented in Julia [60] and available at [59].\nAppendix D: Structure learning without any prior knowledge on the topology\nHere, we present results on structure learning in the presence of unobserved nodes. We reproduce the experiment\npresented in Fig. 4, but in the most challenging setting where no information on the network topology is available,\nand the super-set of edges that we initially consider corresponds to a fully-connected graph. The results are given in\nFig. 10, and are similar for all four synthetic network types.The edge reconstruction is always better than random\nguess, but we observe reduction of reconstruction quality with the increase of the number of unobserved nodes. This"
  },
  {
    "id": 827,
    "text": "guess, but we observe reduction of reconstruction quality with the increase of the number of unobserved nodes. This\nis expected due to the appearance of degeneracy where several models are consistent with the observation data. As a\nresult SLICER produces a graph, which is compatible with the observed data and as such can be used as an effective\nmodelforprediction[8],evenifitisnotthetrue graph. Moreover,haveinmindthatourmethodrecoversthenetwork\nsimultaneously with learning model parameters.\n[1] MarkNewman. Networks. Oxforduniversitypress,2018. [6] JeanPouget-AbadieandThibautHorel. Inferringgraphs\n[2] Alain Barrat, Marc Barthelemy, and Alessandro Vespig- fromcascades: Asparserecoveryframework. InInterna-\nnani. Dynamical processes on complex networks. Cam- tional Conference on Machine Learning, pages 977–986.\nbridge university press, 2008. PMLR, 2015.\n[3] Praneeth Netrapalli and Sujay Sanghavi. Learning the [7] AndreyYLokhov. Reconstructingparametersofspread-"
  },
  {
    "id": 828,
    "text": "bridge university press, 2008. PMLR, 2015.\n[3] Praneeth Netrapalli and Sujay Sanghavi. Learning the [7] AndreyYLokhov. Reconstructingparametersofspread-\ngraph of epidemic cascades. ACM SIGMETRICS Per- ingmodelsfrompartialobservations.AdvancesinNeural\nformance Evaluation Review, 40(1):211–222, 2012. Information Processing Systems, 29, 2016.\n[4] Manuel Gomez-Rodriguez, Jure Leskovec, and Andreas [8] Mateusz Wilinski and Andrey Y Lokhov. Prediction-\nKrause. Inferring networks of diffusion and influence. centric learning of independent cascade dynamics from\nACM Transactions on Knowledge Discovery from Data partialobservations. InInternationalConferenceonMa-\n(TKDD), 5(4):1–37, 2012. chine Learning, pages 11182–11192. PMLR, 2021.\n[5] Bruno Abrahao, Flavio Chierichetti, Robert Kleinberg, [9] Isaak Neri and D´esir´e Boll´e. The cavity approach to\nand Alessandro Panconesi. Trace complexity of network parallel dynamics of ising spins on a graph. Jour-"
  },
  {
    "id": 829,
    "text": "and Alessandro Panconesi. Trace complexity of network parallel dynamics of ising spins on a graph. Jour-\ninference. In Proceedings of the 19th ACM SIGKDD in- nal of Statistical Mechanics: Theory and Experiment,\nternational conference on Knowledge discovery and data 2009(08):P08009, 2009.\nmining, pages 491–499, 2013."
  },
  {
    "id": 830,
    "text": "20\n(cid:6)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:8) (cid:6)(cid:5)(cid:9)\n(cid:11)\n(cid:14)(cid:13)(cid:12)(cid:15)(cid:17)(cid:19)(cid:18)(cid:3)(cid:14)(cid:20)(cid:17)(cid:19)(cid:13)(cid:3)(cid:13)(cid:16)(cid:17)\n(cid:6)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:8) (cid:6)(cid:5)(cid:9)\n(cid:11)\n(cid:14)(cid:13)(cid:12)(cid:15)(cid:17)(cid:19)(cid:18)(cid:3)(cid:14)(cid:20)(cid:17)(cid:19)(cid:13)(cid:3)(cid:13)(cid:16)(cid:17)\n(cid:6)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:8) (cid:6)(cid:5)(cid:9)\n(cid:11)"
  },
  {
    "id": 831,
    "text": "(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:8) (cid:6)(cid:5)(cid:9)\n(cid:11)\n(cid:14)(cid:13)(cid:12)(cid:15)(cid:17)(cid:19)(cid:18)(cid:3)(cid:14)(cid:20)(cid:17)(cid:19)(cid:13)(cid:3)(cid:13)(cid:16)(cid:17)\n(cid:6)(cid:4)(cid:5)\n(cid:5)(cid:4)(cid:10)\n(cid:5)(cid:4)(cid:9)\n(cid:5)(cid:4)(cid:8)\n(cid:5)(cid:4)(cid:7)\n(cid:5)(cid:4)(cid:5)\n(cid:6)(cid:5)(cid:7) (cid:6)(cid:5)(cid:8) (cid:6)(cid:5)(cid:9)\n(cid:11)\n(cid:14)(cid:13)(cid:12)(cid:15)(cid:17)(cid:19)(cid:18)(cid:3)(cid:14)(cid:20)(cid:17)(cid:19)(cid:13)(cid:3)(cid:13)(cid:16)(cid:17)\n(a) (b) 3-regular tree (c) random 3-regular\nτ\ni\n(cid:4)(cid:5)(cid:4)(cid:6) (cid:4)(cid:5)(cid:7)(cid:9)(cid:6)\nα (cid:4)(cid:5)(cid:8)(cid:4)(cid:6) (cid:4)(cid:5)(cid:9)(cid:4)(cid:6)\nij\n(d) Erdos-Renyi (e) scale-free\nτ\nj\nobserved - fraction(100−ξ)%\nunobserved - fractionξ%"
  },
  {
    "id": 832,
    "text": "α (cid:4)(cid:5)(cid:8)(cid:4)(cid:6) (cid:4)(cid:5)(cid:9)(cid:4)(cid:6)\nij\n(d) Erdos-Renyi (e) scale-free\nτ\nj\nobserved - fraction(100−ξ)%\nunobserved - fractionξ%\nFIG. 10: Structure learning starting with a fully-connected graph. Average ROC curve surface, as a function of the\nnumber of available cascades for different network types in the case where a fraction ξ of nodes is unobserved and\nthere is no prior knowledge about the topology. Each point is averaged over 5 different networks and 5 different sets\nof parameters α∗ (sampled from a uniform distribution in [0,1]). All networks contain N =100 nodes and all but\nij\nthe tree have average degree equal to ⟨k⟩=3. Unobserved nodes were picked at random. All cascades had length\nT =5.\n[10] Brian Karrer and Mark EJ Newman. Message passing [18] Gino Del Ferraro and Erik Aurell. Dynamic message-\napproach for general epidemic models. Physical Review passing approach for kinetic spin models with reversible"
  },
  {
    "id": 833,
    "text": "approach for general epidemic models. Physical Review passing approach for kinetic spin models with reversible\nE, 82(1):016101, 2010. dynamics. Physical Review E, 92(1):010102, 2015.\n[11] Yashodhan Kanoria and Andrea Montanari. Majority [19] Munik Shrestha, Samuel V Scarpino, and Cristopher\ndynamicsontreesandthedynamiccavitymethod. 2011. Moore. Message-passing approach for recurrent-state\n[12] Erik Aurell and Hamed Mahmoudi. Three lemmas on epidemic models on networks. Physical Review E,\ndynamiccavitymethod. Communications in Theoretical 92(2):022821, 2015.\nPhysics, 56(1):157, 2011. [20] Andrey Y Lokhov and David Saad. Optimal deploy-\n[13] ErikAurellandHamedMahmoudi. Dynamicmean-field ment of resources for maximizing impact in spreading\nand cavity methods for diluted ising systems. Physical processes. Proceedings of the National Academy of Sci-\nReview E, 85(3):031119, 2012. ences, 114(39):E8138–E8146, 2017."
  },
  {
    "id": 834,
    "text": "and cavity methods for diluted ising systems. Physical processes. Proceedings of the National Academy of Sci-\nReview E, 85(3):031119, 2012. ences, 114(39):E8138–E8146, 2017.\n[14] Fabrizio Altarelli, Alfredo Braunstein, Luca Dall’Asta, [21] Andrey Y Lokhov and David Saad. Scalable in-\nandRiccardoZecchina. Largedeviationsofcascadepro- fluence estimation without sampling. arXiv preprint\ncessesongraphs. PhysicalReviewE,87(6):062115,2013. arXiv:1912.12749, 2019.\n[15] Fabrizio Altarelli, Alfredo Braunstein, Luca Dall’Asta, [22] ThomasBarthel. Thematrixproductapproximationfor\nand Riccardo Zecchina. Optimizing spread dynamics on the dynamic cavity method. Journal of Statistical Me-\ngraphs by message passing. Journal of Statistical Me- chanics: Theory and Experiment, 2020(1):013217, 2020.\nchanics: TheoryandExperiment,2013(09):P09011,2013. [23] Hanlin Sun, David Saad, and Andrey Y Lokhov. Com-"
  },
  {
    "id": 835,
    "text": "chanics: TheoryandExperiment,2013(09):P09011,2013. [23] Hanlin Sun, David Saad, and Andrey Y Lokhov. Com-\n[16] AndreyYLokhov,MarcM´ezard,HirokiOhta,andLenka petition, collaboration, and optimization in multiple\nZdeborova´. Inferring the origin of an epidemic with a interacting spreading processes. Physical Review X,\ndynamic message-passing algorithm. Physical Review E, 11(1):011048, 2021.\n90(1):012801, 2014. [24] BoLiandDavidSaad. Impactofpresymptomatictrans-\n[17] Andrey Y Lokhov, Marc M´ezard, and Lenka Zdeborov´a. mission on epidemic spreading in contact networks: A\nDynamicmessage-passingequationsformodelswithuni- dynamic message-passing analysis. Physical Review E,\ndirectional dynamics. Physical Review E, 91(1):012811, 103(5):052303, 2021.\n2015."
  },
  {
    "id": 836,
    "text": "21\n[25] Erik Aurell, David Machado Perez, and Roberto Mulet. [42] Keqi Han, Yuan Tian, Yunjia Zhang, Ling Han, Hao\nA closure for the master equation starting from the dy- Huang, and Yunjun Gao. Statistical estimation of diffu-\nnamic cavity method. Journal of Physics A: Mathemat- sionnetworktopologies.In2020IEEE36thInternational\nical and Theoretical, 56(17):17LT02, 2023. ConferenceonDataEngineering(ICDE),pages625–636.\n[26] Stefano Crotti and Alfredo Braunstein. Matrix prod- IEEE, 2020.\nuct belief propagation for reweighted stochastic dynam- [43] JessicaHoffmannandConstantineCaramanis. Learning\nics over graphs. Proceedings of the National Academy of graphs from noisy epidemic cascades. Proceedings of the\nSciences, 120(47):e2307935120, 2023. ACM on Measurement and Analysis of Computing Sys-\n[27] FreyaBehrens,BarboraHudcov´a,andLenkaZdeborova´. tems, 3(2):1–34, 2019.\nBacktracking dynamical cavity method. Phys. Rev. X, [44] WilliamTrouleau,JalalEtesami,MatthiasGrossglauser,"
  },
  {
    "id": 837,
    "text": "[27] FreyaBehrens,BarboraHudcov´a,andLenkaZdeborova´. tems, 3(2):1–34, 2019.\nBacktracking dynamical cavity method. Phys. Rev. X, [44] WilliamTrouleau,JalalEtesami,MatthiasGrossglauser,\n13:031021, Aug 2023. Negar Kiyavash, and Patrick Thiran. Learning hawkes\n[28] Davide Ghio, Antoine LM Aragon, Indaco Biazzo, and processes under synchronization noise. In Interna-\nLenka Zdeborova´. Bayes-optimal inference for spread- tional Conference on Machine Learning, pages 6325–\ning processes on random networks. arXiv preprint 6334. PMLR, 2019.\narXiv:2303.17704, 2023. [45] Wei Chen, Chi Wang, and Yajun Wang. Scalable influ-\n[29] Jonathan S Yedidia, William T Freeman, Yair Weiss, encemaximizationforprevalentviralmarketinginlarge-\net al. Understanding belief propagation and its general- scale social networks. In Proceedings of the 16th ACM\nizations. Exploring artificial intelligence in the new mil- SIGKDD international conference on Knowledge discov-"
  },
  {
    "id": 838,
    "text": "izations. Exploring artificial intelligence in the new mil- SIGKDD international conference on Knowledge discov-\nlennium, 8(236-239):0018–9448, 2003. ery and data mining, pages 1029–1038, 2010.\n[30] Marc Mezard and Andrea Montanari. Information, [46] Michael Shapiro and Edgar Delgado-Eckert. Finding\nphysics,andcomputation.OxfordUniversityPress,2009. the probability of infection in an sir network is np-hard.\n[31] Jacob Goldenberg, Barak Libai, and Eitan Muller. Talk Mathematical biosciences, 240(2):77–84, 2012.\nofthenetwork: Acomplexsystemslookattheunderlying [47] Wei Chen, Yajun Wang, and Siyu Yang. Efficient in-\nprocess of word-of-mouth. Marketing letters, 12(3):211– fluence maximization in social networks. In Proceedings\n223, 2001. of the 15th ACM SIGKDD international conference on\n[32] David Kempe, Jon Kleinberg, and E´va Tardos. Max- Knowledge discovery and data mining, pages 199–208,\nimizing the spread of influence through a social net- 2009."
  },
  {
    "id": 839,
    "text": "[32] David Kempe, Jon Kleinberg, and E´va Tardos. Max- Knowledge discovery and data mining, pages 199–208,\nimizing the spread of influence through a social net- 2009.\nwork. In Proceedings of the ninth ACM SIGKDD in- [48] Nan Du, Le Song, Manuel Gomez Rodriguez, and\nternational conference on Knowledge discovery and data Hongyuan Zha. Scalable influence estimation in\nmining, pages 137–146, 2003. continuous-time diffusion networks. Advances in neural\n[33] William Ogilvy Kermack and Anderson G McKendrick. information processing systems, 26, 2013.\nAcontributiontothemathematicaltheoryofepidemics. [49] Edith Cohen, Daniel Delling, Thomas Pajor, and Re-\nProceedingsoftheroyalsocietyoflondon.SeriesA,Con- nato F Werneck. Sketch-based influence maximization\ntaining papers of a mathematical and physical character, and computation: Scaling up with guarantees. In Pro-\n115(772):700–721, 1927. ceedings of the 23rd ACM International Conference on"
  },
  {
    "id": 840,
    "text": "taining papers of a mathematical and physical character, and computation: Scaling up with guarantees. In Pro-\n115(772):700–721, 1927. ceedings of the 23rd ACM International Conference on\n[34] Matt J Keeling and Ken TD Eames. Networks and epi- ConferenceonInformationandKnowledgeManagement,\ndemic models. Journal of the royal society interface, pages 629–638, 2014.\n2(4):295–307, 2005. [50] Brendan Lucier, Joel Oren, and Yaron Singer. Influence\n[35] HaoHuang,KeqiHan,BeichengXu,andTingGan. Re- at scale: Distributed computation of complex contagion\nconstructingdiffusionnetworksfromincompletedata. In in networks. In Proceedings of the 21th ACM SIGKDD\nIJCAI, volume 2022, pages 3085–3091, 2022. International Conference on Knowledge Discovery and\n[36] Daryl J Daley and Joe Gani. Epidemic modelling: an Data Mining, pages 735–744, 2015.\nintroduction. Number 15. Cambridge University Press, [51] HungTNguyen,TriPNguyen,TamNVu,andThangN\n2001. Dinh. Outward influence and cascade size estimation in"
  },
  {
    "id": 841,
    "text": "introduction. Number 15. Cambridge University Press, [51] HungTNguyen,TriPNguyen,TamNVu,andThangN\n2001. Dinh. Outward influence and cascade size estimation in\n[37] Emre Sefer and Carl Kingsford. Convex risk minimiza- billion-scale networks. Proceedings of the ACM on Mea-\ntiontoinfernetworksfromprobabilisticdiffusiondataat surementandAnalysisofComputingSystems,1(1):1–30,\nmultiplescales. In2015IEEE31stInternationalConfer- 2017.\nence on Data Engineering, pages 663–674. IEEE, 2015. [52] B´elaBolloba´s. Randomgraphs. InModerngraphtheory,\n[38] RitabrataDutta,AntoniettaMira,andJukka-PekkaOn- pages 215–252. Springer, 1998.\nnela. Bayesian inference of spreading processes on net- [53] Albert-La´szlo´ Baraba´si and R´eka Albert. Emergence of\nworks.ProceedingsoftheRoyalSocietyA:Mathematical, scalinginrandomnetworks. science,286(5439):509–512,\nPhysical and Engineering Sciences, 474(2215):20180129, 1999.\n2018. [54] Tom Fawcett. An introduction to roc analysis. Pattern"
  },
  {
    "id": 842,
    "text": "Physical and Engineering Sciences, 474(2215):20180129, 1999.\n2018. [54] Tom Fawcett. An introduction to roc analysis. Pattern\n[39] Kareem Amin, Hoda Heidari, and Michael Kearns. recognition letters, 27(8):861–874, 2006.\nLearningfromcontagion(withouttimestamps). InInter- [55] Mark EJ Newman. The structure and function of com-\nnational Conference on Machine Learning, pages 1845– plex networks. SIAM review, 45(2):167–256, 2003.\n1853. PMLR, 2014. [56] Wayne W Zachary. An information flow model for con-\n[40] Adisak Supeesun and Jittat Fakcharoenphol. Learning flict and fission in small groups. Journal of anthropolog-\nnetworkstructuresfromcontagion. InformationProcess- ical research, 33(4):452–473, 1977.\ning Letters, 121:11–16, 2017. [57] Ryan A. Rossi and Nesreen K. Ahmed. The network\n[41] Alfredo Braunstein, Alessandro Ingrosso, and data repository with interactive graph analytics and vi-\nAnna Paola Muntoni. Network reconstruction from sualization. In Proceedings of the Twenty-Ninth AAAI"
  },
  {
    "id": 843,
    "text": "Anna Paola Muntoni. Network reconstruction from sualization. In Proceedings of the Twenty-Ninth AAAI\ninfection cascades. Journal of the Royal Society Conference on Artificial Intelligence, 2015.\nInterface, 16(151):20180844, 2019."
  },
  {
    "id": 844,
    "text": "22\n[58] Amanda L Traud, Peter J Mucha, and Mason A passing/, 2021.\nPorter. Social structure of Facebook networks. Phys. [60] Stefan Karpinski Jeff Bezanson. The julia programming\nA, 391(16):4165–4180, Aug 2012. language.\n[59] Mateusz Wilinski and Andrey Y Lokhov.\nhttps://github.com/mateuszwilinski/dynamic-message-"
  },
  {
    "id": 845,
    "text": "COMPREHENSIVE DESIGN AND WHOLE-CAVITY SIMULATION OF A MULTI-\nBEAM INDUCTIVE OUTPUT TUBE USING A 3rd HARMONIC DRIVE ON THE GRID\nH.P. Freund,1,2,3 T. Bui,1 R.L. Ives,1 T. Haberrmann,1 and M. Read1\n1Calabazas Creek Research, Inc., San Mateo, CA 94404\n2Institute for Research in Electronics and Applied Physics, University Maryland, College Park, Maryland 20742\n3University of New Mexico, Albuquerque, New Mexico, 87131\nIn this paper, we discuss the design and whole-cavity simulation of a Multi-Beam Inductive Output\nTube (MBIOT) that uses a 3rd harmonic component to the drive voltage on the grid. High-efficiency inductive\noutput tubes (IOTs) are typically characterized by efficiencies up to 70 − 75%. However, the achievement of\nefficiencies greater than 80% would substantially reduce the operating costs of next-generation accelerators.\nTo achieve this goal, we consider the addition of a 3rd harmonic component to the drive signal on the grid."
  },
  {
    "id": 846,
    "text": "To achieve this goal, we consider the addition of a 3rd harmonic component to the drive signal on the grid.\nWe anticipate that the MBIOT will be used to provide the rf power to drive RF linacs. We discuss and model\nan 8-beam MBIOT with a 700 MHz resonant cavity using beams with a voltage of 35 kV and an average\ncurrent of 7.25 A yielding a perveance of about 1.1 µP. We simulate this MBIOT using the NEMESIS\nsimulation code which has been extended using a three-dimensional Poisson solver based upon the Petsc\npackage from Argonne National Laboratory. The effect of the 3rd harmonic on the efficiency is greatest when\nthe phase of the 3rd harmonic is shifted by π radians with respect to the fundamental drive signal and with 3rd\nharmonic powers greater than about 50% of the fundamental drive power. For the present example, we show\nthat efficiencies approaching 82% are possible. Designs for the MBIOT input coupler, grids and output cavity"
  },
  {
    "id": 847,
    "text": "that efficiencies approaching 82% are possible. Designs for the MBIOT input coupler, grids and output cavity\nhave been developed based on these simulations and will be discussed.\nIndex Terms – Inductive Output Tube, Harmonic Grid Drive, High-Efficiency\nI. INTRODUCTION the application of the 3rd harmonic component to the drive\nHigh-efficiency inductive output tubes (IOTs) are on the IOT gun(s).\ntypically characterized by efficiencies up to 70 − 75%.\nHowever, the achievement of efficiencies greater than 80%\nwould substantially reduce the operating costs of next-\ngeneration radio frequency linear accelerators (RF linacs).\nTo achieve this goal, we consider the addition of a 3rd\nharmonic component to the drive signal on the grid of a\nmulti-beam inductive output tube (MBIOT) [1,2]. The 3rd\nharmonic drive component in IOT guns has been\nconsidered to apply such a gun as the injector [3] of radio\nfrequency linear accelerators (RF linacs).\nIOTs are sometimes referred to as klystrodes® [4] and"
  },
  {
    "id": 848,
    "text": "considered to apply such a gun as the injector [3] of radio\nfrequency linear accelerators (RF linacs).\nIOTs are sometimes referred to as klystrodes® [4] and\nare used for high power applications such as the drivers for\nFig. 1: Schematic illustration of an inductive output tube, where\nRF Linacs [5-8]. A schematic illustration of an IOT is\nthe output cavity is located between the pole pieces [9].\nshown in Fig. 1 [9]. In an IOT, a pre-bunched beam is\ngenerated by applying a radio-frequency signal to the grid\nWe consider an 8-beam MBIOT with a 700 MHz\nof an electron gun which is accelerated to higher energies\nresonant cavity and using an annular beam with a voltage\nby a DC potential before injection into a resonant output\nof 35 kV, an average current of 7.25 A yielding a perveance\ncavity. The modulated beam is preconditioned to excite the\nof about 1.1 µP. This MBIOT is simulated using the\nresonant mode of the output cavity, after which the spent"
  },
  {
    "id": 849,
    "text": "cavity. The modulated beam is preconditioned to excite the\nof about 1.1 µP. This MBIOT is simulated using the\nresonant mode of the output cavity, after which the spent\nNEMESIS simulation code [1,9] which has been extended\nbeam is directed into a collector.\nby the incorporation of a three-dimensional Poisson solver\nRecent development of a high-power MBIOT [10]\nusing the Petsc package available from Argonne National\nintended for the European Spallation Neutron Source [11]\nLaboratory [12]. NEMESIS has previously been\nachieved an efficiency of 65% with a peak power of 1.2\nsuccessfully validated by comparison with the K5H90W-2\nMW at a frequency of 705 MHz. The cost of RF power for\nIOT developed by Communications & Power Industries\nsuch facilities is an important driver for accelerator\nLLC (CPI) [9]. It is found that the effect of the 3rd harmonic\noperations, and the production of an IOT with still higher\non the efficiency is greatest when the phase of the 3rd"
  },
  {
    "id": 850,
    "text": "LLC (CPI) [9]. It is found that the effect of the 3rd harmonic\noperations, and the production of an IOT with still higher\non the efficiency is greatest when the phase of the 3rd\nefficiencies could provide significant cost reductions. A\nharmonic is shifted by π radians with respect to the\n5% increase in the efficiency reduces the power required\nfundamental drive signal and with 3rd harmonic powers\nfor a 200-kW device by 10 kW. This is a significant\ngreater than about 50% that of the fundamental drive\noperational cost savings for systems running continuously\npower. For the present example, we show that efficiencies\nor at high duty. This is the prime motivation for studying\napproaching 82% are possible."
  },
  {
    "id": 851,
    "text": "The organization of the paper is as follows. The efficiency, each beam must propagate in its own beam\nnumerical model incorporated in the NEMESIS code and tunnel. Each beam includes its own interaction gap. The\nwhole-cavity simulations with NEMESIS are described in Poisson solver, therefore, must include the region within\nSection. II. The designs for the input coupler, grids and each beam tunnel as well as the overall cavity to include\noutput cavity are described in Section II. A summary and the fields which the beams would experience in the gaps.\ndiscussion are given in Section IV. The accuracy improvement is achieved because a Dirichlet\nboundary condition can be prescribed for each beam\nII. WHOLE-CAVITY SIMULATION tunnel. In addition, if an inner conductor is automatically\ngenerated, then the Dirichlet boundary condition can be\nThe numerical formulation in the NEMESIS code [1,9] prescribed to this inner conductor which improves"
  },
  {
    "id": 852,
    "text": "generated, then the Dirichlet boundary condition can be\nThe numerical formulation in the NEMESIS code [1,9] prescribed to this inner conductor which improves\nis similar to particle-in-cell simulation codes. Integration accuracy for a given number of radial nodes. The\nof the dynamical equations is performed in time, so the efficiency is improved by not having to deal with the\ncode can implicitly treat particles that might turn around. boundary condition of the cavity axis.\nThis can be important in high-efficiency designs where We use an algorithm to generate the mesh of the beam\nparticles lose a great deal of energy. tunnels for one cavity. The algorithm can easily be\nNEMESIS contains an equivalent (LRC) circuit model extended for multiple cavities along the main axis. The\nfor the cavity voltage with a model for the circuit fields beam tunnels are assumed to have the same (minor) radius"
  },
  {
    "id": 853,
    "text": "for the cavity voltage with a model for the circuit fields beam tunnels are assumed to have the same (minor) radius\ntaken from Kosmahl and Branch [13] and scaled using the r , and their centers span equal angle distance and on the\nb\ncavity voltage. The integration of particle trajectories uses same (major) radius R from the cavity axis. The algorithm\nb\nthe circuit fields obtained in this fashion as well as an takes advantage of the structure of the finite difference\nanalytic model for the focusing fields and two- or three- mesh to construct the mesh of the beam tunnels only once.\ndimensional Poisson solvers for the space charge fields. At every axial (z) node, the same indices of the (r, θ) nodes\nThe numerical procedure is illustrated in Fig. 2. The prescribing the beam tunnels are repeated.\nprocedure in stepping from t  t + ∆t begins with a 4th Let R i be the radius of the inner conductor, N r the number"
  },
  {
    "id": 854,
    "text": "procedure in stepping from t  t + ∆t begins with a 4th Let R i be the radius of the inner conductor, N r the number\norder Runge-Kutta integration of the equivalent circuit of r nodes from the inner conductor to the outer conductor,\nequations. We typically take 100 steps per wave period to Nθ the number of angles, d\nr\nthe mesh r-size, dθ the mesh θ–\nensure accuracy. Once the circuit equations have been size, and N the number of beam tunnels. In addition, the\nb\nstepped, electrons are injected into the cavity if the time number of local angles nθ is also needed to construct each\ncoincides with the bunch phase. After injection, the tunnel. It’s noted that nθ << Nθ for an accurate\ntrajectories are integrated using a Boris push [14], which representation of the tunnels. We assume the first tunnel is\nincludes the RF and magnetostatic fields, and the Poisson on the x axis. The pseudo algorithm is as follows."
  },
  {
    "id": 855,
    "text": "includes the RF and magnetostatic fields, and the Poisson on the x axis. The pseudo algorithm is as follows.\nsolver is called to obtain the electrostatic fields. At this The local angle distance is δ θ =2π/nθ. For each tunnel m\npoint, the source current is calculated by averaging the in [0,N\nb\n] set the global angle of the tunnel center θ\nm\n= mdθ.\ncurrent obtained using (x t , v t + ∆t/2 ) and (x t + ∆t , v t + ∆t/2 ). This Then for each local angle n in [0,nθ]\nensures 2nd order accuracy for the overall procedure. 1. Set the local angle in the tunnel θ\nn\n= nδ θ,\nFinally, we test whether any particles have left the system\n2. Construct the local coordinates (r,θ) on the tunnel\nl l\n(from either end of the cavity or by striking the wall), and,\nradius using (r ,θ),\nif necessary, eject them from the simulation. This is b n\n3. Transform cylindrical coordinates from local (r,θ)\nrepeated as many times as necessary to simulate any given l l\nto global (r,θ) using (R ,θ),\npulse time. b m"
  },
  {
    "id": 856,
    "text": "3. Transform cylindrical coordinates from local (r,θ)\nrepeated as many times as necessary to simulate any given l l\nto global (r,θ) using (R ,θ),\npulse time. b m\n4. Search for global mesh indices (i,j) of (r,θ) space\nusing R\ni\n, d\nr\n, dθ,\n5. Obtain one node on the tunnel in the global mesh,\nfrom the global mesh indices (i,j) and (6) store the\npair (i,j) for later boundary condition assignment.\n6. Finally, apply the Dirichlet boundary condition to\neach mesh node on the tunnels along the cavity z\naxis except for the tunnel gap. The axial location of\nthe gaps is preset.\nThe three-dimensional Poisson solver presents the single\ngreatest computational load in NEMESIS. As a result, we\nhave parallelized the three-dimensional Petsc solver using\nthe Message Passing Interface (MPI). Running on a\nWindows laptop computer with four cores, an example of\nthe improvements in the overall runtime is shown in Fig. 3\nwhere we plot the normalized run time versus the number"
  },
  {
    "id": 857,
    "text": "Windows laptop computer with four cores, an example of\nthe improvements in the overall runtime is shown in Fig. 3\nwhere we plot the normalized run time versus the number\nFig. 2: Flow chart of the simulation procedure [1].\nof cores for the simulations discussed below. The runtime\ndecreased by a factor of more than three by engaging all\nTo simulate a three-dimensional Poisson solver for a\nfour cores.\nmultiple beam device, and improve accuracy and\n2"
  },
  {
    "id": 858,
    "text": "1.2\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n1 2 3 4\n3\nemitnuR\ndezilamroN\nFig. 5: Schematic illustration of a side view of the multi-\nL\ncav\nBeam Tunnel 2R\ntunnel\nR\nBeam Tunnel\nGap cav\nl\ngap\nBeam Tunnel\nR\ncenter\nBeam Tunnel\nNumber of Cores\nFig. 3: Normalized runtime for NEMESIS using the\ng\nparallelized three-dimensional Poisson solver. cente\nbeam cavity showing individual beam tunnels.\nThe inclusion of the 3rd harmonic is done via the\nfollowing model for the drive current, where I is the peak We consider an 8-beam MBIOT. A schematic\np\ncurrent, ε denotes the ratio of the 3rd harmonic to the illustration of the cross section of an MBIOT with six\nfundamental and ϕ is the phase shift of the 3rd harmonic beams is shown in Fig. 4 for clarity, and a side view of the\nrelative to the fundamental. cavity is shown in Fig. 5.\nWe refer to these simulations as whole-cavity\n2\n  t   t  simulations because we include all the beams in the"
  },
  {
    "id": 859,
    "text": "relative to the fundamental. cavity is shown in Fig. 5.\nWe refer to these simulations as whole-cavity\n2\n  t   t  simulations because we include all the beams in the\nI(t)= I sinπ +εsin3π +ϕ , (1) simulation including when they are within the beam\np   τ   τ \nwidth width tunnels or in the gaps. This contrasts with many\nsimulations of multi-beam IOTs and klystrons in which\nfor t < τ < 1/f and zero otherwise, where f is the wave\nwidth only a single beam is simulated and the result is then scaled\nfrequency and τ denotes the portion of the wave period\nwidth by the number of beams.\nover which electrons are drawn off the grid. The ratio of\nThe configuration consists of a multi-beam cavity tuned\nthe average to peak current using this model for the drive\nto 700 MHz with R/Q = 80 Ω and a loaded Q = 40 with a\ncurrent is\nradius of about 11.0 cm, a length of 9.144 cm, and with the\nI avg = 1+ε2 τ width . (2) gap center located 4.571 cm downstream from the entrance"
  },
  {
    "id": 860,
    "text": "current is\nradius of about 11.0 cm, a length of 9.144 cm, and with the\nI avg = 1+ε2 τ width . (2) gap center located 4.571 cm downstream from the entrance\nto the cavity. The optimal gap length and the optimal\nI 2 τ\np period position of the gap center were found through a multiple-\nparameter series of simulations. The optimal gap length\nwas found to be about 3.9 cm. The electron beam voltage\nand average current are 35.155 kV and 7.25 A (over the\nCavity eight beams) respectively. A solenoidal focusing field is\nWall used with an amplitude of 126 G which is close to the\nR Brillouin field when the electron cyclotron frequency Ω e =\ntunnel √2ω , where ω is the electron plasma frequency [15].\npe pe\nThe ratio of the average to peak current is 0.15. In the\nR absence of any 3rd harmonic drive, this implies that the\ncav\nratio of the width of the pulse to the resonant period (= 1/f,\nBeam\nwhere f = 700 GHz) is 0.30. Note that from Eq. (2) the ratio"
  },
  {
    "id": 861,
    "text": "R absence of any 3rd harmonic drive, this implies that the\ncav\nratio of the width of the pulse to the resonant period (= 1/f,\nBeam\nwhere f = 700 GHz) is 0.30. Note that from Eq. (2) the ratio\nof the average to peak current depends upon the resonant\nTunnel\nR period, the bunch width and the relative strength of the 3rd\ncenter\nharmonic. In this work we held I /I and the resonant\navg peak\nperiod fixed so that the bunch width varied with the relative\nstrength of the 3rd harmonic.\nThe variation in the performance versus the cavity radius\n(R ) is shown in Fig. 6 with (blue) and without (red) the\ncav\n3rd harmonic drive for a cathode radius (R ) of 1.05 cm\ncathode\nFig. 4: Schematic illustration the cross section of a 6-beam and a beam tunnel radius (R beamtun ) of 1.60 cm. Here we\nMBIOT. observe that the performance decreases with increasing\ncavity radius and we focus on a cavity radius of 11.0 cm."
  },
  {
    "id": 862,
    "text": "210000\n82\n205000\n80\n200000\n78\n195000\n76\n190000\n74\n185000\n10.5 11.0 11.5 12.0 12.5 13.0 13.5 14.0\n4\n)W(\nrewopP\nEfficiency\n(%)\nε = 1.0\nϕ = 3.14\nR = 1.60 cm\nbeamtun\nR = 1.05 cm cathode\nR/Q = 80\nQ = 40\nR (cm)\ncavity\nFig. 6: Variation in the output power and efficiency as\nfunctions of the cavity radius. The blue (red) line\ncorresponds to the case with (without) the 3rd\nharmonic drive.\nThe variation in the performance versus the beam tunnel\nradius is shown in Fig. 7 both with (blue) and without (red)\nthe 3rd harmonic drive and for a cavity radius of 11.0 cm\nand a cathode radius of 0.975 cm. It is clear from the figure\nthat the performance peaks for a beam tunnel radius of\nabout 1.60 cm (as used in Fig. 6). Hence, we will focus on\nthis beam tunnel radius.\n210000 82\n205000\n80\n200000 78\n195000 76\n190000\n74\n185000\n72\n180000 70\n175000\n1.20 1.30 1.40 1.50 1.60 1.70 1.80 1.90\n)W(\nrewoP\nEfficiency\n(%)\nε = 1.0\nϕ = 3.14\nR = 11.000 cm\ncav\nR = 0.975 cm\ncathode\nR/Q = 80\nQ = 40\nR (cm)\nbeamtun"
  },
  {
    "id": 863,
    "text": "195000 76\n190000\n74\n185000\n72\n180000 70\n175000\n1.20 1.30 1.40 1.50 1.60 1.70 1.80 1.90\n)W(\nrewoP\nEfficiency\n(%)\nε = 1.0\nϕ = 3.14\nR = 11.000 cm\ncav\nR = 0.975 cm\ncathode\nR/Q = 80\nQ = 40\nR (cm)\nbeamtun\nFig. 7: Variation in the output power and efficiency as\nfunctions of the beam tunnel radius. The blue (red)\nline corresponds to the case with (without) the 3rd\nharmonic drive.\n210000\n82\n205000 80\n200000\n78\n195000\n76\n190000\n74\n185000\n0.80 0.85 0.90 0.95 1.00 1.05 1.10\n)W(\nrewoP\nEfficiency\n(%)\nWe now consider the performance as a function of the\ncathode radius using the optimized values of the cavity\nradius (11.0 cm) and beam tunnel radius (1.60 cm). The\nvariation in the output power and efficiency versus the\ncathode radius is shown in Fig. 8 for both the absence (red)\nand presence of the 3rd harmonic drive (blue). We observe\nthat the use of the 3rd harmonic drive results in an increase\nin power of about 8.5%. The performance improves as the"
  },
  {
    "id": 864,
    "text": "and presence of the 3rd harmonic drive (blue). We observe\nthat the use of the 3rd harmonic drive results in an increase\nin power of about 8.5%. The performance improves as the\ncathode radius is increased up to a cathode radius if 1.05\ncm which is the largest radius that we consider for the\nMBIOT design. Overall efficiency of about 82% is\nindicated at this cathode radius when the 3rd harmonic is\napplied to the drive.\nIII. MBIOT COMPONENT DESIGN\nIn this section we discuss three elements of the MBIOT\ndesign: the grid, the input coupler and the output cavity.\nA. The Grids\nA program goal was to replace the Pyrolytic Graphite\n(PG) grid in the IOT by a molybdenum (moly) grid, which\nis much easier to fabricate, and still preserves the existing\nbeam optics. A moly grid will also operate at a lower\ntemperature due to its lower emissivity and provide a\nhigher design safety factor. However, it has a higher\nthermal expansion than PG; therefore, a detailed thermal"
  },
  {
    "id": 865,
    "text": "temperature due to its lower emissivity and provide a\nhigher design safety factor. However, it has a higher\nthermal expansion than PG; therefore, a detailed thermal\nand structural analysis of the electron gun was needed to\nensure its emitter-grid spacing at temperature, and beam\noptics of the PG gridded and moly gridded gun were\ncomparable.\nSimulating a PG grid is problematic because of its\northotropic properties and spherical shape. Its thermal\nconductivity in the ab plane is two orders of magnitude\nlarger than in the c plane. Its thermal expansion in the ab\nplane is almost one order of magnitude larger than in the c\nplane. Orthotropic materials require the ability to prescribe\nmaterial coordinates for the orthotropic parts in the model,\nso that proper coordinate transformation can be performed\nin the finite element formulation. Unfortunately, neither\nAnsys nor SolidWorks simulation provides the option to\nprescribe a spherical coordinate system. They permit only"
  },
  {
    "id": 866,
    "text": "in the finite element formulation. Unfortunately, neither\nAnsys nor SolidWorks simulation provides the option to\nprescribe a spherical coordinate system. They permit only\nCartesian and cylindrical coordinates. If either Ansys or\nε = 1.0 SolidWorks were to allow spherical coordinates, we will\nϕ = 3.14 just need to segment the existing grid into two parts, one\n8.5%\nfor the flat rim with a Cartesian coordinates, and one for\nR = 11.000 cm the spherical section with the spherical coordinates. Due to\ncav\nR = 1.60 cm this shortcoming, the PG grid is analyzed with two\nbeamtun\napproximations: (a) Orthotropic PG with cylindrical\nR/Q = 80\ncoordinates (b) Isotropic PG with ab plane properties. Q = 40\nFigure 9 shows the model of the grid with its support\nassembly. This model provides the grid displacement only.\nR (cm)\ncathode Thus, for the sake of hot spacing comparison between PG\nFig. 8: Variation in the output power and efficiency as and moly grids, one can think of a nondisplaced emitter as"
  },
  {
    "id": 867,
    "text": "R (cm)\ncathode Thus, for the sake of hot spacing comparison between PG\nFig. 8: Variation in the output power and efficiency as and moly grids, one can think of a nondisplaced emitter as\nfunctions of the cathode radius. The blue (red) line the referenced position. For the heat transfer model, the\ncorresponds to the case with (without) the 3rd copper stem is air cooled, the grid top radiates to a heat sink\nharmonic drive. kept at 150oC, and the grid bottom is radiated by the emitter"
  },
  {
    "id": 868,
    "text": "kept at 1050oC and loaded by the electron beam of 2.5 W. As mentioned above, the isotropic PG temperature at\nAir flow rate can be adjusted to obtain the same stem 560oC was very likely more realistic and comparable to the\ntemperature when the grid material is changed to moly. For moly grid temperature at 472oC. The center of the PG grid\nthe mechanical model, the bottom of the stem is kept fixed, moving away from a fixed emitter was in the range of 40.5\nand the grid rim is assumed to bond to the grid deck. Other μm to 78.0 μm, which was two orders of magnitude greater\npart interfaces are also assumed to be bonded. than that of the moly grid at 0.52 μm.\nFig. 9: CAD model of the grid and supports.\nWith approximated orthotropic PG, for which the\nmaterial, cylindrical coordinates were applied to the grid,\nthe c plane properties will have larger effects making the\ngrid less conductive and lower thermal expansion along the\nlateral surface. Temperature would be more uniform in the"
  },
  {
    "id": 869,
    "text": "the c plane properties will have larger effects making the\ngrid less conductive and lower thermal expansion along the\nlateral surface. Temperature would be more uniform in the\nspherical section; consequently, lower compression and\nlower axial displacement than those of approximated Fig. 10: Ansys model of the moly gridded gun. Only a\nisotropic PG. Due to the thinness of the grid, the isotropic quarter of the CAD model is used.\napproximation of pyrolytic graphite was more realistic than\nthe actual orthotropic properties with incorrect material\ncoordinates particularly for thermal calculation. However,\nthe isotropic formulation required positive Poisson’s ratio\nthat the PG ab plane did not. This was very likely leading\nto inaccurate stress and displacement.\nResults of the analysis for the model shown in Figure 9\nare summarized in Table 1. It shows the orthotropic PG\ngrid temperature being the highest at 886oC, and isotropic\nPG at 568oC.\nMaterial Max Tensile Stress Rim Center"
  },
  {
    "id": 870,
    "text": "are summarized in Table 1. It shows the orthotropic PG\ngrid temperature being the highest at 886oC, and isotropic\nPG at 568oC.\nMaterial Max Tensile Stress Rim Center\nTemp Strength (MPa) Displ Displ\n(°C) (MPa) (µm) (µm)\nOrtho 886 124 38.3 45.3 40.5\nPG\nIso PG 560 124 62.8 51.8 78.0\nMoly 472 430 215 43.5 0.52\nTable 1: Grid thermal and mechanical characteristics. The\nemittance structure is assumed to be fixed with\nzero displacement for comparison. The stress is\nFig. 11: Temperature of the moly gridded gun.\ncalculated at the screw holes.\n5"
  },
  {
    "id": 871,
    "text": "Since it was difficult to accurately obtain the hot spacing With the above hot dimension, we performed the beam\nof the PG grid to use as a benchmark design, we decided to optics simulation including optimization to achieve the\npursue the full analysis of the emitter and grid structures to required emission for moly grid.\nobtain the emitter-moly-grid spacing at temperature, and\nuse it to verify the beam optics. This model for the moly B. The Input Coupler\ngridded gun is shown in Fig. 10. The same boundary\nconditions as shown in Fig. 9 were used in addition to the The coupler was designed to simultaneously drive the\nboundary conditions being imposed at the OD of the grids of eight guns at both the fundamental (~700 MHz)\nemitter support. Only one quarter of the model is used in and the third harmonic (~2100 MHz). Initially it was hoped\nthe simulation to reduce computer time. To further reduce that the coupler would combine the frequencies through"
  },
  {
    "id": 872,
    "text": "the simulation to reduce computer time. To further reduce that the coupler would combine the frequencies through\nthe compute time, the radiation exchanges between the heat separate feeds. However, no practical geometry with two\nshields were analytically derived via circuit representation feeds was found, and it was decided to combine the feeds\nof the surface and geometrical resistances to scale the with an external coaxial hybrid T.\nsurface emissivity and area accordingly. The volumetric The design of the coupler was complicated by the basic\nheat generated in the emitter was iteratively prescribed to fact that the roots of the eigenfunctions of a circular cavity\nachieve the emitter surface temperature of 1050oC. The – Bessel functions – are not uniformly spaced. Thus, it is\ntemperature of the moly gridded gun structure is shown in not possible with a simple circular cavity to have both the"
  },
  {
    "id": 873,
    "text": "temperature of the moly gridded gun structure is shown in not possible with a simple circular cavity to have both the\nFig. 11. The corresponding axial displacement is shown in fundamental and third harmonic resonant at the same\nFig. 12, and its close up in Fig. 13. The emitter and grid frequency. With a 700 MHz fundamental in the TM\n01\nOD move closer to each other from their cold positions by mode, the TM mode will have a resonance at 2515 MHz.\n03\n105 μm. The emitter-grid ID spacing was also reduced, but This required the introduction of a structure to modify\nless, by 73.8 μm. the modes. The chosen arrangement was an annulus at the\nminimum of the third harmonic to primarily influence the\nfundamental. The geometry is shown in Fig. 14. There are\neight electron guns evenly arranged at a radius of 90.24\nmm (3.560 in). The gun grids are connected to the cavity\nby short coaxial lines, the inner conductors of which are\nhollow and terminate at the outer cavity wall. This allows"
  },
  {
    "id": 874,
    "text": "mm (3.560 in). The gun grids are connected to the cavity\nby short coaxial lines, the inner conductors of which are\nhollow and terminate at the outer cavity wall. This allows\nfor the heater leads and the air outlet of the grid cooling.\nThe design also includes tuners to adjust both resonances,\nallowing for the cavity to be used over a range of\nfrequencies.\nFig. 12: Axial displacement of the moly gridded gun.\nFig. 14: Geometry of the input cavity.\nThe result is a tunable cavity that provides the required\ngrid voltage (~62 V) with an input of 4 kW at the\nfundamental. This implies a gain of 20 dB. But a hybrid T\nin the input coax will reduce that gain by about 4 dB, so\nthe net gain will be about 16 dB. For the same input power,\nthe voltage at the third harmonic will be about 35 V.\nFig. 13: Close up of the axial displacement of the moly\ngridded gun.\n6"
  },
  {
    "id": 875,
    "text": "The cavity was simulated using ANSYS HFSS. As The output of the cavity is at the cathode potential. High\nshown in Fig. 14, the model included the actual coupling voltage isolation of the input was provided by including\ncavity and the electron gun grid-driving structure. The grid insulators as shown in Fig. 17.\nwas simulated using a lumped-load with an impedance With the insulator included, a scan was made of the\nprovided by the gun manufacturer. resonant frequency and grid voltage as a function of the\ncavity height and top and side tuners. A cavity radius of\n172 mm gave a tuning that is close to being centered on\n700 MHz. The results of scanning the side and top tuners\nare shown in Fig 18. With adjustment of both tuners,\nsimultaneous resonances at both the fundamental and 3rd\nharmonic can be achieved from 698 MHz to 703 MHz.\nFig. 15: Fundamental on the midplane of the cavity.\nFig. 18: Plot of the resonant frequencies as a function of"
  },
  {
    "id": 876,
    "text": "harmonic can be achieved from 698 MHz to 703 MHz.\nFig. 15: Fundamental on the midplane of the cavity.\nFig. 18: Plot of the resonant frequencies as a function of\nAt resonance, the complex magnitude of the RF electric\nthe top tuner height for several side tuner heights.\nfields is given in Fig. 15 and Fig. 16. As expected, (and\nThe cavity radius is 172 mm. Intersections of the\ndesired) the fields are azimuthally symmetric.\nfundamental and the (dashed) 3rd harmonic/3 lines\ngive the operating frequencies.\n703 25\n702 20\n701\n15\n700\n10\n699\n698 5\n697 0\n0 10 20\nFig. 19: Plot of the resonant frequencies of the points of\nFig. 16: Third harmonic on the midplane of the cavity.\nintersection from Fig. 18.\nFig. 20: Gun grid voltage with the cavity driven with 2 kW\nFig. 17: Quarter model of the approach for the isolation.\nat the fundamental.\nThe insulator is shown in yellow. The bottom\nplate (blue) is at ground potential while the top\nFigure 19 shows the intersection frequencies. Figures 20"
  },
  {
    "id": 877,
    "text": "at the fundamental.\nThe insulator is shown in yellow. The bottom\nplate (blue) is at ground potential while the top\nFigure 19 shows the intersection frequencies. Figures 20\n(red) is at the cathode potential.\nand 21 show the grid voltages for the parameter space. The\n7\nycneuqerF\ntnanoseR\n)zHM(\nthgieH\nrenuT\nediS\n)mm(\nResonant…\nTop Tuner Height (mm)"
  },
  {
    "id": 878,
    "text": "voltage at the fundamental is almost independent of the The RF Design of the eight-beam output cavity has been\ntuner heights. The third harmonic voltage is weakly created using HFSS. It features eight reentrant drift tubes\ndependent on the top tuner height, but has a significant positioned in a circle. The RF power is extracted from the\ndependence on the side tuner height. An input power of 4 output cavity through a center coaxial line. Fig. 22 shows\nkW at the fundamental will provide about 61 V required to the HFSS model of a 1/8th slice of the output cavity. The\nfully modulate the electron guns. The same power at the beam tunnel is in the middle of the slice. The cavity has\nthird harmonic will produce between 30 V and 45 V. eight tuning posts protruding from the outer cavity wall for\nadjusting the resonant frequency in cold test after brazing\nthe cavity. In Fig. 22 the tuning posts are sectioned by the\nsymmetry planes.\nFig 21. Gun grid voltage with the cavity driven with 2 kW"
  },
  {
    "id": 879,
    "text": "adjusting the resonant frequency in cold test after brazing\nthe cavity. In Fig. 22 the tuning posts are sectioned by the\nsymmetry planes.\nFig 21. Gun grid voltage with the cavity driven with 2 kW\nat the third harmonic.\nTo summarize, a resonant structure has been designed to\nprovide coupling from a single input to the eight electron\ngun grids. With appropriate adjustment of tuners, the\ncavity can be tuned from 698 MHz to 702 MHz. Four\nkilowatts is required to drive the gun grids at the\nfundamental.\nFig. 23: HFSS model of a 1/8th slice of the Output Cavity.\nThe normalized electric field vectors are shown on\na radial plane through the beam tunnel.\nThe resonant frequency of the output cavity is 700 MHz,\nthe external Q is 120, and the R/Q in the beam center is 185\nΩ (R/Q related to the stored energy in 1/8th of the cavity\nvolume). Figure 23 shows the HFSS model including the\nelectric field vectors on a radial plane through the beam\ntunnel."
  },
  {
    "id": 880,
    "text": "Ω (R/Q related to the stored energy in 1/8th of the cavity\nvolume). Figure 23 shows the HFSS model including the\nelectric field vectors on a radial plane through the beam\ntunnel.\nFig. 22: HFSS model of a 1/8th slice of the Output Cavity.\nC. The Output Cavity\nFig. 24: Simulated output cavity frequency and external Q\nversus length of the tuning stubs.\nFor the output cavity we chose a coaxial cavity design\noperating in the fundamental mode. A coaxial cavity\nThe main adjustment of the cavity resonant frequency\ndesign allows flexibility in the size of the “bolt circle”\nwill be accomplished by machining the cavity diameter.\nwhere the beams are placed and optimization of the electric\nThe fine-adjustment of the cavity frequency will be done\nfield (amplitude and uniformity) in the drift tube gap.\nin cold test after brazing the cavity. Figure 24 shows how\n8"
  },
  {
    "id": 881,
    "text": "the resonant frequency can be adjusted through the depth compared the “outside” edge of the beam. This is shown in\nof the tuning posts. The adjustment will also impact the the electric field contour plot in Fig. 26. Unfortunately,\nexternal Q. there are some limitations on how close to the cavity center\nThe external Q of the output cavity will then be adjusted the beams can be placed or how large the inner conductor\nthrough the length of the coax short, as shown in Fig. 25. can be made.\nWhile all cavity parameters are influenced by the various HFSS simulations with driven modal solution types were\ncold-tuning features, the frequency dependence on the performed to ensure that the RF electric field doesn’t\nlength of the coaxial short is small and has a negative slope. exceed acceptable limits. The results in Fig.27 show a\nThis will allow to essentially adjust frequency and external maximum electric field of 8.7 MV/m.\nQ independently."
  },
  {
    "id": 882,
    "text": "This will allow to essentially adjust frequency and external maximum electric field of 8.7 MV/m.\nQ independently.\nFig. 27: HFSS model showing a 1/16th slice of the output\ncavity. The results depict the electric field values\non the surfaces (and the symmetry plane in the\nFig. 25: Simulated resonant frequency and external Q of\nbackground). The enlarged inset shows the area\nthe output cavity versus length of the coaxial\nwith the highest voltage gradients on the radius of\nshort.\nthe upstream drift tube “nose”.\nAll major components and subassemblies of the MBIOT\nhave been modeled in Solidworks. Figure 28 shows the\nmodel of the output cavity and the bottom part of the\ncollector.\nFig. 28: Cross-sectional view (Solidworks model) of the\nFig. 26: Contour plot of the normalized electric field (1/8th MBIOT output cavity.\nslice) on a plane through the middle of the\ninteraction gap. The RF exits the MBIOT output cavity through a coaxial\nline and a coaxial output window. The RF design of the"
  },
  {
    "id": 883,
    "text": "slice) on a plane through the middle of the\ninteraction gap. The RF exits the MBIOT output cavity through a coaxial\nline and a coaxial output window. The RF design of the\nUsing a coaxial line in the center of the output cavity output window and the transition to a standard 4-1/8” coax\nensured that the RF electric fields at all eight beam was created using the code CASCADE. Figure 29 shows\nlocations are the same, thus allowing uniform extraction of the output window geometry in a cross-sectional view. The\nthe RF power from all the beams. There is however some chart in Fig, 30 shows a low voltage standing wave ratio\nnon-uniformity within the area of each beam since the\nelectric field is higher towards the “inside” edge of the\nbeam (beam edge closer to the center of the cavity)\n9"
  },
  {
    "id": 884,
    "text": "(VSWR) for this window design at the 700 MHz operating The program also investigated a moly grid to replace the\nfrequency. existing PG grid used in IOT guns. The analysis showed\nthat the moly grid would operate at a lower temperature,\nthough compensation is required for the difference in\nthermal expansion.\nFinally, we designed an output cavity to extract energy\nfrom the eight electron beams and produce RF power\nextracted through a coaxial window on the tube axis. The\ndesign incorporates tuning plugs to ensure precise\nfrequency control in the assembled output cavity.\nThis device, if successfully built and tested, would\nrepresent a significant advanced in RF source technology\nin this frequency range. It would provide a higher\nefficiency, more compact, and lower cost alternative to\nFig. 29: RF Output Window geometry. existing RF sources.\nACKNOWLEDGEMENTS\nThis material is based upon work supported by the\nU.S. Department of Energy, Office of Science, Office of"
  },
  {
    "id": 885,
    "text": "Fig. 29: RF Output Window geometry. existing RF sources.\nACKNOWLEDGEMENTS\nThis material is based upon work supported by the\nU.S. Department of Energy, Office of Science, Office of\nHigh Energy Physics, under Grant No. DE-SC0019800.\nREFERENCES\n[1] H.P. Freund, R.L. Ives and W.D. Sessions, High-\nEfficiency Inductive Output Tubes Using a Third\nFig 30: RF Output Window, VSWR versus frequency\nHarmonic Drive on the Grid, IEEE Trans. Plasma Sci.\nsimulated using CASCADE.\n48, 3827 (2020).\n[2] W. Sessions and H.P. Freund, US Patent 10734182,\nIV. SUMMARY AND CONCLUSION Ultra-High Efficiency Single-Beam and Multi-Beam\nInductive Output Tubes (awarded 4 August 2020).\nIn this paper, we have described whole-cavity [3] P. Sprangle, J. Peñano, B. Hafizi, D. Gordon, S. Gold,\nsimulations with NEMESIS and component design for an A. Ting, and C. Mitchell, “High average current\n8-beam MBIOT with the inclusion of a 3rd harmonic electron guns for high-power free electron lasers,”"
  },
  {
    "id": 886,
    "text": "8-beam MBIOT with the inclusion of a 3rd harmonic electron guns for high-power free electron lasers,”\ncomponent on the drive. The numerical formulation is an Phys. Rev. ST-AB vol. 14, 020702 (2011).\noutgrowth of the NEMESIS simulation code [1.9] which [4] Klystrode® is a registered trademark synonymous\nhas been extended by the incorporation of a three- with the CPI Eimac-brand IOT.\ndimensional Poisson solver based upon the Petsc package\n[5] D.M. Priest and M. Shrader, The klystrode-an unusual\navailable from Argonne National Laboratory.\ntransmitting tube with potential for UHF-TV, Proc.\nIt was found that the effect of the 3rd harmonic on the\nIEEE, vol. 70, pp. 1318-1325, Nov. 1982.\nefficiency is greatest when the phase of the 3rd harmonic is\nshifted by π radians with respect to the fundamental drive [6] J.C. Whitaker, Power Vacuum Tubes Handbook. New\nYork: CRC Press, 1999.\nsignal and with 3rd harmonic powers greater than about"
  },
  {
    "id": 887,
    "text": "shifted by π radians with respect to the fundamental drive [6] J.C. Whitaker, Power Vacuum Tubes Handbook. New\nYork: CRC Press, 1999.\nsignal and with 3rd harmonic powers greater than about\n50% that of the fundamental drive power [9]. For the [7] A.S. Gilmour, Microwave Tubes. Dedham, MA:\npresent example, we show that efficiencies approaching Artech House 1986.\n82% are possible by this means and that improvements in [8] R.J. Barker, J.H Booske, N.C. Luhman, and G.S.\nthe performance due to the 3rd harmonic can reach 8.5%. Nusinovich, eds., Modern Microwave and Millimeter\nWe remark that the NEMESIS formulation with the Wave Power Electronics. Piscataway, NJ: IEEE\nincorporation of the three-dimensional Poisson solver Press/Wiley Interscience, 2005.\nallows for the whole-cavity simulation of multi-beam [9] H.P. Freund, W.H. Miner, Jr., J. Verboncoueur, Y. Li,\nklystrons as well. Application of NEMESIS to the and E. Wright, Time-domain simulation of inductive"
  },
  {
    "id": 888,
    "text": "klystrons as well. Application of NEMESIS to the and E. Wright, Time-domain simulation of inductive\nsimulation of a multi-beam klystron under development at output tubes, IEEE Trans. Plasma Sci., vol. 35, pp.\nCalabazas Creek Research will be reported in a future 1081–1088 (2007).\npaper.\n[10] A. Beunas, C. Bel, C. Robert, J.C. Racamier, D.\nWe also present the design and analysis of a compact\nBussiere, D. Bariou, K.H. Khalifa, and V. Hermann,\ninput coupler suitable for both fundamental and 3rd\nManufacturing and performance results of a 1.2 MW\nharmonic drive of the electron gun grid. The input coupler\npeak, 704 MHz, multi-beam inductive output tube,\nis tunable from 698-703 MHz.\npaper presented at the 2017 18th International Vacuum\n10"
  },
  {
    "id": 889,
    "text": "Electronics Conference, London, United Kingdom, 24 Thuc Bui received his M.S.\n– 26 April, 2017. from the University of\n[11] M. Jensen et al., High power RF sources for the ESS California at Berkeley and\nRF systems, Proceedings of the 27th Linear Engineer degree in Applied\nAccelerator Conference, Geneva, Switzerland, 31 Mechanics from Stanford\nAugust – 5 September, 2014, pp. 2014. University. He performed\nextensive research in finite\n[12] www.anl.gov/mcs/petsc-portable-extensible-toolkit-\nelement methods, developed\nfor-scientific-computation.\nand implemented three-\n[13] H.G. Kosmahl and G.M. Branch, Generalized dimensional, linear and\nrepresentation of the electric fields in interaction gaps\nquadratic tetrahedral and\nof Klystrons and traveling wave tubes, IEEE Trans.\nhexahedral elements to solve\nElectron Devices, vol. ED-20, pp. 621-629, July 1973.\nproblems in elasticity,\n[14] C.K. Birdsall and A.B. Langdon, Plasma Physics via compressible fluids, supersonic viscous flows,"
  },
  {
    "id": 890,
    "text": "Electron Devices, vol. ED-20, pp. 621-629, July 1973.\nproblems in elasticity,\n[14] C.K. Birdsall and A.B. Langdon, Plasma Physics via compressible fluids, supersonic viscous flows,\nComputer Simulation, New York: McGraw-Hill, electrostatic, magnetostatic and Helmholtz fields. He uses\n1985, p. 59. object-oriented programming techniques, design patterns\nR.C. Davidson, Theory of Nonneutral Plasmas, Reading, and finite element methods to solve charged particle beam\nMA: Benjamin, 1974, p. 7. problems. He is the principal author of Beam Optics\nAnalyzer (BOA) and was responsible for the development\nDr. H.P. Freund is a theoretical plasma physicist currently of the particle pusher, emission algorithms, Poisson and\nstudying coherent radiation sources such as free-electron Curl-Curl field solvers. The particle pusher and the field\nlasers, traveling wave solvers, using the finite element method, are intimately\ntubes, klystrons and integrated with the framework originally developed by"
  },
  {
    "id": 891,
    "text": "lasers, traveling wave solvers, using the finite element method, are intimately\ntubes, klystrons and integrated with the framework originally developed by\ninductive output tubes, and RPI, Simmetrix and further improved by Mr. Bui to utilize\ncyclotron, Cerenkov, and its novel and world class adaptive meshing facility. For the\norbitron masers by both Curl-Curl solver in magnetostatic and Helmholtz\nanalytical and numerical problems, he implemented the vector finite element\nmethods. He has published method to strongly enforce the tangential continuity of the\nmore than 180 papers in field. In collaboration with NCSU, Mr. Bui implemented\nrefereed journals, made the Monte Carlo method with a secondary emission model\nnumerous contributions to of elastically backscattered electrons from the first\nbooks and published principle. He then extended this model to simulate the\nproceedings, and coauthored a book entitled Principles of power density field inside the solid due to backscattered"
  },
  {
    "id": 892,
    "text": "books and published principle. He then extended this model to simulate the\nproceedings, and coauthored a book entitled Principles of power density field inside the solid due to backscattered\nFree-electron Lasers [Springer, Cham, Switzerland, 2018, electrons for heat transfer analysis. He also used iterative\n3rd edition]. In addition to this scholarly activity, Dr. methods Nelder-Mead and Implicit Filtering in BOA to\nFreund has also made contributions to more popular optimize the emitter shape, size, electrode spacing and\nscientific literature with contributions on free-electron focusing magnetic field to achieve to desired current and\nlasers published in Scientific American magazine and the beam shape. He has recently completed a parallelized 3D\nAcademic Press Encyclopedia of Science and Technology. Poisson solver using the finite difference method in\nThe article in Scientific American has been translated and Nemesis to push particles in a multiple beam devices."
  },
  {
    "id": 893,
    "text": "The article in Scientific American has been translated and Nemesis to push particles in a multiple beam devices.\nrepublished in Veda a Technika (Science and Technology),\nwhich was published by the Czechoslovak Academy of\nSciences. Dr. Freund has extensive experience in the Dr. R. Lawrence Ives (M’82-SM’90-LM’16) received his\nsimulation of microwave tubes. He has developed Ph.D. from N.C. State University in plasma physics in\nimportant simulation codes for treating free-electron lasers 1984 and began his microwave career in the Gyrotron\nand masers, cyclotron masers and gyrotrons, traveling Department at Varian Associates, Inc. in Palo Alto, CA. In\nwave tubes, and klystrons and inductive output tubes. that position he was responsible for designing electron\nguns, gyrotron circuits, collectors, and waveguide\ncomponents.\n11"
  },
  {
    "id": 894,
    "text": "He founded Calabazas His graduate work was on the generation and propagation\nCreek Research, Inc. in of Intense Relativistic Electron Beams (IREBs). Following\n1994, which is involved in his postdoctoral work, Dr. Read worked at the Naval\nsoftware development, Research Laboratory (NRL)\nmicrowave tube and and from 1983 to 1986 was\ncomponent design, and head of the Gyrotron\nuses of microwave power\nOscillators and Plasma\nfor environmental,\nInteractions Section of\nresearch, defense, and\nPlasma Physics Division. In\nheating applications. Dr.\n1986, he became a Principal\nIves was principal\nResearch Engineer with\ninvestigator on programs\nPhysical Sciences Inc., where\nto develop long life, high\nhe was responsible for research programs on microwave\ncurrent density cathodes, long life, high quantum\nand electron beam generation and their applications. In\nefficiency photocathodes, high power RF windows, and\n1999, Dr. Read joined Calabazas Creek Research, where"
  },
  {
    "id": 895,
    "text": "and electron beam generation and their applications. In\nefficiency photocathodes, high power RF windows, and\n1999, Dr. Read joined Calabazas Creek Research, where\nmultiple beam klystrons. Current areas of research include\nuse of Atomic Layer Deposition to reduce cooling channel he is responsible for electron gun and high power\ncorrosion, direct coupled gyrotrons, multiple beam triode microwave device research. He is currently Chief Scientist\namplifiers, and application of additive manufacturing to at CCR.\nRF sources and components.\nDr. Ives also provides consulting support to several\ncommercial companies on electron guns, cathodes, and RF\nsource development.\nDr. Habermann received a Ph.D. (Dr. rer. nat.) in Physics\nfrom the University of\nWuppertal (Germany) in\n1999, where he conducted\nresearch in field emission.\nHe investigated carbon\nnanotubes, CVD diamond\nthin films, diamond-coated\nsilicon microstructures for\ncold cathode applications,\nas well as researched the\nbasic mechanisms of"
  },
  {
    "id": 896,
    "text": "He investigated carbon\nnanotubes, CVD diamond\nthin films, diamond-coated\nsilicon microstructures for\ncold cathode applications,\nas well as researched the\nbasic mechanisms of\n'parasitic' field emission\nimpacting superconducting RF cavities used in particle\naccelerators. From 2000 to 2019 he worked at\nCommunications and Power Industries in Palo Alto, CA,\non the development and manufacturing of klystrons. Dr.\nHabermann managed multiple programs to develop,\nproduce and deliver prototypes or initial production runs of\nnew vacuum electron devices. He contributed to numerous\nprograms within the company and provided engineering\nsupport in production. In 2019 Dr. Habermann joined\nCalabazas Creek Research where he is supporting various\nR&D programs. He is also responsible for the company’s\nQuality Assurance.\nDr. Read received his Ph.D. in 1975 from Cornell\nUniversity in Electrical Engineering and Plasma Physics.\n12"
  },
  {
    "id": 897,
    "text": "HITS DIFF : From Truth Discovery to Ability\nN S\nDiscovery by Recovering Matrices with the\nConsecutive Ones Property\nZixuan Chen Subhodeep Mitra R. Ravi Wolfgang Gatterbauer\nNortheastern University Google Carnegie Mellon University Northeastern University\nchen.zixu@northeastern.edu mitradeep1@gmail.com ravi@andrew.cmu.edu w.gatterbauer@northeastern.edu\nis to come up with meaningful MCQs (including question Abstract—We analyze a general problem in a crowd-sourced\nsetting where one user asks a question (also called item) and stems and choices) related to the topics of the class and\nother users return answers (also called labels) for this question. answerthequestionsfromothers.Inthisway,theinitiativeof\nDifferent from existing crowd sourcing work which focuses on and interactions between students are encouraged, and their\nfinding the most appropriate label for the question (the “truth”),\nperformances can be used as another important measure"
  },
  {
    "id": 898,
    "text": "finding the most appropriate label for the question (the “truth”),\nperformances can be used as another important measure\nour problem is to determine a ranking of the users based on\n(e.g.,a“participation”score)towardsthegradeassessment,\ntheir ability to answer questions. We call this problem “ability\ndiscovery” to emphasize the connection to and duality with the in addition to traditional exam scores. To assess students,\nmore well-studied problem of “truth discovery”. Kiyanafirstsimplycountshowmanytimesastudentanswers\nTo model items and their labels in a principled way, we draw questions, which is the traditional way for an instructor to\nupon Item Response Theory (IRT) which is the widely accepted\ngive a participation score for students in a forum. However,\ntheory behind standardized tests such as SAT and GRE. We\nin this way, the grades are biased towards students who\nstart from an idealized setting where the relative performance"
  },
  {
    "id": 899,
    "text": "theory behind standardized tests such as SAT and GRE. We\nin this way, the grades are biased towards students who\nstart from an idealized setting where the relative performance\nof users is consistent across items and better users choose better answeralotofquestionsrandomly.Thesecondattemptisto\nfittinglabelsforeachitem.Wepositthataprincipledalgorithmic require all students to answer the same number of questions\nsolution to our more general problem should solve this ideal andrankthembyhowmanyquestionstheyanswercorrectly,\nsetting correctly and observe that the response matrices in this\nwhich still requires a lot of efforts for her to figure out\nsetting obey the Consecutive Ones Property (C1P). While C1P is\nall the correct answers and suffers from the problem that\nwellunderstoodalgorithmicallywithvariousdiscretealgorithms,\nwe devise a novel variant of the HITS algorithm which we call eachquestionhasquitedifferentdifficulties.Kiyanawonders"
  },
  {
    "id": 900,
    "text": "wellunderstoodalgorithmicallywithvariousdiscretealgorithms,\nwe devise a novel variant of the HITS algorithm which we call eachquestionhasquitedifferentdifficulties.Kiyanawonders\n“HITSNDIFFS” (or HND), and prove that it can recover the whether there is a more principled way for ranking students\nidealC1P-permutationincaseitexists.Unlikefastcombinatorial by their abilities to answer questions correctly.\nalgorithms for finding the consecutive ones permutation (if it\nexists), HND also returns an ordering when such a permutation\nExample2(Crowdworkersranking). Daiyuwantstorelease\ndoes not exist. Thus it provides a principled heuristic for our\nproblemthatisguaranteedtoreturnthecorrectanswerintheideal a human intelligence task which consists of a set of ques-\nsetting.OurexperimentsshowthatHNDproducesuserrankings tions at a crowdsourcing platform like Amazon Mechanical\nwith robustly high accuracy compared to state-of-the-art truth Turk [1]. Suffering from low-quality answers from the crowd"
  },
  {
    "id": 901,
    "text": "with robustly high accuracy compared to state-of-the-art truth Turk [1]. Suffering from low-quality answers from the crowd\ndiscoverymethods.WealsoshowthatournovelvariantofHITS\nworkers, she wonders whether there is a better way to select\nscales better in the number of users than ABH, the only prior\ntopcrowdworkersinsteadofsimplysettingthresholdsforthe\nspectral C1P reconstruction algorithm.\nIndex Terms—truth discovery, item response theory, consecu- number of tasks they have finished or finished successfully.\ntive ones property\nThe above examples motivate our problem of “ability dis-\ncovery” which ranks the users (students/workers) based on\nI. INTRODUCTION\ntheir abilities to answer questions correctly.\nMotivation. We first present a couple of examples from a\nThe ability discovery problem. We have m users and n\nclass to a more general crowdsourcing context. items.1 Each item has up to k labels,2 and the labels usually"
  },
  {
    "id": 902,
    "text": "The ability discovery problem. We have m users and n\nclass to a more general crowdsourcing context. items.1 Each item has up to k labels,2 and the labels usually\nExample1(Studentranking). Kiyana,aninnovativeinstruc- varybetweenitems(theitemsarethus“heterogeneous”).Each\ntor for an online class who suffered from the leakage of user chooses up to one label to each of the items, and two\nprevious exam questions and difficulty of creating new ones, users may choose the same label to the same item. Our goal\nnoticesalotofinteractionsinstudentforumslikePiazza[2] is to derive a principled way for determining a ranking of the\nand pilots a new learning approach. Since students are users in terms of their ability to pick correct labels for the\nwilling to ask and answer questions, Kiyana aims to utilize items based solely on the user responses.\nsuch communication for both practice and assessment of the"
  },
  {
    "id": 903,
    "text": "willing to ask and answer questions, Kiyana aims to utilize items based solely on the user responses.\nsuch communication for both practice and assessment of the\nclass by requiring students to suggest and answer multiple- 1Weuseitem/questionandlabel/option/answer/choiceinterchangeably.\nchoice questions (MCQs) themselves. The task of students 2In other words, the item(s) with the most unique labels has k different\nlabels.Labelscanbeproposedeitherfromquestionersoranswerers.\n1\n3202\nceD\n12\n]IS.sc[\n1v31000.1042:viXra"
  },
  {
    "id": 904,
    "text": "Connection to truth discovery. Ability discovery can be theory[4],[22],[29].Wearguethatanyprincipledsolutionto\nconsidered a dual problem of the widely studied truth discov- ranking users by their abilities should be able to recover the\neryproblem[79].Thesetupissimilar;thedifferenceisthatthe correct ranking when responses are consistent with abilities.\ntruth discovery problem measures success in finding the truth (2) We propose a novel yet simple adaptation of the HITS\n(thus the correct label for each item) whereas our problem algorithm [31] that we call HITSNDIFFS (HND) for ranking\nfocuses on finding the correct ranking of the users by their users based on their abilities. We prove the surprising result\nrelativeabilities.Whilethetruthdiscoveryproblemoccursina that HND recovers the consecutive ones ranking of users\nwiderangeofproblemsrelatedtocrowdsourcingandhasbeen when a unique such order exists. Unlike fast combinatorial"
  },
  {
    "id": 905,
    "text": "widerangeofproblemsrelatedtocrowdsourcingandhasbeen when a unique such order exists. Unlike fast combinatorial\nofintensefocusforthedatamanagementcommunity[79],the algorithms for finding the C1P ordering if it exists, HND\nability discovery problem gets little attention and is usually can also deal with the general case when no such order\ntreated as a sub-problem: if one knows the truth, it is easier exists. This makes HND an ideal candidate for our problem\nto rank the users based on the choices they make. In turn, if (and even becomes an exact algorithm in special settings).\none knows the order of user abilities, it is easier to determine We compare HND against ABH [4], which is the only other\nthe truth. However, we show in Section IV that it is not spectral approach that has these properties, and give intuitive\nstraightforward to rank users correctly, even when given the and experimental evidence for why HND performs better."
  },
  {
    "id": 906,
    "text": "straightforward to rank users correctly, even when given the and experimental evidence for why HND performs better.\ncorrect answers to the questions beforehand, which means (3)WeshowhowItemResponseTheory(IRT)[64],whichis\neven the perfect truth discovery method is not guaranteed to widely deployed in educational testing, provides a natural and\nperform well for the ability discovery problem. Furthermore, mathematically principled theory (including generative mod-\nwe argue that ability discovery is much more than a sub- els)formodelingheterogeneousitemrankingthatincludesthe\nproblem of truth discovery and highlight its importance in C1P as a special case of consistent responses.\ntwo aspects: 1) It has different application scenarios as we (4) We conduct extensive experiments on synthetic datasets\ndiscussed in the examples. 2) Different from correctness of generated by 3 polytomous IRT models and show that HND\nanswers,userabilitiesareabstractandcannottobemeasured"
  },
  {
    "id": 907,
    "text": "discussed in the examples. 2) Different from correctness of generated by 3 polytomous IRT models and show that HND\nanswers,userabilitiesareabstractandcannottobemeasured\ncan outperform other existing truth discovery approaches in\ndirectly, which makes ability discovery results valuable but terms of accuracy of the user ranking. We also show (both\nalso hard to evaluate with no acknowledged ground truth. in theory and with experiments) that HND has better scala-\nAssumptions. Similar as in truth discovery, we assume an bility and accuracy than ABH (which is the only other C1P\nobjective total order on the labels of each item based on their reconstruction approach known today that can be used for the\ncorrectness,andatotalorderontheusersbasedontheirlatent general ranking problem).\none-dimensional abilities for choosing the correct labels.\nOutline. Section II defines our problem, draws the connec-\nOur approach. We first define an idealized scenario in"
  },
  {
    "id": 908,
    "text": "one-dimensional abilities for choosing the correct labels.\nOutline. Section II defines our problem, draws the connec-\nOur approach. We first define an idealized scenario in\ntiontotheC1Pproperty,andintroducesIRTasgeneralization\nwhich the user responses are consistent with their abilities\nof C1P. Section III introduces our approach, gives its formal\nacross the items and characterize it as the response matrix\npropertiesandcomparesittocloselyrelatedwork.SectionIV\nhavingtheConsecutiveOnesProperty(C1P).Wethensuggest\npresents experiments. Section V discusses additional related\nanefficientspectralmethodthatwecallHITSNDIFFS(HND)\nwork on truth discovery before Section VI concludes. Our\nfor reconstructing such ideal orderings if they exist. We prove\ncode is available online [9].\nthat in the ideal error-free scenario (better users always make\nbetter choices) our method is guaranteed to find the correct\nII. FORMALSETUP\nranking, which puts our approach on a stronger theoretical"
  },
  {
    "id": 909,
    "text": "that in the ideal error-free scenario (better users always make\nbetter choices) our method is guaranteed to find the correct\nII. FORMALSETUP\nranking, which puts our approach on a stronger theoretical\nfooting as a cross between a heuristic and an exact algorithm.\nA. Ability discovery problem formulation\nImportantly, our method generalizes to the general non-ideal\ncase and allows us to compare it with existing truth discovery Consider a setting with m users choosing among k options\nmethods for ranking users. One key innovation in our work foreachofnitems.Itemsareheterogeneousinthattheyhave\nis the use of Item Response Theory (IRT) [64] to model both different options, as is the case in MCQs used in standardized\nlabel rankings as well as the propensity of users of different testsettings(seeFigure1a).Thissetupisdifferentfromtypical\nabilities to choose such labels, and the previously not made multiclass classification [12] where all n items have the same"
  },
  {
    "id": 910,
    "text": "abilities to choose such labels, and the previously not made multiclass classification [12] where all n items have the same\nconnection to C1P. We utilize 3 different generative models class of k labels. To emphasize the difference, we refer to our\nfrom the IRT literature to generate realistic synthetic data. setup as heterogeneous multiclass classification.\nExperiments on these data show that (i) our new method is User responses can be represented in one-hot encoding as\nmoreaccuratethanexistingtruthdiscoverymethods,and(ii)it a (m×kn) binary response matrix C (see Figure 1b) where\ncan also serve as scalable approach that reconstructs the C1P each row represents the choices of a user and each column\norder if it exists and generalizes much better on non-ideal represents an option for some item. The number of non-zeros\ninputs than the only other C1P order reconstruction method in C is mn and the number of non-zeros in each row n."
  },
  {
    "id": 911,
    "text": "inputs than the only other C1P order reconstruction method in C is mn and the number of non-zeros in each row n.\nthat works in the non-ideal scenario. Our goal is to rank the users by their abilities to choose\nContributions. (1) We connect the notion of consistent the “best” options for each item. Each user u has a latent\nj\nresponses in heterogeneous multiclass classification to the one-dimensional selection ability θ that represents the user’s\nj\nwell-known Consecutive Ones Property (C1P) from seriation ability to choose the best of options for each item.\n2"
  },
  {
    "id": 912,
    "text": "Definition 1 (Ability discovery). Given m users and their\nchoicesamongk options3 fornheterogeneousitemsasbinary\nresponse matrix C. Rank the m users by their abilities to\ncorrectly choose options for each item.\nSeveralapproachesonhomogeneousitemsassumetheprob-\nabilitiesofusersgettingacorrectanswertobeidenticalacross\nquestions and encode user abilities as a (k×k) dimensional\nconfusion matrix per user [12], [79]. In our setting, this is\n1 2 3\nnot the case: each option h for item t i may have a different 1 A A A\n“discrimination score” α . In general, the higher α for 2 A A C\nih ih\noption h is, the more “discriminating” it is (the option’s prob- 3 A B C\n4 B C C\nability of being chosen more quickly increases with student\nability). The probability of answering a question correctly is\nthen a function of the user ability and all the question option\ndiscrimination scores. This setup builds upon Item Response\nTheory (IRT) [39], [64], summarized in detail in Section II-D."
  },
  {
    "id": 913,
    "text": "then a function of the user ability and all the question option\ndiscrimination scores. This setup builds upon Item Response\nTheory (IRT) [39], [64], summarized in detail in Section II-D.\nExample 3. Figure 1a shows m=4 users answering n=3\nMCQs. Each question has k=3 choices labeled A to C in\ndecreasingorderoffit.Figure1bshowsanexampleresponse\nmatrix C′ and its binary form C. Assuming that users’\nchoices are “consistent” with their abilities (i.e. correctness\nof labeling increases with ability), the only possible ranking\nof users for the observed C′ is 1, 2, 3, 4, or its reverse.\nFigure 1c illustrates an IRT model between the latent user\nabilities and the probability of picking the correct answer\nwhen the correct ranking is 1, 2, 3, 4. For example, user u 2\nhastheabilitytolabelitemst andt correctly,thususeru 1 2 2\nchooses the correct answer A for both items. Our problem is\nto rank the users by their mastery of the subject based only\non the users’ choices without knowing the true labels."
  },
  {
    "id": 914,
    "text": "chooses the correct answer A for both items. Our problem is\nto rank the users by their mastery of the subject based only\non the users’ choices without knowing the true labels.\nB. The ideal case with consistent responses\nWe call a response matrix to be “consistent” if there is an\nunambiguousorderingofusersaccordingtotheirabilitiesthat\nis reflected in their responses across the items. This way, the\nability is a unique skill that is tested across their responses\nto all items. In this ideal case, if a user u chooses a better\n1\noption for an item t than user u , then u must also choose\n1 2 1\nan equal or better option for any other item t to reflect this\n2\nconsistency. This implies that there is an implicit ordering\namong the choices for each item from best to worst and the\nbetter users pick better options for every item.\nMore formally, assume that the user abilities θ are all\nj\ndistinct, and also that for every item t , the discrimination\ni"
  },
  {
    "id": 915,
    "text": "better users pick better options for every item.\nMore formally, assume that the user abilities θ are all\nj\ndistinct, and also that for every item t , the discrimination\ni\nα over all the options are distinct, then there is a unique\nih\nlinear ordering of the users, and of the options for each item.\nDefinition 2 (Consistent Responses). A response matrix C is\nconsistent if there exists an assignment of user abilities θ and\nitem discriminations α, s.t. for any pair of users u and u\n1 2\nwith θ >θ , and for any item t where u chooses option h\n1 2 i 1 1\nand u chooses option h , we have α >α .\n2 2 ih1 ih2\n3TosimplifythediscussionanddifferentfromSectionI,weassumehere\nthateachitemhasexactlyk choices.Foritemswithk′<k choices,wecan\nassumethemtohavek−k′ morechoicesandnobodychoosingthem.\nsresu\n4=m\nResponse matrix C' Binary response matrix C\nn=3 items k⋅n=9 item choices\n1A 1B1C2A 2B2C3A 3B3C\n1 1 0 0 1 0 0 1 0 0\n2 1 0 0 1 0 0 0 0 1\n3 1 0 0 0 1 0 0 0 1\n4 0 1 0 0 0 1 0 0 1\nProbability 1 t 1 t 2 t 3"
  },
  {
    "id": 916,
    "text": "n=3 items k⋅n=9 item choices\n1A 1B1C2A 2B2C3A 3B3C\n1 1 0 0 1 0 0 1 0 0\n2 1 0 0 1 0 0 0 0 1\n3 1 0 0 0 1 0 0 0 1\n4 0 1 0 0 0 1 0 0 1\nProbability 1 t 1 t 2 t 3\nof picking\nthe correct\nanswer A 0\nu 4 u 3 u 2 u 1 User ability\nsresu\n4=m\nBipartite response graph\nm=4 users 1 2 3 4\n(a)\nk=3 item choices 1A 1B 1C 2A 2B 2C 3A 3B 3C\nn=3 items 1 2 3\n(b)\n(c)\nFig. 1: (a) Ability discovery problem: m=4 users choose one from\nk=3choicesoflabelsA,B,Cforeachofn=3items.(b)Input:the\n(m×k)responsematrixC′,orequallyitsflattened(m×kn)binary\nresponsematrix C.(c) Model:the probabilityof pickingthe correct\nanswer in terms of the user ability for Items 1,2,3. The abilities of\nall 4 users are marked on the horizontal axis.\nC. Relation to Consecutive ones Property (C1P)\nWe observe that consistent response matrices, when row-\nsorted according to user abilities, satisfy a widely studied\nordering property in seriation, called the consecutive ones\nproperty (C1P) [4], [22], [29]. We follow the notation from"
  },
  {
    "id": 917,
    "text": "sorted according to user abilities, satisfy a widely studied\nordering property in seriation, called the consecutive ones\nproperty (C1P) [4], [22], [29]. We follow the notation from\nseriation theory and call it a P-matrix.\nDefinition 3 (C1P, P-matrix & pre-P-matrix [4]). A binary\nmatrixsatisfiesC1PandiscalledaP-matrixifineachcolumn,\nall the 1’s are consecutive. If the rows of a matrix can be\npermuted so it becomes a P-matrix, we call it a pre-P-matrix.\nIn other words, no 0’s appear between any two 1’s in\na column in a P-matrix (see C in Figure 1b). To see that\nconsistent responses with users sorted by abilities θ give a P-\nmatrix, suppose for a contradiction that a column correspond-\ning to an option for an item has two or more blocks of ones.\nThen the users corresponding to the zeros in between these\nblocks will have chosen another option for which the quality\nis strictly higher or lower than that of this option since the\noption qualities are assumed to be distinct. But this violates"
  },
  {
    "id": 918,
    "text": "blocks will have chosen another option for which the quality\nis strictly higher or lower than that of this option since the\noption qualities are assumed to be distinct. But this violates\nconsistency since the rows are ordered by user ability.\nObservation1(ConsistentResponsesimplyC1P). Aresponse\nmatrix C is consistent iff it is a pre-P-matrix.\nConsequently, ranking the users in a scenario of consistent\nresponses corresponds to the problem of determining a per-\nmutation of the rows of C so that the result obeys C1P.\nState-of-the-art on C1P. Booth and Leuker [7] (“BL”)\nproposed the fastest known algorithm for finding all possible\npermutations of the rows that reconstruct the C1P ordering in\ntime linear in the number of nonzero entries in the matrix,\n3"
  },
  {
    "id": 919,
    "text": "taking time O(mn) in our setting. However, their method\nDichotomous (k=2) Polytomous (k>2)\nfails to produce an ordering of the rows when the matrix\nGLAD 1 1PL1 GRM k-1\nis not a pre-P-matrix, and therefore cannot be used as a k=2\ngeneral heuristic for simulated or real-world datasets that are b=0 a=1\nnot ideal. In contrast, Atkins et al. [4] (“ABH”) proposed k=2\n2PL 2 Bock 2(k-1)\nan elegant spectral method to determine whether a matrix\nobeys C1P, thus giving a rare continuous method to solve c=0 b 0 =-∞\nk=2 & c=1/k\na combinatorial ordering problem. Moreover, it can also be 3PL 3 Samejima 2k\nadapted as a heuristic when the input matrix is not a pre-P-\nmatrix.Tothebestofourknowledge,thisistheonlycurrently Fig. 2: Correspondences between the discussed IRT models. Orange\nknown method that can be used for ability discovery that is numbers show number of free parameters per question. Arrows\nmean“specializesinto.”Dashedarrowsimplyspecializationrequires"
  },
  {
    "id": 920,
    "text": "known method that can be used for ability discovery that is numbers show number of free parameters per question. Arrows\nmean“specializesinto.”Dashedarrowsimplyspecializationrequires\nalso guaranteed to recover a C1P ranking if it exists.\nspecial assumptions: Bock to GRM: holds only approximately after\nOur goal. Our goal is to develop a fast and principled fixing aBock=h·aGRM, Samejima to 3PL: for k=2 when c=1/k.\nh\nalgorithm that just like ABH (i) returns a P-matrix in the\nspecial case of pre-P matrix inputs, and (ii) can solve the\nproblem in the more general case when the response matrix model[57]considersadifficultyparameterforeachoptionand\nis not pre-P. As we show, the performance of ABH quickly a discrimination parameter for each question:\ndrops in the non-ideal setting (the general IRT setting in\nP (θ)=P∗ (θ)−P∗ (θ)\nSection II-D) which makes it unusable for ability discovery. ih ih i,h+1\nWeshowthatourmethodismorerobustandgeneralizesbetter, P∗ (θ)=σ (cid:0) a (θ−b ) (cid:1) = 1"
  },
  {
    "id": 921,
    "text": "P (θ)=P∗ (θ)−P∗ (θ)\nSection II-D) which makes it unusable for ability discovery. ih ih i,h+1\nWeshowthatourmethodismorerobustandgeneralizesbetter, P∗ (θ)=σ (cid:0) a (θ−b ) (cid:1) = 1\nthus being the first method with a useful accuracy for ability ih i ih 1+e−ai(θ−bih)\ndiscovery that is also guaranteed to solve the ideal case. −∞=b <b <...<b <b =∞\ni0 i1 i,k−1 ik\nThe Bock model [6] furthermore assigns a discrimination\nD. Relation to Item Response Theory (IRT)\nparameter to each option, and the Samejima model [56] takes\nBrief introduction of IRT models. A large body of work into account random guessing by adding a dummy option.\nfrom psychological and educational researchers called Item Connection between IRT and C1P. We introduced the\nResponse Theory (IRT) [39], [64] studies the mathematical definition of consistent responses in Section II-B, When a\nfunctionsrelatingtheprobabilityofanexaminee’sresponseon response matrix is consistent, it is a pre-P-matrix and can be"
  },
  {
    "id": 922,
    "text": "functionsrelatingtheprobabilityofanexaminee’sresponseon response matrix is consistent, it is a pre-P-matrix and can be\natestitemtoanunderlyingability.Allmajoreducationaltests, permutedtobecomeaP-matrixwhichhastheconsecutiveones\nsuch as the Scholastic Aptitude Test (SAT) [39] and Graduate property (C1P). If the response matrix is a pre-P-matrix, the\nRecord Examination (GRE) [30], are based on IRT. IRT corresponding response function of the probability for a user\nforms a mathematically principled, experimentally validated, tochooseaspecificoptionh∈{0,...,k−1}canbeexpressed\nand widely used theory on how users answer items. Figure 2 as the difference between two Heaviside step functions:\nsummarizestheconnectionsbetweendifferentIRTmodelsand\nSection C contains even more details on IRT. P ih (θ)=H(θ−b ih )−H(θ−b i,h+1 )\nBinaryIRTmodelscanbeseenasvariationsofthestandard\nfor appropriately chosen b such that:\nlogistic or sigmoid function σ : R → [0,1] defined by ih"
  },
  {
    "id": 923,
    "text": "BinaryIRTmodelscanbeseenasvariationsofthestandard\nfor appropriately chosen b such that:\nlogistic or sigmoid function σ : R → [0,1] defined by ih\nσ(x) = 1+ ex ex = 1+ 1 e−x , which is widely used in machine −∞=b i0 <b i1 <...<b i,k−1 <b ik =∞\nlearning models and a smooth relaxation of the Heaviside\nstep function H(x) = I(x > 0) [43]. These models describe Notice that this response function is exactly the GRM model\nthe probability P (θ) for a student with ability θ to answer a inthelimitofa→∞:Whenevertheuserabilityθ isbetween\ni\nquestion i correctly. Here we only show the 3PL IRT model b and b , then this student chooses option h.\nih i,h+1\ndue to limited space, where θ is the latent ability for each Tosummarize,IRTmodelscanbeseenasarelaxedversion\nuser;a,b,carelatentitemfactorscharacterizingthequestions of the response function in the ideal case when the response\nand their options, representing discrimination, difficulty and matrix can be permuted to obey C1P. As widely accepted"
  },
  {
    "id": 924,
    "text": "and their options, representing discrimination, difficulty and matrix can be permuted to obey C1P. As widely accepted\nrandom guessing respectively (Other models can be derived models for MCQs, they strongly support our principle of\nbased on Figure 2): satisfying the more strict C1P in the ideal case and can be\nused to generate data in non-ideal cases (see Section IV).\nP (θ)=c +(1−c )σ (cid:0) a (θ−b ) (cid:1) =c + 1−c i\ni i i i i i 1+e−ai(θ−bi) III. AFAMILYOFHITSALGORITHMS\nMultinomial models measure the probability P (θ) for a We review the HITS algorithm and variants that have\nih\nstudent with ability θ to choose an option h for a question i. been proposed for truth discovery. We then describe a natural\nThus different from binary models whose parameters belong averaging version of HITS that we call “AVGHITS.” Our key\nto questions, multinomial IRT models assign parameters to observation is that the eigenvector corresponding to the 2nd"
  },
  {
    "id": 925,
    "text": "to questions, multinomial IRT models assign parameters to observation is that the eigenvector corresponding to the 2nd\neachoption.Forexample,theGradedResponseModel(GRM) largesteigenvalueofitsupdatematrixreconstructstheuser(or\n4"
  },
  {
    "id": 926,
    "text": "row) ordering with C1P if one exists and is unique.4 We then approach does not require normalization. In the following\ndescribe a variant that we call “HITSNDIFFS” (or HND in matrix formulation, let Crow represent the row-normalized\nshort) on a tripartite graph to find this eigenvector efficiently: response matrix C:\nit keeps an additional vector of differences between adjacent\nscores and updates it in the loop of the AVGHITS algorithm s←Croww w←1−exp (cid:0) C⊤log(1−s) (cid:1)\nto compute the ordering of users that we require. We prove\nthatHNDreturnsthecorrectorderingwhentheuserresponses Investment[47]calculatestheabilityofusersasthesumof\nare consistent and compare its time complexity and expected thescoresoftheirchosenoptions,weightedbytheuserability\naccuracy for non-consistent responses with other methods. theyinvestedinthepreviousiteration.PooledInvestment[47]\nRequired background from spectral graph theory. We extends Investment by using a different formula for the item"
  },
  {
    "id": 927,
    "text": "Required background from spectral graph theory. We extends Investment by using a different formula for the item\nsay that v is an eigenvector of (n×n) quadratic matrix A scores. Both variants use non-linear scaling of the item scores\nwith eigenvalue λ if v̸=0 and Av=λv. If A is symmetric, with different user-specified hyperparameters.\nthen all eigenvalues are real [33]. We use indices to refer to Our method. We build upon this key idea of updating\neigenvalues sorted algebraically: λ ≥ λ ≥ ...λ ≥ λ . scores in a bipartite graph of users and items by iteratively\n1 2 n−1 n\nWe write v to refer to the eigenvector corresponding to summingscoresfromonesidetoupdatetheother.However,in\ni\neigenvalue λ and will be cavalier in referring to it as “the i- contrast to other methods, we focus on the 2nd largest instead\ni\nth largest eigenvector” when we really mean “the eigenvector of the dominant eigenvector of a new variant and show that"
  },
  {
    "id": 928,
    "text": "i\nth largest eigenvector” when we really mean “the eigenvector of the dominant eigenvector of a new variant and show that\ncorresponding to the i-th largest eigenvalue.” this approach is guaranteed to recover the correct ranking in\nIfthematrixisnon-negativeandirreducible,thenaccording case of consistent responses. As we will show in Section IV,\nto the Perron-Frobenius theorem [21], [42], [48] the first no other existing truth discovery method can do that.\neigenvector is also the largest by amplitude (λ =max |λ |)\n1 i i\nand the corresponding eigenvector v 1 is positive. B. “AVGHITS”\nA. “HITS” and its variants for truth discovery In our setup, there are nk different choices (k choices for\nHubs and Authorities (HITS) [31]5 is a classic spectral each of n items). Consider a bipartite graph G=(L∪R,E)\nthat corresponds to the (m×nk) response matrix C: Partition\napproach that several truth discovery approaches have built\nL contains a vertex for each of m users: L = {u ,...,u }."
  },
  {
    "id": 929,
    "text": "that corresponds to the (m×nk) response matrix C: Partition\napproach that several truth discovery approaches have built\nL contains a vertex for each of m users: L = {u ,...,u }.\nupon. The original aim of the algorithm is to rate web pages 1 m\nPartition R is a collection of n vertex sets R = {I ,...,I },\nby two scores: authority and hub score. These scores are 1 n\none for each item. Each set I contains k ≤ k vertices I =\nrecursively defined such that the hub scores are proportional i i i\n{c ,...,c }wherec representsoptionhofitemi.Weadd\nto the sum of the authority scores of the nodes they point to, i1 iki ih\nan edge to between a user u and an option c E if user j\nand the authority scores are proportional to the sum of the j ih\nchooses option h for item i.\nscoresofthehubspointingtothem,thusreflectingamutually\nconsistent set of scores [44]. To make our derivations easier to follow, we will con-\nveniently assume that each item i has the same number"
  },
  {
    "id": 930,
    "text": "scoresofthehubspointingtothem,thusreflectingamutually\nconsistent set of scores [44]. To make our derivations easier to follow, we will con-\nveniently assume that each item i has the same number\nIn the context of truth discovery, the authority and hub\nk = k of options. Notice however, that this is not required\nscores can be interpreted as the user abilities and option i\ncorrectness scores, respectively. Let C represent the m×n for our approach. We further assume C to be connected.\nbinaryresponsematrixwhereC =1iffuserj choosesitem This requirement applies to all spectral truth ranking methods\nj,i\ni, and s be the m-dimensional user score vector, and w be including HITS as otherwise the relative ranking of users\nthe n-dimensional item weight vector. In matrix notation, the (or items) from different components can’t be established.6\nFinally, define s as a (m×1) user score vector and w as a\nscores are recursively defined as:\n(kn×1) option weight vector denoting weights for each of"
  },
  {
    "id": 931,
    "text": "Finally, define s as a (m×1) user score vector and w as a\nscores are recursively defined as:\n(kn×1) option weight vector denoting weights for each of\ns←βCw w←αC⊤s the kn options according to their order in C.\nwhere α and β are normalization constants and C⊤ is the\nWe call AVGHITS the modification of the HITS update\nrule that uses averages instead of sums to iteratively update\ntranspose of C. The algorithm starts from an initial assign-\nthe userscores and option weights in bothdirections: theuser\nment, such as s = e and iteratively updates then normalizes\nscore s is updated to be the average of the weights of all the\nwands.Theuserscoresswillconvergetothe1steigenvector j\noptionsthatuserj picked,andanoptionweightc isupdated\nof the matrix CC⊤. ih\nto be the average of the scores of all users who picked it. In\nTruthFinder[73]modifiesHITSbyfirsttakingtheaverage\nthe following matrix formulation, let Crow represent the row-\ninsteadofthesumofthechosenitemscoresasuserscoresand"
  },
  {
    "id": 932,
    "text": "TruthFinder[73]modifiesHITSbyfirsttakingtheaverage\nthe following matrix formulation, let Crow represent the row-\ninsteadofthesumofthechosenitemscoresasuserscoresand\nnormalized and Ccol the column-normalized response matrix\ninterpretingthemasprobabilitiesoftheusersbeingcorrecton\nC. At each iteration (until convergence), we update the user\nany question. It then defines an item’s score as the probability\nscore vector s and the option weight vector w as follows:\nof it being true given the independent probabilities of all the\nusers choosing the item. When appropriately initialized, the s←Croww w←(Ccol)⊤s\n4Weconsideranorderinganditsreverseorderingtobethesame.\n5HITSoriginallystoodfor“Hyperlink-InducedTopicSearch.” 6PageRankachievestheconnectivitywiththeteleportoperation.\n5"
  },
  {
    "id": 933,
    "text": "By combining the above two update equations, we can\nm-1=3 user diffs 𝑠! \"#$$ 𝑠% \"#$$ 𝑠 & \"#$$\nupdateuserscoresbetweeniterationsdirectlybyreplacingthe\ntwo normalized response matrices with one update matrix U: m=4 users s s s s\n1 2 3 4\ns←Crow(Ccol)⊤s (1)\n(cid:124) (cid:123)(cid:122) (cid:125) n⋅k=9 item choices w 1 w 2 w 3 w 4 w 5 w 6 w 7 w 8 w 9\nU\nItem 1 Item 2 Item 3\nTheseiterationsarenotyetveryhelpful.Indeed,weobserve\nthat the largest eigenvector of U is the all-ones vector e, and -1 1 0 ⋯ 0 0 0 ⋯ 0\nthis is the vector of user scores that AVGHITS converges to. 0 -1 1 ⋱ 0 1 0 ⋯ 0\nIt turns out that it is the eigenvector corresponding to the 2nd S= ⋮ ⋱ ⋱ ⋱ 0 T= 1 1 ⋱ 0\nlargest eigenvalue of U that we seek. 00 ⋯ 00 --11 11 ⋮ ⋮ ⋱ 0\n1 1 ⋯ 1\nC. Our algorithm “HITSNDIFFS” (HND)\nFig.3: HITSNDIFFS usesa3-partitegraphofoptionweights,user\nInthefollowing,weshowasimplealgorithmtofindthe2nd scores,anduserdiffs.ContrastthisgraphwithFigure1.Theupdate"
  },
  {
    "id": 934,
    "text": "Fig.3: HITSNDIFFS usesa3-partitegraphofoptionweights,user\nInthefollowing,weshowasimplealgorithmtofindthe2nd scores,anduserdiffs.ContrastthisgraphwithFigure1.Theupdate\nlargesteigenvectororderingofUandprovethatitcanbeused equations (see Algorithm 1) use two re-shaping matrices S and T.\nto find the unique consecutive ones ordering of the response\nmatrixC.By“theeigenvectorordering”,wemeantheranking\nWe can now get a user difference score update rule:\nof entries in this eigenvector in terms of their values. For\nexample, v 1 = {0.36,0.8,0.48} and v 2 = {0.48,0.64,0.6} sdiff ←Ss=SCrow(Ccol)⊤s=SCrow(Ccol)⊤Tsdiff (2)\nhave the identical ordering {3,1,2} or its reverse {1,3,2}. (cid:124) (cid:123)(cid:122) (cid:125)\nOur algorithm does not return the 2nd largest eigenvector of Udiff\nU but instead returns a vector with the identical ordering. In other words, Udiff =SUT is a “difference update” matrix"
  },
  {
    "id": 935,
    "text": "Our algorithm does not return the 2nd largest eigenvector of Udiff\nU but instead returns a vector with the identical ordering. In other words, Udiff =SUT is a “difference update” matrix\nThe 2nd largest eigenvector of a matrix can be found using that is used to update sdiff from one iteration to the next.\na variant of the deflation method [41], [43], which we will With these update equations, sdiff converges to the largest\ndiscuss in detail in Section III-F. Here we present a novel, eigenvectorofUdiff.Ouralgorithm HND thatimplementsthis\nconceptually simple, and faster algorithm that we term “HITS\nis described in Algorithm 1.\nand DIFFS” (HITSNDIFFS or HND) that extends AVGHITS\nWe now prove the connection between the 1st eigenvector\nfrom a bipartite to a tripartite graph and whose iterative of Udiff and the 2nd largest eigenvector of U.\nupdates converge to a user ranking that is guaranteed to be"
  },
  {
    "id": 936,
    "text": "from a bipartite to a tripartite graph and whose iterative of Udiff and the 2nd largest eigenvector of U.\nupdates converge to a user ranking that is guaranteed to be\nC1P in the ideal case, and that performs well also in more Lemma 1 (Eigenvector correspondence). x is the 2nd largest\ngeneral settings. This approach leverages particular properties\neigenvectorofUiffy=SxisthelargesteigenvectorofUdiff.\nof our problem that don’t apply to the 2nd largest eigenvector\nProof sketch. Due to the limit of space, we only provide\norderings of any matrix from more general settings.\nthe high-level ideas of our proofs in the paper. First, we can\nFirst, we propose a new intermediate step that calculates\nfind out that each row of U has sum 1. Using this, we can\ndifferences between user scores in the iterative updates of\nprove that the largest eigenvector of U is in the direction of\nAVGHITS. Rather than updating the user scores iteratively,\nthe all ones vector e=1 if the largest eigenvalue of U has"
  },
  {
    "id": 937,
    "text": "prove that the largest eigenvector of U is in the direction of\nAVGHITS. Rather than updating the user scores iteratively,\nthe all ones vector e=1 if the largest eigenvalue of U has\nHITSNDIFFS updates the differences between adjacent user m\nmultiplicity1(i.e.thegraphisasingleconnectedcomponent).\nscores by using a suitably modified update matrix, and results\nLet x be an eigenvector of U that is not in the direction of\nin the scores converging to the ordering according to the\nthe all ones vector, i.e. x̸=αe. Note that TS=(I −ee⊤)\nsecond largest eigenvector of U. Furthermore, this modifica- m 1\nand each row of SU sums to 0. Then,\ntion only adds a linear overhead of computing the user score\ndifference vectors and normalizing it in every iteration. As Ux=λx\nwe will show in Theorem 2, when the response matrix obeys\nSUx=λSx\nC1P, then HND reconstructs the correct ordering of the rows.\nAs shown in Figure 3, we define a new vector sdiff of SU(I m −ee⊤ 1 )x=λSx"
  },
  {
    "id": 938,
    "text": "SUx=λSx\nC1P, then HND reconstructs the correct ordering of the rows.\nAs shown in Figure 3, we define a new vector sdiff of SU(I m −ee⊤ 1 )x=λSx\ndifferences in user scores with entries sdiff = s −s , SUTSx=λSx\nj j+1 j\nj ∈[m−1].Thisisequaltosd j iff =SswhereS∈R(m−1)×m is Udiffy=λy, where y=Sx (3)\nshowninFigure3.Inthereversedirection,thereareinfinitely\nmany vectors s that can be generated from a given sdiff, all Therefore, Udiff has exactly the same eigenvalues as U\nshifted by different constants. Since we only want a final except the largest eigenvalue 1, and the eigenvectors of Udiff\nordering of users, we can WLOG set the first element of the are the differences between the entries of the corresponding\nvector s to be 0. The transformation then is s=Tsdiff where eigenvector of U. Thus we prove the lemma.\nT∈Rm×(m−1) is the lower unit triangular matrix7.\nTheorem 1 (2nd eigenvector of AVGHITS recovers C1P). If\nC is a pre-P-matrix with a unique consecutive ones ordering"
  },
  {
    "id": 939,
    "text": "T∈Rm×(m−1) is the lower unit triangular matrix7.\nTheorem 1 (2nd eigenvector of AVGHITS recovers C1P). If\nC is a pre-P-matrix with a unique consecutive ones ordering\n7Itisthisfixingthatintuitivelykeepstheordering,butchangestheactual\namplitudes. of its rows and each row has the same row sum, then this\n6"
  },
  {
    "id": 940,
    "text": "Algorithm1:HITSNDIFFS(HND-power):Afastimplemen- Our solution to this symmetry-breaking problem is moti-\ntationofequation(2)tocalculatethe2ndeigenvectorordering vated by the following observation: users with higher ability\nof U=Crow(Ccol)⊤ tend to converge on the correct option as a majority answer,\nInput: ResponsematrixC,randomlyinitializeduserscoress while users with lower ability at the other end of the ordering\n0\nOutput: User scores s tend to answer randomly. This idea is similar to the main\n1: sdiff ←sd 0 iff // initialize user score differences argument in [35] that experts tend to answer similar correct\n2: repeat answers.Thus thelowerendof theuserordering hasahigher\n3: s←Tsdiff // update user scores entropy in the choices picked than the higher quality end.\n4: w←(Ccol)⊤s // update option weights Notice that this idea is also implicit in IRT models with ran-\n5: s←Croww // update user scores dom guessing where users with low ability choose uniformly"
  },
  {
    "id": 941,
    "text": "4: w←(Ccol)⊤s // update option weights Notice that this idea is also implicit in IRT models with ran-\n5: s←Croww // update user scores dom guessing where users with low ability choose uniformly\n6: sdiff ←Ss // update user score differences\nrandomamongtheoptions(hencehavehighentropy),whereas\n7: Normalize sdiff to be a unit vector\n8: until convergence or iteration limit users of high ability pick the single correct choice.\n9:\ns←Tsdiff We operationalize this idea in a new heuristic that is very\neffectiveinpractice:Givenarankingoftheusers,wecompute,\nfor the top and the bottom user decile, the average entropy of\nthe chosen item options across all items. We pick the side\nordering of the rows of C is given by the ranking of the rows\nwith lower entropy as the users with higher quality. We use\nsorted by values in the 2nd largest eigenvector of U.\nthis “decile entropy method” for both HND and ABH in our\nexperiments.\nProof sketch.WecanfirstprovethatifCisapre-P-matrix"
  },
  {
    "id": 942,
    "text": "sorted by values in the 2nd largest eigenvector of U.\nthis “decile entropy method” for both HND and ABH in our\nexperiments.\nProof sketch.WecanfirstprovethatifCisapre-P-matrix\nwith a unique consecutive ones ordering of its rows and each\nrow has the same row sum, U is an R-matrix (defined in\nE. Why HND works better than ABH\n[4]) where in each row and column, the entries closer to the HND and ABH rely on strikingly similar intuitions about\ndiagonal are larger than or equal to the further entries. Using spectral properties of matrices: HND ranks users by the 2nd\nthis, we can prove every entry in Udiff is non-negative by largest eigenvector of U (whose difference is the largest\ncomputing each entry in Udiff step by step according to its eigenvector of Udiff = SCrow(Ccol)⊤T), whereas it can be\ndefinition, which means Udiff is a non-negative matrix. We shown that ABH ranks users by the 2nd smallest eigenvector"
  },
  {
    "id": 943,
    "text": "definition, which means Udiff is a non-negative matrix. We shown that ABH ranks users by the 2nd smallest eigenvector\ncannowapplythePerron-FrobeniusTheorem[21],[48]:there of the Laplacian matrix L of CC⊤ (whose difference is the\nexists a non-negative eigenvector of Udiff corresponding to smallest eigenvector of M=SLT). In an ideal scenario with\nthe largest eigenvalue of Udiff. We know Udiff has exactly the consistent responses (thus in an IRT scenario with very large\nsameeigenvaluesasU,exceptthelargesteigenvalue1,andthe discrimination scores), both methods are guaranteed to return\neigenvectors of Udiff are the differences between the elements the correct C1P ordering.8\nof the corresponding eigenvector of U. Since the differences We canalso expectthe accuracyto be identical in theother\nbetween the elements of the eigenvector corresponding to the extreme scenario where all questions have 0 discrimination."
  },
  {
    "id": 944,
    "text": "between the elements of the eigenvector corresponding to the extreme scenario where all questions have 0 discrimination.\n2nd largest eigenvalue of U (largest eigenvalue of Udiff) is Buthowcanweexpecttheiraccuracytocompareinthemore\nnon-negative, that eigenvector of U is monotonic. Therefore, general scenario?\nsorting the rows according to the second largest eigenvector We interpret the general IRT scenario as random pertur-\nordering of the corresponding update matrix U gives a P- bations [61] from the ideal C1P case. Now notice that the\nmatrix, proving the theorem. smallest eigenvector of M is identical to the largest eigenvec-\ntor of βI −M where β is larger than all the entries and\nTheorem 2. If C is a pre-P-matrix with a unique C1P m−1\nall the eigenvalues of M.9 Thus the comparison of HND and\norderingofitsrowsandeachrowhasthesamerowsum,then\nABH corresponds to the largest eigenvector of Udiff against\nHND reconstructs the consistent ordering of the users taking"
  },
  {
    "id": 945,
    "text": "orderingofitsrowsandeachrowhasthesamerowsum,then\nABH corresponds to the largest eigenvector of Udiff against\nHND reconstructs the consistent ordering of the users taking\nβI − M. Since both matrices have all non-negative entries\nlinear time in the number of nonzeros in U per iteration.\nin the ideal scenario, we know from the Perron-Frobenius\nProof sketch. From Lemma 1, we know by converting the theorem [21], [48] that all values in their largest eigenvector\nconverged largest eigenvector of Udiff back into a user score, are non-negative.\nwe regain the ordering of the rows according to values in the The user score of the kth user equals to the cumulative\nsecond largest eigenvector of U. This, along with Theorem 1, sum of the first k−1 entries in the eigenvector. In the ideal\nproves this theorem that HND detailed in Algorithm 1 recon- case when C is a C1P matrix, every entry of sdiff is non-"
  },
  {
    "id": 946,
    "text": "proves this theorem that HND detailed in Algorithm 1 recon- case when C is a C1P matrix, every entry of sdiff is non-\nstructs the ideal consistent ordering. negative so s give us a perfect ranking of the students. In the\nnon-ideal scenario when C is not a C1P matrix and the users\nD. Decile entropy-based symmetry breaking\n8Recall that they are guaranteed to return the same ordering, but not the\nNotice that reversing the order of a P-matrix still leaves sameeigenvector.\nit as a P-matrix. Thus all methods for solving C1P suffer 9To see that, assume Av = λv. Then (A+βI)v = Av+βIv =\nλv+βv = (λ+β)v. Thus if v is an eigenvector of A with eigenvalue\nfromanaturalsymmetrybreakingproblem:theyhavetodecide\nλ,thenvisalsoaneigenvectorofthespectrallyshiftedmatrixA+βI,but\nbetweentheorderreturnedbyanalgorithmoritsexactinverse. witheigenvalueλ+β.\n7"
  },
  {
    "id": 947,
    "text": "are permuted by their abilities, the sign of the entries in sdiff be obtained by using the power method on B. We next argue\nchange. When the eigenvector is even, a simple sign change (and later show experimentally in Section IV-C) that using\nin one of the entries does not influence the entire ranking of the deflation method is slightly less efficient than HND (in\ns but when the eigenvector has a large variance, a simple sign addition to being not as simple to formulate as HND-power).\nchange in one large entry can break the entire ranking. For The most widely known deflation method [41], [43]\nexample, if the kth entry of sdiff is quite large but the sign is only works for symmetric matrices and does not apply to\nnegative, the error of the ranking of the (k+1)th students can the asymmetric U. [68] presents several more variants of the\nbe very large. deflationmethodincludingsomethatworkfornon-symmetric"
  },
  {
    "id": 948,
    "text": "negative, the error of the ranking of the (k+1)th students can the asymmetric U. [68] presents several more variants of the\nbe very large. deflationmethodincludingsomethatworkfornon-symmetric\nBased on our observation above, we expect HND to work matrices. Most of those methods either require matrix-matrix\nbetterthanABHasthevarianceoflargesteigenvectorofβI− multiplication or both the left dominant eigenvector and the\nM should be much larger compared to Udiff. The result is rightdominanteigenvector.TheonlyexceptionisWilkinson’s\nverifiedwithdedicatedexperimentsinSectionIV-D:Figure6a vector annihilation [70] which only needs the right dominant\nshowsourobservationsonthevariancesofthesdiff forβI−M eigenvector (which we know is a unit vector in the direction\nand Udiff. Figure 6b and Figure 6c verify that ABH is less oftheallonesvectorinourcase).However,[68]claimedthat\naccurate and less stable than HND. thismethodisdifficulttoapplyinpracticebecauseoftheneed"
  },
  {
    "id": 949,
    "text": "accurate and less stable than HND. thismethodisdifficulttoapplyinpracticebecauseoftheneed\nto conduct annihilation between the power iterations and we\nF. Complexity Comparison\nfound no open-source implementation.\nWe analyze the asymptotic time complexity of the var- ForourexperimentsinSectionIVweimplementHotelling’s\nious methods. We compare HND against (i) existing C1P matrixdeflation[69]whichusesboththeleftandrightlargest\nreconstructionalgorithms BL and ABH,and(ii)thedeflation eigenvectors and thus requires one more round of the power\nmethod[41],[43]asanalternativemethodtocomputethe2nd iteration.10 The experimental result in Section IV-C verifies\nlargest eigenvector for AVGHITS. that HND is not just conceptually simpler but also slightly\nThetimecomplexitydependsonthenumberofusersm,the\nmore efficient than the deflation method.\nnumber of questions n, and the number of iterations t which\nABH [4]. To reconstruct the C1P ordering, ABH requires\nmay be different for different methods."
  },
  {
    "id": 950,
    "text": "more efficient than the deflation method.\nnumber of questions n, and the number of iterations t which\nABH [4]. To reconstruct the C1P ordering, ABH requires\nmay be different for different methods.\nthe computation of the Fiedler vector [18], which is the\nWeassumet≪nandt≪mandthusonlyfocusonmand\neigenvector corresponding to the 2nd smallest eigenvalue of\nn. Notice that although the response matrix C is a (m×kn)- the related Laplacian matrix L.\nmatrix, it has only O(mn) non-zero entries since every user\nThe original ABH paper [4] does not propose an explicit\ncan pick only one label per question.\nsolutionandinsteadreferstotheLanczosalgorithm[32],[46],\nHITSNDIFFS. A naive way to calculate the ranking is\nwhose time complexity is O(dmt) with d being the average\nto first compute Udiff and then use the power method on it.\nnumberofnon-zeroentriesinarowofagiven(m×m)matrix\nHowever, computing Udiff requires a matrix-matrix multipli-\nA. When the Laplacian matrix is dense as in our scenarios"
  },
  {
    "id": 951,
    "text": "numberofnon-zeroentriesinarowofagiven(m×m)matrix\nHowever, computing Udiff requires a matrix-matrix multipli-\nA. When the Laplacian matrix is dense as in our scenarios\ncations before the iterations with time complexity O(m2n). the time complexity of the Lanczos algorithm is O(m2t). It\nSinceUdiff isa(m−1)×(m−1)matrix,thetimecomplexity\nis efficient for eigenvector computations on large symmetric\ntorunthepowermethodonUdiff isO(m2)periteration.This\nmatrices[10],andthestate-of-the-artFiedlervectorsolver[26]\ngives a total time complexity for the naive implementation\nuses Lanczos.\nas O(m2n) + O(m2t) = O(m2n). By instead running the\nHowever, implementation by libraries such as Scipy [66]\nmutual updates of w, s and sdiff as described in Algorithm 1,\nand Tenpy [24], require the full matrix as input, which\nwe can replace matrix-matrix multiplications with several\nwould require us to compute the Laplacian matrix first. This\nmatrix-vector multiplications and thereby get a more effi-"
  },
  {
    "id": 952,
    "text": "we can replace matrix-matrix multiplications with several\nwould require us to compute the Laplacian matrix first. This\nmatrix-vector multiplications and thereby get a more effi-\ncalculation involves matrix-matrix multiplications and, as we\ncient implementation of HITSNDIFFS in O(mnt) . In other show in Section IV-C, results in O(m2n) .\nwords, the speed-up results from applying the associativity\nWe provide another solution for ABH which is not in the\nlaw and replacing the calculation s ← (SCrow(Ccol)⊤T)sdiff\noriginal paper [4]. Similar to how we implement HND as\nwith s ← S(Crow((Ccol)⊤(Tsdiff))). One detail is that we\nAlgorithm1,wecanalsoimplementABHbyusingthepower\nneed however to implement Line 3 differently. Materializing\nmethodonthematrixβI −Mtogetitslargesteigenvector\nthe matrix T would take O(m2). Instead, we calculate the m−1\nwithouthavingmatrix-matrixmultiplications.Aswediscussed\nentries for s from sdiff via cumulative summation (e.g. via"
  },
  {
    "id": 953,
    "text": "the matrix T would take O(m2). Instead, we calculate the m−1\nwithouthavingmatrix-matrixmultiplications.Aswediscussed\nentries for s from sdiff via cumulative summation (e.g. via\nin Section III-E, this largest eigenvector of βI − M is\nnumpy.cumsum in Python). m−1\nidentical to the smallest eigenvector of M which can be used\nThedeflationmethod.Theorem1showedthatourproblem\nto compute the order of the 2nd smallest eigenvector of L in\ncan be formulated as finding the 2nd largest eigenvector v\n2 the same way as Algorithm 1. The total time complexity for\nof U. The problem of finding the eigenvector corresponding\nthis algorithm is O(mnt+m2t) . When m and n are close\nto the second largest eigenvalue of a given matrix A can be\norm<n,thetimecomplexitybecomesO(mnt)whichisthe\nsolvedwiththedeflationmethod [41],[43].Theideaistofirst\ncalculate the dominant eigenvector v , and then eliminate the\n1 10Wefirstcalculatethedominantlefteigenvectorviapoweriteration,then"
  },
  {
    "id": 954,
    "text": "solvedwiththedeflationmethod [41],[43].Theideaistofirst\ncalculate the dominant eigenvector v , and then eliminate the\n1 10Wefirstcalculatethedominantlefteigenvectorviapoweriteration,then\ninfluence of the v from A to get a new matrix B whose 1st\n1 deflate the matrix, and then calculate the dominant right eigenvector on the\neigenvector is the 2nd eigenvector of A. Then v 2 of A can deflatedmatrix.\n8"
  },
  {
    "id": 955,
    "text": "same as HND. However, when m ≫ n, the time complexity guessingintoaccountsoitmodelstheeducationaltestscenario\nbecomes O(m2t), which is larger than O(mnt) of HND. where students try to maximize their scores. Bock and GRM\nBL [7]. The original paper by Booth and Leuker (BL) [7] models with no random guessing models the crowdsourcing\nfor reconstructing the C1P property can work directly on the scenario where workers usually do not guess.\ninitial response matrix and runs in O(mk +n+f), where Bydefault,wesetuserabilityθtobewithin[0,1],itemdif-\nf is the non-zero entries in the matrix. In our setup where ficulty b to be within [−0.5,0.5], and the item discrimination\nf = O(mn), the time complexity is O(mn). Therefore, BL a to be within [0,10], all uniformly random. Besides varying\nis the fastest method when it works. Since it cannot be used the number of users, items and options, Section IV-B also"
  },
  {
    "id": 956,
    "text": "is the fastest method when it works. Since it cannot be used the number of users, items and options, Section IV-B also\nfor solving ability discovery in general, we are not using it in has experiments with shifted b’s (chosen to achieve a certain\nour experiments. percentage of users giving correct answers).11\nHITS [31], Truthfinder [73], Investment [47], PooledIn- Methods and their implementations. For a thorough\nvestment [47]. All these methods are iteration-based variants evaluation, we created three alternative implementations of\nof HITS that take at least O(mnt) and differ in how they HND and two alternative implementations of ABH. HND-\niterate between the user and the item scores. In practice, powerfollowsAlgorithm1whichonlyinvolvesmatrix-vector\nonly HITS can be defined as an eigenvector problem with multiplications. ABH-power is our novel kimplementation of\na closed-form solution and efficient linear algebra implemen- ABH that avoids matrix-matrix multiplications by using the"
  },
  {
    "id": 957,
    "text": "a closed-form solution and efficient linear algebra implemen- ABH that avoids matrix-matrix multiplications by using the\ntation.Truthfinderconvergesinpractice,whileInvestmentand power method on the matrix βI − M. ABH-direct is\nm−1\nPooledInvestmentcannotconvergeandreturndifferentresults the implementation of ABH using the Lanczos algorithm as\ndepending on initialization. Our approach in contrast comes suggested by [4]. Section III-F discussed the drawback of\nwith the same computational properties as HITS: guarantee requiring matrix-matrix multiplications. We use an efficient\nof convergence, unique solution, an intuitive formulation as a sparseLinearAlgebraPythonlibrarycalledScipy[66]. HND-\nspectral problem, and an efficient matrix implementation. direct implements HND similarly by directly computing the\n2nd largest eigenvector of U by using the Arnoldi algorithm\nIV. EXPERIMENTS\n[3], which can be considered the general version of the"
  },
  {
    "id": 958,
    "text": "2nd largest eigenvector of U by using the Arnoldi algorithm\nIV. EXPERIMENTS\n[3], which can be considered the general version of the\nOur experiments compare the accuracy of ability discovery Lanczos algorithm on asymmetric matrices, also using Scipy.\nand the scalability of the various methods. The main take- HND-deflation implements HND with the deflation method\naways from the experiments are: 1) HND robustly returns discussed also in Section III-F. For HND-power, ABH-power\nrankings for users with accuracy on average better than or andHND-deflation,thecriterionforconvergenceisamaximal\nequaltoothertruthdiscoverymethods;2)HNDiscompetitive L2-norm of 10−5 over the change. For our experiments (other\nwithtwo“cheatingcompetitors”(thatareprovidedtheground than Section IV-C) we used “HND-power” for HND and\ntruth information about the correct options for each question “ABH-direct”for ABH sincetheyturnedouttobethefastest"
  },
  {
    "id": 959,
    "text": "truth information about the correct options for each question “ABH-direct”for ABH sincetheyturnedouttobethefastest\nthat is usually not available); 3) HND has better scalability as implementations.\na C1P reconstruction algorithm than ABH.\nWe also implemented HITS [31], TruthFinder [73], In-\nvestment and PooledInvestment [47]. Since none of those\nA. Experimental setup\niteration-based approaches (except HITS) allows an efficient\nEnvironment. All scalability experiments are run on Intel\nmatrix formulation, our implementation in Python uses loops\nXeonE5-2680CPUswithanexclusiveenvironmentand128G\nand is not efficient. We thus do not report scalability exper-\nallocated memory. We implemented HND in Python 3.8.1.\niments on those methods as native implementation in C++\nBenchmarks.[59]providesanextensivebenchmarkforthe\nwouldbringthosecloseto HND asdiscussedinSectionIII-F.\ntruth discovery problem, and [79] provides a broad survey of\nForInvestmentandPooledInvestment(whichdonotconverge)"
  },
  {
    "id": 960,
    "text": "wouldbringthosecloseto HND asdiscussedinSectionIII-F.\ntruth discovery problem, and [79] provides a broad survey of\nForInvestmentandPooledInvestment(whichdonotconverge)\nexisting truth discovery methods. Two points stand out: 1)\nweuse10iterationsinsteadoftuningthenumberofiterations.\nAll open-source datasets used in the two papers lack ground\nTwocheatingbaselines.ToshowtheeffectivenessofHND,\ntruthforuserabilities.2)All20datasetsfallunderthesetting\nwe also compare HND with two “cheating competitors” that\nwith homogeneous items. As we discussed in Section I, it is\nare given additional ground truth information about questions\neasy to understand the lack of ground truth for the ability\nthat is usually not available: True-answer has information\ndiscovery problem because the user abilities are abstract and\nabout which choices are correct for each question (which is\nnot able to be obtained from external knowledge. To make up\nusually not known in our scenario) and then ranks users by"
  },
  {
    "id": 961,
    "text": "about which choices are correct for each question (which is\nnot able to be obtained from external knowledge. To make up\nusually not known in our scenario) and then ranks users by\nfor it, we first create synthetic data based on the polytomous\nthe number of correctly answered questions. GRM-estimator\nmodelsfromItemResponseTheory(IRT)(recallSectionII-D).\nuses a Python package called GIRTH [58] that estimates the\nMoreover,weuseareal-worldMCQdatasetwithapproximate\nparametersofaGRMmodelincludinguserabilities.However,\n(but not accurate enough) ground truth as a supplementary\nit requires knowing the order of options for each question by\nevidence to verify the usefulness of HND in Section IV-E.\ncorrectness.\nPolytomous synthetic data generator. We use the three\npolytomous IRT models from Section II-D (GRM [57],\n11For experiments with GRM data, we use the data generator from the\nBock [6] and Samejima [56]) to generate synthetic data sets"
  },
  {
    "id": 962,
    "text": "polytomous IRT models from Section II-D (GRM [57],\n11For experiments with GRM data, we use the data generator from the\nBock [6] and Samejima [56]) to generate synthetic data sets\nGIRTHpackagewhichrequiresatleastk=3options.WeimplementedBock\nwith known ground truth. Samejima model takes random andSamejimageneratorsourselvesandthusoptionscanstartfromk=2.\n9"
  },
  {
    "id": 963,
    "text": "Our comparison with this approach is notable because it is where users answer different number of questions, we vary\nthetheoretically“best”modeltofitdatageneratedbythesame the probability of questions to be answered. For each pair of\nsynthetic GRM process. question and user, there is the probability of p for the user\nB. Accuracy on synthetic data\nto answer the question. We see that HND performs well even\nwhen the dataset is not complete.\nAccuracy. To determine the accuracy of a method, we\n6. C1P (Figure 4h).13 In addition to the three multinomial\ncalculateSpearman’srankcorrelationcoefficient[60]between\nIRT models, we also generate response matrices that are\nthe returned user ranking and the ground truth ranking by\nconsistent and can be reconstructed to be a P-matrix. These\ntheir actual abilities. Spearman’s correlation is defined as\nresponses correspond to a random GRM instance with very\nthe Pearson correlation between the rankings of two scoring"
  },
  {
    "id": 964,
    "text": "their actual abilities. Spearman’s correlation is defined as\nresponses correspond to a random GRM instance with very\nthe Pearson correlation between the rankings of two scoring\nstrong discrimination a. We use these matrices to verify the\nfunctions and ranges between −1 and 1. It is similar to\neffectivenessof HND inreconstructingaC1Ppermutation.To\nKendall’s correlation, yet strictly preferred if there are ties\navoid ties in the rankings and provide a unique C1P ordering,\nin the data [49]. There can be negative accuracy at times\nwe set both the user ability θ and the difficulty parameter b\n(not shown in Figure 4), which means the returned ranking\nto be within [0,1], randomly chosen. We see that HND and\nisnegativelycorrelatedorrandomwhosecoefficientisnear0.\nABH are indeed the only two methods that can reconstruct\nSetup. We conduct experiments to determine the accuracy\nthe C1P permutation if there exists one.\nas function of the 1) number of items n, 2) number of users"
  },
  {
    "id": 965,
    "text": "Setup. We conduct experiments to determine the accuracy\nthe C1P permutation if there exists one.\nas function of the 1) number of items n, 2) number of users\nm, 3) number of options k, 4) option difficulties b , and 5) Summary. HITSNDIFFS is a robust method that outper-\nih\nprobability of questions to be answered p. In the first experi- forms the other approaches in most setups, especially those\nment,weusedatageneratedaccordingtothethreepolytomous with high discrimination. We see this as vindication for\nmodels (GRM, Bock, Samejima) from Section II-D to also designing an approach based on the principle that consistent\nshow the robust performance of HND for all three models. In answers need to be solved correctly. HND is also competitive\nother experiments, we only use data generated according to even against the two cheating approaches which have the best\nthe Samejima model since it is the most general one to avoid item answer given (i.e. they have access to an oracle that can"
  },
  {
    "id": 966,
    "text": "the Samejima model since it is the most general one to avoid item answer given (i.e. they have access to an oracle that can\nredundancy. To verify the ability of algorithms to recover a solve the entire problem of truth discovery). Moreover, we\nC1P ranking, we also generate data that 6) follows the consis- verified that HND and ABH are indeed the only ones that\ntent response property (which as discussed in Section II-C is can reconstruct a C1P permutation if it exists.\nthe case for IRT models when the discrimination a → ∞).\nih\nEveryquestionhasthesamenumberofoptions,andeveryuser\nanswerseveryquestion.Bydefault,wesettheusersm=100,\nC. Scalability experiments\nitems n=100, and options k =3.\n1. Varying number of questions n (Figures 4a to 4c12).\nFigures 5a and 5b show the scalability of our various\nHND has better than or equal accuracy as the other methods\neven including the two cheating competitors over data gener-\nimplementations of ABH and HND, as well as the GRM-\nestimator w.r.t."
  },
  {
    "id": 967,
    "text": "HND has better than or equal accuracy as the other methods\neven including the two cheating competitors over data gener-\nimplementations of ABH and HND, as well as the GRM-\nestimator w.r.t.\natedbyallthreemodels.NoticethattheGRM-estimatorworks\npoorlyforSamejimabecauseitdoesnottakerandomguessing number of users (m) and questions (n). Each shown data\ninto account. pointisthemedianover5runs,andwesetatimeoutof1,000\n2. Varying number of users m (Figure 4d). HND works seconds. HND vs. ABH. Figure 5a shows that ABH-direct\nhere also better than or equal to other approaches (except for and HND-direct scale with O(m2k) in the number of users\nthe data point with low m where the cheating competitors as predicted in Section III-F. The theoretic time complexity\nwin). of ABH-power is O(m2t) when m is much larger than n,\n3. Varying number of options k (Figure 4e). HND stays and thus it also takes quadratic time. In contrast, HND-power"
  },
  {
    "id": 968,
    "text": "win). of ABH-power is O(m2t) when m is much larger than n,\n3. Varying number of options k (Figure 4e). HND stays and thus it also takes quadratic time. In contrast, HND-power\ntop and accurate, even slightly outperforming True-answer. canscalelinearlyandisabout20%fasterthan HND-deflation\n4. Varying question difficulties b (Figure 4f). Here, on average for m>1000 users as it needs only one round of\nih\nwe change the difficulty range from the default [−0.5,0.5] thepowermethod.Figure5bshowsthatalthough ABH-direct\nto 7 different ranges, [−1,0], [−0.75,0.25], [−0.5,0.5], is slightly faster for fixed few users, all implementations are\n[−0.25,0.75],[0,1],[0.25,1.25],[0.5,1.5],whileuserabilities efficient even for a large number of questions.\nθ remain at [0,1]. Thus even the least able user has a high\nj GRM-estimator. As a representative of max likelihood\nprobabilitytoansweradifficultquestionintheeasiestsetting,\nparameter estimation, the GRM-estimator is expected to per-"
  },
  {
    "id": 969,
    "text": "j GRM-estimator. As a representative of max likelihood\nprobabilitytoansweradifficultquestionintheeasiestsetting,\nparameter estimation, the GRM-estimator is expected to per-\nwhile even the best user can be incorrect for some easy\nform best on GRM data. However, Figure 5 shows that such\nquestionsinthehardestsetting.Thex-axishereistheaverage\nparameter estimation is by orders of magnitude slower than\naccuracyonthequestionsacrossalltheusers.Inallscenarios,\nHND.\nwe see HND outperforms other competitors.\nSummary.HNDscalesasymptoticallyandpracticallybetter\n5. Varying probability p of answering a question (Fig-\nthan the other existing C1P reconstruction algorithm ABH in\nure 4g). To show that HND works for more general scenarios\nthe number of users. Moreover, our intuitive Algorithm 1 is\n12TheGRMestimatordoesnotworkwhenthequestionnumberislarge. slightly faster than an adaptation of the deflation method.\n10"
  },
  {
    "id": 970,
    "text": "1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n25 50 100 200 400 800 1600\nNumber of questions\ngniknar\nresu\nfo\nycaruccA\n1.0\n0.8\n0.6\n0.4 ABH Invest\nHnD PooledInv\nHITS True-Answer 0.2\nTruthFinder GRM-estimator\n0.0\n25 50 100 200 400 800 1600\nNumber of questions\n(a)Varyingn(GRM)\ngniknar\nresu\nfo\nycaruccA\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n25 50 100 200 400 800 1600\nNumber of questions\n(b)Varyingn(Bock)\ngniknar\nresu\nfo\nycaruccA\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n25 50 100 200 400 800 1600\nNumber of users\n(c)Varyingn(Samejima)\ngniknar\nresu\nfo\nycaruccA\n(d)Varyingm(Samejima)\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0 2 3 4 5 6\nNumber of options\ngniknar\nresu\nfo\nycaruccA\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0 35 40 45 50 55 60 65 70\nAverage user accuracy\n(e)Varyingk (Samejima)\ngniknar\nresu\nfo\nycaruccA\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0 0.6 0.7 0.8 0.9 1.0\nProbability\n(f)Varyingb ih (Samejima)\ngniknar\nresu\nfo\nycaruccA\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0 25 50 100 200 400 800 1600\nNumber of questions\n(g)Varyingp(Samejima)\ngniknar\nresu\nfo\nycaruccA\n(h)Varyingn(C1P)"
  },
  {
    "id": 971,
    "text": "Probability\n(f)Varyingb ih (Samejima)\ngniknar\nresu\nfo\nycaruccA\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0 25 50 100 200 400 800 1600\nNumber of questions\n(g)Varyingp(Samejima)\ngniknar\nresu\nfo\nycaruccA\n(h)Varyingn(C1P)\nFig. 4: Section IV-B: Results of accuracy experiments (the legend is in the first figure).\n103\n102\n101\n100\n101\n102\n103\n101 102 103 104 105\nNumber of users\n)ces(\nemit\nnoitucexE\nGRM-estimator\n103\nA\nA\nB\nB\nH\nH\n-\n-\nP\nD\no\nir\nw\ne\ne\nct\nr Quadratic 102\nHnD-Direct 101 HnD-Deflation\nHnD-Power Linear 100\n101\n102\n103\n101 102 103 104 105\nNumber of questions\n(a)Scalabilitywithusers(m).\n)ces(\nemit\nnoitucexE\nbetter stability and accuracy of the HND rankings. Figure 6b\nQuadratic confirms that the ranking of a user is more stable for HND.\nFigure 6c shows the resulting increase in the accuracy of\nLinear HND over ABH. This confirms our original goal to develop\na spectral method that can achieve the same C1P ranking as\nABH, yet generalizing better in the non-ideal case."
  },
  {
    "id": 972,
    "text": "Linear HND over ABH. This confirms our original goal to develop\na spectral method that can achieve the same C1P ranking as\nABH, yet generalizing better in the non-ideal case.\n(b)Scalabilitywithitems(n). E. Accuracy experiments on real-world data\nFig. 5: Section IV-C: Scalability experiments with n = 100 items AsmentionedinSectionIV-A,wedonotknowanyexisting\nand increasing numbers of users m in (a), or m = 100 users benchmark with a known true ranking of users by their\nand increasing numbers of items n in (b). The experiments confirm abilities. In order to still verify the performance of HND on\nthat our method (HnD) scales linearly in the number of items and\nreal-world datasets we use the ranking of the “True-answer”\nusers,whereasABH(eventryingvariousalternativemethods)hasan\nbaselineasthegroundtruth.Noticethatalthoughthisbaseline\nunavoidable quadratic scalability in the number of users.\nperforms well in our synthetic experiments, it is far from the"
  },
  {
    "id": 973,
    "text": "baselineasthegroundtruth.Noticethatalthoughthisbaseline\nunavoidable quadratic scalability in the number of users.\nperforms well in our synthetic experiments, it is far from the\nperfectgoldstandard(sometimesevenoutperformedbyHND)\nD. Stability experiments for ABH and HND so the experimental result in this subsection should be seen\nWe next experimentally verify our prediction from Sec- only as a supplementary evidence. The six used real world\ntion III-E that HND generalizes better from the ideal case MCQ datasets are from [35].\nthan ABH. In this setup, we fix m = 100 users, n = 100 Figure 7 shows the average experimental result of six\nitems, k = 3 options, with user abilities and item difficulties datasets where PooledInvestment and HITS perform slightly\nequallyspacedbetween[0,1]and[−0.5,0.5]respectively.For better than HND. However, we need to emphasize several\noneitem,alltheoptiondifficultiesarethesame.Allitemshave points: (1) All datasets are very small in terms of question"
  },
  {
    "id": 974,
    "text": "oneitem,alltheoptiondifficultiesarethesame.Allitemshave points: (1) All datasets are very small in terms of question\nidentical discrimination a and all the options in one question numbers (from 20 to 36) but have double user numbers on\nhave equally spaced a (as in the GRM model). average,whichindicatestheirlimiteddiscrimination.(2)There\nWe then vary the question discrimination scores and com- isnoconsistentwinneronallsixdatasets(seedetailedresultin\npare the (i) variance of the respective eigenvectors used Section D), an observation also made by [79] for the related\nfor ranking; (ii) the normalized average difference in rank truth discovery problem. (3) All other models except ABH\nbetween each user’s ranking;14 and (iii) the average accuracy with poor performance tend to have more similar accuracy\nofthepredictedrankingsforHNDandABHacrossrepeatedly than HND while HND topsthembyfaron2ofthe6datasets,\nsampled response matrices. which shows the novelty of HND and its usefulness on data"
  },
  {
    "id": 975,
    "text": "ofthepredictedrankingsforHNDandABHacrossrepeatedly than HND while HND topsthembyfaron2ofthe6datasets,\nsampled response matrices. which shows the novelty of HND and its usefulness on data\nFigure6ashowsourobservationfromSectionIII-Ethatthe of different distributions.\nvariance of the largest eigenvector of Udiff of HND is much\nV. ADDITIONALRELATEDWORK\nsmaller than βI −M, which is expected to lead to the\nm−1\nIn this section, we discuss additional approaches for the\n13In the experiments, PooledInv returned all negative coefficients but we truth discovery problem. This is in addition to existing C1P\nconsideritsrankingtobethereverseone.\nreconstructionalgorithmsdiscussedinSectionII-CandHITS-\n14Heredifferencemeanstheaveragedifferenceofeachuser’srankingsfrom\ndifferentruns,scaleddownto[0,1]bytheusernumber. based truth discovery approaches discussed in Section III-A.\n11"
  },
  {
    "id": 976,
    "text": "0.010\n0.009\n0.008\n0.007\n0.006\n0.005\n0.004\n20 21 22 23 24\nQuestion discrimination\nrotcevnegie\ntsegral\neht fo\necnairaV\n0.35\n0.30\n0.25\nI-M of ABH\nUdiff of HnD 0.20 0.15\n0.10\n0.05\n20 21 22 23 24\n(a)Varianceofeigenvectorusedby Question discrimination\nHNDorABH\ntnemecalpsid\nresu dezilamroN\n1.0\nABH\nHnD 0.8\n0.6\n0.4\n0.2\n0.0\n20 21 22 23 24\nQuestion discrimination\n(b)Normalizeduserdisplacement\ngniknar\nresu\nfo ycaruccA\nABH\nHnD\n(c)Accuracyofuserranking\nFig. 6: Section IV-D: Stability experiments: (a) The variance of the eigenvector used by HND is smaller than that used by ABH, which\nmakesitmorerobusttoperturbationsfromtheidealC1Pcase.ThisleadstoHNDhavinglowerdifferenceintheuserranking(b)andhigher\naccuracy (c).\ntion, yet focus on the copying relationships between sources.\nAccuracy of user ranking\nIn our setup, no information is copied between users. [53]\nPooledInv 80.85\ndiscusses the problem of how much training data is needed\nInv 75.60 to gain high-quality models. [45] studies truth discovery in"
  },
  {
    "id": 977,
    "text": "PooledInv 80.85\ndiscusses the problem of how much training data is needed\nInv 75.60 to gain high-quality models. [45] studies truth discovery in\nquantitative applications, such as percentage annotation and\nTF 67.12\nobject counting.\nHITS 77.16\nCrowdsourcing. The ability discovery problem is closely\nABH 9.92 connected to the truth discovery problem which occur in a\nwide range of data management problems related to crowd-\nHnD 76.34\nsourcing[34],[79].Variouscrowdsourcingsystemshavebeen\n0 20 40 60 80 100 proposed[17],[78],andthecrowdsourcingapproachhasbeen\nFig. 7: Section IV-E: Correlation of user ranking on real-world refined for various tasks, such as query answering [20], entity\ndatasets with the “True-answer” baseline that serves us as approx- resolution [8], annotating Twitter data [19], top-k algorithms\nimate gold standard user ranking. Notice that the true ranking of\n[75] and various other labeling tasks [25], [63], [67].\nusers by ability is not known."
  },
  {
    "id": 978,
    "text": "imate gold standard user ranking. Notice that the true ranking of\n[75] and various other labeling tasks [25], [63], [67].\nusers by ability is not known.\nExpert finding. The expert finding problem [37], [74] also\naims to assess the trustworthiness of users. The difference is\nSpectral approaches. Dalvi et al. [11] proposed two meth-\nthatitfocusesonfindingexpertswithexpertise(skills)specific\nto a given question while our ability discovery problem aims\nods that output the user abilities and item labels with the\nto assess an overall user ability.\nhelp of eigenvector computation. Ghosh et al. [23] proposed\na method that only outputs the item labels, which involves\nVI. CONCLUSIONS\ncalculating the first eigenvector of a symmetric matrix. Both\nWe proposed HITSNDIFFS, a novel variant of HITS,\napproachesworkonlyforbinaryproblemsandarenotobvious\nwith surprising theoretical and practical properties for ability\nto generalize for k >2 options."
  },
  {
    "id": 979,
    "text": "We proposed HITSNDIFFS, a novel variant of HITS,\napproachesworkonlyforbinaryproblemsandarenotobvious\nwith surprising theoretical and practical properties for ability\nto generalize for k >2 options.\ndiscovery. On the theoretical side, we showed that 1) C1P\nOther truth discovery approaches. [35] proposes the\nof the response matrix models consistent solutions for the\nconcept of experts and utilizes the observation that experts\nproblem; 2) our method reconstructs the correct user rankings\naremorelikelytoreachconsensusonasetofsinglequestions\nin the consistent case; 3) does so in linear time and 4) can\n(called hyper-questions in the paper) to conduct majority vote\nhandle more general cases (in contrast to other linear discrete\non hyper-questions instead of single questions but cannot\nalgorithms). On the practical side, we showed that HND\nquantify user abilities nor rank them. [40] relies on embed-\nhandles the problem of ability discovery with robust accuracy"
  },
  {
    "id": 980,
    "text": "algorithms). On the practical side, we showed that HND\nquantify user abilities nor rank them. [40] relies on embed-\nhandles the problem of ability discovery with robust accuracy\ndings that cannot be easily converted into a ranking on users.\nand greater scalability in terms of the number of users than\n[36]usesconfidenceandfocusonlong-taildata.[12],[28]are\nthe only existing C1P reconstruction algorithm that works for\noptimization-based methods that only consider homogeneous\ngeneral cases.\nquestions (recall Section II-A).\nOther truth discovery problems. Many approaches have ACKNOWLEDGMENT\nbeen proposed for truth discovery. Most have different setups\nThis work was supported in part by the National Science\nand are not applicable to our problem. [62], [72] change the\nFoundation(NSF)underawardnumbersIIS-1762268andIIS-\nsetupoftheproblembyassigningdifferenttaskstotwogroups\n1956096.\nofworkers,wherethefirstgroupanswersthequestionsandthe\nsecondgroupevaluatestheanswers.[16],[52],[77],[80]work"
  },
  {
    "id": 981,
    "text": "setupoftheproblembyassigningdifferenttaskstotwogroups\n1956096.\nofworkers,wherethefirstgroupanswersthequestionsandthe\nsecondgroupevaluatestheanswers.[16],[52],[77],[80]work\non the problem of how to assign questions to only a subset of\nthesources.[14],[15]payattentiontothesourcesofinforma-\n12"
  },
  {
    "id": 982,
    "text": "REFERENCES [22] D. R. Fulkerson and O. A. Gross, “Incidence matrices and interval\ngraphs.”PacificJournalofMathematics,vol.15,no.3,pp.835–855,\n[1] “Amazonmechanicalturk,”https://www.mturk.com/,2023. 1965.[Online].Available:http://dx.doi.org/10.2140/pjm.1965.15.835\n[2] “Piazza,”https://piazza.com/,2023. [23] A. Ghosh, S. Kale, and P. McAfee, “Who moderates the moderators?:\n[3] W. E. Arnoldi, “The principle of minimized iterations in the crowdsourcingabusedetectioninuser-generatedcontent,”inEC,2011,\nsolution of the matrix eigenvalue problem,” Quarterly of applied pp. 167–176. [Online]. Available: https://doi.org/10.1145/1993574.\nmathematics, vol. 9, no. 1, pp. 17–29, 1951. [Online]. Available: 1993599\nhttp://www.jstor.org/stable/43633863 [24] J. Hauschild and F. Pollmann, “Efficient numerical simulations with\n[4] J.E.Atkins,E.G.Boman,andB.Hendrickson,“Aspectralalgorithm TensorNetworks:TensorNetworkPython(TeNPy),”SciPostPhys.Lect."
  },
  {
    "id": 983,
    "text": "[4] J.E.Atkins,E.G.Boman,andB.Hendrickson,“Aspectralalgorithm TensorNetworks:TensorNetworkPython(TeNPy),”SciPostPhys.Lect.\nfor seriation and the consecutive ones problem,” SIAM Journal on Notes, p. 5, 2018, code available from https://github.com/tenpy/tenpy.\nComputing, vol. 28, no. 1, pp. 297–310, 1998. [Online]. Available: [Online]. Available: https://scipost.org/10.21468/SciPostPhysLectNotes.\nhttps://doi.org/10.1137/S0097539795285771 5\n[5] A. Birnbaum, “Some latent traint models,” in Statistical theories of\n[25] H. Hu, Y. Zheng, Z. Bao, G. Li, J. Feng, and R. Cheng,\nmental test scores, F. M. Lord and M. R. Novick, Eds. Addison-\n“Crowdsourced POI labelling: Location-aware result inference and\nWesley, 1968, ch. 17. [Online]. Available: https://psycnet.apa.org/ task assignment,” in ICDE, 2016, pp. 61–72. [Online]. Available:\nrecord/1968-35040-000\nhttps://doi.org/10.1109/ICDE.2016.7498229\n[6] R. D. Bock, “Estimating item parameters and latent ability when"
  },
  {
    "id": 984,
    "text": "record/1968-35040-000\nhttps://doi.org/10.1109/ICDE.2016.7498229\n[6] R. D. Bock, “Estimating item parameters and latent ability when\n[26] Y. Hu, J. Scott, and H. MC73, “A fast multilevel fiedler and profile\nresponsesarescoredintwoormorenominalcategories,”Psychometrika,\nreductioncode,”inRAL-TR-2003-36,2003.\nvol. 37, no. 1, pp. 29–51, 1972. [Online]. Available: https:\n[27] N. Q. V. Hung, H. H. Viet, T. T. Nguyen, M. Weidlich, H. Yin, and\n//doi.org/10.1007/BF02291411\nX. Zhou, “Computing crowd consensus with partial agreement,” IEEE\n[7] K.S.BoothandG.S.Lueker,“Testingfortheconsecutiveonesproperty,\nTrans. Knowl. Data Eng., vol. 30, no. 1, pp. 1–14, 2018. [Online].\nintervalgraphs,andgraphplanarityusingPQ-treealgorithms,”Journal\nAvailable:https://doi.org/10.1109/TKDE.2017.2750683\nof Computer and System Sciences, vol. 13, no. 3, pp. 335–379, 1976.\n[28] H. Kajino, Y. Tsuboi, and H. Kashima, “A convex formulation for\n[Online].Available:https://doi.org/10.1016/S0022-0000(76)80045-1"
  },
  {
    "id": 985,
    "text": "[28] H. Kajino, Y. Tsuboi, and H. Kashima, “A convex formulation for\n[Online].Available:https://doi.org/10.1016/S0022-0000(76)80045-1\nlearningfromcrowds,”inAAAI,2012,pp.73–79.[Online].Available:\n[8] C.Chai,G.Li,J.Li,D.Deng,andJ.Feng,“Cost-effectivecrowdsourced\nhttps://doi.org/10.1609/aaai.v26i1.8105\nentity resolution: A partial-order approach,” in SIGMOD, 2016, pp.\n969–984.[Online].Available:https://doi.org/10.1145/2882903.2915252 [29] D. Kendall, “Incidence matrices, interval graphs and seriation in\n[9] Z. Chen, S. Mitra, R. Ravi, and W. Gatterbauer, “HITSnDIFFS: From archeology,”PacificJournalofmathematics,vol.28,no.3,pp.565–570,\ntruth discovery to ability discovery by recovering matrices with the 1969.[Online].Available:https://msp.org/pjm/1969/28-3/p08.xhtml\nconsecutive ones property: Code and experiments,” 2023. [Online]. [30] N. M. Kingston and N. J. Dorans, “The feasibility of using item"
  },
  {
    "id": 986,
    "text": "consecutive ones property: Code and experiments,” 2023. [Online]. [30] N. M. Kingston and N. J. Dorans, “The feasibility of using item\nAvailable:https://github.com/northeastern-datalab/HITSnDIFFS/ responsetheoryasapsychometricmodelforthegreaptitudetest,”ETS\n[10] J. K. Cullum and R. A. Willoughby, Lanczos algorithms for large Research Report Series, vol. 1982, no. 1, 1982. [Online]. Available:\nsymmetric eigenvalue computations: Vol. I: Theory. SIAM, 2002. http://dx.doi.org/10.1002/j.2333-8504.1982.tb01298.x\n[Online].Available:https://doi.org/10.1137/1.9780898719192 [31] J.M.Kleinberg,“Authoritativesourcesinahyperlinkedenvironment,”\n[11] N. Dalvi, A. Dasgupta, R. Kumar, and V. Rastogi, “Aggregating J. ACM, vol. 46, no. 5, pp. 604–632, 1999. [Online]. Available:\ncrowdsourced binary ratings,” in WWW, 2013, pp. 285–294. [Online]. https://doi.org/10.1145/324133.324140\nAvailable:https://doi.org/10.1145/2488388.2488414 [32] C. Lanczos, “An iteration method for the solution of the eigenvalue"
  },
  {
    "id": 987,
    "text": "Available:https://doi.org/10.1145/2488388.2488414 [32] C. Lanczos, “An iteration method for the solution of the eigenvalue\n[12] A. P. Dawid and A. M. Skene, “Maximum likelihood estimation of problem of linear differential and integral operators,” 1950. [Online].\nobserver error-rates using the EM algorithm,” Applied statistics, pp. Available:https://doi.org/DOI:10.6028/JRES.045.026\n20–28,1979.[Online].Available:https://doi.org/10.2307/2346806 [33] D.Lay,S.Lay,andJ.McDonald,LinearAlgebraandItsApplications.\n[13] C.DeMars,ItemResponseTheory. OxfordUniversityPress,042010. Pearson,2016.[Online].Available:https://books.google.com/books?id=\n[Online]. Available: https://doi.org/10.1093/acprof:oso/9780195377033. L8SUoAEACAAJ\n001.0001 [34] G. Li, J. Wang, Y. Zheng, and M. J. Franklin, “Crowdsourced data\n[14] X.L.Dong,L.Berti-Equille,Y.Hu,andD.Srivastava,“Globaldetection management:Asurvey,”inICDE,2017,pp.39–40.[Online].Available:"
  },
  {
    "id": 988,
    "text": "[14] X.L.Dong,L.Berti-Equille,Y.Hu,andD.Srivastava,“Globaldetection management:Asurvey,”inICDE,2017,pp.39–40.[Online].Available:\nofcomplexcopyingrelationshipsbetweensources,”Proceedingsofthe https://doi.org/10.1109/ICDE.2017.26\nVLDB Endowment, vol. 3, no. 1-2, pp. 1358–1369, 2010. [Online]. [35] J. Li, Y. Baba, and H. Kashima, “Hyper questions: Unsupervised\nAvailable:https://doi.org/10.14778/1920841.1921008 targetingofafewexpertsincrowdsourcing,”inCIKM,2017,pp.1069–\n[15] X.L.Dong,L.Berti-Equille,andD.Srivastava,“Integratingconflicting 1078.[Online].Available:https://doi.org/10.1145/3132847.3132971\ndata: the role of source dependence,” Proceedings of the VLDB\n[36] Q.Li,Y.Li,J.Gao,L.Su,B.Zhao,M.Demirbas,W.Fan,andJ.Han,\nEndowment, vol. 2, no. 1, pp. 550–561, 2009. [Online]. Available:\n“A confidence-aware approach for truth discovery on long-tail data,”\nhttps://doi.org/10.14778/1687627.1687690\nProceedings of the VLDB Endowment, vol. 8, no. 4, pp. 425–436,"
  },
  {
    "id": 989,
    "text": "“A confidence-aware approach for truth discovery on long-tail data,”\nhttps://doi.org/10.14778/1687627.1687690\nProceedings of the VLDB Endowment, vol. 8, no. 4, pp. 425–436,\n[16] X. L. Dong, B. Saha, and D. Srivastava, “Less is more: Selecting 2014.[Online].Available:https://doi.org/10.14778/2735496.2735505\nsources wisely for integration,” Proceedings of the VLDB Endowment,\n[37] S. Lin, W. Hong, D. Wang, and T. Li, “A survey on expert finding\nvol. 6, no. 2, pp. 37–48, 2012. [Online]. Available: https://doi.org/10.\ntechniques,” J. Intell. Inf. Syst., vol. 49, no. 2, pp. 255–279, 2017.\n14778/2535568.2448938\n[Online].Available:https://doi.org/10.1007/s10844-016-0440-5\n[17] J. Fan, G. Li, B. C. Ooi, K. Tan, and J. Feng, “icrowd: An adaptive\n[38] Q. Liu, J. Peng, and A. T. Ihler, “Variational inference for\ncrowdsourcing framework,” in SIGMOD, 2015, pp. 1015–1030.\ncrowdsourcing,” in NIPS, 2012, pp. 701–709. [Online]. Available:\n[Online].Available:https://doi.org/10.1145/2723372.2750550"
  },
  {
    "id": 990,
    "text": "crowdsourcing framework,” in SIGMOD, 2015, pp. 1015–1030.\ncrowdsourcing,” in NIPS, 2012, pp. 701–709. [Online]. Available:\n[Online].Available:https://doi.org/10.1145/2723372.2750550\nhttps://dl.acm.org/doi/10.5555/2999134.2999212\n[18] M. Fiedler, “Laplacian of graphs and algebraic connectivity,” Banach\n[39] F. M. Lord, M. R. Novick, and A. Birnbaum, Statistical Theories\nCenter Publications, vol. 25, no. 1, pp. 57–70, 1989. [Online].\nof Mental Test Scores. Addison-Wesley, 1968. [Online]. Available:\nAvailable:https://doi.org/10.4064/-25-1-57-70\nhttps://psycnet.apa.org/record/1968-35040-000\n[19] T. Finin, W. Murnane, A. Karandikar, N. Keller, J. Martineau,\nand M. Dredze, “Annotating named entities in twitter data with [40] S. Lyu, W. Ouyang, H. Shen, and X. Cheng, “Truth discovery by\ncrowdsourcing,” in NAACL HLT 2010 Workshop on Creating Speech claimandsourceembedding,”inCIKM. ACM,2017,pp.2183–2186."
  },
  {
    "id": 991,
    "text": "crowdsourcing,” in NAACL HLT 2010 Workshop on Creating Speech claimandsourceembedding,”inCIKM. ACM,2017,pp.2183–2186.\nandLanguageDatawithAmazon’sMechanicalTurk,2010,pp.80–88. [Online].Available:https://doi.org/10.1145/3132847.3133069\n[Online].Available:https://aclanthology.org/W10-0713/ [41] L. W. Mackey, “Deflation methods for sparse PCA,” in NIPS,\n[20] M. J. Franklin, D. Kossmann, T. Kraska, S. Ramesh, and R. Xin, D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, Eds., 2008, pp.\n“CrowdDB: answering queries with crowdsourcing,” in SIGMOD, 1017–1024. [Online]. Available: https://proceedings.neurips.cc/paper/\n2011, pp. 61–72. [Online]. Available: https://doi.org/10.1145/1989323. 2008/hash/85d8ce590ad8981ca2c8286f79f59954-Abstract.html\n1989331 [42] C. D. Meyer, Ed., Matrix Analysis and Applied Linear Algebra.\n[21] G.Frobenius,“U¨bermatrizenausnichtnegativenelementen,”Sitzungs- Society for Industrial and Applied Mathematics, 2000. [Online]."
  },
  {
    "id": 992,
    "text": "[21] G.Frobenius,“U¨bermatrizenausnichtnegativenelementen,”Sitzungs- Society for Industrial and Applied Mathematics, 2000. [Online].\nberichtederKo¨niglichPreussischenAkademiederWissenschaften,pp. Available:https://doi.org/10.1137/1.9781611977448\n456–477, 1912. [Online]. Available: https://commons.wikimedia.org/ [43] K.P.Murphy,ProbabilisticMachineLearning:Anintroduction. MIT\nwiki/File:Ueber Matrizen aus nicht negativen Elementen.pdf Press,2022,p.255.[Online].Available:probml.ai\n13"
  },
  {
    "id": 993,
    "text": "[44] M. Newman, Networks: An Introduction. New York, NY, USA: with item response theory,” in ACL/IJCNLP 2021. Association for\nOxford University Press, Inc., 2010. [Online]. Available: https: Computational Linguistics, 2021, pp. 1141–1158. [Online]. Available:\n//doi.org/10.1093/acprof:oso/9780199206650.001.0001 https://doi.org/10.18653/v1/2021.acl-long.92\n[45] R. W. Ouyang, L. M. Kaplan, A. Toniolo, M. Srivastava, and T. J. [66] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy,\nNorman, “Aggregating crowdsourced quantitative claims: Additive and D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright,\nmultiplicative models,” IEEE Transactions on Knowledge and Data S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov,\nEngineering,vol.28,no.7,pp.1621–1634,2016.[Online].Available: A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, ˙I. Polat,"
  },
  {
    "id": 994,
    "text": "Engineering,vol.28,no.7,pp.1621–1634,2016.[Online].Available: A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, ˙I. Polat,\nhttps://doi.org/10.1109/TKDE.2016.2535383 Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold,\n[46] B. N. Parlett and D. S. Scott, “The lanczos algorithm with selective R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M.\northogonalization,” Mathematics of Computation, vol. 33, no. 145, pp. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy\n217–238,1979.[Online].Available:https://doi.org/10.2307/2006037 1.0 Contributors, “SciPy 1.0: Fundamental Algorithms for Scientific\n[47] J.PasternackandD.Roth,“Knowingwhattobelieve(whenyoualready Computing in Python,” Nature Methods, vol. 17, pp. 261–272, 2020.\nknowsomething),”inCOLING,C.HuangandD.Jurafsky,Eds.,2010, [Online].Available:https://doi.org/10.1038/s41592-019-0686-2"
  },
  {
    "id": 995,
    "text": "knowsomething),”inCOLING,C.HuangandD.Jurafsky,Eds.,2010, [Online].Available:https://doi.org/10.1038/s41592-019-0686-2\npp.877–885.[Online].Available:https://aclanthology.org/C10-1099/ [67] P. Welinder and P. Perona, “Online crowdsourcing: rating annotators\n[48] O. Perron, “Zur Theorie der Matrices,” Mathematische Annalen, and obtaining cost-effective labels,” in CVPRW, 2010, pp. 25–32.\nvol. 64, no. 2, pp. 248–263, 1907. [Online]. Available: https: [Online].Available:https://doi.org/10.1109/CVPRW.2010.5543189\n//doi.org/10.1007/BF01449896 [68] P. A. White, “The computation of eigenvalues and eigenvectors\n[49] M.-T. Puth, M. Neuhaeuser, and G. D. Ruxton, “Effective use of of a matrix,” Journal of the Society for Industrial and Applied\nspearman’sandkendall’scorrelationcoefficientsforassociationbetween Mathematics, vol. 6, no. 4, pp. 393–437, 1958. [Online]. Available:\ntwo measured traits,” Anim. Behav., vol. 102, pp. 77–84, April 2015. http://www.jstor.org/stable/2098714"
  },
  {
    "id": 996,
    "text": "two measured traits,” Anim. Behav., vol. 102, pp. 77–84, April 2015. http://www.jstor.org/stable/2098714\n[Online].Available:https://doi.org/10.1016/j.anbehav.2015.01.010 [69] ——, “Hotelling’s matrix deflation,” Journal of the Society for\n[50] G. Rasch, Studies in mathematical psychology: I. Probabilistic IndustrialandAppliedMathematics,vol.6,no.4,pp.414–415,1958.\nmodels for some intelligence and attainment tests. Danmarks [Online].Available:http://www.jstor.org/stable/2098714\nPaedagogiske Institut, 1960. [Online]. Available: https://psycnet.apa. [70] ——, “Wilkinson’s vector annihilation,” Journal of the Society for\norg/record/1962-07791-000 Industrial and Applied Mathematics, vol. 6, no. 4, p. 414, 1958.\n[51] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, [Online].Available:http://www.jstor.org/stable/2098714\nL. Bogoni, and L. Moy, “Learning from crowds,” J. Mach. [71] J. Whitehill, T. fan Wu, J. Bergsma, J. R. Movellan, and P. L."
  },
  {
    "id": 997,
    "text": "L. Bogoni, and L. Moy, “Learning from crowds,” J. Mach. [71] J. Whitehill, T. fan Wu, J. Bergsma, J. R. Movellan, and P. L.\nLearn. Res., vol. 11, pp. 1297–1322, 2010. [Online]. Available: Ruvolo,“Whosevoteshouldcountmore:Optimalintegrationoflabels\nhttps://dl.acm.org/doi/10.5555/1756006.1859894 from labelers of unknown expertise,” in NIPS, 2009, pp. 2035–2043.\n[52] T. Rekatsinas, X. L. Dong, and D. Srivastava, “Characterizing and [Online].Available:https://dl.acm.org/doi/10.5555/2984093.2984321\nselectingfreshdatasources,”inSIGMOD,2014,pp.919–930.[Online]. [72] J. Yang, J. Fan, Z. Wei, G. Li, T. Liu, and X. Du, “A\nAvailable:https://doi.org/10.1145/2588555.2610504 game-based framework for crowdsourced data labeling,” The VLDB\n[53] T. Rekatsinas, M. Joglekar, H. Garcia-Molina, A. G. Parameswaran, Journal, vol. 29, no. 6, pp. 1311–1336, 2020. [Online]. Available:\nand C. Re´, “Slimfast: Guaranteed results for data fusion and source https://doi.org/10.1007/s00778-020-00613-w"
  },
  {
    "id": 998,
    "text": "and C. Re´, “Slimfast: Guaranteed results for data fusion and source https://doi.org/10.1007/s00778-020-00613-w\nreliability,” in SIGMOD, 2017, pp. 1399–1414. [Online]. Available: [73] X. Yin, J. Han, and P. S. Yu, “Truth discovery with multiple\nhttps://doi.org/10.1145/3035918.3035951 conflicting information providers on the web,” IEEE Trans. Knowl.\n[54] F. Samejima, “Estimation of latent ability using a response pattern of Data Eng., vol. 20, no. 6, pp. 796–808, 2008. [Online]. Available:\ngradedscores.”Psychometrikamonographsupplement,1969.[Online]. https://doi.org/10.1109/TKDE.2007.190745\nAvailable:https://doi.org/10.1007/BF03372160 [74] S. Yuan, Y. Zhang, J. Tang, W. Hall, and J. B. Cabota`, “Expert\n[55] ——, “A general model for free-response data.” Psychometrika finding in community question answering: a review,” Artif. Intell.\nMonograph Supplement, 1972. [Online]. Available: https://psycnet.apa. Rev., vol. 53, no. 2, pp. 843–874, 2020. [Online]. Available:"
  },
  {
    "id": 999,
    "text": "Monograph Supplement, 1972. [Online]. Available: https://psycnet.apa. Rev., vol. 53, no. 2, pp. 843–874, 2020. [Online]. Available:\norg/record/1972-28083-001 https://doi.org/10.1007/s10462-018-09680-6\n[56] ——, “A new family of models for the multiple-choice item.” [75] X. Zhang, G. Li, and J. Feng, “Crowdsourced top-k algorithms: An\nTennessee Univ Knoxville Dept of Psychology, Tech. Rep., 1979. experimental evaluation,” PVLDB, vol. 9, no. 8, pp. 612–623, 2016.\n[Online].Available:https://apps.dtic.mil/sti/citations/ADA080350 [Online].Available:http://www.vldb.org/pvldb/vol9/p612-zhang.pdf\n[57] ——, “Graded response model,” in Handbook of modern item [76] Y. Zhang, X. Chen, D. Zhou, and M. I. Jordan, “Spectral methods\nresponse theory. Springer, 1997, pp. 85–100. [Online]. Available: meet EM: A provably optimal algorithm for crowdsourcing,” J. Mach.\nhttps://doi.org/10.1007/978-1-4757-2691-6 5 Learn. Res., vol. 17, pp. 102:1–102:44, 2016. [Online]. Available:"
  },
  {
    "id": 1000,
    "text": "https://doi.org/10.1007/978-1-4757-2691-6 5 Learn. Res., vol. 17, pp. 102:1–102:44, 2016. [Online]. Available:\n[58] R. Sanchez, “Girth: Item Response Theory in Python.” [Online]. http://jmlr.org/papers/v17/14-511.html\nAvailable:https://github.com/eribean/girth [77] Y. Zheng, R. Cheng, S. Maniu, and L. Mo, “On optimality of jury\n[59] A. Sheshadri and M. Lease, “Square: A benchmark for research selectionincrowdsourcing,”inEDBT. OpenProceedings.org,2015,pp.\non computing crowd consensus,” in First AAAI conference on 193–204.[Online].Available:https://doi.org/10.5441/002/edbt.2015.18\nhuman computation and crowdsourcing, 2013. [Online]. Available: [78] Y.Zheng,G.Li,andR.Cheng,“DOCS:domain-awarecrowdsourcing\nhttps://ojs.aaai.org/index.php/HCOMP/article/view/13088 system,”PVLDB,vol.10,no.4,pp.361–372,2016.[Online].Available:\n[60] C. Spearman, “The proof and measurement of association between http://www.vldb.org/pvldb/vol10/p361-zheng.pdf"
  },
  {
    "id": 1001,
    "text": "[60] C. Spearman, “The proof and measurement of association between http://www.vldb.org/pvldb/vol10/p361-zheng.pdf\ntwo things,” The American Journal of Psychology, vol. 15, no. 1, pp. [79] Y. Zheng, G. Li, Y. Li, C. Shan, and R. Cheng, “Truth\n72–101,1904.[Online].Available:https://doi.org/10.2307/1422689 inference in crowdsourcing: Is the problem solved?” PVLDB,\n[61] G. W. Stewart and J. guang Sun, Matrix perturbation theory. vol. 10, no. 5, pp. 541–552, 2017. [Online]. Available: https:\nAcademic Press, 1990. [Online]. Available: https://www.worldcat.org/ //doi.org/10.14778/3055540.3055547\ntitle/matrix-perturbation-theory/oclc/908946968 [80] Y. Zheng, J. Wang, G. Li, R. Cheng, and J. Feng, “QASCA: A\n[62] T. Sunahase, Y. Baba, and H. Kashima, “Pairwise HITS: quality-aware task assignment system for crowdsourcing applications,”\nQuality estimation from pairwise comparisons in creator-evaluator in SIGMOD, 2015, pp. 1031–1046. [Online]. Available: https:"
  },
  {
    "id": 1002,
    "text": "Quality estimation from pairwise comparisons in creator-evaluator in SIGMOD, 2015, pp. 1031–1046. [Online]. Available: https:\ncrowdsourcing process,” in AAAI, vol. 31, no. 1, 2017, pp. 977–984. //doi.org/10.1145/2723372.2749430\n[Online].Available:https://doi.org/10.1609/aaai.v31i1.10634\n[63] A. Tarasov, S. J. Delany, and C. Cullen, “Using crowdsourcing for\nlabelling emotional speech assets,” W3C EmotionML Workshop,\n2010. [Online]. Available: https://www.w3.org/2010/10/emotionml/\npapers/tarasov.pdf\n[64] W. J. Van Der Linden and R. K. Hambleton, “Item response theory:\nBrief history, common models, and extensions,” in Handbook of\nmodern item response theory. Springer, 1997, pp. 1–28. [Online].\nAvailable:https://doi.org/10.1007/978-1-4757-2691-6 1\n[65] C. Vania, P. M. Htut, W. Huang, D. A. Mungra, R. Y. Pang,\nJ. Phang, H. Liu, K. Cho, and S. R. Bowman, “Comparing test sets\n14"
  },
  {
    "id": 1003,
    "text": "APPENDIXA to form CC⊤. The method has two main parts: showing that\nNOMENCLATURE if C is a P-matrix, then (1) CC⊤ has the special property\nof being an R-matrix (which we will define below), and (2)\nm number of users the 2nd smallest eigenvector of the related Laplacian matrix\nn number of items L = D−CC⊤ is monotonic (i.e. its elements are in either\nk number of choices (or options) for each item\nincreasing or decreasing order).\nu user j\nj\nθ ability of user j Here D is a diagonal matrix filled with the row sums of\nj\nt item i CC⊤. Given these two facts, they prove that if C is a pre-\ni\nc ih choice (or option) h for item i P matrix, then the row permutations induced by sorting the\na ih discriminationofoptionhforitemiindiscrimination-difficulty values of 2nd smallest eigenvector of L can reconstruct the\nparameterization\nC1P property for C.\nb difficulty of option h for item i in discrimination-difficulty\nih\nparameterization\nDefinition4(R-matrix[4]). AmatrixAiscalledanR-matrix"
  },
  {
    "id": 1004,
    "text": "parameterization\nC1P property for C.\nb difficulty of option h for item i in discrimination-difficulty\nih\nparameterization\nDefinition4(R-matrix[4]). AmatrixAiscalledanR-matrix\nα discrimination (or slope) of option h for item i in slope-\nih if it is symmetric and\nintercept parameterization\nβ difficulty(orintercept)ofoptionhforitemiinslope-intercept\nih\nparameterization A ji ≥A jh forj <i<h\nc i random guessing parameter in some IRT models for item i A ≤A fori<h<j\nA jth row of some matrix A ji jh\nj:\nA ith column of some matrix A\n:i Intuitively,thedefinitiondescribesamatrixwherethevalues\nA (j,i) entry of matrix A\nji fall off as we move away from the diagonal along any row.\nλ the i-th largest eigenvalue\nv i theeigenvectorcorrespondingtothei-thlargesteigenvalue(for Since for CC⊤, each matrix entry represents the dot product\ni\nsimplicity, also referred to as “i-th largest eigenvector”) oftheresponsesoftwousers.Theentriesrepresentthenumber"
  },
  {
    "id": 1005,
    "text": "i\nsimplicity, also referred to as “i-th largest eigenvector”) oftheresponsesoftwousers.Theentriesrepresentthenumber\nC (m×kn) binary response matrix of common responses for a pair of users. When the response\nD thediagonalmatrixofasquarematrix.Ofsize(m×m)when\nmatrix is sorted by user ability, these values are highest for\nit represents the diagonal matrix of CC⊤ used by ABH [4]\nusers with themselves and fall off as we move away along\nL theLaplacianmatrixofasquarematrix.Ofsize(m×m)when\nitrepresentstheLaplacianmatrixofCC⊤ usedby ABH [4]: the correct linear ordering of the abilities represented in the\nL=D−CC⊤ correctly sorted response matrix.\ns (m) user score vector where s j denotes score of user j Our method. Given a pre-P matrix C with a unique\nw (kn) option weight vector where w denotes the weight of\ni consecutive ones ordering, we use our update matrix U =\noption ((i−1) mod k)+1 for item ((i−1) div k)+1"
  },
  {
    "id": 1006,
    "text": "w (kn) option weight vector where w denotes the weight of\ni consecutive ones ordering, we use our update matrix U =\noption ((i−1) mod k)+1 for item ((i−1) div k)+1\ne the ones vector 1 for some constant r∈N+ Crow(Ccol)⊤ to find this orderings of users that reconstruct\nr\nCcol (m×kn) column-normalized response matrix the C1P property for the matrix C. To do this, we will prove\nCrow (m×kn) row-normalized response matrix the following two statements: (1) Lemma 6 shows that if C\nU (m×m)updatematrixU=Crow(Ccol)⊤ usedby AVGHITS is a P-matrix, then U is an R-matrix. (2) Lemma 7 shows\nUdiff ((m−1)×(m−1)) difference update matrix Udiff = SUT that the 2nd largest eigenvector of U is monotonic. These\nused by HND to update user score differences sdiff two statements imply that if C is a pre-P matrix, then the\nsdiff (m−1)userdifferencevectorwithsdiff =s −s ,j ∈[m−1]\nj j+1 j eigenvector corresponding to the 2nd largest eigenvalue of U\nS ((m−1)×m) matrix to compute the differences in a vector"
  },
  {
    "id": 1007,
    "text": "sdiff (m−1)userdifferencevectorwithsdiff =s −s ,j ∈[m−1]\nj j+1 j eigenvector corresponding to the 2nd largest eigenvalue of U\nS ((m−1)×m) matrix to compute the differences in a vector\ncan be used to find the unique ordering that reconstruct the\nT (m×(m−1)) matrix to reconstruct a vector based on the\ndifferences C1P property for the matrix C.\nM ((m−1)×(m−1)) matrix M=SLT Withoutlossofgenerality,weassumethateveryitemoption\nI identity matrix (quadratic matrix with 1’s on the diagonal)\nwas chosen by at least one student. If an option was never\nt number of iterations\nchosen, this option has no information and can be removed.\ne the vector with the ith element 1 and all other elements 0\ni Thus in the following proof, we assume that every column of\n[m] set {1,2,...,m}\nC contains at least one 1.\nWe first recall a well-known lemma that we will use in the\nAPPENDIXB proofs.\nPROOFS\nLemma 2 (Constant row sums). If all rows of a non-negative"
  },
  {
    "id": 1008,
    "text": "[m] set {1,2,...,m}\nC contains at least one 1.\nWe first recall a well-known lemma that we will use in the\nAPPENDIXB proofs.\nPROOFS\nLemma 2 (Constant row sums). If all rows of a non-negative\nIn this section, we prove our main Theorem 2. Before the squarematrixAsumtoascalarb,thenthelargesteigenvalue\nproof, we first review some background.15 of A is b with the corresponding eigenvector in the direction\nThe method of Atkins et al. (ABH). Given a pre-P of e=1 .\nn\nmatrixCwithauniqueconsecutiveonesordering,thespectral\nProof of Lemma 2. It is easy to see that e is an eigenvector\nmethod of Atkins et al. [4] (which we refer to as ABH) finds\nwitheigenvalueb:sinceeveryrowinAsumstob,everyentry\nthis ordering of users that reconstruct the C1P property for C\nin Ae is b. That b is the largest eigenvalue follows from the\nbyusingtheproductoftheresponsematrixCanditstranspose\nfact that the spectral radius ρ(A) ≤ ||A|| for every matrix\nnorm [42], including the induced ∞-norm ||A|| which is"
  },
  {
    "id": 1009,
    "text": "byusingtheproductoftheresponsematrixCanditstranspose\nfact that the spectral radius ρ(A) ≤ ||A|| for every matrix\nnorm [42], including the induced ∞-norm ||A|| which is\n15Thereferencesintheappendixrefertothefullreference,whichincludes ∞\n5morepapers. the maximum absolute row sum.\n15"
  },
  {
    "id": 1010,
    "text": "Note that if there are multiple connected components in add additional columns with exactly one 1 and m−1 zeros\nthe user-option bipartite graph, there is no way to get a total each to the matrix C until each row has the same number of\nordering on the users or items, since we cannot compare 1s. Note that adding such columns does not affect the C1P\nbetween the different connected components; we can only get property, since each additional column has only a single 1. In\nan ordering for each connected component separately. Thus, practice, the ordering of users might be affected by padding\nin the sequence, we assume a single connected component, the matrix with columns in this way, so we only make this\nandthus,multiplicity1ofthelargesteigenvalueoftheupdate assumption of equal row sums for proving results related to\nmatrix U. the C1P property of the ideal case.\nLemma 3 (U is row-stochastic). Each row of U has sum 1. Lemma 5. If the response matrix C is a P-matrix and each"
  },
  {
    "id": 1011,
    "text": "matrix U. the C1P property of the ideal case.\nLemma 3 (U is row-stochastic). Each row of U has sum 1. Lemma 5. If the response matrix C is a P-matrix and each\nrowhasthesamerowsum,theupdatematrixUissymmetric.\nProof of Lemma 3. Define r as the sum of elements of the\nj\njth row of U=Crow(Ccol)⊤, i.e. r = (cid:80)m U . We want to Proof of Lemma 5. First we assume each user chooses nans\nj i=1 ji\nshow that r =1,∀j ∈[m]. options. Consider any i,j ∈[m],i̸=j. We need to show that\nj\nSince U=Crow(Ccol)⊤, the jth row of U, U , is created U =U , where\nj: ij ji\nfrom the corresponding row Crow of Crow and all the rows\nj:\nCcol of Ccol: wid (cid:88) th(C)\ni: U = CrowCcol\nji jh ih\nm m m\n(cid:88) (cid:88) (cid:88) h=1\nr = Crow·((Ccol)⊤) = Crow·Ccol =Crow· Ccol\nj j: :i j: i: j: i: width(C)\n(cid:88)\ni=1 i=1 h=1 U = CrowCcol\nij ih jh\nBut recall that by construction, (cid:80)m Ccol is the ones vector h=1\ni=1 i:"
  },
  {
    "id": 1012,
    "text": "r = Crow·((Ccol)⊤) = Crow·Ccol =Crow· Ccol\nj j: :i j: i: j: i: width(C)\n(cid:88)\ni=1 i=1 h=1 U = CrowCcol\nij ih jh\nBut recall that by construction, (cid:80)m Ccol is the ones vector h=1\ni=1 i:\ne (every option is chosen by at least one student or otherwise To contribute to the sum, the respective elements of Crow\nthe whole column is removed). So, and Ccol have to both be nonzero. For all i ̸= j, Crow and\njh\nr j =Cr j o : w· (cid:88) m Cc i: ol =Cr j o : w·e=1 C al i s c h o o l m be e i a n n g s b C o r t o h w n a o n n d z C er c o ol im ar p e li b e o s th C n jh on a z n e d ro. C ih are 1, which\nih jh\ni=1 Because each user answers the same number of questions\nThe last step follows from the definition of the row- nans, all nonzero entries in Crow are equal, and therefore also\nnormalized binary response matrix Crow. Crow =Crow = 1 ifbotharenonzero.Alsobydefinitionof\njh ih nans"
  },
  {
    "id": 1013,
    "text": "normalized binary response matrix Crow. Crow =Crow = 1 ifbotharenonzero.Alsobydefinitionof\njh ih nans\nLemma 4 (1st eigenvector of AVGHITS). If the largest Ccol, C j c h ol = C i c h ol if they are both nonzero since they appear\neigenvalue of U has multiplicity 1, the fixed point of the in the same column of Ccol. Combining the two equations,\nAVGHITS update rule is in the direction of e=1 m C j ro h wC i c h ol =C i r h owC j c h ol for all i̸=j. Therefore, also U ij =U ji\nfor all i̸=j, and thus U is symmetric.\nProof of Lemma 4. The AVGHITS update rules above imply\nfor any iteration t>0, Lemma 6. If the response matrix C is a P-matrix and each\nrow has the same row sum, then the update matrix U is an\ns(t) =Us(t−1)\nR-matrix.\n=⇒ s(t) =Uts(0)\nProof of Lemma 6. We need to show that neither of\nLetλ bethelargesteigenvalueofUwiththecorresponding\n1\neigenvector v 1 . By the power rule, if λ 1 has multiplicity 1, U ji >U jk fori<k <j (4)"
  },
  {
    "id": 1014,
    "text": "Proof of Lemma 6. We need to show that neither of\nLetλ bethelargesteigenvalueofUwiththecorresponding\n1\neigenvector v 1 . By the power rule, if λ 1 has multiplicity 1, U ji >U jk fori<k <j (4)\ns(t) converges in the direction of v as t goes to infinity U <U forj <i<k (5)\n1 ji jk\n(barring the very unlikely random initialization of s(0) that\nis orthogonal to v ). is true, where\n1\nNotethatUissquareandnon-negativesinceCrow andCcol\nwidth(C)\n(cid:88)\nare both non-negative. By Lemma 3, each row of U has sum U = CrowCcol\nji jh ih\n1, and we can then use Lemma 2. This gives us the result,\nh=1\nsince the fixed point of AVGHITS is in the direction of the width(C)\n(cid:88)\neigenvector corresponding to the largest eigenvalue λ 1 of U U jk = C j ro h wC k co h l\n(provided λ has multiplicity 1) and Lemma 2 implies U has\n1 h=1\nlargesteigenvalue1withthecorrespondingeigenvectore.\nAnalogous to the proof for the symmetry (Lemma 5), we"
  },
  {
    "id": 1015,
    "text": "(provided λ has multiplicity 1) and Lemma 2 implies U has\n1 h=1\nlargesteigenvalue1withthecorrespondingeigenvectore.\nAnalogous to the proof for the symmetry (Lemma 5), we\nWe can now prove our main result in Theorem 1 using have CrowCcol = CrowCcol for all nonzero C ,C ,C . In\njh ih jh kh jh ih kh\nthe following three lemmas. In those lemmas, we make the order for (4) to hold, there would have to be more non-zero\nsimplifying assumption that each row of the matrix C has CrowCcol than CrowCcol. This in turn would imply that among\njh ih jh kh\nthe same row sum (i.e. the same number of 1s in each row), thecolumnsofCwhichhavea1inthejthrow(andtherefore\nwhich means each user chooses the same number of options. nonzero entries in the jth row of Crow) there are strictly more\nThis is WLOG, because given any arbitrary matrix C, we can 1’s in the ith row than the kth row. But since i<k <j, the\n16"
  },
  {
    "id": 1016,
    "text": "columnwhichhasa1intheithrowbutnotthekthrowleads eigenvalue 1, and the eigenvectors of Udiff are the differences\nto a violation of the consecutive ones property. between the elements of the corresponding eigenvector of U.\n(5) leads to a similar contradiction. Since the differences between the elements of the eigenvector\ncorresponding to the 2nd largest eigenvalue of U (largest\nLemma 7. IfCisaP-matrixandeachrowhasthesamerow\neigenvalueof Udiff)are non-negative,thateigenvector of U is\nsum, the 2nd largest eigenvector of U is monotonic.\nmonotonic.\nProof of Lemma 7. Wewillprovethisinawaysimilarto[4].\nDefine the matrices S ∈ R(m−1)×m and T ∈ Rm×(m−1) as The above thre lemmas imply that if we start with a pre-P\nmatrix C with a consistent row sum, sorting the rows accord-\nin Section III-C.\nNote that TS = (I −ee⊤). Note that for any vector v, ing to the second eigenvector ordering of the corresponding\nm 1"
  },
  {
    "id": 1017,
    "text": "matrix C with a consistent row sum, sorting the rows accord-\nin Section III-C.\nNote that TS = (I −ee⊤). Note that for any vector v, ing to the second eigenvector ordering of the corresponding\nm 1\nSv = (v −v ,v −v ,...,v −v )⊤. Similarly, the ith row update matrix U gives a P-matrix, proving Theorem 1.\n2 1 3 2 r r−1\nof SU is just the difference between the (i+1)th and ith rows From Lemma 1, we know by converting the converged\nlargest eigenvector of Udiff back into a user score, we regain\nof U. However, as shown in Lemma 3, each row of U sums\nthe ordering of the rows according to values in the second\nto 1. This implies each row of SU sums to 0.\nlargest eigenvector of U. This, along with Theorem 1, proves\nLet x be an eigenvector of U that is not in the direction of\nTheorem 2 that an algorithm for computing the largest eigen-\nthe all ones vector e=1 , i.e. x̸=αe. Then,\nm vectorofUdiff (HNDdetailedinAlgorithm1)reconstructsthe\nUx=λx ideal consistent ordering.\nSUx=λSx\nAPPENDIXC"
  },
  {
    "id": 1018,
    "text": "the all ones vector e=1 , i.e. x̸=αe. Then,\nm vectorofUdiff (HNDdetailedinAlgorithm1)reconstructsthe\nUx=λx ideal consistent ordering.\nSUx=λSx\nAPPENDIXC\nSU(I −ee⊤)x=λSx\nm 1 ITEMRESPONSETHEORY(IRT)MODELS\nSUTSx=λSx\nSectionC-Astartswiththebinary(ordichotomous)models\nUdiffy=λy, where y=Sx (6) that model the probability that a student answers a question\ncorrectly. Section C-B then discusses multinomial (or poly-\nThe equivalence between the 2nd and 3rd lines above is\ntomous) models that model the probability for a student to\nbecause each row of SU sums to 0 as shown above, so\nchoose a specific option of a question.\nSUe = 0. So, any eigenvalue λ of U is also an eigenvalue\nof Udiff, except for the eigenvalue of U corresponding to the\nA. Binary (or Dichotomous) Models\ne eigenvector leading to Se=0. Therefore, Udiff has exactly\nThe binary IRT models are variations of binary logistic\nthe same eigenvalues as U except the largest eigenvalue 1,"
  },
  {
    "id": 1019,
    "text": "e eigenvector leading to Se=0. Therefore, Udiff has exactly\nThe binary IRT models are variations of binary logistic\nthe same eigenvalues as U except the largest eigenvalue 1,\nand the eigenvectors of Udiff are the differences between the regression with one latent trait x: P(x) = σ(αx+β). The\nintuition is that the probability of a correct answer increases\nentries of the corresponding eigenvector of U, which proves\nwith a student’s ability and decreases with the question’s\nLemma 1.\nThen we prove that Udiff is a non-negative matrix. First, difficulty.Theabilityofastudentiscapturedbyasinglelatent\nfactor θ.\nwe think about SU. As we explained before, every row of it\n1PL (1-Parameter Logistic) model. This is the simplest\nsumsto0.Moreover,foreachtherthrowofSUwhichisthe\nlogisticIRTmodelandisalsocalledtheRaschmodel[50].Itis\ndifference between the rth and (r+1)th row of U, the first r\ncalled1PLasithasonlyoneparameterb calleddifficultyfor"
  },
  {
    "id": 1020,
    "text": "logisticIRTmodelandisalsocalledtheRaschmodel[50].Itis\ndifference between the rth and (r+1)th row of U, the first r\ncalled1PLasithasonlyoneparameterb calleddifficultyfor\nentries are non-positive while all the others are non-negative i\nevery question i which shifts the logistic function with regard\nbecause U is an R-matrix.\nto student abilities. The probability for a user with ability θ\nThen we turn to the multiplication between SU and T,\nwhich generates Udiff. The entry at the j-th row and the i-th to answer a question i correctly is given by the following\ncolumn of Udiff is the product of the j-th row of SU and response function:\nthe i-th column of T. This product is actually the sum of the P (θ)=σ (cid:0) θ−b (cid:1) = 1\nlast (m−i) entries of the j-th row of SU, which also equals i i 1+e−(θ−bi)\nto the sum of the j-th row of SU minus the first i entries.\nThus the probability of getting a question correct increases\nRecall that the first r entries are non-positive while all the"
  },
  {
    "id": 1021,
    "text": "to the sum of the j-th row of SU minus the first i entries.\nThus the probability of getting a question correct increases\nRecall that the first r entries are non-positive while all the\nwithalargerstudentabilityθ andasmallerquestiondifficulty\nothers are non-negative for the r-th row of SU, and each row\nb .\ni\nof SU sums to 0. If j ≥ i, the first i entries of the j-th row\n2PL (2-Parameter Logistic) model [5]. The 2PL model\nof SU are all non-positive so the product is non-negative. If\nadds a second parameter a called discrimination to every\ni\nj < i, the last m−i entries of the j-th row of SU are all\nquestion i. Intuitively, the discrimination models how well a\nnon-negative so the product is non-negative.\nquestioncanseparatecompetentfromlesscompetentstudents:\nTherefore, we prove every entry in Udiff is non-negative,\nHigh discrimination implies that the probability of answering\nwhich means Udiff is a non-negative matrix.\nit correctly increases strongly with student ability θ. The new"
  },
  {
    "id": 1022,
    "text": "High discrimination implies that the probability of answering\nwhich means Udiff is a non-negative matrix.\nit correctly increases strongly with student ability θ. The new\nWe can now apply the Perron-Frobenius Theorem [21],\nresponse function is:\n[48]: there exists a non-negative eigenvector of Udiff corre-\nsponding to the largest eigenvalue of Udiff. Note that Udiff P (θ)=σ (cid:0) a (θ−b ) (cid:1) = 1\nhas exactly the same eigenvalues as U, except the largest i i i 1+e−ai(θ−bi)\n17"
  },
  {
    "id": 1023,
    "text": "Notice this is exactly the logistic function after changing\n1.0\nfrom slope-intercept σ(α θ +β ) to discrimination-difficulty\ni i (cid:0) (cid:1)\nparameterizationσ a (θ−b ) .Inotherwords,the2PLmodel\ni i 0.8\nis isomorph to logistic regression. It specializes into the 1PL\nmodelbytyingalldiscriminationparameterstothesamevalue 0.6\na =1.\ni 0.4\nGLAD. This model was proposed in the crowdsourcing\nliterature[71]andalsousesthelogisticfunction.Interestingly, 0.2\nit is a specialization of the 2PL model with all difficulty\n0.0\nparameters tied by b i = 0. A special property of the GLAD 0.5 0.0 0.5\nmodel is that for a student whose ability is 0, the probability User Ability\nto answer any question correctly is 50%. The new response\nfunction is:\nP (θ)=σ (cid:0) a θ (cid:1) = 1 i i 1+e−aiθ\n3PL (3-Parameter Logistic) model [5]. The 3PL model\nadds a third parameter c to each question i that model the\ni\nprobability that a student without low ability can randomly"
  },
  {
    "id": 1024,
    "text": "3PL (3-Parameter Logistic) model [5]. The 3PL model\nadds a third parameter c to each question i that model the\ni\nprobability that a student without low ability can randomly\nguess the correct answer. This is motivated in a multiple-\nchoice question (MCQ) setting where the best strategy for a\nstudent who does not know the answer is to pick one answer\nrandomly. A reasonable setting for c is 1/k, where k is the\nnumber of choices (or options). The 3PL model specializes to\nthe 2PL model by tying all random guesses to 0: c =0. The\ni\nnew response function is\nP (θ)=c +(1−c )σ (cid:0) a (θ−b ) (cid:1) =c + 1−c i i i i i i i 1+e−ai(θ−bi)\nB. Multinomial (Polytomous) Models\nMultinomial IRT models model the probability of a student\npicking a particular choice among k options. They rely on a\ngeneralisationofthelogisticfunctiontomultipleinputscalled\nthe softmax activation function (or multinomial logit), used in multinomial logistic regression. The softmax σ :Rk →[0,1]k"
  },
  {
    "id": 1025,
    "text": "generalisationofthelogisticfunctiontomultipleinputscalled\nthe softmax activation function (or multinomial logit), used in multinomial logistic regression. The softmax σ :Rk →[0,1]k\ndefines a probability of a choice among several outcomes\nfor k ≥ 2 as σ(x)\nh\n= exh/ (cid:80)k\nl=1\nexl for h = 1...k.\nThe various multinomial IRT models can thus be seen as\nvariants of multinomial logistic regression. We next discuss\nthree important polytomous models and their connections.\nBock. Bock’s nominal category model [6] is exactly multi-\nnomial logistic regression in slope-intersection parameteri-\nzation. In this model, each option h has a discrimination\nparameter α and an intercept β . The probability for a user h h\nwith ability θ to pick option h for question i is given by the\nfollowing response function:\neαihθ+βih\nP (θ)= (7) ih (cid:80)k−1eαilθ+βil\nl=0\nThe option with the largest α is the correct option (students\nwith large enough ability θ will always choose that option)."
  },
  {
    "id": 1026,
    "text": "eαihθ+βih\nP (θ)= (7) ih (cid:80)k−1eαilθ+βil\nl=0\nThe option with the largest α is the correct option (students\nwith large enough ability θ will always choose that option).\nAlso, notice that the function is actually over-parameterized:\nDividing by eαi0θ+βi0 gives a normalized representation with\n2(k−1) independent parameters per question. With the same\nnormalization,Bockrecoversthebinary2PLmodelfork =2.\n(see e.g. [43, section 2.5.3]).\nytilibaborP\nGRM vs. Bock parameterization\n1.0\nGRM\nBock\n0.8\n0.6\n0.4\n0.2\n0.0\n0.5 0.0 0.5\nUser Ability\n(a)\nytilibaborP\nGRM and Bock (as relaxed C1P)\nGRM\nBock\n(b)\nFig. 8: (a) Example illustrating that GRM, for all practical purposes\ncanbeseenasaspecialcaseofBock.ShownareGRMwitha=8, b =(−0.2,0.2)andBockwithα =(8,16),β =(1.6,0).Three h h h\ncurvescorrespondtotheprobabilityforuserstochoosethreeoptions\nrespectively. (b): Example illustrating that GRM and Bock (like all\nIRT models discussed in this paper) can be seen as relaxed versions"
  },
  {
    "id": 1027,
    "text": "curvescorrespondtotheprobabilityforuserstochoosethreeoptions\nrespectively. (b): Example illustrating that GRM and Bock (like all\nIRT models discussed in this paper) can be seen as relaxed versions\nof a response matrix with the C1P property. Shown here are GRM\nwith a = 50, b = (−0.4,0.4) and Bock with α = (50,100),\nh h\nβ =(20,0).NoticethesimilaritywithFigure1(b)afteraccounting\nh\nfor the linear ordering of the responses.\nGraded Response Model (GRM) [54], [55], [57]. The\ngraded-response model deals with ordered polytomous cate-\ngories such as ratings (e.g. strongly disagree, disagree, agree,\nand strongly agree, used in attitude surveys). GRM postulates\nthat there are k+1 steps per question (0,...,k). A student\nchoosing an option h ∈ {0,...,k−1} passes the first h+1\nstepsbutfailsinsteph+2.Thuseverystudentpassesthefirst\nstep but no student passes the last step. The model uses the\n2PLresponsefunctionasacumulativeprobabilityfunctionfor\na student with ability θ to pass step h as:"
  },
  {
    "id": 1028,
    "text": "step but no student passes the last step. The model uses the\n2PLresponsefunctionasacumulativeprobabilityfunctionfor\na student with ability θ to pass step h as:\nP∗ (θ)=σ (cid:0) a (θ−b ) (cid:1) = 1 ih i ih 1+e−ai(θ−bih)\n−∞=b <b <...<b <b =∞\ni0 i1 i,k−1 ik\nwhere P∗ (θ) = (cid:80)k P (θ). The option response function\nih l=h il\nis then P = P∗ −P∗ . For a question i with k options,\nih ih i,h+1\nthere are k free parameters in all including a discrimination\nparameter a and k − 1 difficulty or location parameters\ni\nb ,h∈{1,...,k−1}.\nih\nNoticethatthelogitsinthecumulativeresponsefunctionP∗\nforGRMallhavethesameslopesa ,whichisdescribedasthe i\nhomogeneous case of the graded response model [57]. While\nthe correspondence is not exact, GRM can be interpreted as\nanapproximatespecialcaseoftheBockmodel(seeFigure8).\nSamejima.Similartowhat3PLaddsto2PL,Samejima[56]\nintroduced a model with random guessing based on the Bock\nmodel. The model postulates a latent “don’t-know” choice"
  },
  {
    "id": 1029,
    "text": "Samejima.Similartowhat3PLaddsto2PL,Samejima[56]\nintroduced a model with random guessing based on the Bock\nmodel. The model postulates a latent “don’t-know” choice\nnumberedzeroalongwiththegivenk options.Astudentwith\nlow enough ability will guess randomly among the options.\nNotice that this model does not exactly specialize to the 3PL\nmodel for k = 2. Unlike 3PL which is able to set a random\nguessingprobability,theSamejimamodelfixestheprobability\nof a student randomly picking the correct answer to be 1/k.\n18"
  },
  {
    "id": 1030,
    "text": "1.0\n0.8\n0.6\n0.4\n0.2\n0.0 25 50 100 200 400 800 1600\nNumber of users\ngniknar\nresu\nfo\nycaruccA\n1.0\n0.8\n0.6\n0.4 ABH Invest\nHnD PooledInv\nHITS True-Answer 0.2 TruthFinder GRM-estimator\n0.0 3 4 5 6 7\nNumber of options\n(a)Varyingm(GRM)\ngniknar\nresu\nfo\nycaruccA\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00 30 40 50 60 70 80\nAverage user accuracy\n(b)Varyingk (GRM)\ngniknar\nresu\nfo\nycaruccA\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0 0.6 0.7 0.8 0.9 1.0\nProbability\n(c)Varyingb ih (GRM)\ngniknar\nresu\nfo\nycaruccA\n(d)Varyingp(GRM)\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0 25 50 100 200 400 800 1600\nNumber of users\ngniknar\nresu\nfo\nycaruccA\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0 2 3 4 5 6\nNumber of options\n(e)Varyingm(Bock)\ngniknar\nresu\nfo\nycaruccA\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00 20 30 40 50 60 70\nAverage user accuracy\n(f)Varyingk (Bock)\ngniknar\nresu\nfo\nycaruccA\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0 0.6 0.7 0.8 0.9 1.0\nProbability\n(g)Varyingb ih (Bock)\ngniknar\nresu\nfo\nycaruccA\n(h)Varyingp(Bock)\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n2.5 5 10 20 40"
  },
  {
    "id": 1031,
    "text": "gniknar\nresu\nfo\nycaruccA\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0 0.6 0.7 0.8 0.9 1.0\nProbability\n(g)Varyingb ih (Bock)\ngniknar\nresu\nfo\nycaruccA\n(h)Varyingp(Bock)\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n2.5 5 10 20 40\nQuestion discrimination\ngniknar\nresu\nfo\nycaruccA\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n2.5 5 10 20 40\nQuestion discrimination\n(i)Varyinga ih (GRM)\ngniknar\nresu\nfo\nycaruccA\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n2.5 5 10 20 40\nQuestion discrimination\n(j)Varyinga ih (Bock)\ngniknar\nresu\nfo\nycaruccA\n(k)Varyinga ih (Samejima)\nFig. 9: Section D-A: Results of supplementary accuracy experiments (the legend is in the first figure).\nThe probability for a user with ability θ to pick an option h One major exception appears in Figures 9c and 9g. There\nfor a question i is given by the response function: we see that when the average accuracy is low for Bock and\nGRMmodel,HNDgivestheapproximatereverseranking.The\nP (θ)=\neαihθ+βih + eαi0θ\nk\n+βi0\nreason is when the difficulty is larger than the user ability,"
  },
  {
    "id": 1032,
    "text": "GRMmodel,HNDgivestheapproximatereverseranking.The\nP (θ)=\neαihθ+βih + eαi0θ\nk\n+βi0\nreason is when the difficulty is larger than the user ability,\nih (cid:80)k eαilθ+βil good users do not return consistent correct answers but worse\nl=0\nusers all choose the worst answer for such models without\nFor β →−∞, this model recovers the Bock model.\n0 random guessing. Thus all the methods tend to believe the\nmajority in such arguably unrealistic setup. Still, we see that\nAPPENDIXD\nforSamejima,HNDperformsrobustlywellbecauseSamejima\nSUPPLEMENTARYEXPERIMENTS\nmodelsrandomguessingsowhenthedifficultyishigh,thebad\nA. Accuracy on synthetic data users tend to answer random option so all methods know how\nIn addition to all experiments we show in Section IV-B, to find the good users. In all the realistic scenarios, we see\nwe also conduct an experiment to determine the accuracy HND outperforms other competitors."
  },
  {
    "id": 1033,
    "text": "we also conduct an experiment to determine the accuracy HND outperforms other competitors.\nas function of option discrimination a . Moreover, in Sec- Varying question discriminations a (Figures 9i to 9k).\nih ih\ntion IV-B, we focus on the most general polytomous IRT Recall from Section II-D that larger discriminations imply\nmodel, Samejima to avoid redundancy. Here, we also present that a question can more easily “discriminate” between able\nthe results on the other two models GRM and Bock based on and less able users, and the correct option can more easily\nthe same setup we use in Section IV-B. be picked by good users. Here, we change the discrimination\nSupplementary accuracy experiments on Bock and rangefromthedefault[0,a ]witha =10to5different\nmax max\nGRM data (Figures 9a to 9h). In Section IV-B, Figures 4a ranges, a\nmax\n∈ {2.5,5,10,20,40}. We see that HND keeps\nand 4b show the robust performance of HND for datasets high accuracy except when a\nmax\n=2.5. When a\nmax\nis small,"
  },
  {
    "id": 1034,
    "text": "max\n∈ {2.5,5,10,20,40}. We see that HND keeps\nand 4b show the robust performance of HND for datasets high accuracy except when a\nmax\n=2.5. When a\nmax\nis small,\ngenerated by Bock and GRM model with varying n but the performance between good users and bad users are close,\nfor other experiments we focus on the most general model, which means the dataset is not good at evaluating users.\nSamejima. Here we provide supplementary experimental re-\nsuts on data generated by Bock and GRM model with the\nsamesetup aswhat wedo ondata generatedby theSamejima B. Accuracy on real-world data\nmodell in Section IV-B (corresponding to Figures 4d to 4g).\nThe experimental results generally fits our observation in Here we include the details of the real datasets used in\nSection IV-B. Figure 10 and the experimental results on each individual\n19"
  },
  {
    "id": 1035,
    "text": "Dataset #users #questions #options result Accuracy of user ranking Accuracy of user ranking\nTrue-answer 92.82 True-answer 93.34\nChinese 50 24 5 Figure 11a GRM-estimator 92.60 GRM-estimator 93.68\nEn I g T lish 6 3 3 6 3 2 0 5 5 4 F F i i g g u u r r e e 1 1 1 1 b c Poole H d I I I T n n T S v v F 69.63 8 8 8 8 9 . . 0 4 7 . 0 3 43 Poole H d I I I T n n S v v 8 8 9 9 .5 .9 9 3 9 1.80\nMedicine 45 36 4 Figure 11d ABH 87.41 ABH 89.24\nPokemon 55 20 6 Figure 11e HnD 92.81 HnD 93.65\n50 60 70 80 90 100 80.082.585.087.590.092.595.097.5100.0\nScience 111 20 5 Figure 11f (a)100students (b)2692students\nFig. 10: Summary of real datasets Fig. 12: Section D-C: Accuracy experiments on synthetically simu-\nlateddatawithcorrelationofuserrankingsfollowingparametersfrom\nreal world data described in [13]. Shown are average and standard\nAccuracy of user ranking Accuracy of user ranking deviation over 10 runs.\nPooledInv 82.72 PooledInv 49.77\nInv 76.24 Inv 45.48\nTF 66.57 TF 43.71"
  },
  {
    "id": 1036,
    "text": "Accuracy of user ranking Accuracy of user ranking deviation over 10 runs.\nPooledInv 82.72 PooledInv 49.77\nInv 76.24 Inv 45.48\nTF 66.57 TF 43.71\nHITS 79.34 HITS 45.51 parameters of real-world data [13] or estimated distribution\nABH 9.07 ABH 11.38\n[65]. In this way, we have realistic data with the ground truth.\nHnD 66.83 HnD 54.88\n0 20 40 60 80 100 0 20 40 60 80 100 Experiments on simulated American Experience test\n(a)Chinese (b)English\ndata. DeMars [13] presents a detailed analysis on the Amer-\nAccuracy of user ranking Accuracy of user ranking\nican Experience test with 40 questions and 2692 participat-\nPooledInv 85.44 PooledInv 86.69\ning students. On page 87, the book presents estimates of\nInv 87.47 Inv 86.90\nparameters for a binary 3PL IRT model. We generate 10\nTF 85.61 TF 78.42\nHITS 86.67 HITS 86.37 small datasets with 100 students and 40 questions, fitting the\nABH 22.16 ABH 46.66 size of a real-world class and 10 large datasets with 2692"
  },
  {
    "id": 1037,
    "text": "TF 85.61 TF 78.42\nHITS 86.67 HITS 86.37 small datasets with 100 students and 40 questions, fitting the\nABH 22.16 ABH 46.66 size of a real-world class and 10 large datasets with 2692\nHnD 72.63 HnD 92.89 students and 40 questions following the original dataset size\n0 20 40 60 80 100 0 20 40 60 80 100 provided in [13]. In datasets of both settings, the 40 questions\n(c)IT (d)Medicine\nfollow exactly those estimated parameters from the book to\nAccuracy of user ranking Accuracy of user ranking\nPooledInv 96.10 PooledInv 84.41 simulate a realistic scenario, and the student (user) abilities\nInv 78.76 Inv 78.73 follow the normal distribution N(0,1) as indicated in [13].\nTF 69.78 TF 58.61 We then conduct experiments on each of them and take the\nHITS 83.47 HITS 81.58 average accuracy. Our results in Figure 13b clearly show the\nABH 10.32 ABH 0.86 stronger and more stable performance of HND compared to\nHnD 95.01 HnD 75.79\nother competitors except for the cheating estimators."
  },
  {
    "id": 1038,
    "text": "ABH 10.32 ABH 0.86 stronger and more stable performance of HND compared to\nHnD 95.01 HnD 75.79\nother competitors except for the cheating estimators.\n0 20 40 60 80 100 0 20 40 60 80 100\n(e)Pokemon (f)Science\nExperiments on simulated half-moon data. Besides the\nFig. 11: Detailed result on each dataset in Figure 7 simulated data based on the data distribution of the American\nExperiencetest,whichexactlyfitsourbackgroundofstudents\nand questions, we also explore the performance of HND on\ndataset of Figure 7.16 otherdata.Priorwork[65]providesadetailedanalysisofIRT\nWe like to remind that these datasets do not come with parameters over 29 datasets. They consider different natural\na ground truth on user ranking and we instead inferred an language understanding models as users in our scenario and\napproximate ranking by using the “True answer” baseline use the performances of those models to estimate the binary"
  },
  {
    "id": 1039,
    "text": "approximate ranking by using the “True answer” baseline use the performances of those models to estimate the binary\nexplained in Section IV-A as the reference ranking against 3PL IRT parameters (Section C-A) of the tasks (questions in\nwhich to compare. We have seen in the earlier experimental our scenario) in the datasets. One important take-away from\nresults that sometimes HND gives better answers than “True the paper is that the distribution of log discrimination versus\nanswer.” Thus the experimental results should be seen only as difficultytendstohaveahalf-moonpattern,whichmeansmost\na supplementary evidence. of the discriminative questions are either easy or difficult.\nFollowing this, we develop a simulated question generator\nC. Accuracy on realistic simulated data with the half-moon distribution of log discrimination versus\ndifficulty as shown in Figure 13a. As for other parameters,\nAsmentionedinSectionIV-A,wedonotknowanyexisting"
  },
  {
    "id": 1040,
    "text": "difficulty as shown in Figure 13a. As for other parameters,\nAsmentionedinSectionIV-A,wedonotknowanyexisting\nwe set user ability to follow the normal distribution N(0,1)\nbenchmark with a known true ranking of users by their\nand random guessing c to be within [0,0.5] as hinted by [65].\nabilities. In order to still verify the performance of HND in\nWe generate 10 datasets with 100 users and 100 questions,\nan as-realistic-as-possible setting, we simulate real-world data\nconduct experiments on them and take the average accuracy\nby generating synthetic data that follow prior published IRT\nas the experimental result. Figure 13b presents the strong\n16Forthefirsttwoexperiments,thecorrelationofABHisactuallynegative.\nperformance of HND, which shows that HND may be also\nWeuseitsabsolutevalueforthesimplicityofpresentation. applicable to other crowdsourcing tasks.\n20"
  },
  {
    "id": 1041,
    "text": "3\n2\n1\n0 1\n21.0 0.5 0.0 0.5 1.0 log a\nb\nAccuracy of user ranking\nTrue-answer 93.35 17.5 GRM-estimator 95.08 15.0\nPooledInv 54.51 12.5\nInv 57.33 10.0 TF 44.50 7.5\nHITS 56.61 5.0 ABH 89.71 2.5 HnD 95.13\n2 4 6 8 10\n0 20 40 60 80 100 Beta coefficient (b)Correlationofuserrankingon\n(a)Distributionofsimulated simulateddatafollowingthehalfmoon\ndatawithhalfmoonshape datadistribution\nFig. 13: Experiments on the simulated half moon data\nD. Implementation details\nTo get an approximate correspondence in discrimination\nbetween Bock and GRM models, when we parameterize a\nih\nfor Bock from [0,x], we also parameterize a for GRM from\ni\n[0, 2x ]. In this way, the average a for Bock is x and\nk+1 ih 2\nthe average GRM’s a is x . As discussed in Section II-D,\ni k+1\na = x for GRM is approximately the same case as\ni k+1\na = hx forBockforeachh.Thismeansa = x forGRM\nih k+1 i k+1\napproximates the case that the average a for Bock equals\nih\n( (cid:80) hx )/k = (k+1)kx/k = x. In this way, we get similar\nk+1 2(k+1) 2"
  },
  {
    "id": 1042,
    "text": "i k+1\na = hx forBockforeachh.Thismeansa = x forGRM\nih k+1 i k+1\napproximates the case that the average a for Bock equals\nih\n( (cid:80) hx )/k = (k+1)kx/k = x. In this way, we get similar\nk+1 2(k+1) 2\naverage discriminations for Bock and GRM.\nAnother minor detail is that when we generate the C1P\ndata, directly applying the distribution where all user abilities\nare distributed evenly within [0,1] will result in completely\nsymmetric response matrix and thus no method (even with\nsymmetry breaking) could possibly determine the side with\nbetter users. Therefore we set 10% of the users to be within\n[0,0.5]and90%userstobewithin[0.5,1]withoutinfluencing\nthe property that the response matrices are consistent.\nAPPENDIXE\nDETAILEDANALYSIS\nA. Dawid-Skene (DS) for homogenous items\nBesides the Item Response Theory, another widely used\nmodel for item labeling was proposed by Dawid and Skene\n(DS)[12].Herewefirst introducetheDSmodelandcompare\nit against thhe IRT models."
  },
  {
    "id": 1043,
    "text": "Besides the Item Response Theory, another widely used\nmodel for item labeling was proposed by Dawid and Skene\n(DS)[12].Herewefirst introducetheDSmodelandcompare\nit against thhe IRT models.\nThe DS model assumes homogenous items, i.e. they are\nof the same type. It is parameterized by one latent stochastic\nconfusionmatrixperuserwhereoff-diagonalentriesrepresent\nthe probabilities that a user mislabels an item from one\nclass with another while the diagonal elements correspond to\nprobabilitiesofmakingaccuratechoices.Forexample,assume\nm=5 users have to choose one of k =3 labels dog, cat, or\nrabbit for each of n=10 images. One entry in the confusion\nmatrixforauserspecifiestheprobabilitythatthatuserchooses\nthe label ‘dog’ when the true label is ‘cat.’ The approach\nassumes identical parameters for each item and thus requires\nmk(k−1)parametersformusers.Theconfusionmatricesand\ntrue labels are jointly estimated by maximizing the likelihood\nof observed labels via the EM algorithm [12].\nrebmun"
  },
  {
    "id": 1044,
    "text": "mk(k−1)parametersformusers.Theconfusionmatricesand\ntrue labels are jointly estimated by maximizing the likelihood\nof observed labels via the EM algorithm [12].\nrebmun\nnoitaretI\n700 600\n500\n400 300\n200 100\n0\n101 102 103 104 105 (a)Theinfluenceofβ.Each Number of questions\niterationnumberisdividedbythe\nsmallestnumber.\nsnoitareti\nfo rebmuN\nABH-Power HnD-Deflation HnD-Power\n(b)Iterationnumberswithvarying\nn\nFig. 14: Detailed Analysis on ABH-power\nThis model is widely used and studied in crowdsourcing\napplications for homogenous questions [27], [38], [51], [76],\nand a recent survey [79] on crowdsourcing recommends it\nas an implementation with little overhead. However, DS is\nnot suited to model a scenario where every question can be\ndifferent, such as in multiple-choice questions testing student\nunderstanding.IRT,incontrasttoDS,usesone(ormore)latent\nparameters for each question.\nB. Detailed Analysis on ABH-power\nAs discussed in Section III-F, it is possible theoretically"
  },
  {
    "id": 1045,
    "text": "understanding.IRT,incontrasttoDS,usesone(ormore)latent\nparameters for each question.\nB. Detailed Analysis on ABH-power\nAs discussed in Section III-F, it is possible theoretically\nto implement ABH in a similar way as HND whose time\ncomplexity is O(mnt). However, instead of O(mnt), the\ntime complexity of ABH-Power is O(mnt+m2t), which can\nbe larger than O(mnt) when m >> n. We start from the\nalgorithm to analyze ABH-Power in detail.\nThe algorithm of ABH-Power. Algorithm 2 presents our\nnovel approach for implementing ABH which we call ABH-\nPower. The main difference from Algorithm 1 are Lines 5\nand 6. Note that C is a (m × n) matrix but both D and\nI are (m × m) matrices which are much larger than C\nwhen m is much larger than n. In this case, ABH-Power has\ntime complexity O(m2t), which corresponds to the quadratic\nexecution time in Figure 5a. However, this does not explain\nwhy ABH-Power is also slower in Figure 5b. Besides, we\nobserve that when question number is large (e.g., 10000) and"
  },
  {
    "id": 1046,
    "text": "execution time in Figure 5a. However, this does not explain\nwhy ABH-Power is also slower in Figure 5b. Besides, we\nobserve that when question number is large (e.g., 10000) and\nusernumberissmall(e.g.,100),eachiterationofABH-Power\nand HND takes similar time. We believe that the reason for\nABH-Power being slower also in Figure 5b is due to the\nrequired number of iterations until convergence. We discuss\nthis next:\nThe influence of β. We discussed in Section III-E the\ncomparison between HND and ABH. To implement ABH-\npower,weneedtorelyonthematrixβI −M,andβ needs\nm−1\nto be no smaller than all the entries and eigenvalues of M. In\npractice,wesetβ tobethelargestentryinthediagonalmatrix\nof CC⊤ to fulfill the requirements. Therefore, in practice, β\nis very large. Figure 14a shows an interesting study on the\nchoice of β, which shows that the iteration number of ABH-\npowerislinearintermsofβ,whichmeansthatthelargebeta\nwe use results in more iterations compared to HND."
  },
  {
    "id": 1047,
    "text": "choice of β, which shows that the iteration number of ABH-\npowerislinearintermsofβ,whichmeansthatthelargebeta\nwe use results in more iterations compared to HND.\nNumber of iterations. The above analysis shows us why\nABH can be much slower than HND. However, to fully\n21"
  },
  {
    "id": 1048,
    "text": "Algorithm 2: ABH-Power installed.figures.ipynbprovidesanotebooktoreproduceevery\nfigure of experimental results in this paper.\nInput: ResponsematrixC,randomlyinitializeduserscoress\n0\nOutput: User scores s\n1: sdiff ←sd 0 iff // initialize user score differences\n2: repeat\n3: s←Tsdiff // update user scores\n4: w←C⊤s // update option weights\n5: s←Ds−Cw // update user scores\n6: sdiff ←βIsdiff−Ss // update user score differences\n7: Normalize sdiff to be a unit vector\n8: until convergence or iteration limit\n9:\ns←Tsdiff\nunderstandwhyinbothFigure5b,itseemsABH-Powertakes\nlonger than linear execution time, we need more detailed\nevidence of the number of iterations. Figure 14b shows\nthe numbers of iterations against varying number of users\nand questions respectively, corresponding to Figure 5b. Each\nshown data point is also the median iteration number over\nthe 5 runs. Figure 14b shows that generally as the number of\nquestions increase, the number of iterations increase as well."
  },
  {
    "id": 1049,
    "text": "shown data point is also the median iteration number over\nthe 5 runs. Figure 14b shows that generally as the number of\nquestions increase, the number of iterations increase as well.\nThis explains why in Figure 5b, ABH-Power is not linear.\nAPPENDIXF\nREPRODUCIBILITY\nIn this subsection, we briefly discuss our code repository\nstructureongithub[9](seeFigure15).Amoredetailedintro-\nduction can be found in the Readme.md file in the repository.\nFig. 15: Repository structure\nThe methods folder contains all the methods including\nvarious implementations of HITSnDIFFs and ABH, majority\nvote, “True-Answer” baseline, HITS-based approaches and\nGRM-estimator using the girth package. The experiments\nfolder contains all the experiments reported in the paper\nincluding various accuracy and efficiency experiments on\nsynthetic datasets, experiments on real-world datasets and\nexperiments to compare HnD and ABH. The datasets folder\ncontains the real-world datasets we use in the experiments."
  },
  {
    "id": 1050,
    "text": "synthetic datasets, experiments on real-world datasets and\nexperiments to compare HnD and ABH. The datasets folder\ncontains the real-world datasets we use in the experiments.\nReadme.md describes the repository structure. With the help\nof requirements.txt, all necessary python packages can be\n22"
  },
  {
    "id": 1051,
    "text": "Resource-Limited Automated Ki67 Index Estimation in Breast\nCancer\nJessicaGliozzo GiosuèMarinò ArturoBonometti\njessica.gliozzo@unimi.it giosue.marino@studenti.unimi.it arturo.bonometti@hunimed.eu\nDipartimentodiInformatica, DipartimentodiInformatica, DepartmentofBiomedicalSciences,\nUniversitàdegliStudidiMilano UniversitàdegliStudidiMilano HumanitasUniversity\nMilan,Italy Milan,Italy PieveEmanuele-Milan,Italy\nEuropeanCommission,JointResearch DepartmentofPathology,IRCCS\nCentre(JRC) HumanitasClinicalandResearch\nIspra,Italy Hospital\nRozzano–Milan,Italy\nMarcoFrasca∗ DarioMalchiodi\nmarco.frasca@unimi.it dario.malchiodi@unimi.it\nDipartimentodiInformatica, DipartimentodiInformatica&DSRC,\nUniversitàdegliStudidiMilano UniversitàdegliStudidiMilano\nMilan,Italy Milan,Italy\nCININationalLaboratoryinArtificial CININationalLaboratoryinArtificial\nIntelligenceandIntelligentSystems IntelligenceandIntelligentSystems\nRome,Italy Rome,Italy\nABSTRACT CCSCONCEPTS"
  },
  {
    "id": 1052,
    "text": "CININationalLaboratoryinArtificial CININationalLaboratoryinArtificial\nIntelligenceandIntelligentSystems IntelligenceandIntelligentSystems\nRome,Italy Rome,Italy\nABSTRACT CCSCONCEPTS\nThepredictionoftumorprogressionandchemotherapyresponse •Computingmethodologies→Neuralnetworks;•Appliedcom-\nhasbeenrecentlytackledexploitingTumorInfiltratingLympho- puting→Bioinformatics;•Generalandreference→Perfor-\ncytes(TILs)andthenuclearproteinKi67asprognosticfactors.Re- mance.\ncently,deepneuralnetworks(DNNs)havebeenshowntoachieve\ntopresultsinestimatingKi67expressionandsimultaneousdeter- KEYWORDS\nminationofintratumoralTILsscoreinbreastcancercells.However,\nTumor infiltrating lymphocytes, Ki67 protein, Resource-limited\ninthelasttenyearstheextraordinaryprogressinducedbydeep\nlearning,Resource-limiteddevices,DNNcompression,Deeplearn-\nmodelsproliferatedatleastasmuchastheirresourcedemand.The\ning.\nexorbitant computational costs required to query (and in some"
  },
  {
    "id": 1053,
    "text": "learning,Resource-limiteddevices,DNNcompression,Deeplearn-\nmodelsproliferatedatleastasmuchastheirresourcedemand.The\ning.\nexorbitant computational costs required to query (and in some\ncasesalsotostore)adeepmodelrepresentastronglimitationin ACMReferenceFormat:\nJessicaGliozzo,GiosuèMarinò,ArturoBonometti,MarcoFrasca,andDario\nresource-limited contexts, like that of IoT-based applications to\nMalchiodi.2023.Resource-LimitedAutomatedKi67IndexEstimationin\nsupporthealthcarepersonnel.Tothisend,weproposearesource\nBreastCancer.In2023the10thInternationalConferenceonBioinformatics\nconsumption-awareDNNfortheeffectiveestimateofthepercent-\nResearchandApplications(ICBRA)(ICBRA2023),September22–24,2023,\nageofKi67-positivecellsinbreastcancerscreenings.Ourapproach\nBarcelona,Spain.ACM,NewYork,NY,USA,8pages.https://doi.org/10.\nreducedupto75%and89%theusageofmemoryanddiskspace 1145/3632047.3632072\nrespectively,upto1.5×theenergyconsumption,andpreserved\norimprovedtheoverallaccuracyofabenchmarkstate-of-the-art"
  },
  {
    "id": 1054,
    "text": "reducedupto75%and89%theusageofmemoryanddiskspace 1145/3632047.3632072\nrespectively,upto1.5×theenergyconsumption,andpreserved\norimprovedtheoverallaccuracyofabenchmarkstate-of-the-art\n1 INTRODUCTION\nsolution.Encouragedbysuchpositiveresults,wedevelopedand\nThenuclearproteinKi67hasbeenintroducedasaproliferative\nstructuredtheadoptedframeworksoastoallowitsgeneralpurpose\nmarker to be used along with Tumor Infiltrating Lymphocytes\nusage,alongwithapublicsoftwarerepositorytosupportitsusage.\n(TILs)asafeatureeffectivelydrivingthepredictionoftumorpro-\ngressionandchemotherapyresponse[31,48].Ki67estimationin\n∗Correspondingauthor surgical pathology is a specific example of a task in which the\nresultoftheevaluationbearsasignificantclinicalconsequence\ninmanycancertypes.Indeed,thepathologypracticeconsiders\nPermissiontomakedigitalorhardcopiesofpartorallofthisworkforpersonalor differentpositivitycut-offstodiscriminatebetweenlesionswith"
  },
  {
    "id": 1055,
    "text": "inmanycancertypes.Indeed,thepathologypracticeconsiders\nPermissiontomakedigitalorhardcopiesofpartorallofthisworkforpersonalor differentpositivitycut-offstodiscriminatebetweenlesionswith\nclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed differentoverallprognosisortherapyresponse[41,49].Forthis\nforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation\nreason,anhighaccuracyinKi67estimationisessentialtoavoid\nonthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored.\nForallotheruses,contacttheowner/author(s). overorunder-gradingofthescrutinizedsample.Inbreastpathol-\nICBRA2023,September22–24,2023,Barcelona,Spain ogythistaskmayoccupyamajorroleintheroutineofpathologists\n©2023Copyrightheldbytheowner/author(s).\ngiventhehighnumberofsurgicalsamplesproduceddailyinboth\nACMISBN979-8-4007-0815-2/23/09.\nhttps://doi.org/10.1145/3632047.3632072 referralandperipheralpathologycentres,asaconsequenceofthe\n3202\nceD\n22\n]MQ.oib-q[\n1v41000.1042:viXra"
  },
  {
    "id": 1056,
    "text": "ICBRA2023,September22–24,2023,Barcelona,Spain Gliozzoetal.\neffectivenessofthewomanhealthscreeningpoliciesindifferent adopted.Tothisend,Sahaetal.[44]initiallyproposedtousea\ncountries.Thesetasksrequirethesegmentationandvisualcount GammamixturemodelwithExpectation-Maximizationtoidentify\nofcellsbypathologists,whichisarelativelytime-consumingtask seedpointsandpatchselection,whicharefedtoaCNNmodelhav-\nsubjecttohighintra-andinter-operatorvariability,relatedtothe ingacustomdecisionlayerbasedondecisiontrees.Themethod\nexperienceofeachprofessional[8,20].Furthermore,intheevalua- achieved91%ofF-score.Zhangetal.[54]exploredtheuseofCNN\ntionofallthesefeaturesitshouldalsobeconsideredthepresence forKi67imageclassification,whereaprobabilityheatmapcanbe\nofadifferentdegreeofintra-tumoralheterogeneity,dependingon obtainedfromrepositioningtheclassifiedpatchestotheiroriginal\nthecancerhistotypeandthetypeoftissuesample[57].Thus,the location.Theratiooftumorcellsintheheatmapcanbeusedas"
  },
  {
    "id": 1057,
    "text": "thecancerhistotypeandthetypeoftissuesample[57].Thus,the location.Theratiooftumorcellsintheheatmapcanbeusedas\ndevelopmentofautomaticcellcountapproachesenablesafaster anindicatorforKi67expression.Moreover,theyexploredtheuse\nandmorereliablediagnosis/prognosis.Indeed,eveniftheevalua- ofsingleshotmultiboxdetectorstodetectKi67positiveandnega-\ntionofasingleKi67slidemaybearelativelyfastoperation,the tivecells.Fulawkaetal.[16]appliedaCNNDenseNetmodelwith\nin-seriesestimationofalargenumberofcancersamplesistime fuzzyinterpretationtoobtainabinarymask,whichisthenusedto\nconsumingandmaybenefitofautomatedtoolsabletoprovideand segmentbreastcancercellsandcomputeKi67index.Negahbaniet\nintegratesuchdatainthehistopathologyreport,independentlyof al.[39]introducedapipelineforcellclassificationanddetectionof\nthecomputationalresourcesavailable.Thislatteraspectisalsoof bothKi67andTILsthatexploitsaU-Netarchitecturewithnovel"
  },
  {
    "id": 1058,
    "text": "thecomputationalresourcesavailable.Thislatteraspectisalsoof bothKi67andTILsthatexploitsaU-Netarchitecturewithnovel\ncriticalimportance,asitshouldbeconsideredthateventhough residualdilatedinceptionmodules(seesection2.2forfurtherde-\nscreeningprogramsarecommonlyemployedatleastinwestern tails).However,theresourcesrequiredtorunDLapplicationsare\ncountries,noteveryfacilitymaydisposeofenoughcomputational nottrivial,beingthesemodelstypicallyoverparameterized[3],and\nresourcestorunDeepLearning(DL)software[24,40].Alongwith thereforerequiringahighamountofcomputationalresources.We\ntheproblemofKi67estimation,alsotheidentificationofTIL-score referheretothesizeofthelearnedDLmodels,andnottotheever-\nisrelevantinbreastcancerbecauseithasaprognosticrole,asa growingsizeofthedatasetstobeprocessed[28],orthesizeofthe\ncomponentoftheimmunesystemfightingtumorprogression[47]. singleimages,astheseissueshavebeenaddressedinliterature(see,"
  },
  {
    "id": 1059,
    "text": "componentoftheimmunesystemfightingtumorprogression[47]. singleimages,astheseissueshavebeenaddressedinliterature(see,\nTILestimationfromhistologicalimagespresentchallengessimilar e.g.,[25]andtheuseoftiles/patches[5]).Justtostatesomeexam-\ntothoserelatedtoKi67(i.e.thetaskeasilybecomestime-consuming ples,dependingontheirspecificimplementation,thespaceneeded\nandoperator-dependent),thusalsothisactivitycantakeadvantage tostoreaCNNtrainedforimageclassificationpurposes,suchas\nfromtheapplicationofDL. thepreviouslymentionedDenseNet,ResNet,orAlexNetmodels,\nThesuccessfulresultsofdeepconvolutionalmodelsarenotsur- variesfromtenstohundredsofmegabytes;thisrequirementjumps\nprising,sincethesemodelsprovedtobeoneofthemosteffective uptoseveraltensgigabytesifweconsidermodernLargeLanguage\nandflexibletoolstotacklehealthcareproblems[4,13].Thesemeth- Models (e.g., 11 billions of parameters for some variants of the"
  },
  {
    "id": 1060,
    "text": "andflexibletoolstotacklehealthcareproblems[4,13].Thesemeth- Models (e.g., 11 billions of parameters for some variants of the\nodsarecurrentlyappliedtoanalyseawidespectrumofmedical T5Text-To-TextTransformerModel[42],meaningaround44ter-\ndata types, such as (I) clinical images, (II) biosignals, (III) high- abytesofRAMwhenusing4bytesperparameter).Tofacetheissue\ndimensionalomicsdata,and(IV)ElectronicHealthRecords,achiev- relatedtotheexcessiveresourceusage,recentworksfocusedon\ningstate-of-the-artperformanceonseveraltasks,includingthe specificallydesigningdeepmodelsformobiledevices[55],which\nrecognitionofmitoses,nodalmetastases,orthemeasurementof however,despitetheireffectiveness,isapplicabletonovelarchi-\nprognosticmarkers[23,26,27,52]. tecturesbutnottotheexistingones.Indeed,mostavailableDNNs\nTheapplicationofDLmodelsisalsopervasiveintheanalysisof havebeenpre-trained(oftenusingamorethanremarkableamount"
  },
  {
    "id": 1061,
    "text": "prognosticmarkers[23,26,27,52]. tecturesbutnottotheexistingones.Indeed,mostavailableDNNs\nTheapplicationofDLmodelsisalsopervasiveintheanalysisof havebeenpre-trained(oftenusingamorethanremarkableamount\nbreastcancerhistopathologicalimages,whereitiseffectivetotackle ofresources)withtheaimofdeployingthemontostandardcom-\ndifferenttasks:imageclassification[2,10,36],cellcontoursand putingfacilities.Suchaspectcaneasilyhampertheadoptionof\nnucleidetection[37]andKi67-indexestimation.Regardingbreast DL-basedmedicalsoftware,especiallyondeviceswithlimitedre-\ncancerimageclassification,state-of-the-artmethodsarebasedon sources,likesmartphonesorIoT-devices[1,18,30].Further,this\nconvolutional neural networks. Nawaz et al. [38] proposed the isevenmorerelevantindevelopingcountries,characterizedby\nuseoftheconvolutionalarchitectureDenseNettoperformmulti- pooreconomicconditions—mainlyintheruralareas—andoften"
  },
  {
    "id": 1062,
    "text": "useoftheconvolutionalarchitectureDenseNettoperformmulti- pooreconomicconditions—mainlyintheruralareas—andoften\nclassclassificationofhistopathologicalimagesfromtheBreakHis evenbyshortageofphysicians,whichwouldhighlybenefitfrom\ndataset[46],reachinganaccuracyof95.4%.Xieetal.[51]used AIsystemstosupportclinicaldecisions.Low-resourceDLmodels\ntheInception_V3andInception_ResNet_V2networkstoclassify canimprovetheexploitationandapplicabilityofthebasicwearable\nbreastcancerhistopathologicalimagesthroughtransferlearning, networkeddevicesavailable,andfosteraspeed-uptowardrevolu-\noutperformingmanypreviouslyproposedarchitectures(AlexNet, tionarychangesinthehealthcaresystemsofthesecountries[21].\nCSDCNN,LeNet).Zhuetal.[56]usedafully-CNNexploitingcon- Ontheotherside,cloud-basedsolutionsmightbeusedforhosting\nvolution and deconvolution layers to directly regress a density largeDLmodelsinordertoquerythemonline.However,apart"
  },
  {
    "id": 1063,
    "text": "volution and deconvolution layers to directly regress a density largeDLmodelsinordertoquerythemonline.However,apart\nmapshowingthepositionofcells.Interestingly,theyevaluatedthe privacyandsecurityissuesinducedbysensibledata,itwouldalso\nmodelalsoonhistopathologicalbreastlymphnodeimages.Xueet requireanactiveInternetconnection,whichmightbeaproblem\nal.[53]useAlexNetandResNetwithEuclideanlosstopredictthe inpoorruralareas.Moreover,Internetandcloud-basedsolutions\nnumberofcellsinimagepatches.TheuseofDLforcellcountingis haveacostwhichhastobetakenintoaccount,inrelationwiththe\nsowell-establishedinliteraturethatanImageJplugin[14]nowen- (scarce)availablebudget.\nablestheuseofU-Netmodelstoperformcelldetectionongeneric Insuchacontext,thisstudyproposesanautomatedAImethod\nbiomedicalimagedata.Finally,theestimationofKi67-scorefrom forcellclassificationanddetectionofKi67andTILs,withapar-\nimmunohistochemicalimagesisanothertaskwhereDLislargely ticularattentiontotheresourceusage.Weshowoursolutionto"
  },
  {
    "id": 1064,
    "text": "Resource-LimitedAutomatedKi67IndexEstimationinBreastCancer ICBRA2023,September22–24,2023,Barcelona,Spain\nbecompetitivewiththestate-of-the-artsolution,PathoNet[39], anetworklayer.Thecompressionratio isdefinedastheratio𝜓\naCNNshowntobetop-performinginclassifyingbreastcancer betweenthesizesoftheuncompressedandcompressedmatrix,\nimagesbelongingtothreeclassesKi67-immunopositive,Ki67-im- thatis𝜓 =size(𝑾o)/size(𝑾),wheresize(𝑥)isthememorysizeof\nmunonegativeandtumorinfiltratinglymphocytes,whilereducing 𝑥.Ingeneral,uppercaseboldfacesymbolswilldenotematrices,and\nitsRAManddiskrequirementsupto4×and9×,respectively,along thecorrespondinglowercaseletterswillrefertomatrixentries(e.g.,\nwithitsenergyconsumption.Moreover,onceverifiedtheeffective- 𝑤owillbeanentryof𝑾o).Vectors—precisely,rowvectors—will\nnessofourmethodology,wehavedoneafurthersteptowardsthe berenderedusingitalicboldface(e.g.,𝒙).\nrealizationofamoregeneralframeworktoreducetheresource\n2.2.2 CompressionFramework. Thefirstphaseoftheproposed"
  },
  {
    "id": 1065,
    "text": "realizationofamoregeneralframeworktoreducetheresource\n2.2.2 CompressionFramework. Thefirstphaseoftheproposed\ndemandofexistingpre-trainedDLmodels.Inthissense,PathoNet\nmethodologyconcernsthereuseofanexistingpre-trainedmodel,\ncanbeconsideredasausecaseofsuchamethodology,relatively\n‘toy’foritsmemorysizeofaround13𝑀𝐵,butwewillargumentin ascommonnowadaysgiventhelargenumberofpubliclyavailable\npre-trainedmodelsdevelopedforvariousapplications.Toshow\nthediscussionattheendofthepaperhowrecentresearchesalready\nthe power and flexibility of the proposed approach we thereby\nproposedDNNsforthesameproblemrequiringseveralhundreds\ncompressthepre-trainedPathoNetnetwork,retrievedfromthe\nofmegabytesofRAM.Summarizing,weprovideaPythonmodule\nsamerepositoryholdingthepreviouslymentioneddata.\nto1)firstlettheexistingpre-trainedmodeltoundergolossycom-\npressionofitslayers,andthenlosslessstorageofthecompressed\nThePathoNetmodel. PathoNetisaCNNproposedfortheaccu-\nlayers;2)toruninmainmemorythemodelintheresource-saver"
  },
  {
    "id": 1066,
    "text": "pressionofitslayers,andthenlosslessstorageofthecompressed\nThePathoNetmodel. PathoNetisaCNNproposedfortheaccu-\nlayers;2)toruninmainmemorythemodelintheresource-saver\nratedetectionandcountoftumoralcells,appropriatelystainedfor\nformatandefficientlyqueryingitdirectlyinsuchaformat,without\nKi67andTILs,frombiopsyimagesofmalignantbreasttumors.The\nneedtoreconvertittotheoriginaluncompressedformat.\ncellspresentintheseimageshavebeenlabelledasKi67positive\nThepaperisorganizedasfollow:Sect.2illustratestheuseddata,\ntumorcells,Ki67negativetumorcells,andlymphocytesinfiltrating\nmodelandoverallmethodology.Theobtainedresultsaredescribed\nthecancerarea.Themodelisdesignedtodetectandclassifycells\ninSect.3anddiscussedinSect.4.Someconcludingremarksend\naccordingtothesethreeclasses[39].\nthepaper.\nTodealwiththesizevariabilityofcellsfromimagetoimage,\nPathoNetismostlycomposedofdilatedinceptionlayers.Onthe\n2 METHODS\nonehand,thissolvestheproblemofchoosingafixedkernelsize"
  },
  {
    "id": 1067,
    "text": "thepaper.\nTodealwiththesizevariabilityofcellsfromimagetoimage,\nPathoNetismostlycomposedofdilatedinceptionlayers.Onthe\n2 METHODS\nonehand,thissolvestheproblemofchoosingafixedkernelsize\nThissectiondepictstheapplicationoftheproposedframeworkto byusingdifferentkernelsizesinonemodule;ontheotherone,it\ntheconsideredcasestudy.Moreprecisely,Sect.2.1describesthe enlargesthenetworkstructurewithoutsufferingfromthevanish-\nuseddataset,whileSect.2.2illustratestheframeworkadoptedto inggradientproblemwhenincreasingthenumberofkernels.In\nlearnthelow-resourcesmodel,aswellashowtorunqueriesinthe particular,residualdilatedinceptionmodules(RDIMs)areusedin\ncompressedstorageformat. theencoderanddecoderpartofthenetwork.Eachofthesemodules\nconsistsoftwoparallelparts,thefirstcomposedbytwostacked\n2.1 ProblemDefinitionandData\nconvolutionallayerswithkernelsize3,andthesecondbuiltby\nWerequestedtotheauthorsthebreastcancerinvasiveductalcarci- stackingtwo3×3dilatedconvolutionallayerswithdilation4.Tore-"
  },
  {
    "id": 1068,
    "text": "convolutionallayerswithkernelsize3,andthesecondbuiltby\nWerequestedtotheauthorsthebreastcancerinvasiveductalcarci- stackingtwo3×3dilatedconvolutionallayerswithdilation4.Tore-\nnomadataset(SHIDC-B-Ki-67-V1.0)1,whichisalreadydividedina ducethenumberofparameters,andconsequentlythepossibilityof\ntrainandatestset(containing1656and700images,respectively). overfitting,theoutputsofthesetwopartsarenotconcatenatedbut\nEachimageisassociatedwithaJSONfilecontainingthenuclei summedup.Overall,PathoNetutilizesaU-Net-likestructure[43],\ncoordinates(positiononXandYaxesofthecellcenter)andthe wheremostconvolutionallayersarereplacedbyRDIMs.Thestruc-\ngroundtruthlabelforeachnucleus(where1=Ki67-immunopositive, ture of the network is the following: the input layer is initially\n2=Ki67-immunonegativeand3=TIL).Fromtheprovidedtraining processedusingtwoconvolutionallayers,inturnstackedoverfour\nset,wecreatedavalidationsetrandomlyselecting20%ofthela- encoderRDIMsandfourdecoderRDIMs;thelatterarefollowed"
  },
  {
    "id": 1069,
    "text": "set,wecreatedavalidationsetrandomlyselecting20%ofthela- encoderRDIMsandfourdecoderRDIMs;thelatterarefollowed\nbeledimages.Wecheckedthattheobtainedtrain(composedbythe bythree1×1convolutionallayerswithlinearactivationfunction,\nremaining80%oftheimages)andvalidationsethadanaverage usedtoproducethethree-channeloutputofthemodel(see[39],\nnumberofcellsperimage(i.e.“avg./IMG”)similartothevalues Figure5,foravisualrepresentation).\npublishedinTable2ofthereferencepaper[39].Theobtainedvalues\nLossycompressionofnetworklayers. Thisstepiscrucialtoachieve\nofavg./IMGarepresentedinTable1.Thetrainsetundergoesdata\ngoodcompressionratioswhilenotaffectingthemodelaccuracy.\naugmentationbyflippingimagesw.r.t.XandYaxesandapplying\nAmongthevastplethoraofcompressiontechniquesproposedin\nrotations,asdoneinthereferencepaper.\nliterature(see,e.g.[11]forasurvey),wedesignedandexploited\n2.2 Algorithm themostsuitabletechniquesforthestructureofPathoNet.Wedid\nnotconsiderweightorstructuralpruning,sincethemodelnetwork"
  },
  {
    "id": 1070,
    "text": "literature(see,e.g.[11]forasurvey),wedesignedandexploited\n2.2 Algorithm themostsuitabletechniquesforthestructureofPathoNet.Wedid\nnotconsiderweightorstructuralpruning,sincethemodelnetwork\n2.2.1 Notationandpreliminarydefinitions. Mostmemoryanddisk\nstructureisalreadytunedbytheauthors,withawell-conceived\nrequirementsofaconvolutionalneuralnetwork(CNN)aredue\nandpreciseorganizationoftheindividualblocks.Weightprun-\ntothestorageofweightconnectionsforeachlayerinthemodel.\ning,forinstance,isrewardedwhenattaininghighsparsitylevels\nThus, the compression of an existing CNN mainly maps to the (>0.5),whichinturnallowtheusageofcompressedformatssuch\nproblem of finding out a succinct approximation𝑾 of a given\nasCSC.However,anexcessivepruningofconvolutionallayers\nmatrix/tensor𝑾orepresentingthelearnedconnectionweightsof\ncanhighlydeterioratethepredictivecapabilitiesofthemodel[34].\n1https://github.com/SHIDCenter/PathoNet Additionally,ouraimisalsotonotslowerthemodelexecution,"
  },
  {
    "id": 1071,
    "text": "ICBRA2023,September22–24,2023,Barcelona,Spain Gliozzoetal.\nTable1:Averagenumberofcellsperimage(avg./IMG)andnumberofannotatedcells(#cells)foreachset,presentedacrosscell\nlabels,whereimmunopositive(Ki67+)andimmunonegative(Ki67-)representtheresultofKi67staining.\nTraining+\nTraining Validation Test\nValidation\navg./IMG #cells avg./IMG #cells avg./IMG #cells avg./IMG #cells\nKi67+ 21.21 35106 20.88 27664 22.48 7442 22.51 15755\nKi67- 45.31 75010 45.04 59683 46.31 15327 46.63 32643\nTIL 1.88 3112 1.81 2402 2.15 710 1.97 1380\nalongwithobtainingitsspacereduction.Accordingly,weoperated and the entropy of the resulting discrete distribution for\naweightquantizationofconvolutionallayers,consistinginbuild- representativeweights[7].\ningthematrix𝑾 usingalimitednumberofdistinctweights,each Aretrainingphaseisfinallyappliedafterquantization,ensur-\nrepresentedusinglessbitsthaneachentryin𝑾o.Theideaisto\ningthattheupdatedweightsalwaysassumevaluesinthesetof"
  },
  {
    "id": 1072,
    "text": "representedusinglessbitsthaneachentryin𝑾o.Theideaisto\ningthattheupdatedweightsalwaysassumevaluesinthesetof\ncastconnectionweightsintocategoriesandsubstitutingallweights representatives[22].\ninacategorywitharepresentative.Thisapproach,namedweight\nLosslessstorageofthecompressednetwork. Thethirdphaseof\nsharing(WS),allowstosaveroomwhencombinedwiththeIndex\nourframeworkisthedesignofasuitableroom-saverformatforthe\nMapstorageformat,whichconsistsinreplacingweightswiththeir\nmodel,tailoredforthecompressionschemesusedintheprevious\ncategory/representativeindex,atthecostofstoringseparatelythe\nstep,andabletoperformthemodelexecutionwithoutre-expanding\nvectorofrepresentativeweights.Theadvantageofthisformatis\nthenetwork.Asmentionedabove,anefficientsolutiontoexploitthe\nthatitonlyaddsonelevelmemoryaccessandalmostpreserves\nquantizedtensorsisrepresentedbytheIndexMap(IM)format[22].\nthedottimeefficiencyoftheoriginalmodel.Itisworthpointing"
  },
  {
    "id": 1073,
    "text": "thatitonlyaddsonelevelmemoryaccessandalmostpreserves\nquantizedtensorsisrepresentedbytheIndexMap(IM)format[22].\nthedottimeefficiencyoftheoriginalmodel.Itisworthpointing\noutthatincasethemodeltocompresscontainsalsoothertypes Representativesarestoredinavector𝒄 = {𝑐 1 ,...,𝑐 𝑘},whosein-\ndicesareentriesofanewmatrix/tensor𝑴.Thus,if𝑤o ∈ 𝑾o is\noflayers,e.g.,fully-connectedlayers,specificandmoresuitable\nlossycompressionandlosslessstorageapproachescanbelever-\nassociatedwithcentroid,say,𝑐 2,thenthecorrespondingentryin\n𝑴issetto2.When𝑾o(andaccordingly𝑴)hasdimension𝑛×𝑚,\naged,exploitingforinstancepruning+quantizationcompression\ndenotedby𝑏and𝑏¯thenumberofbitsusedtostoreoneentryof\nandaddressmapstorage[32,33].\n𝑾oandof𝑴,respectively,thecompressionratioobtainedis:\nThestrategyusedtogroupweightsandtoselectrepresenta-\ntivesdistinguishesthefourstate-of-the-artquantizationalgorithms 𝑏𝑛𝑚\nconsideredinthisstudyanddescribedinthefollowing. 𝜓 =\n𝑏¯𝑛𝑚+𝑏𝑘"
  },
  {
    "id": 1074,
    "text": "Thestrategyusedtogroupweightsandtoselectrepresenta-\ntivesdistinguishesthefourstate-of-the-artquantizationalgorithms 𝑏𝑛𝑚\nconsideredinthisstudyanddescribedinthefollowing. 𝜓 =\n𝑏¯𝑛𝑚+𝑏𝑘\nClustering-basedWS(CWS).Thisstrategygroupsweights Forinstance,when𝑘 ≤256,𝑏¯=8isenoughtorepresent28 =\ninto𝑘clustersviathe𝑘-meansalgorithm[35],andusesthe\n256differentindices,andassumingatypicalFP32formatfor𝑾o\nresultingcentroids{𝑐\n1\n,...,𝑐 𝑘}asrepresentativestoreplace\n(𝑏 = 32),thecompressionratiowouldbe𝜓 ≈ 4.Weremarkthat\ntheweightsinthecorrespondingcluster[22].\nthisformatdoesnotinduceanyinformationloss,whileneeding\nProbabilisticWS(PWS).ThistechniqueisbasedontheProb-\nonlyoneadditionalmemoryaccesstoretrieveagivenweight.\nabilisticQuantizationmethod[33],inwhicharandomized\nalgorithmtransformseachweight𝑤o ∈𝑾oinoneof𝑘dis- Modelinferenceintheresource-saverformat. Thefinalstepof\ntinctrepresentatives𝑐 1 ,...,𝑐 𝑘.Anicefeatureofthismethod ourframeworkisthecomputationofmatrix/tensorproductsdi-"
  },
  {
    "id": 1075,
    "text": "tinctrepresentatives𝑐 1 ,...,𝑐 𝑘.Anicefeatureofthismethod ourframeworkisthecomputationofmatrix/tensorproductsdi-\nconsistsinthefactthattheobtained𝑾 canbeseenasthe rectlyinthecompressedformatusedatpreviousstep.Without\nvalueofanunbiasedestimatorfor𝑾o(see[33]fordetails). lossofgenerality,wecanassumethatthelayerweightsarerepre-\nUniformQuantization(UQ).Inthisscheme,whichachieved sentedbyamatrix.Tocomputetheoutput𝒐 = 𝒙 ·𝑾 ofagiven\ntopcompressionperformanceinrecentapplicationstoCNNs compressedlayerwithweightmatrix𝑾 ontheinput𝒙,thatis\ncompression[6],representativesareselectedbyuniformly 𝑜 𝑖 =(cid:205) 𝑗 𝑥 𝑗 𝑤 𝑗𝑖usingIMformat,weperform𝑜 𝑖 =(cid:205) 𝑗 𝑥 𝑗 𝑐 𝑚𝑗𝑖 .Froma\npartitioningtheentireweightdomaininto𝑘subintervals2. memoryconsumptionstandpoint,thisdoesnotneedtoexpandthe\nSuchaselectionhasbeenproventoyieldanoutputentropy compressedmatrix,anditstillkeepsthemodelmemoryfootprint\nwhichisasymptoticallysmallerthanthatofanyotherquan- 𝜓 timessmallerthantheoriginalone."
  },
  {
    "id": 1076,
    "text": "Suchaselectionhasbeenproventoyieldanoutputentropy compressedmatrix,anditstillkeepsthemodelmemoryfootprint\nwhichisasymptoticallysmallerthanthatofanyotherquan- 𝜓 timessmallerthantheoriginalone.\ntizer, regardless of the source statistics, when the source\nImplementation. ThePathoNetsourcecodewasimplemented\ndensityfunctionissufficientlysmoothed[19].\ninPython3,usingTensorflowandKeras.Ourcompressiontech-\nEntropyConstrainedScalarQuantization(ECSQ):Thisisa\nniquesandtheretrainingprocedureshavebeenimplementedinthe\ntechniqueleveraginganiterativeoptimizationalgorithmto\nsameprogrammingenvironment.Ourimplementation,availableon\ndeterminetheoptimalnumberofgroups.Itisdrivenbythe\nGitHub3,allows:(i)toperformthecompressionandretrainingof\njointoptimizationoftheexpectedvalueforthequantization\nPathoNetwithdifferentquantizationtechniques;(ii)tocomputethe\ndistortion(ameasureofthedistancebetween𝑾o and𝑾)\ncompressionratio(relativetothememoryspace);(iii)toevaluate"
  },
  {
    "id": 1077,
    "text": "PathoNetwithdifferentquantizationtechniques;(ii)tocomputethe\ndistortion(ameasureofthedistancebetween𝑾o and𝑾)\ncompressionratio(relativetothememoryspace);(iii)toevaluate\n2Notethattheactualnumberofsubintervals𝑘canbelowerthantheinputvaluedue thecompressedmodelspaceondiskandthepredictiontimeratio\ntotheinternalselectionofthe𝛿hyperparameterofthemethod(see[6]forfurther\ndetails). 3https://github.com/GliozzoJ/pathonet_compression"
  },
  {
    "id": 1078,
    "text": "Resource-LimitedAutomatedKi67IndexEstimationinBreastCancer ICBRA2023,September22–24,2023,Barcelona,Spain\ncomparedtotheuncompressedversionofPathoNet.Thereposi- Table2:Valuesofcumulativelearningrate(clr)andnumber\ntoryalsocontainsaJupyternotebookallowingthereplicationof\nofgroups𝑘selectedbythetuningprocess.\nourresultsviadirectexecutionofthecompressedmodelsrelying\nonIMrepresentationandanothernotebooktoestimateenergy Quantization CWS PWS ECSQ UQ\nconsumption.\nclr 0.00001 0.0001 0.00001 0.0001\nk 1024 4096 4096 4096\n3 RESULTS\nWetestedthefourconsideredquantizationapproaches(i.e.,CWS,\nPWS,UQ,andECSQ;seeSect.2.2)onthebreastcancerdataset for all compression methods, with the first choice ensuring us-\nprovidedbytheauthorsofPathoNet.AsmentionedinSect.2.1,we ing1byteforeachindex,andtheother2choicesensuringlower\nexecutedanholdoutprocedureinwhichthetrainingsetisexploited compressionbutpotentiallyhigherperformance.Notethatthis"
  },
  {
    "id": 1079,
    "text": "executedanholdoutprocedureinwhichthetrainingsetisexploited compressionbutpotentiallyhigherperformance.Notethatthis\ntoretrainthemodelafterquantizationandavalidationsetisused bidimensionalgridyields12combinationsforeachmethod,fora\ntotuneasetofhyperparametersbymeansofgridsearch.Indeed, totalof48experiments;thispreventedusfromusingmorerefined\ntheapproachesadoptedtocompressthenetworkpresentsome grids.Afterchoosingthebestcouple,afinalretrainingusingall\nhyperparametersthatinfluencetheobtainedperformance.Oneof theaugmentedtrainingsetproducesthecompressedmodel.The\nthemisthenumber𝑘 ofgroups,whichhasadirecteffectonthe cumulativelearningrateandthenumberofgroupsselectedatthe\nfinalsizeofthecompressedmodel.Moreover,sincewehaveto endoftuningareshowedinTable2.Thenetworkconfiguration\nretrainthenetworkafterthequantizationstep,alltheclassical waskeptasintheoriginalPathoNetmodel,wheneverpossible,"
  },
  {
    "id": 1080,
    "text": "retrainthenetworkafterthequantizationstep,alltheclassical waskeptasintheoriginalPathoNetmodel,wheneverpossible,\nhyperparametersrelatedtothetrainingofaneuralnetwork(e.g., andalltheotherexperimentaldetailsarereportedinourpublic\nlearningrate,batchsize,patience,etc.)playanimportantrole.In repository4.\nparticular, we have considered the cumulative learning rate𝑐𝑙𝑟\nand the number of groups𝑘, since they impacted more on the 3.3 Experimentalresults.\nmodelaccuracy.Oncethebestmodelisselected(seeSect.2.1),the Thegeneralizationperformance,intermsofF1-scoreforthethree\ncorrespondinghyperparametersareusedtotrainthenetworkon classes,RMSEandaggregatedcut-offaccuracyforKi67-indexand\nthetrainandvalidationsets,andthegeneralizationperformance TIL-scoreispresentedinTable3andcomparedtothesamemetrics\nofthecompressedmodelisassessedonthetestset. computedontheoriginaluncompressedPathoNetmodel5.\nAswecanseefromtheresultsofthisfirstsetofexperiments"
  },
  {
    "id": 1081,
    "text": "ofthecompressedmodelisassessedonthetestset. computedontheoriginaluncompressedPathoNetmodel5.\nAswecanseefromtheresultsofthisfirstsetofexperiments\n3.1 EvaluationMetrics (rows2–5inTable3),thecompressednetworksachievecompa-\nrableperformancew.r.t.theoriginalPathoNetmodelintermsof\nThesuccessfulapplicationofacompressionstrategyshouldlead\nF1-scorefortheclassesKi67immunopositiveandimmunonegative.\ntoamodelthat(I)retainssimilargeneralizationperformancew.r.t.\nInterestingly, the compression methods UQ and ECSQ obtain a\ntheuncompressedmodel,(II)leadstoareductionintermsofspace\nsignificantimprovementinF1-scorefortheTILclass(3%forUQ\noccupancy,and(III)keepsareasonableexecutiontimewhenused\nandslightlylowerforECSQ).ConsideringRMSE,theperformances\ntomakeinferencesonthetestset.\nofcompressednetworksareslightlyworsefortheKi67-index,but\nInparticular,thegeneralizationperformancesareevaluatedin\ntheyachievealwaysbetterresultsfortheTIL-score.Moreover,all\ntermsofF1-score,RMSE(RootMeanSquaredError)andaggregated"
  },
  {
    "id": 1082,
    "text": "Inparticular,thegeneralizationperformancesareevaluatedin\ntheyachievealwaysbetterresultsfortheTIL-score.Moreover,all\ntermsofF1-score,RMSE(RootMeanSquaredError)andaggregated\nthecompressednetworksmatchtheuncompressedonesforthe\ncut-offaccuracyforTILsandKi67(asdefinedinthereferencepaper).\nKi67-indexcut-offaccuracywhilealwaysconsistentlyimproving\nMoreover,twoadditionalmetricsareused:\nthecorrespondingmetricforTIL-score.Inparticular,theimprove-\n• Compressionratio:theratioofthememorysizeneededby mentincut-offaccuracyforTIL-scorerangesfrom4.4%to13.1%,\ntheuncompressedoverthecompressedmodel(cfr.Sect.2.2); dependingontheappliedquantizationapproach.Overall,allthe\n• Timeratio:theratiobetweenevaluationtimesonthetestset compressed models almost halve the space occupancy in RAM\noftheuncompressedoverthecompressedmodel. whilebringingaslowdownduringexecutionoflessthan20%.\nBestcompression. AsshowninTable2,thegridsearchprocess\n3.2 Hyperparametersoptimization"
  },
  {
    "id": 1083,
    "text": "oftheuncompressedoverthecompressedmodel. whilebringingaslowdownduringexecutionoflessthan20%.\nBestcompression. AsshowninTable2,thegridsearchprocess\n3.2 Hyperparametersoptimization\nledtotheselectionofanhighnumberofgroups𝑘,whichwasequal\nThe train and validation sets (Section 2.1) are used to perform to4096inmostcases.Theperformanceofthecompressedmodels\nthetuningofhyperparameters,i.e.,thecumulativelearningrate iscompetitivewiththeoriginalPathoNetmodel,andsomemetrics\n𝑐𝑙𝑟 forthefinetuningofweightsafterquantization(Section2.2), areoftenbetter(especiallytheonesrelatedtoTILs).Thisbehaviour\nandnumberofclusters𝑘,bymeansofgridsearch.Obviously,the\nisexpected,sinceanhighernumberofrepresentativesgivesbet-\nvalidationsetisnotaugmentedinthisphase.Thebestcombination terchancestopreservethenetworkstructure.Ontheotherhand,\nofhyperparametersistheonethatgivesthelowestRMSEonthe itisinterestingtoevaluateifalowernumberofrepresentatives\nvalidationset.Thenthebestmodelisretrainedonthecomplete"
  },
  {
    "id": 1084,
    "text": "ofhyperparametersistheonethatgivesthelowestRMSEonthe itisinterestingtoevaluateifalowernumberofrepresentatives\nvalidationset.Thenthebestmodelisretrainedonthecomplete\n4https://github.com/GliozzoJ/pathonet_compression\naugmentedtrainingset.\n5TheresultsreportedfortheuncompressednetworkinTable3aredifferentfrom\nAsoutlinedin[33],thecumulativelearningrateneedstobe theonesshowedinthereferencepaper[39].WecontactedtheauthorsofPathoNet\nsmaller than the learning rate used to train the original model: andtheyagreedwiththecorrectnessoftheF1-scoreresultsusingdatasetSHIDC-\naccordingly,thegridfor𝑐𝑙𝑟 hasbeensetto[0.001,0.0001,0.00001, B-Ki-67-V1.0.Moreover,weimplementedthefunctiontocomputetheRMSEand\ncut-offaccuracy,whichisnowpartofthePathoNetpackageavailableonGitHub\n0.000001].Thenumberofgroups𝑘hasbeenchosenin[256,1024,4096]\n(https://github.com/SHIDCenter/PathoNet/blob/master/evaluation.py)."
  },
  {
    "id": 1085,
    "text": "ICBRA2023,September22–24,2023,Barcelona,Spain Gliozzoetal.\nTable3:Generalizationperformanceoftheuncompressed(firstrow)andcompressedPathoNetmodels.Ki67+standsfor\nimmunopositives,Ki67-forimmunonegatives,whilethelasttwocolumnsrespectivelyrepresenttheRAMcompressionand\ntheevaluationtimeratios(thehigher,thebetter).Thebestresultsforeachmetricarehighlightedinbold.\nF1-score RMSE Cut-offAccuracy\nExperiment Ki67+ Ki67- TIL Ki67-index TIL-score Ki67-index TIL-score Space Time\nUncompressed 0.853 0.776 0.348 0.050 0.054 0.913 0.826 - -\nCWS 0.852 0.774 0.355 0.054 0.044 0.913 0.870 1.942 0.833\nPWS 0.848 0.774 0.351 0.053 0.021 0.913 0.957 1.789 0.838\nECSQ 0.852 0.776 0.375 0.054 0.039 0.913 0.913 1.789 0.837\nUQ 0.852 0.775 0.378 0.056 0.036 0.913 0.913 1.916 0.835\nCWS-k=256 0.853 0.778 0.365 0.053 0.041 0.913 0.870 3.937 0.843\nPWS-k=256 0.835 0.746 0.193 0.050 0.028 0.957 0.957 3.937 0.839\nECSQ-k=256 0.848 0.781 0.355 0.049 0.023 0.913 0.913 3.937 0.837"
  },
  {
    "id": 1086,
    "text": "CWS-k=256 0.853 0.778 0.365 0.053 0.041 0.913 0.870 3.937 0.843\nPWS-k=256 0.835 0.746 0.193 0.050 0.028 0.957 0.957 3.937 0.839\nECSQ-k=256 0.848 0.781 0.355 0.049 0.023 0.913 0.913 3.937 0.837\nUQ-k=256 0.852 0.777 0.374 0.055 0.038 0.913 0.913 3.937 0.840\ncanleadtocompetitiveresultswhilesignificantlyreducingthe breastcancer,ahighernumberofTILsindicatesahigheractivation\nspaceoccupancyinmainmemory.Fromapracticalstandpoint, oftumor-suppressingadaptiveimmunityandanincreasedrateof\nausercouldhaveonlylimitedcomputationalresourcesavailable responseafteradjuvantanthracycline-basedchemotherapy[45].\nandwillingtostilluseacompressedCNNevenattheexpenses Forexample,inlungcancer,theevaluationofPDL-1immunohisto-\nofamarginaldecreaseingeneralizationperformance.Totestthis chemicalpositivitybecamecriticalafterthedemonstrationofthe\nsituation, we executed again the experiments performed in the efficacyofanti-PDL1drugPembrolizumab[17].Theinclusionof"
  },
  {
    "id": 1087,
    "text": "situation, we executed again the experiments performed in the efficacyofanti-PDL1drugPembrolizumab[17].Theinclusionof\nprevioussectionbutavoidingthemodelselectionprocess.Inpar- patientsinimmunochemotherapyprotocolswithPembrolizumab\nticular,thenumberofgroupsisfixedto256forallquantization monotherapyoritscombinationspassesthroughPDL1positivecell\nmethodsandthecumulativelearningrateasthebestvalueselected count,sinceinter-observervariabilitycaneasilyshiftthetherapeu-\nbymeansofgridsearchintheprevioussetofexperiments(see ticplanduetothelowamountofpositivecellsneededtoreachthe\nTable2).Theotherhyperparametersremainedunchanged.The TPScut-off[9].Indeed,attemptstouseDNNforTPScomputation\nresultsareshowedinTable3(rows6–9).Quitesurprisingly,com- inlungcanceralreadyexistinliterature[50].\npressedPathoNetmodelsinthissettingtendtopreserveoreven Moreover,giventhegrowingknowledgeaboutdiseasemolec-"
  },
  {
    "id": 1088,
    "text": "pressedPathoNetmodelsinthissettingtendtopreserveoreven Moreover,giventhegrowingknowledgeaboutdiseasemolec-\nimprovetheirperformancewhenusingmorerepresentatives(see, ulartherapytargetsandinterestinPrecisionMedicine,afuture\ne.g.,theCWSmethod),whichhastheappreciatedbenefitthatthe increaseinthenumberofroutinelyanalyzedimmunohistochemical\nspacecompressionisstillincreased,namelyto≈4×theoriginal predictiveandprognosticmarkersbecomesareasonableeducated\nuncompressedPathoNetmodel.AnexceptionisthePWSmethod, guess[15].Inthiscontext,theapplicationofML-basedmodelson\nwhichshowsalowerabilitytoselectinformativerepresentatives prognostic/predictiveimmunohistochemicalpanelswillshortenthe\nwhentheirnumberislimited,confirmingtheresultsin[34].Onthe timerequiredfortheirevaluation,andincreasetheoverallaccuracy\notherside,CWSmethodexhibitedanoverallhigherstabilityand ofthetests.Theemploymentofsuchmodelswilllikelyreducethe"
  },
  {
    "id": 1089,
    "text": "otherside,CWSmethodexhibitedanoverallhigherstabilityand ofthetests.Theemploymentofsuchmodelswilllikelyreducethe\neffectivenessinchoosingtherepresentativeweights,asconfirmed costsandtimerelatedtotheevaluationofeachbiomarker,there-\nbythethefactthatitperformedbestduringmodelselectionwhen foremitigatingthepathologistworkload;reducingtheresource\nnotusingthemaximum𝑘allowed(Table2). demandofsuchcomputationalmodelswilltherebystillfavourtheir\napplicability.\n4 DISCUSSION Secondly,aparttherelevanceofcellcountoperationempha-\nsizedsofar,ourworkinducesasecondbenefitrelatedtothemodel\nInthisresearch,wefocusedontheproblemoftheautomaticcom-\ncompression,intermsofbothRAManddiskoccupancyreduction.\nputationofKi67andTILindicesinbreastcancer,withtheprimary\nIngeneral,thediskspacerequiredtostoreaCNNislowerthan\ngoalofatleastpreservethetopperformanceobtainedforthistask\ntheamountofRAMneededtoloadandquerythemodel,dueto\nintheliterature,alongwithaspecialattentiontolimittheresulting"
  },
  {
    "id": 1090,
    "text": "goalofatleastpreservethetopperformanceobtainedforthistask\ntheamountofRAMneededtoloadandquerythemodel,dueto\nintheliterature,alongwithaspecialattentiontolimittheresulting\noptimizedformatsavailabletoserializemodelsondisk(e.g.,the\nmodelresourceusage,tonothamperitspracticalapplicability.We diskserializationprovidedbythelzmaPythonmodule6).Thedisk\nshowedhowtoobtainaCNNexhibitingperformancecompetitive\nstoragereduction(upto9×,UQquantizationmethod),represents\nwithPathoNet(referenceCNNfortheproblem),whileyieldinga\nanadvantageinsituationsinwhichtheserializedrepresentation\nmodelmuchmoreresource-cautious,throughanovelcompression\nofmodelsisusedtosharethelatteramongseveralactorsinadis-\nframework.Oursolutionisaround4×smallerintermsofmemory\ntributedframework.Thishappensnotablyinthefederatedlearning\nfootprintand9×intermsofdisksize(Figure1),withreferenceto\nsetting[29],characterizedbyaprivacy-preservingcommunication\nPathoNet,whilestillperformingthesameorbetter.Thisisatfirst"
  },
  {
    "id": 1091,
    "text": "footprintand9×intermsofdisksize(Figure1),withreferenceto\nsetting[29],characterizedbyaprivacy-preservingcommunication\nPathoNet,whilestillperformingthesameorbetter.Thisisatfirst\nloopinwhichedgecomputingdevicestrain“partial”machinelearn-\nofclinicalinterest,beingKi67pivotalinthedefinitionofpatients’\ningmodelsonlocallyacquireddataandsendthemtoacentralized\ntreatmentandfortheevaluationoftheirprognosis[12].Similarly,\nserverthatmergesthemandshareswithalldevicestheresulting\nthenumberofTILsshowapositivecorrelationwithpatientssur-\nvivalandtherapyresponse.Infactatleastincertainsubtypesof 6https://docs.python.org/3/library/lzma.html"
  },
  {
    "id": 1092,
    "text": "Resource-LimitedAutomatedKi67IndexEstimationinBreastCancer ICBRA2023,September22–24,2023,Barcelona,Spain\ndoneafurthersteptosettleageneralframeworkforcompress-\ningpre-trainedDLmodels,thatcoupledwithapubliclyavailable\nrepository,allowstoextendourapproachtootherproblemsand\ndeepmodels.Thisstudyservestherebyasapotentialreferencefor\nnon-expertuserswhoneedtodownsizeexistingAItoolsbasedon\ndeeporconvolutionalneuralnetworks,avoidingbuildingcompact\nDNNsfromscratch,mainlyforproblemswherethetraininghas\nveryhighcosts.Oursolutionallowstoextendexistingmodelsto\ncontextswheresufficientlypowerfulhardwareisnotavailableor\nwheredeviceshaveinherentlyfewcomputationalresources.Our\npipelinecanhandlebothfully-connectedandconvolutionallayerin\nFigure1:Diskcompressionratioofthetestedmethodswith compressionstep,anditisnotlimitedtoagivenstorageformatin\nrespecttotheoriginalsizeofPathoNet(12.802MBytes). thefinalcompressedlayerrepresentationstep.Themainlimitation"
  },
  {
    "id": 1093,
    "text": "respecttotheoriginalsizeofPathoNet(12.802MBytes). thefinalcompressedlayerrepresentationstep.Themainlimitation\noftheoverallframeworkliesinitsapplicabilitytoonlydeepmodels\n(whichhowevercoverthemajorityofapplicationdomains),and\n“global”model.Havingthepossibilitytosendcompressedmod-\namongthem,tofeed-forwardarchitectures,excludingforinstance\nelsbackandforthwouldleadinthiscasetoavaluablesavingof\nrecurrentneuralnetworks(RNNs),justtostateanexample.Apo-\nnetworkbandwidth,whichisthebottleneckresource.\ntentialfuturedevelopmentofthisstudywouldindeedextendthe\nMotivated by the promising results obtained, we have made\npresentsolutiontoalsosupportsuchmodels.\nour compression methodology ready and general enough to be\nappliedinotherdomainswithsimilarneeds.Inthissense,PathoNet\nACKNOWLEDGMENTS\ncanbeseenasa‘toyexample’forstatingtheusefulnessofour\nmethodology,beingitsmemoryfootprintslightlylessthan13MB, ThisworkwassupportedbytheItalianMURPRINproject“Multi-"
  },
  {
    "id": 1094,
    "text": "ACKNOWLEDGMENTS\ncanbeseenasa‘toyexample’forstatingtheusefulnessofour\nmethodology,beingitsmemoryfootprintslightlylessthan13MB, ThisworkwassupportedbytheItalianMURPRINproject“Multi-\ntakingintoaccount4bytesforeachmodelparameterasafloating criteriadatastructuresandalgorithms:fromcompressedtolearned\npoint number. Nevertheless, for the computation of Ki67-index indexes,andbeyond”(Prot.2017WR7SHH).Partofthisworkwas\ncurrentresearcheshavealsoproposedmuchbiggermodels:see done while D. Malchiodi was visiting scientist at Inria Sophia-\ne.g.[16],whereanensembleofthreeDenseNet121modelsisused Antipolis/I3S CNRS Université Côte d’Azur (France). We thank\ntoprovideafinalprediction,whichroughlyrequires320MBof M.Sc.FarzinNegahbaniandhisresearchteamatShirazUniversity\nRAM.Althoughinsmallscale,wegiveanideaofhowmuchthe forthepromptassistancewiththePathoNetpackage.\ncompressionofPathoNetimpactsontheenergyconsumption.We\nleveragedthePythonpackagecodecarbonandobtained0.000573 REFERENCES"
  },
  {
    "id": 1095,
    "text": "compressionofPathoNetimpactsontheenergyconsumption.We\nleveragedthePythonpackagecodecarbonandobtained0.000573 REFERENCES\nkWh used for querying the original model on 500 images, and [1] AlaaAwadAbdellatif,LutfiSamara,AmrM.Mohamed,AimanErbad,Carla-\n0.000380kWhtoquerythecompressedone(i.e.UQ,𝑘 =256).7Our Fabiana Chiasserini, Mohsen Guizani, Mark Dennis O’Connor, and James\nLaughton.2020.I-Health:LeveragingEdgeComputingandBlockchainforEpi-\nrepositoryprovidesadedicatedJupyternotebooktospecifically demicManagement.ArXivabs/2012.14294(2020).\ncomparetheenergyconsumption(seeSect.2),inadditiontothe [2] HananAljuaid,NazikAlturki,NajahAlsubaie,LuciaCavallaro,andAntonio\nLiotta.2022. Computer-aideddiagnosisforbreastcancerclassificationusing\ncodeabletoreproducealltheexperimentsproposedinthisstudy,\ndeepneuralnetworksandtransferlearning.Comput.Methods.Programs.Biomed.\nandtothescriptsallowingtoautomaticallyloadserializedsaved 223(2022),106951."
  },
  {
    "id": 1096,
    "text": "deepneuralnetworksandtransferlearning.Comput.Methods.Programs.Biomed.\nandtothescriptsallowingtoautomaticallyloadserializedsaved 223(2022),106951.\nmodels,deserializethem,andruntheirexecution. [3] ZeyuanAllen-Zhu,YuanzhiLi,andYingyuLiang.2019.LearningandGeneral-\nizationinOverparameterizedNeuralNetworks,GoingBeyondTwoLayers.In\nAdvancesinNeuralInf.Process.Syst.,Vol.32.CurranAssociates,Inc.\n5 CONCLUSIONS [4] YoshuaBengio,IanGoodfellow,andAaronCourville.2017.Deeplearning.Vol.1.\nMITpress,Massachusetts,USA.\nThisstudyintroducesanautomatedDLapproachfortheKi67-im-\n[5] AdamG.Berman,WilliamR.Orchard,MarcelGehrung,andFlorianMarkowetz.\nmunonegative,Ki67-immunopositive,andTILscelldetectionon 2021.PathML:Aunifiedframeworkforwhole-slideimageanalysiswithdeep\nstainedimages,withaspecificattentiontotheresultingmodelre- learning.medRxiv(2021). https://doi.org/10.1101/2021.07.07.21260138\n[6] YoojinChoi,MostafaEl-Khamy,andJungwonLee.2020.UniversalDeepNeural"
  },
  {
    "id": 1097,
    "text": "stainedimages,withaspecificattentiontotheresultingmodelre- learning.medRxiv(2021). https://doi.org/10.1101/2021.07.07.21260138\n[6] YoojinChoi,MostafaEl-Khamy,andJungwonLee.2020.UniversalDeepNeural\nsourcedemand.Ourframeworkexhibitsperformancecompetitive NetworkCompression.IEEEJ.Sel.TopicsSignalProcess.14,4(2020),715–726.\nwithstate-of-the-artmodelsfortheestimationofKi67andTILin- [7] PhilipAChou,TomLookabaugh,andRobertMGray.1989.Entropy-constrained\nvectorquantization.IEEETrans.Acoust.Speech,SignalProcess.37,1(1989),31–42.\ndicesinbreastcancer.Further,itsuccessfullytacklestheproblemof\n[8] YulRiChung,MinHyeJang,SoYeonPark,GyungyubGong,Woo-HeeJung,\nreducingmodelsizeandcomputationalresourceneed,toenhance KoreanBreastPathologyKi-67StudyGroup,etal.2016.Interobservervariability\nitsapplicabilityinlow-resourcescontext,andtolimittheenergy ofKi-67measurementinbreastcancer.J.Pathol.Transl.Med.50,2(2016),129.\n[9] WendyACooper,PrudenceARussell,MayaCherian,EdwinaEDuhig,David"
  },
  {
    "id": 1098,
    "text": "itsapplicabilityinlow-resourcescontext,andtolimittheenergy ofKi-67measurementinbreastcancer.J.Pathol.Transl.Med.50,2(2016),129.\n[9] WendyACooper,PrudenceARussell,MayaCherian,EdwinaEDuhig,David\nconsumption.Withreferencetoastate-of-the-arttop-performing\nGodbolt,PeterJJessup,ChristineKhoo,ConnullLeslie,AnnabelleMahar,DavidF\nsolution,weobtainedamodelupto4×and9×smallerinterms Moffat,etal.2017. Intra-andinterobserverreproducibilityassessmentofPD-\nofRAManddiskspacerespectively,whilereducingtheenergy L1biomarkerinNon–Smallcelllungcancer. Clin.CancerRes.23,16(2017),\n4569–4577.\nconsumptionofaround1.5×,andsubstantiallypreservingclassi-\n[10] TayeGirmaDebelee,FriedhelmSchwenker,AchimIbenthal,andDerejeYohannes.\nficationaccuracy.Oncefavorablydemonstratedtheeffectiveness 2020.Surveyofdeeplearninginbreastcancerimageanalysis.EvolvingSystems\n11(2020),143–163.\nofourapproachintheestimationofKi67andTILscores,wehave\n[11] LeiDeng,GuoqiLi,SongHan,LupingShi,andYuanXie.2020.ModelCompres-"
  },
  {
    "id": 1099,
    "text": "11(2020),143–163.\nofourapproachintheestimationofKi67andTILscores,wehave\n[11] LeiDeng,GuoqiLi,SongHan,LupingShi,andYuanXie.2020.ModelCompres-\nsionandHardwareAccelerationforNeuralNetworks:AComprehensiveSurvey.\n7EnergyconsumptionwascomputedonLinux-6.2.6-76060206-generic-x86_64-with- Proc.IEEE108,4(2020),485–532.\nglibc2.35,CPUIntel(R)Core(TM)i7-9750HF2.60GHz,GPUNVIDIAGeForceRTX [12] MitchDowsett,TorstenONielsen,RogerA’Hern,JohnBartlett,RCharles\n2060. Coombes,JackCuzick,MatthewEllis,NLynnHenry,JudithCHugh,Tracy"
  },
  {
    "id": 1100,
    "text": "ICBRA2023,September22–24,2023,Barcelona,Spain Gliozzoetal.\nLively,etal.2011.AssessmentofKi67inbreastcancer:recommendationsfrom [36] AndréLSMeirelles,TahsinKurc,JoelSaltz,andGeorgeTeodoro.2022.Effective\ntheInternationalKi67inBreastCancerworkinggroup.J.Natl.CancerInst.103, activelearningindigitalpathology:Acasestudyintumorinfiltratinglympho-\n22(2011),1656–1664. cytes.Comput.MethodsProgramsBiomed.220(2022),106828.\n[13] JanEgger,ChristinaGsaxner,AntonioPepe,KelseyLPomykala,FredericJonske, [37] MuhammadFirozMridha,MdHamid,MuhammadMostafaMonowar,AshfiaJan-\nManuelKurz,JianningLi,andJensKleesiek.2022. Medicaldeeplearning–a natKeya,AbuQuwsarOhi,MdIslam,Jong-MyonKim,etal.2021.Acompre-\nsystematicmeta-review.Comput.MethodsProgramsBiomed.(2022),106874. hensivesurveyondeep-learning-basedbreastcancerdiagnosis.Cancers13,23\n[14] ThorstenFalk,DominicMai,RobertBensch,ÖzgünÇiçek,AhmedAbdulkadir, (2021),6116."
  },
  {
    "id": 1101,
    "text": "[14] ThorstenFalk,DominicMai,RobertBensch,ÖzgünÇiçek,AhmedAbdulkadir, (2021),6116.\nYassineMarrakchi,AntonBöhm,JanDeubner,ZoeJäckel,KatharinaSeiwald, [38] MajidNawaz,AdelASewissy,andTaysirHassanASoliman.2018.Multi-class\netal.2019.U-Net:deeplearningforcellcounting,detection,andmorphometry. breastcancerclassificationusingdeeplearningconvolutionalneuralnetwork.\nNat.Methods16,1(2019),67–70. Int.J.Adv.Comput.Sci.Appl9,6(2018),316–332.\n[15] MatteoFassan,AldoScarpa,AndreaRemo,GiovannaDeMaglio,Giancarlo [39] FarzinNegahbani,RasoolSabzi,BitaPakniyatJahromi,DenaFirouzabadi,Fateme\nTroncone,AntonioMarchetti,ClaudioDoglioni,GiuseppeIngravallo,Giuseppe Movahedi,MahsaKohandelShirazi,ShayanMajidi,andAmirrezaDehghanian.\nPerrone,PaolaParente,etal.2020.Currentprognosticandpredictivebiomarkers 2021.PathoNetintroducedasadeepneuralnetworkbackendforevaluationof\nforgastrointestinaltumorsinclinicalpractice.Pathologica112,3(2020),248. Ki-67andtumor-infiltratinglymphocytesinbreastcancer.Sci.Rep.11,1(2021),"
  },
  {
    "id": 1102,
    "text": "forgastrointestinaltumorsinclinicalpractice.Pathologica112,3(2020),248. Ki-67andtumor-infiltratinglymphocytesinbreastcancer.Sci.Rep.11,1(2021),\n[16] LukaszFulawka,JakubBlaszczyk,MartinTabakov,andAgnieszkaHalon.2022. 1–13.\nAssessmentofKi-67proliferationindexwithdeeplearninginDCIS(ductal [40] SeungPark,AnilVParwani,RaymondDAller,LechBanach,MichaelJBecich,\ncarcinomainsitu).Sci.Rep.12,1(2022),3166. StephanBorkenfeld,AlexisBCarter,BruceAFriedman,MarcialGarciaRojo,\n[17] LeenaGandhi,DelvysRodríguez-Abreu,ShirishGadgeel,EmilioEsteban,En- AndrewGeorgiou,etal.2013.Thehistoryofpathologyinformatics:Aglobal\nriquetaFelip,FláviaDeAngelis,ManuelDomine,PhilipClingan,MaximilianJ perspective.J.Pathol.Inform.4(2013),7.\nHochmair,StevenFPowell,etal.2018.Pembrolizumabpluschemotherapyin [41] GilPatrusPenaandJosédeSouzaAndrade-Filho.2009.Howdoesapathologist\nmetastaticnon–small-celllungcancer.N.Engl.J.Med.378,22(2018),2078–2092. makeadiagnosis?Arch.Pathol.Lab.Med.133,1(2009),124–132."
  },
  {
    "id": 1103,
    "text": "metastaticnon–small-celllungcancer.N.Engl.J.Med.378,22(2018),2078–2092. makeadiagnosis?Arch.Pathol.Lab.Med.133,1(2009),124–132.\n[18] LauraGarcía,JesúsTomás,LorenaParra,andJaimeLloret.2019.Anm-health [42] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,\napplicationforcerebralstrokedetectionandmonitoringusingcloudservices. MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2020. Exploringthe\nInt.J.Inf.Manag.45(2019),319–327. LimitsofTransferLearningwithaUnifiedText-to-TextTransformer.J.Mach.\n[19] HGishandJPierce.1968.Asymptoticallyefficientquantizing.IEEETrans.Inf. Learn.Res.21,1,Article140(jan2020),67pages.\nTheory14,5(1968),676–683. [43] OlafRonneberger,PhilippFischer,andThomasBrox.2015.U-Net:Convolutional\n[20] DouglasSGomes,SimoneSPorto,DéboraBalabram,andHeleniceGobbi.2014. NetworksforBiomedicalImageSegmentation.InMedicalImageComputingand"
  },
  {
    "id": 1104,
    "text": "[20] DouglasSGomes,SimoneSPorto,DéboraBalabram,andHeleniceGobbi.2014. NetworksforBiomedicalImageSegmentation.InMedicalImageComputingand\nInter-observervariabilitybetweengeneralpathologistsandaspecialistinbreast Computer-AssistedIntervention–MICCAI2015.SpringerInternationalPublishing,\npathologyinthediagnosisoflobularneoplasia,columnarcelllesions,atypical Cham,234–241.\nductalhyperplasiaandductalcarcinomainsituofthebreast.Diagn.Pathol.9,1 [44] MonjoySaha,ChandanChakraborty,InduArun,RosinaAhmed,andSanjoy\n(2014),1–9. Chatterjee.2017.AnadvanceddeeplearningapproachforKi-67stainedhotspot\n[21] JonathanGuoandBinLi.2018.TheApplicationofMedicalArtificialIntelligence detectionandproliferationratescoringforprognosticevaluationofbreastcancer.\nTechnologyinRuralAreasofDevelopingCountries.HealthEquity2,1(2018), Sci.Rep.7,1(2017),3213.\n174–181. PMID:30283865. [45] RobertoSalgado,CarstenDenkert,SDemaria,NSirtaine,FKlauschen,Gian-"
  },
  {
    "id": 1105,
    "text": "TechnologyinRuralAreasofDevelopingCountries.HealthEquity2,1(2018), Sci.Rep.7,1(2017),3213.\n174–181. PMID:30283865. [45] RobertoSalgado,CarstenDenkert,SDemaria,NSirtaine,FKlauschen,Gian-\n[22] SongHan,HuiziMao,andWilliamJ.Dally.2016.DeepCompression:Compress- carloPruneri,SWienert,GertVandenEynden,FrederickLBaehner,Frederique\ningDeepNeuralNetworkwithPruning,TrainedQuantizationandHuffman Pénault-Llorca,etal.2015. Theevaluationoftumor-infiltratinglymphocytes\nCoding.In4thInternationalConferenceonLearningRepresentations,ICLR2016, (TILs)inbreastcancer:recommendationsbyanInternationalTILsWorking\nSanJuan,PuertoRico,May2-4,2016,ConferenceTrackProceedings.SanJuan, Group2014.Ann.Oncol.26,2(2015),259–271.\nPuertoRico. http://arxiv.org/abs/1510.00149 [46] FabioASpanhol,LuizSOliveira,CarolinePetitjean,andLaurentHeutte.2015.\n[23] M.G.Hanna,O.Ardon,V.E.Reuter,S.J.Sirintrapun,C.England,D.S.Klimstra, Adatasetforbreastcancerhistopathologicalimageclassification.IEEETrans."
  },
  {
    "id": 1106,
    "text": "[23] M.G.Hanna,O.Ardon,V.E.Reuter,S.J.Sirintrapun,C.England,D.S.Klimstra, Adatasetforbreastcancerhistopathologicalimageclassification.IEEETrans.\nandM.R.Hameed.2021.Integratingdigitalpathologyintoclinicalpractice.Mod. Biomed.Eng.63,7(2015),1455–1462.\nPathol.35,2(Oct2021),152–164. [47] SashaEStantonandMaryLDisis.2016.Clinicalsignificanceoftumor-infiltrating\n[24] NadiaHarbeck,FrédériquePenault-Llorca,JavierCortes,MichaelGnant,Nehmat lymphocytesinbreastcancer.J.ImmunoTher.Cancer4,1(2016),1–7.\nHoussami,PhilipPoortmans,KathrynRuddy,JaniceTsang,andFatimaCardoso. [48] PankajTaneja,DejanMaglic,FumitakeKai,SinanZhu,RobertDKendig,AFry\n2019.Breastcancer.Nat.Rev.Dis.Primers5,1(2019),1–31. Elizabeth,andKazushiInoue.2010.Classicalandnovelprognosticmarkersfor\n[25] DaisukeHirahara,EichiTakaya,MizukiKadowaki,YasuyukiKobayashi,and breastcancerandtheirclinicalsignificance.Clin.Med.Insights:Oncol.4(2010)."
  },
  {
    "id": 1107,
    "text": "[25] DaisukeHirahara,EichiTakaya,MizukiKadowaki,YasuyukiKobayashi,and breastcancerandtheirclinicalsignificance.Clin.Med.Insights:Oncol.4(2010).\nTakuyaUeda.2021.Effectofthepixelinterpolationmethodfordownsampling [49] MiaomiaoTao,ShuChen,XianquanZhang,andQiZhou.2017.Ki-67labeling\nmedicalimagesondeeplearningaccuracy.J.comput.commun.9,11(2021). indexisapredictivemarkerforapathologicalcompleteresponsetoneoadjuvant\n[26] SebastianKleinandDanGDuda.2021.MachineLearningforFutureSubtypingof chemotherapyinbreastcancer:ameta-analysis.Medicine96,51(2017),e9384.\ntheTumorMicroenvironmentofGastro-EsophagealAdenocarcinomas.Cancers [50] XiangyunWang,PeilinChen,GuangtaiDing,YishiXing,RongrongTang,Chao-\n13,19(2021),4919. longPeng,YizhouYe,andQiangFu.2021.Dual-scalecategorizationbaseddeep\n[27] KyubumLee,JohnH.Lockhart,MengyuXie,RituChaudhary,RobbertJ.C.Slebos, learningtoevaluateprogrammedcelldeathligand1expressioninnon-smallcell"
  },
  {
    "id": 1108,
    "text": "[27] KyubumLee,JohnH.Lockhart,MengyuXie,RituChaudhary,RobbertJ.C.Slebos, learningtoevaluateprogrammedcelldeathligand1expressioninnon-smallcell\nElsaR.Flores,ChristineH.Chung,andAikChoonTan.2021.DeepLearningof lungcancer.Medicine100,20(2021),pe25994.\nHistopathologyImagesattheSingleCellLevel.Front.Artif.Intell.4(2021),137. [51] JuanyingXie,RanLiu,JosephLuttrellIV,andChaoyangZhang.2019. Deep\n[28] JohannLi,GuangmingZhu,CongHua,MingtaoFeng,PingLi,XiaoyuanLu, learningbasedanalysisofhistopathologicalimagesofbreastcancer.Front.Genet.\nJuanSong,PeiyiShen,XuXu,LinMei,etal.2021. Asystematiccollectionof 10(2019),80.\nmedicalimagedatasetsfordeeplearning.arXivpreprintarXiv:2106.12864(2021). [52] H.Xu,F.Cong,andT.H.Hwang.2021. MachineLearningandArtificial\n[29] TianLi,AnitKumarSahu,AmeetTalwalkar,andVirginiaSmith.2020.Federated Intelligence-drivenSpatialAnalysisoftheTumorImmuneMicroenvironmentin"
  },
  {
    "id": 1109,
    "text": "[29] TianLi,AnitKumarSahu,AmeetTalwalkar,andVirginiaSmith.2020.Federated Intelligence-drivenSpatialAnalysisoftheTumorImmuneMicroenvironmentin\nLearning:Challenges,Methods,andFutureDirections.IEEESignalProcess.Mag. PathologySlides.Eur.Urol.Focus7,4(Jul2021),706–709.\n37,3(2020),50–60. [53] YaoXue,NilanjanRay,JudithHugh,andGilbertBigras.2016.Cellcountingby\n[30] YeLiu,LiqiangNie,LeiHan,LumingZhang,andDavidS.Rosenblum.2015. regressionusingconvolutionalneuralnetwork.InComputerVision–ECCV2016\nAction2Activity:RecognizingComplexActivitiesfromSensorData.InProceed- Workshops:Amsterdam,TheNetherlands,October8-10and15-16,2016,Proceedings,\ningsofthe24thInternationalConferenceonArtificialIntelligence(BuenosAires, PartI14.Springer,Springer,Cham,Amsterdam,TheNetherlands,274–290.\nArgentina)(IJCAI’15).AAAIPress,1617–1623. [54] RuihanZhang,JunhaoYang,andChunxiaoChen.2018.Tumorcellidentification"
  },
  {
    "id": 1110,
    "text": "Argentina)(IJCAI’15).AAAIPress,1617–1623. [54] RuihanZhang,JunhaoYang,andChunxiaoChen.2018.Tumorcellidentification\n[31] YanMao,QingQu,XiaosongChen,OuHuang,JiayiWu,andKunweiShen. inki-67imagesondeeplearning.Mol.Cell.Biol.15,3(2018),177.\n2016.Theprognosticvalueoftumor-infiltratinglymphocytesinbreastcancer:a [55] XiangyuZhang,XinyuZhou,MengxiaoLin,andJianSun.2018.ShuffleNet:An\nsystematicreviewandmeta-analysis.PloSONE11,4(2016),e0152500. ExtremelyEfficientConvolutionalNeuralNetworkforMobileDevices.InProc.\n[32] GiosuèCataldoMarinòetal.2021.ReproducingtheSparseHuffmanAddress oftheIEEEConf.onComputerVisionandPatternRecognition(CVPR).\nMapCompressionforDeepNeuralNetworks.InReproducibleResearchinPattern [56] RunkaiZhu,DongSui,HongQin,andAiminHao.2017.Anextendedtypecell\nRecognition.SpringerInternationalPublishing,Cham,161–166. detectionandcountingmethodbasedonFCN.In2017IEEE17thInt.Conf.on"
  },
  {
    "id": 1111,
    "text": "Recognition.SpringerInternationalPublishing,Cham,161–166. detectionandcountingmethodbasedonFCN.In2017IEEE17thInt.Conf.on\n[33] GiosuèCataldoMarinòetal.2021.Compressionstrategiesandspace-conscious BioinformaticsandBioengineering(BIBE).IEEE,Washington,DC,USA,51–56.\nrepresentationsfordeepneuralnetworks.In202025thInt.Conf.onPattern [57] DovileZilenaite,AllanRasmusson,RenaldasAugulis,JustinasBesusparis,Aida\nRecognition(ICPR).IEEE,Milan,Italy,9835–9842. Laurinaviciene,BenoitPlancoulaine,ValerijusOstapenko,andArvydasLauri-\n[34] GiosuèCataldoMarinò,AlessandroPetrini,DarioMalchiodi,andMarcoFrasca. navicius.2020.Independentprognosticvalueofintratumoralheterogeneityand\n2023. Deepneuralnetworkscompression:Acomparativesurveyandchoice immuneresponsefeaturesbyautomateddigitalimmunohistochemistryanalysis\nrecommendations.Neurocomputing520(2023),152–170. inearlyhormonereceptor-positivebreastcarcinoma. Front.Oncol.10(2020),\n[35] J.B.McQueen.1967.Somemethodsofclassificationandanalysisinmultivariate 950."
  },
  {
    "id": 1112,
    "text": "recommendations.Neurocomputing520(2023),152–170. inearlyhormonereceptor-positivebreastcarcinoma. Front.Oncol.10(2020),\n[35] J.B.McQueen.1967.Somemethodsofclassificationandanalysisinmultivariate 950.\nobservations.InProc.offifthBarkleysymp.onmath.statist.andprob.281–297."
  },
  {
    "id": 1113,
    "text": "DISTRIBUTIONAL REINFORCEMENT LEARNING-BASED ENERGY\nARBITRAGE STRATEGIES IN IMBALANCE SETTLEMENT\nMECHANISM\nSeyedSoroushKarimiMadahi BertClaessens ChrisDevelder\nGhentUniversity BEEBOP GhentUniversity\nseyedsoroush.karimimadahi@ugent.be\nABSTRACT\nGrowthinthepenetrationofrenewableenergysourcesmakessupplymoreuncertainandleadsto\nanincreaseinthesystemimbalance. Thistrend,togetherwiththesingleimbalancepricing,opens\nanopportunityforbalanceresponsibleparties(BRPs)toperformenergyarbitrageintheimbalance\nsettlementmechanism. Tothisend,weproposeabatterycontrolframeworkbasedondistributional\nreinforcementlearning(DRL).Ourproposedcontrolframeworktakesarisk-sensitiveperspective,\nallowingBRPstoadjusttheirriskpreferences: weaimtooptimizeaweightedsumofthearbitrage\nprofitandariskmeasurewhileconstrainingthedailynumberofcyclesforthebattery. Weassess\nthe performance of our proposed control framework using the Belgian imbalance prices of 2022"
  },
  {
    "id": 1114,
    "text": "profitandariskmeasurewhileconstrainingthedailynumberofcyclesforthebattery. Weassess\nthe performance of our proposed control framework using the Belgian imbalance prices of 2022\nandcomparetwostate-of-the-artRLmethods,deepQlearningandsoftactor-critic. Resultsreveal\nthatthedistributionalsoftactor-criticmethodcanoutperformothermethods. Moreover,wenote\nthatourfullyrisk-averseagentappropriatelylearnstohedgeagainsttheriskrelatedtotheunknown\nimbalancepriceby(dis)chargingthebatteryonlywhentheagentismorecertainabouttheprice.\nKeywords Batteryenergystoragesystems(BESS)·Distributionalsoftactor-critic(DSAC)·Imbalancesettlement\nmechanism·Reinforcementlearning(RL)·Risk-sensitiveenergyarbitrage\n1 Introduction\nClimatechangehasbeenamotivationfortransitioningtowardadecarbonizedelectricitygridonboththesupplyand\nthedemandside. TheEuropeanCommissionaimstoreachcarbonneutralityby2050[1]. Toachievethistarget,the"
  },
  {
    "id": 1115,
    "text": "Climatechangehasbeenamotivationfortransitioningtowardadecarbonizedelectricitygridonboththesupplyand\nthedemandside. TheEuropeanCommissionaimstoreachcarbonneutralityby2050[1]. Toachievethistarget,the\npenetrationofrenewableenergysources(RES)needstodramaticallyincrease. TheInternationalRenewableEnergy\nAgency’sreportof2023statesthatthetotalpowercapacityofRESintheworldgrewfrom1.57TWin2013to3.37TW\nin2022[2]. However,thistrendmakeselectricitygenerationmoreuncertainduetothedependenceofRESproduction\nonweatherconditions. Consequently,theincreaseintheshareofRESleadstoanincreaseinthemismatchbetween\ngenerationandconsumption.\nGiventhispotentiallyincreasingmismatchbetweenproductionandconsumption,transmissionsystemoperators(TSOs)\narefacingchallengesinmaintainingthebalanceofthegrid. FollowingtheliberalizationoftheEuropeanelectricity\nsystem,thebalancingresponsibilityofTSOshasbeenoutsourcedtobalanceresponsibleparties(BRPs)[3]. Each"
  },
  {
    "id": 1116,
    "text": "system,thebalancingresponsibilityofTSOshasbeenoutsourcedtobalanceresponsibleparties(BRPs)[3]. Each\nunbalancedBRPispenalizedbyanimbalancepriceattheendofeachimbalancesettlementperiod. Accordingto\ntheelectricitybalancingguideline(EBGL),publishedbytheEuropeannetworkoftransmissionsystemoperatorsfor\nelectricity(ENTSO-E),themainobjectiveoftheimbalancesettlementmechanismistomakesurethatBRPssupport\nthesystembalanceinanefficientwayandtostimulatemarketparticipantsinrestoringthesystembalance[4]. Also,\nEBGLstatesthatasingleimbalancepricingmethodshouldbeusedtocalculatetheimbalancecost: thesettlementprice\nshouldbethesameforbothnegativeandpositiveimbalances. SuchasingleimbalancepricingencouragesBRPsto\ndeviatefromtheirday-aheadnominationtohelptheTSOwithbalancingthegridandtoreducetheircost. Thewide\nusageofRESinadditiontothesingleimbalancepricingprovidesanopportunityforBRPstoreducetheircostusingan\narbitragestrategyintheimbalancesettlementmechanism. Forthispurpose,recentlybatteryenergystoragesystems\n3202"
  },
  {
    "id": 1117,
    "text": "arbitragestrategyintheimbalancesettlementmechanism. Forthispurpose,recentlybatteryenergystoragesystems\n3202\nceD\n32\n]GL.sc[\n1v51000.1042:viXra"
  },
  {
    "id": 1118,
    "text": "(BESS)haveattractedtheattentionofBRPsduetotheirfastresponsetime[5],highefficiency[6],andsignificant\ndecreasesincostofrecentbatterytechnology[7].\nEnergyarbitrageintheimbalancesettlementmechanismischallengingbecauseofhighuncertaintiesinimbalance\nprice and near real-time decision-making. Due to these mentioned challenges, as well as the recent change in the\nimbalancepricingmethodology,fewresearchworkshavebeenconductedonthearbitrageintheimbalancesettlement\nmechanism[3,8–13].AlthoughmostofthecitedstudieshaveformulatedcontrolstrategiesforBESSusingmodel-based\noptimizationmethods(suchasstochasticoptimizationandrobustoptimization),wearguethatmodel-basedoptimization\nmethodsarenotthemostappropriateforobtaininganarbitragestrategy. Althoughitispossible,toformulatetheenergy\narbitrageproblemasanonlinearprogrammingproblem,becauseofthenonconvexnatureofsuchnonlinearproblems,\nthereisnoefficientwaytofindtheoptimalsolutionforthem[14]. Hence,linearizationtechniques(suchaspiecewise"
  },
  {
    "id": 1119,
    "text": "thereisnoefficientwaytofindtheoptimalsolutionforthem[14]. Hence,linearizationtechniques(suchaspiecewise\nlinearapproximation)areappliedtotransformanonlinearproblemintoalinearormixed-integerconvexproblem.\nHowever,applyingthesetechniquesmayresultinanintractableoptimizationproblemoraninaccurateapproximation\noftheproblem. Moreover,thesemodel-basedmethodsneeda(probabilistic)forecasterforfutureimbalancepricesto\naddressuncertaintyinfutureprices. Instochasticoptimization,suchuncertaintiescanbehandledbygeneratingasetof\nscenarios. Yet,asimbalancepricesarehighlyuncertain,alargenumberofscenariosarerequiredtocorrectlyreflectthe\nimbalancepricedistribution,whichincreasesthecomputationalburdentotheextentthattheproblemmaybecome\ncomputationallyintractable. Ontheotherhand,althoughrobustoptimizationdoesnotneedasmanyscenarios[15],its\nobtainedsolutionmightbeaveryconservativestrategyandnotnecessarilythemosteconomicalone[16]."
  },
  {
    "id": 1120,
    "text": "computationallyintractable. Ontheotherhand,althoughrobustoptimizationdoesnotneedasmanyscenarios[15],its\nobtainedsolutionmightbeaveryconservativestrategyandnotnecessarilythemosteconomicalone[16].\nGiventheabovechallenges, fewresearchworkshavefocusedonriskmanagementinthearbitrageprobleminthe\nimbalancesettlementmechanism. Generally,marketparticipantshavedifferentriskpreferences. Forexample,BRPs\nhavemoreconservativearbitragestrategiesintheimbalancesettlementmechanismbecauseofhighlyvolatileimbalance\nprices. Inotherwords,BRPsassignhigherweightstoscenarioswithlowerrevenuesanddeviatefromrisk-neutral\ndecision-making. Thus, to provide a more practical solution, a risk-averse perspective needs to be considered in\nthe arbitrage strategy, while the previous studies have ignored risk management. Moreover, a battery’s lifetime\nmainlydependsonitscharging/dischargingoperations. Frequentlyswitchingbetweencharginganddischargingcan"
  },
  {
    "id": 1121,
    "text": "mainlydependsonitscharging/dischargingoperations. Frequentlyswitchingbetweencharginganddischargingcan\nsignificantlyreducethebatterycyclelifeandthusdecreasethenetprofit duetoanincreasedoperationalcostofthe\nBESS.Wenotethatexistingworkshavenotinvestigatedtheimpactofthebatterycyclelifeonthearbitragestrategy.\nInsummary,shortcomingsandweaknessesinpreviousstudiesofarbitragestrategiesarethat: (i)adoptmodel-based\nmethods(whicharecomplextosolve)(ii)donotconsiderarisk-sensitiveperspective;and(iii)neglectbatterycyclelife\nconstraints. Toaddresstheseshortcomings(furtherelaboratedinSection2),inthispaper,weproposeadistributional\nreinforcementlearning(DRL)-basedcontrolframeworkforarisk-sensitiveenergyarbitragestrategyintheimbalance\nsettlementmechanismforBESS.Theproposedcontrolframework(Section3)aimstomaximizethearbitrageprofit\naswellasariskmeasurebyconstrainingthedailynumberofcyclesforthebattery. WebelieveDRLmethodsare"
  },
  {
    "id": 1122,
    "text": "settlementmechanismforBESS.Theproposedcontrolframework(Section3)aimstomaximizethearbitrageprofit\naswellasariskmeasurebyconstrainingthedailynumberofcyclesforthebattery. WebelieveDRLmethodsare\npropermethodsforriskmanagement,sincetheylearnthecompleteprobabilitydistributionofrandomreturnsinsteadof\ntheexpectedreturn. TheproposedcontrolframeworkcanbetunedaccordingtotheriskpreferenceofBRPsfroma\nfullyrisk-averseperspectivetoafullyrisk-seekingone. Inthispaper,westartfromtwostate-of-the-artreinforcement\nleaning(RL)methods,i.e.,deepQlearning(DQN),asavalue-basedmethod,andsoftactor-critic(SAC),asapolicy\ngradientmethod. WeextendthesevanillaDQNandSACmethodswithadistributionalperspective(i.e.,DDQN,DSAC)\nandarisk-awarecomponentinthelossfunction(Section4). Theperformanceoftheproposedcontrolframeworkis\nevaluatedontheBelgianimbalancepricesof2022(Sections5and6). Overall,ourcontributionsinthispaperarethat\nweproposeaDRL-basedcontrolframework\n(i) foraBESSwhileconsideringaconstraintonthedailynumberofcycles;"
  },
  {
    "id": 1123,
    "text": "weproposeaDRL-basedcontrolframework\n(i) foraBESSwhileconsideringaconstraintonthedailynumberofcycles;\n(ii) thatachievesarisk-sensitivearbitragestrategywithatunablerisktolerancebyoptimizingaweightedsumof\nthearbitragerevenueandariskmeasureintheimbalancesettlementmechanism;\n(iii) forwhichwecomparetheperformanceofvalue-basedandpolicygradientRLmethodsinahighlyuncertain\ntradingmarket.\n2 BackgroundandRelatedWork\nEnergyarbitrageisatechniquetoachievefinancialprofitsbypurchasingenergywhenthepriceischeapandsellingit\nwhenthepriceisexpensive[17]. Energyarbitragecanbeperformedwithinoneelectricitymarket(e.g.,day-ahead\nmarket[18]orintra-daymarket[19])totakeadvantageofvaryingpricesatdifferenthours. Moreover,energyarbitrage\nstrategiesbetweenseveralelectricitymarketshavebeendevelopedtobenefitfromapricedifferencebetweentwoor\nmoreelectricitymarkets,forinstance,energyarbitragebetweenday-aheadandintra-daymarkets[20],day-aheadand"
  },
  {
    "id": 1124,
    "text": "moreelectricitymarkets,forinstance,energyarbitragebetweenday-aheadandintra-daymarkets[20],day-aheadand\nreal-time markets [12], day-ahead market and imbalance settlement mechanism [21], or day-ahead, intra-day, and\nreal-timemarkets[22].\n2"
  },
  {
    "id": 1125,
    "text": "Therecentchangeintheimbalancepricecalculation[4]andanincreaseinimbalancepriceshaveopenedupanew\narbitrageopportunityinelectricitymarkets. Figure1demonstratestheriseinBelgianimbalancepricesinrecentyears.\nHowever,onlyfewstudieshavebeenconductedonenergyarbitrageintheimbalancesettlementmechanism,duetothe\nhighriskinvolvedinthisarbitrage. In[8],anovelreal-timestochasticmulti-periodmanagementstrategyforavirtual\npowerplantwasproposedtomaximizetheprofitofavirtualpowerplantaswellasminimizetheoperationalgridcost.\nItsolvesasequentialstochasticoptimizationproblemtomanagetheparticipationofaBESSinthereal-timemarket.\nThe authors in [3] first implement a new tailored encoder-decoder architecture to generate improved probabilistic\nforecastsofthefuturesystemimbalance. Then,theysolveabi-levelrobustoptimizationproblemtomaximizethe\nrevenuefromtheparticipationofaBESSintheimbalancesettlement. Theauthorsin[9]introduceanovelstochastic"
  },
  {
    "id": 1126,
    "text": "forecastsofthefuturesystemimbalance. Then,theysolveabi-levelrobustoptimizationproblemtomaximizethe\nrevenuefromtheparticipationofaBESSintheimbalancesettlement. Theauthorsin[9]introduceanovelstochastic\nmodelpredictivecontrol(MPC)approachtooptimizetherevenueofBESSintheimbalancesettlementmechanismby\ntakingintoaccountbatterydegradationcostsandriskaversion. Morespecifically,anattention-basedrecurrentneural\nnetworkisusedtopredictthesystemimbalanceandimbalanceprice. In[10],first,themaximumpotentialprofitfrom\nthereal-timemarketisobtainedusingalinearoptimizationprogramwiththeassumptionofperfectforesightforfuture\nprices. Then,ashrinking-horizoncontrolalgorithmisdevelopedtoobtaintheenergyarbitragestrategyforBESSinthe\nreal-timemarketbyconsideringforecasterrorsinfuturereal-timeprices. Reference[11]proposesastochasticmodel\ntomaximizetheenergyarbitragerevenueofBESSunderuncertaintyinday-aheadandreal-timemarkets. Ahybrid"
  },
  {
    "id": 1127,
    "text": "tomaximizetheenergyarbitragerevenueofBESSunderuncertaintyinday-aheadandreal-timemarkets. Ahybrid\nstochastic-robustoptimizationmethodisproposedin[12]tomaximizetherevenueofBESSparticipantsinday-ahead\nandreal-timemarkets. Theday-aheadmarketproblemissolvedbystochasticoptimization,whilethebiddingand\nofferingstrategyinthereal-timemarketisdeterminedbyrobustoptimization. Theauthorsin[13]proposescontrol\nstrategiesforseasonalthermalenergystoragesystemstointeractwithday-aheadandimbalancemarkets: MPC-based\nandRL-basedcontrollersaredevelopedforeachmarketinteractiontocomparetheperformanceofthesetwocontrollers\ninthedifferentelectricitymarkets.\nThus,mostpreviousresearchworkshaveappliedmodel-basedoptimizationmethodstosolvethearbitrageproblem[3,\n8–12]. Nonetheless,themaindisadvantageofthesemethodsisthattheyrequirelinearizationtechniquestoapproximate\nthenonlinearproblemasalinear(ormixed-integer)convexproblemthatcanresultinaninaccurateapproximation."
  },
  {
    "id": 1128,
    "text": "thenonlinearproblemasalinear(ormixed-integer)convexproblemthatcanresultinaninaccurateapproximation.\nDuetopartiallyknownmodelparametersanduncertaintiesoftherealelectricitymarket,therealmarketissimplified\nintoaconvexmarketmodel,resultinginaninaccurateapproximationoftherealmarketdynamics[23]. Furthermore,\nstochastic optimization can be time-consuming, while robust optimization leads to overly conservative strategies.\nAnotherlimitationofthementionedstudiesisthatonlyoneofthem[9]proposesarisk-sensitivearbitragestrategy.\nMoreover,thepreviousresearchworks[3,8,10–12]ignoretheeffectofabattery’slifetimeonthearbitragestrategy.\nFigure1: TheevolutionofBelgianimbalancepricesfrom2018to2023\nToavoidproblemsofmodel-basedoptimizationmethods,RLmethodscanbeused. Recently,RL,asamodel-free\nmethod,hasattractedresearchers’attentionduetoitsremarkableperformanceinsolvingcomplexsequentialdecision-\nmakingproblemssuchasplayinggames,roboticcontrol,andautonomousdriving. RLcanlearna(near-)optimalpolicy"
  },
  {
    "id": 1129,
    "text": "makingproblemssuchasplayinggames,roboticcontrol,andautonomousdriving. RLcanlearna(near-)optimalpolicy\nforastochasticnonlinearenvironmentbydirectlyinteractingwiththeenvironment[24]. InRL,thereisnospecial\nhypothesisregardingtherewardfunction;itcanbelinearornonlinear. Incontrasttomodel-basedoptimizationmethods,\nmodel-freeRLmethodsdonotneedpriorknowledgeoranexplicitmodeloftheenvironment. Theagent,byinteracting\nwiththeenvironment,capturesuncertaintiesandestimatessystemdynamics. AnotheradvantageofRLmethodsisthat\naftertrainingtheRLagent,itslearnedpolicycanbedirectlyusedinanewtestsettingwithoutrequiringsolvingany\noptimizationproblem. Therefore,RLmethodsareefficienttoolsforreal-timecontrol[25]. RLmethodshavebeen\n3"
  },
  {
    "id": 1130,
    "text": "successfullyappliedtomanypowersystemproblemssuchasthesmartchargingofEVs[26,27],demandresponse[28],\nfrequency control [29], etc. Generally, model-free RL methods can be classified into two categories: value-based\nmethods(e.g.,Q-learning,fittedQ-iteration(FQI),DQN,etc.) andpolicygradientmethods(e.g.,actor-critic,deep\ndeterministic policy gradient (DDPG), SAC, etc.) [30]. In value-based methods, the Q (or V) function is learned\n(estimated)andtheactionischosenbasedonthelearnedQ(orV)functionastomaximizeit. Theauthorsin[31]\nproposedtheDQNmethod,whichcombinesRLwithdeeplearning. AtrainedagentusingDQNreachedhuman-level\nperformanceonmanyAtarivideogames. Ontheotherhand,policygradientmethodsdirectlylearnthepolicy. In[32],\ntheSACmethodhasbeenproposedasanoff-policyactor-criticmethod. InSAC,thepolicyislearnedbyanactor\nnetworkandtheQfunctionisestimatedbyacriticnetwork. Theactoraimstomaximizetheexpectedrewardaswellas"
  },
  {
    "id": 1131,
    "text": "theSACmethodhasbeenproposedasanoff-policyactor-criticmethod. InSAC,thepolicyislearnedbyanactor\nnetworkandtheQfunctionisestimatedbyacriticnetwork. Theactoraimstomaximizetheexpectedrewardaswellas\ntheentropyoftheactor,toencouragetheagenttoexploretheenvironmentmore. Inthispaper,wewillusetheDQN(as\nastate-of-the-artmethodinvalue-basedmethods)andSAC(asastate-of-the-artmethodinpolicygradientmethods)\nmethodstosolvethearbitrageproblem.\n3 ProblemFormulation\nInthissection,theimbalancesettlementmechanismisexplainedindetail(Section3.1)andtheMarkovdecisionprocess\n(MDP)formulationoftheenergyarbitrageproblemintheimbalancesettlementmechanismisprovided(Sections3.2\nand3.3).\n3.1 ImbalanceSettlementMechanism\nBRPsareresponsibleforcontinuouslybalancingtheirindividualdemandandsupply. ButsometimesBRPsdeviate\nfromtheirtradedconsumptionandgenerationduetouncertaintiesinthegrid. ThetotalimbalancevolumeofallBRPs\ninasinglecontrolareaiscalledthetotalsystemimbalance[33]. Positiveandnegativevaluesofthesystemimbalance"
  },
  {
    "id": 1132,
    "text": "inasinglecontrolareaiscalledthetotalsystemimbalance[33]. Positiveandnegativevaluesofthesystemimbalance\nindicatetheexcessandshortageofthegeneration,respectively. ATSOcorrectsthesystemimbalanceinreal-time\nbyactivatingreservecapacitiesofferedinthebalancingmarket[34]. ATSOchargesBRPsfortheirimbalanceat\napricespecifictotheimbalancesettlementperiod(15mininmostEuropeanmarkets). Thismechanismisknown\nas imbalance settlement. The imbalance price is dependent on the reserve volume activated by the TSO [35]. In\neachimbalancesettlementperiod,thenegativeimbalancepriceisequaltothehighestactivatedupwardreserveoffer\n(marginalincrementalprice),andthepositiveimbalancepriceisdeterminedbythelowestactivateddownwardreserve\noffer(marginaldecrementalprice)[36]. Threemainimbalancepricingmethodologiesareusedinvariouscountries: (1)\ndualpricing;(2)two-pricesettlement;and(3)singlepricing[36].\nIn the dual pricing method, the imbalance price is different for positive and negative imbalances. BRPs penalize"
  },
  {
    "id": 1133,
    "text": "dualpricing;(2)two-pricesettlement;and(3)singlepricing[36].\nIn the dual pricing method, the imbalance price is different for positive and negative imbalances. BRPs penalize\nfor negative and positive imbalances using the marginal incremental price (MIP) and marginal decremental price\n(MDP),respectively. ThispricingmethodmotivatesBRPstokeepthebalancewithintheirownportfoliowithoutbeing\nconcernedaboutthetotalsystemimbalance. ThemaindrawbackofthismethodisthatthereisnoincentiveforBRPs\ntodeviatefromtheirnominationtorestorethegrid. Forinstance,ifthetotalsystemimbalanceispositiveandthere\nisaBRPthatcanreducethisimbalance,thenthisBRPisnotincentivized,butevenpenalizedfordeviatingfromits\nday-aheadnomination.\nInthetwo-pricesettlementmethod,similartothedualpricingmethod,differentimbalancepricesareconsideredfor\neachimbalancedirection. ThedifferencewiththedualpricingmethodisthatiftheimbalancedirectionofBRPsis"
  },
  {
    "id": 1134,
    "text": "eachimbalancedirection. ThedifferencewiththedualpricingmethodisthatiftheimbalancedirectionofBRPsis\noppositetothetotalsystemimbalancedirection,theimbalancepriceisthesameastheday-aheadprice. Althoughin\nthispricingmethod,BRPsdonotfacepenaltiesduetotheirdeviationforhelpingTSOwithrestoringthegrid,the\nimbalancepriceisnotattractivetocreateaportfolioimbalanceforsupportingthegrid(typically,dayaheadpricesare\nimbalanceprices).\nInthesinglepricingmethod,theimbalancepriceisthesameforbothimbalancedirectionsanddependsonthetotal\nsystemimbalance. ThispricingmethodprovidesanopportunityforBRPstoreducetheircostbysupportingthegrid.\nForinstance,ifthetotalimbalancepriceisnegativeandaBRPcreatesapositiveimbalance,theBRPwillreceivean\nMIP(imbalanceprice)whichisusuallyhigherthantheday-aheadprice. Insomecountries,e.g.,Germany,despite\nusingthesinglepricingmethod,arbitrageintheimbalancesettlementmechanismisprohibitedandmarketplayersare"
  },
  {
    "id": 1135,
    "text": "usingthesinglepricingmethod,arbitrageintheimbalancesettlementmechanismisprohibitedandmarketplayersare\nexpectedtotradehonestlyinthemarkets[37]. Nonetheless,thearbitrageintheimbalancesettlementmechanismisa\nwin-winsituationforbothBRPsandTSOs. Ontheonehand,BRPscanprofitfromthearbitrageandindirectlyreduce\nthetotalsystemimbalance. Ontheotherhand,thisdecreaseinthetotalsystemimbalanceresultsinalowerimbalance\npricesincetheTSOdoesnotneedtoactivatemoreexpensivereserveoffers.\nAsmentionedearlier,ENTSO-EaimstoharmonizetheimbalancesettlementmechanisminEuropebyimplementing\nthesinglepricingmethodforcalculatingtheimbalancepricewitha15minuteimbalancesettlementperiod. Forthis\n4"
  },
  {
    "id": 1136,
    "text": "reason,thefocusofthispaperisonthesinglepricingmethod. TheBelgianimbalancesettlementmechanismisagood\ncasestudyforthisresearchworkbecausesincethebeginningof2020,itadoptsthesinglepricingmethodwitha15\nminutesettlementperiod[33].\n3.2 MDPFormulationwithoutCycleConstraintConsideration\nTheenergyarbitrageproblemcanbeformulatedasanMDP.AnMDPprovidesamathematicalframeworkforstochastic\nsequentialdecision-makingproblemsandismodeledbyatuple(S,A,R,P,γ),whereS isthestatespace,Aisthe\n(discrete)actionspace,R:S×A→Rrepresentstheimmediaterewardfunction,P :S×S×A→[0,1]denotes\ntheunknownstatetransitionprobabilitydistribution,andγ ∈(0,1]isthediscountfactor[38]. Ateachtimestept,the\nagentobservestheenvironmentstates ∈S andtakesanactiona ∈Abasedonthecurrentstate. Asaconsequence\nt t\nofthetakenaction,theagentreceivesarewardvalueR(s ,a )andmovestoanewstates ∈S withtheprobability\nt t t+1\ndeterminedbythestatetransitionprobabilitydistributionP(s |s ,a ). Intheenergyarbitrageproblem,theagentisa\nt+1 t t"
  },
  {
    "id": 1137,
    "text": "t t t+1\ndeterminedbythestatetransitionprobabilitydistributionP(s |s ,a ). Intheenergyarbitrageproblem,theagentisa\nt+1 t t\ndecisionmakerwhodecidesaboutthecharging/dischargingofBESSateachtimestep. Theenvironmentistheexternal\ncontextwithwhichtheagentinteracts(electricitymarkets,grid,etc.). WedefinetheMDPformulationoftheenergy\narbitrageproblemintheimbalancesettlementmechanismwithoutcycleconstraintsasfollows:\n(i) State: Thestateateachtimestepisexpressedas\ns =(T ,qh,mo,SOC ,πˆimb) (1)\nt qh t t\nwhereT representstheminuteofthequarterhour,qhisthequarterhouroftheday,moisthemonthofthe\nqh\nyear,SOC istheSoCofBESSattimet,andπˆimbistheforecastedimbalancepriceofthecurrentquarter-hour.\nt t\nWeusedaforecastoftheimbalancepricebecausetherealimbalancepriceofthequarterhourisonlycalculated\noncethequarterhourisover.\n(ii) Action: Weconsideradiscreteactionspacewith3possibleactions,asfollows:\na ∈A, A={−P ,0,P } (2)\nt max max\nwhereP isthemaximum(dis-)chargingpoweroftheBESS.Theactiona representsadecisiononthe"
  },
  {
    "id": 1138,
    "text": "(ii) Action: Weconsideradiscreteactionspacewith3possibleactions,asfollows:\na ∈A, A={−P ,0,P } (2)\nt max max\nwhereP isthemaximum(dis-)chargingpoweroftheBESS.Theactiona representsadecisiononthe\nmax t\ncharging/dischargingpowerattimet.\n(iii) Reward: Theobjectiveoftheagentistomaximizetherevenuebybuyingenergywhentheimbalanceprice\nislowandsellingitwhentheimbalancepriceishigh. Hence,therewardfunctiontobemaximizedisthe\nnegativeoftheenergycost,definedasfollows\nr =−a πimb (3)\nt t t\nwhereπimbistherealimbalancepriceofthequarterhourinwhichtlies.\nt\n(iv) State transition function: In the MDP framework, system dynamics are described by a state transition\nprobability function P. This probability function is unknown in the energy arbitrage problem because of\nuncertaintiesintheimbalanceprice. Theagentstrivestoestimatethestateprobabilitydistributionthrough\ninteractions with the environment. However, the state transition for SOC is controlled by a and can be\nt t\nexplicitlyformulatedasbelow."
  },
  {
    "id": 1139,
    "text": "interactions with the environment. However, the state transition for SOC is controlled by a and can be\nt t\nexplicitlyformulatedasbelow.\n SOCtemp :0<SOCtemp <1\n t+1 t+1\nSOC = 0 :SOCtemp <0 (4)\nt+1 t+1\n1 :SOCtemp >1\nt+1\nmin(a ,0) ∆t\nSOCtemp =SOC +(max(a ,0)η + t ) (5)\nt+1 t t cha η C\ndis BESS\nwhereC isthemaximumcapacityoftheBESS,andη andη ,denotethecharginganddischarging\nBESS cha dis\nefficiencyoftheBESS,respectively.\n3.3 MDPFormulationwithCycleConstraintConsideration\nFrequentcharging/dischargingcyclescauseanextracostbecausetheyexpeditethedegradationofBESS.Modelingthe\nagingofBESSiscrucialasitindicatesacapitallossofBESSinvestmentcosts[39]. Duetothedependenceofbattery\nlifetimeonitsoperationalstrategy,thelifetimeofaBESSplaysanimportantroleinthefinancialevaluationofthe\nenergyarbitragestrategy. Usually,thelifetimeofaBESSisdeterminedbythenumberofcompletecharge-discharge\ncyclesbeforeitsnominalcapacitybecomeslowerthanacertainlevelofitsinitialratedcapacity[40].Thus,weconstrain"
  },
  {
    "id": 1140,
    "text": "cyclesbeforeitsnominalcapacitybecomeslowerthanacertainlevelofitsinitialratedcapacity[40].Thus,weconstrain\nthedailynumberofcycles,sinceitalignswiththedesignedlifetimeandguaranteeprovidedbymanufacturers[41].\nTheMDPformulationwithcycleconstraintconsiderationisdescribednext.\n5"
  },
  {
    "id": 1141,
    "text": "(i) State: Thestateisgivenby\ns =(T ,qh,mo,SOC ,πˆimb,ncyc) (6)\nt qh t t t\nt-1\nncyc =\n(cid:88)|min(a\ni\n,0)|∆t\n(7)\nt C\nBESS\ni=0\nwherencycisthedailyconsumednumberofcycles,calculatedusing(7).\nt\n(ii) Action: SimilartotheMDPformulationwithoutcycleconstraints,theactionspaceisdiscretewith3possible\nactions. Theactionisdeterminedasfollows\na =B(u ,ncyc), u ∈A={−P ,0,P } (8)\nt t t t max max\n(cid:26) 0 :u <0∧ncyc >ncyc\nB(u ,ncyc)= t t max (9)\nt t u :else\nt\nwherencyc isthemaximumalloweddailynumberofcyclesandB(.)isabackupcontrollertoensurethe\nmax\ndailycycleconstraint. Thebackupcontrollerisusedtooverridetheagentaction(u )whentheagentwantsto\nt\ndischargethebatteryandthedailynumberofcyclesexceedsthemaximumallowedvalue.\n(iii) Reward: TherewardfunctiondefinitionisthesameasthatoftheMDPformulationwithoutcycleconstraint.\n(iv) Statetransitionfunction: AlsothestatetransitionfunctionisthesameasthatoftheMDPformulationwithout\ncycleconstraint.\n4 ReinforcementLearningMethods"
  },
  {
    "id": 1142,
    "text": "(iv) Statetransitionfunction: AlsothestatetransitionfunctionisthesameasthatoftheMDPformulationwithout\ncycleconstraint.\n4 ReinforcementLearningMethods\nInthispaper,RLmethodsareusedtosolvethearbitrageproblemformulatedasanMDPandfindanarbitragestrategy\nintheimbalancesettlementmechanism. ThegoalinRListolearnapolicythatmaximizestheexpectedlong-term\nreward. NextwedetailthetwoRLmethodsadoptedinthispaper,i.e.,DQNandSAC.Subsequently,weintroducethe\ndistributionalperspectiveonRLandtherisk-sensitiveRLframework.\n4.1 DQN\nClassical tabular RL methods, e.g., Q-learning, suffer from an issue known as the curse of dimensionality. Since\nthesemethodscanonlybeappliedtoproblemswithdiscretestatespace,theycannotbeusedforproblemswithhigh-\ndimensionalorcontinuousstatespace. Inaddition,thesemethodsusuallyneedhandcraftedstaterepresentations[30].\nToovercometheselimitations,theDQNmethodusesadeepneuralnetworkasafunctionapproximatortoestimate"
  },
  {
    "id": 1143,
    "text": "Toovercometheselimitations,theDQNmethodusesadeepneuralnetworkasafunctionapproximatortoestimate\ntheQ-valuefunctionparametrizedbyθ. TheQ-valuefunctionQ (s ,a )islearnedbyminimizingthefollowingloss\nθ t t\nfunction:\n(cid:104) (cid:105)\nL (θ)=E (r +γmaxQ (s ,a)−Q (s ,a ))2 . (10)\nQ (st,at,rt,st+1)∼D t\na\nθ′ t+1 θ t t\nThefirstbenefitofDQNisitsstabilityinlearning. In[31],twotechniquesareusedtostabilizethelearningprocess.\nFirst, the target Q function Q (s ,a ) is used to calculate next state-action values in Eq. (10). Parameters of the\nθ′ t t\ntargetQfunction(θ′)areperiodicallyupdatedwiththemostrecentθ. Second,agentpasttransitionsarestoredinan\nexperiencereplaybuffer(D)andfortrainingQ ,mini-batchesofexperiencesaresampledfromD. Anotherbenefitof\nθ\ntheDQNmethodisthatthismethodisanoff-policymethod. Thekeyadvantageofoff-policymethodsistheircapacity\ntolearnfromhistoricaldatasinceusingthecurrentexperiencesasthetrainingsetcaneasilyoverfitthepolicybecause"
  },
  {
    "id": 1144,
    "text": "tolearnfromhistoricaldatasinceusingthecurrentexperiencesasthetrainingsetcaneasilyoverfitthepolicybecause\nthetrainingsamplesarenotindependent[42]. Inanoff-policysetting,apolicylearnedbytheagentisdifferentfroma\nbehaviorpolicyusedforcollectinghistoricaldata. Usingpasttransitionsfortrainingcansignificantlyimprovesample\nefficiency.\n4.2 SAC\nValue-basedmethodshavesomelimitations. Theapplicationofthesemethodsislimitedtoproblemswithadiscrete\nandlow-dimensionalactionspace. Also,thesemethodslearnadeterministicpolicy,whichmeansforagivenstate,\nanactiontakenbytheagentisalwaysthesame. Thus, keepingabalancebetweenexplorationandexploitationin\nvalue-basedmethodsischallenging. Policygradientmethodssolvetheselimitationsbylearningapolicynetworkthat\noutputstheprobabilityoftakingactionsineachstate. Fromtheexistingpolicygradientmethods,weuseSACbecause\nofitssuperiorsampleefficiencyandstability. Inthisoff-policymethod,thepolicyislearnedbyanactornetworkπ\nϕ"
  },
  {
    "id": 1145,
    "text": "ofitssuperiorsampleefficiencyandstability. Inthisoff-policymethod,thepolicyislearnedbyanactornetworkπ\nϕ\nandtheQfunctionisapproximatedbyacriticnetworkQ . Theobjectiveoftheactoristomaximizetheexpected\nθ\n6"
  },
  {
    "id": 1146,
    "text": "rewardaswellasmaximizetheentropyoftheactortoencouragetheagenttofurtherexploretheenvironment. Theloss\nfunctionoftheactornetwork(J )isgivenby\nπ\nJ (ϕ)=E [αlnπ (a|s)−Q (s,a)] (11)\nπ s∼D,a∼πϕ ϕ θ\nThecriticnetworkestimatesthesoftQ-valuefunction. Thelossfunctionofthecriticnetwork(L )isformulatedas\nQ\nfollows:\nL (θ)=E [(y −Q (s ,a ))2] (12)\nQ (st,at)∼D t θ t t\ny =r +γE [Q (s ,a )−αlnπ (a |s )] (13)\nt t at+1∼πϕ θ′ t+1 t+1 ϕ t+1 t+1\nθ′ =τθ+(1−τ)θ′ (14)\nInEq.(13),y isanestimatedsoft-QvaluethatiscalculatedbyamodifiedBellmanequation(theso-calledsoftBellman\nt\nequation). SimilartotheDQNmethod, thetargetQfunctionisusedtocalculatey . AftereachupdateofQ , the\nt θ\nparametersofQ areupdatedaccordingtoEq.(14)withτ ≪1toslowlytrackthelearnednetwork[43].\nθ′\n4.3 DistributionalRL\nAdistributionalperspectiveonRLwasfirstintroducedin[44].InDRLmethods,theprobabilitydistributionoverreturns\nisestimatedratherthanapointestimateofthemean. DRLmethodsofferseveraladvantages,includingmorestable"
  },
  {
    "id": 1147,
    "text": "isestimatedratherthanapointestimateofthemean. DRLmethodsofferseveraladvantages,includingmorestable\nlearning[44],mitigatingQ-valueoverestimation[45],andprovidingaframeworkforrisk-sensitivelearning[46]. Inthe\nvanillaDQNmethod,thecoreideaistoestimatetheQ-valuefunctionQ . GoingbeyondthevanillaDQNmethod,the\nθ\ndistributionalDQN(DDQN)methodlearnstheprobabilitydistributionofreturns(Z )usingthedistributionalBellman\nθ\nequationasfollows[44]:\nL (θ)=E [D (TZ (s ,a )||Z (s ,a ))] (15)\nZ (st,at)∼D KL θ′ t t θ t t\nTZ(s ,a )= D r +γmaxE [Z(s ,a)] (16)\nt t t a Z∼Z θ′ r+1\nD\nwhereZ isthedistributionofreturns,A=B denotesthattworandomvariablesAandB haveanequalprobability\ndistribution,andTZ indicatestheprobabilitydistributionofTZ. Thedistributionofreturnscanbemodeledasa\nθ\ncategoricaldistributionasbelow.\n(cid:26) (cid:12) V −V (cid:27)\nZ(s ,a )= z (cid:12)z =V + max mini,0≤i<N (17)\nt t i(cid:12) i min N −1\nInEq.(17),V andV arethemaximumandminimumvaluesofrandomreturns,respectively,andN isthenumber\nmin max"
  },
  {
    "id": 1148,
    "text": "Z(s ,a )= z (cid:12)z =V + max mini,0≤i<N (17)\nt t i(cid:12) i min N −1\nInEq.(17),V andV arethemaximumandminimumvaluesofrandomreturns,respectively,andN isthenumber\nmin max\nofbins. IndistributionalSAC(DSAC),thecriticnetworklearnstheprobabilitydistributionofsoftreturns. Theloss\nfunctionofthecriticnetworkinDSACissimilartoEq.(15),butthecalculationofTZ(s ,a )differsasfollows:\nt t\nTZ(s ,a )= D r +γE [Z(s ,a )−αlnπ (a |s )] (18)\nt t t at+1∼πϕ,Z∼Z θ′ t+1 t+1 ϕ t+1 t+1\nSincetheexpectationofZ(s ,a )overZ isequaltoQ(s ,a ),thelossfunctionoftheactornetworkismodifiedas\nt t θ t t\nbelow.\nJ (ϕ)=E [αlnπ (a|s)−E [Z(s,a)]] (19)\nπ s∼D,a∼πϕ ϕ Z∼Zθ\n4.4 Risk-sensitiveRL\nByapproximatingtheprobabilitydistributionofreturns,DRLpresentsapossibilityforlearningarisk-aversepolicy.\nInarisk-neutralRLframework,theagentineachstatetakesanactionthataimstomaximizetheexpectedreturn(Q\nvalue). Ontheotherhand,intherisk-sensitiveRLframework,theagenttakesanactionwiththelowestassociatedrisk."
  },
  {
    "id": 1149,
    "text": "value). Ontheotherhand,intherisk-sensitiveRLframework,theagenttakesanactionwiththelowestassociatedrisk.\nThemainriskinthearbitrageproblemisrelatedtoforecastedimbalanceprices. Thegreatertheinaccuracyinpredicted\nprices,thehighertheassociatedriskoftakingthewrongaction.\nRiskmeasurescanbeusedtoassessthelevelofriskassociatedwithadistributionofreturns[47]. Thelossfunctionof\ntheactornetworkintherisk-sensitiveDSACcanbeformulatedasfollows:\nJ (ϕ)=E [αlnπ (a|s)−E [Z(s,a)]−βΨ[Z(s,a)]], (20)\nπ s∼D,a∼πϕ ϕ Z∼Zθ\nwhereΨ[.]representsariskmeasurefunctionandβ isaparameterthatcontrolsthetrade-offbetweentheexpectation\nvalue and risk. β = 0 represents the risk-neutral attitude of the agent. As β increases, the agent becomes more\nrisk-averse. Inthispaper,value-at-risk(VaR)isappliedastheriskmeasurefunction:\nVaR (Z)=inf{z|CDF (z)≥ρ}, (21)\nρ Z\nwhereρ∈(0,1]isaconfidencelevel. Wewillsetρ=0.1inthispaper.\n7"
  },
  {
    "id": 1150,
    "text": "5 SimulationResults\nWewillevaluatetheperformanceoftheproposedcontrolframework,isexplainedinSections3and4,fortheenergy\narbitrageproblem.\n5.1 ExperimentalSetup\nFigure2showstheoverviewoftheproposedcontrolframework, whichwetestontheBelgianimbalancein2022\nextractedfrom[48]. AsmentionedinSection3.1,Eliapublishestwoimbalanceprices: 15-minute-basedand1-minute-\nbasedprices. ThereferencepricefortheimbalancesettlementofBRPsisthe15-minute-basedpricewhichisthereal\nimbalancepricecalculatedattheendofthequarter-hourperiod. The1-minute-basedprices,ontheotherhand,are\ncalculatedbasedonnon-validateddata,basedontheinstantaneoussystemimbalanceandpricesofcumulativeactivated\nregulationvolumesonaminutebasis. These1-minute-basedpricesarepublishedtoprovideadditionalinformationto\nBRPs[49]. Weusethesenon-validatedpricesasforecastedimbalancepricesofthecorrespondingquarter-hourperiod.\nSincethegranularityoftheforecastedimbalancepricesisoneminute,theRLagenttakesanactioneveryminute. Inthis"
  },
  {
    "id": 1151,
    "text": "Sincethegranularityoftheforecastedimbalancepricesisoneminute,theRLagenttakesanactioneveryminute. Inthis\nwork,theday-aheadscheduleforthebatteryissettozerowhichmeansthatthebatterydoesnottradeintheday-ahead\nmarket. However,futureworkwillextendourproposedcontrolframeworkforarbitrageinboththeday-aheadmarket\nandimbalancesettlement. Totrainandvalidatetheproposedcontrolframework,theimbalancepricedatasetissplitas\nfollows: thefirst20daysofeachmonthareconsideredasthetrainingset,the21stdaytothe25thdayofeachmonth\nareconsideredasthevalidationset,andtheremainingdaysofeachmonthareusedasthetestset. Theconsidered\nBESShasapowerratingof1MWandamaximumcapacityof2MWhwitharound-tripefficiencyof0.9forboth\ncharginganddischarging. SincethemaximumallowedannualnumberofcyclesfortheBESSis400,themaximum\ndailynumberofcyclesissetto1.1. TheRLmethodsaretrainedwith50000episodesandeachepisodeconstitutes\nasimpleday. ThehyperparametersusedforthemethodsarelistedinTable1. Theproposedcontrolframeworkis"
  },
  {
    "id": 1152,
    "text": "dailynumberofcyclesissetto1.1. TheRLmethodsaretrainedwith50000episodesandeachepisodeconstitutes\nasimpleday. ThehyperparametersusedforthemethodsarelistedinTable1. Theproposedcontrolframeworkis\nimplementedinPythonusingthePyTorchpackage.\nWedesignexperimentstoanswerthefollowingquestions:\n• Q1: Whatisthelearnedarbitragestrategywhenthereisnolimitonthedailynumberofcycles?\n• Q2: Howdoesadailynumberofcyclesaffectthelearnedarbitragestrategy?\n• Q3: Whatistheeffectoftherisk-averseperspectiveonthelearnedarbitragestrategy?\nTable1: Methodhyperparameters\nShared DQN SAC\nParameter Value Parameter Value Parameter Value\nDiscountfactorγ 0.9995 learningrate 5×10−4 actorlearningrate 2×10−5\nSoftupdatefactorτ 0.1 criticlearningrate 1×10−4\nExperiencebuffersize 1×106 initialα 1\nMini-batchsize 16384 αlearningrate 3×10−4\nNetworkhiddenlayersize [256,128]\nV 5000\nmax\nV −5000\nmin\nN 11\n5.2 ArbitrageStrategywithoutCycleConstraint(Q1)"
  },
  {
    "id": 1153,
    "text": "Experiencebuffersize 1×106 initialα 1\nMini-batchsize 16384 αlearningrate 3×10−4\nNetworkhiddenlayersize [256,128]\nV 5000\nmax\nV −5000\nmin\nN 11\n5.2 ArbitrageStrategywithoutCycleConstraint(Q1)\nThe learning process of the RL methods for the risk-neutral scenario, without considering the cycle constraint, is\nillustratedinFig.3. TheperformanceofthetrainedRLmethodsonthetestsetisindicatedinTable2. Resultsshow\nthattheDRLmethodsoutperformthestandardRLmethods. Thereasonbehindthisisthatestimatingtheprobability\ndistributionofreturns,ratherthantheexpectationofreturns,canprovideamorestabletrainingtarget. Also,theDRL\nmethods can mitigate instability in the Bellman optimality operator by learning probability distribution of returns.\nTheDDQNmethodincreasestheaveragedailyprofitby17%comparedtotheDQNmethod. DSACimprovesthe\nproportionalreward(definedastheratioofaveragedailyprofittoaveragedailynumberofcycles)by2.1%comparedto"
  },
  {
    "id": 1154,
    "text": "TheDDQNmethodincreasestheaveragedailyprofitby17%comparedtotheDQNmethod. DSACimprovesthe\nproportionalreward(definedastheratioofaveragedailyprofittoaveragedailynumberofcycles)by2.1%comparedto\nSAC.ThecomparisonbetweentheperformanceofthedistributionalandvanillaDQN,andSAC,indicatesthatthe\ndistributionalperspectivecanenhanceDQNresultstoagreaterextent. ThereasonisthattheSACmethodmitigates\ninstabilityintheBellmanoptimalityoperatorbyusinganactornetworkinsteadofthemaxoperatorintheBellman\n8"
  },
  {
    "id": 1155,
    "text": "Figure2: Theoverviewoftheproposedcontrolframework\nequation. Therefore,theimprovementintheDSACresultsismainlyduetostabletrainingtargetforthecriticnetwork.\nHowever,thedistributionalperspectivecanboosttheperformanceofthevanillaDQNbybothprovidingstabletraining\ntargetsandmitigatinginstabilityintheBellmanoptimalityoperator. ResultsalsohighlightthesuperiorityofSACover\nDQN.ThisisbecauseSACcanmitigateQ-valueoverestimationsinDQNbyreplacingthemaxoperator(Eq.(10))with\ntheexpectationoperator(Eq.(13))intheBellmanequation.\n(a) (b)\nFigure3: ThelearningprocessofthefourRLmethodsfortherisk-neutralwithoutcycleconstraintscenario. (a)The\naveragedailyprofitoftheRLmethods. (b)Theaveragedailynumberofcycles.\n9"
  },
  {
    "id": 1156,
    "text": "Table2: EvaluationofRLmethodsonthetestsetintherisk-neutralscenarios\nwithoutcycleconstraint withcycleconstraint\nMethods Profit Cycles Proportionalprofit Profit Cycles Proportionalprofit\n(C/perday) (perday) (C/percycle) (C/perday) (perday) (C/percycle)\nDQN 749.9 3.2 235.6 338.0 0.9 399.1\nDDQN 877.5 3.2 275.9 397.2 1 405.9\nSAC 1147.6 3.7 307.6 504.9 1.1 472.7\nDSAC 1148.5 3.6 314.1 486.4 0.9 541.7\nToanalyzeandstudythelearnedpolicyofthefourRLmethods,thepolicyheatmapsareillustratedinFig.4. SinceSoC\nandforecastedimbalancepricearethetwomostdeterminativefeaturesfortheagent,weshowthelearnedpolicywith\nrespecttothesetwoinputfeatures,whicharealsoinformativetointerpretthepolicy. Figure4showsthattheSACand\nDSACmethodscanlearnamoremeaningfulandsmoothpolicycomparedtotheDQNandDDQNmethods. ForDQN\nandDDQN,theQ-valuefunctionoverestimatesthevalueofrarelyseenstatesandout-of-distribution(OOD)actionsin\ntheseraresatesduetothemaxoperatorandtherelianceoftheestimatedQvaluesoninputsfromthesamedistribution"
  },
  {
    "id": 1157,
    "text": "theseraresatesduetothemaxoperatorandtherelianceoftheestimatedQvaluesoninputsfromthesamedistribution\nasitstrainingset. ThisoverestimationresultsinpoliciesthatchooseOODactions. AccordingtoFig.5,theforecasted\nimbalancepricerarelygoesbeyond850 C/MWh(theprobabilityis1%). ItmeansthattheDQNandDDQNmethods\noverestimateQvaluesforthisareaandtakeOODactions. Figures4and5revealsomecorrelationbetweenthelearned\npolicybyDSACandthepricedistribution. TheagentalwayschargestheBESSwhenthepriceiswithinthelower7%\nquantile(lowerthan−60 C/MWh),regardlessoftheSoClevel. Theagentnevertakesthechargingactionforthe25%\nhighestprices(priceshigherthan380 C/MWh),eveniftheBESSisempty. TheBESSisalwaysdischargedwhenthe\npriceliesintheupper5%quantile(higherthan640 C/MWh). Forthemedianprice(roughly220 C/MWh),theBESS\nisdischargediftheSoCishigherthan60%,doesnothingwhentheSoCisbetween60%and50%,andischargedifthe\nSoCislowerthan50%. Generally,theagentlearnsamilderslopeboundaryforthedischargeaction. IftheBESSwith"
  },
  {
    "id": 1158,
    "text": "isdischargediftheSoCishigherthan60%,doesnothingwhentheSoCisbetween60%and50%,andischargedifthe\nSoCislowerthan50%. Generally,theagentlearnsamilderslopeboundaryforthedischargeaction. IftheBESSwith\nalowSoClevelisdischarged,theagentneedstoquicklyrechargetheBESStomakesureitcanstillmakemoney. This\nquickrechargingincreasestheriskofchargingatahigherprice. Therefore,bydecreasingtheSoC,theareaofidle\nactionbecomeslarger.\n(a) (b)\n(c) (d)\nFigure 4: The projection of the learned policy in the risk-neutral without cycle constraint scenario for (a) DQN,\n(b)DDQN,(c)SAC,and(d)DSAC.\n10"
  },
  {
    "id": 1159,
    "text": "Figure5: Thecumulativedistributionoftheimbalancepricein2022.\n5.3 ArbitrageStrategywithCycleConstraint(Q2)\nFigure6showsthelearningprocessoftheRLmethodsfortherisk-neutralscenariowhenthelimitationisappliedtothe\ndailynumberofcycles. Similartothepreviousscenario,theDSACmethodsurpassesothermethodsbyconvergingtoa\nhigherrewardwithafewernumberofcycles. AccordingtoTable2,althoughtheaveragedailyprofitoftheDSAC\nmethodislessthanthatoftheSACmethod,theDSACmethodearnsthisprofitbyconsumingfewernumberofcycles.\nInotherwords,theDSACmethodachievesa14.6%improvementintheproportionalrewardpercyclecomparedto\ntheSACmethod. Furthermore,theSACandDSACmethodsconvergefasterthantheDQNandDDQNmethodsdue\ntotheirefficientexploration. SinceinDQNandDDQNthelearnedpolicyisdeterministic,theϵ-greedyexploration\ntechniqueneedstobeused. Ontheotherhand, theSACandDSACmethodslearnastochasticpolicyandusethe\nlearnedprobabilitiesforexploration. Thus,insteadofalwaysconsideringafixedexplorationprobabilityofϵforall"
  },
  {
    "id": 1160,
    "text": "techniqueneedstobeused. Ontheotherhand, theSACandDSACmethodslearnastochasticpolicyandusethe\nlearnedprobabilitiesforexploration. Thus,insteadofalwaysconsideringafixedexplorationprobabilityofϵforall\nstates,theprobabilityofexplorationdependsonthecurrentstate. Foragivenstate,whentheprobabilityofoneaction\niscloseto1,theagentalmostalwaysexploitsandhardlyexplores. Conversely,whenprobabilitiesofallactionsare\nclosetoeachother,theagentmostofthetimeexplorestofindthebestactionforthatstate. Consequently,theSACand\nDSACmethodsaremoredataefficientthantheDQNandDDQNmethods.\n(a) (b)\nFigure6: ThelearningprocessofthefourRLmethodsfortherisk-neutralwithcycleconstraintscenario,intermsof\n(a)theaveragedailyprofit,and(b)theaveragedailynumberofcycles.\nThelearnedpolicyofDSACwhenconsideringthecycleconstraintisillustratedinFig.7. Notethatthedisplayed\npolicy is a projection of the learned policy, as the learned policy depends on more than two features and thus is"
  },
  {
    "id": 1161,
    "text": "policy is a projection of the learned policy, as the learned policy depends on more than two features and thus is\nmorecomplicatedthanthefiguresshown. Thelogicbehindthelearnedpolicywithandwithoutthecycleconstraint\nconsideration,whichischargingatcheappricesanddischargingatexpensiveprices,isnearlyidentical. Themain\n11"
  },
  {
    "id": 1162,
    "text": "differencebetweentheselearnedpoliciesisinthesizeoftheidleactionarea. Addingthecycleconstraintmakesthe\nagentmoreconservativeandincreasestheidleactionarea. Moreover, bylimitingthenumberofcycles, theagent\nrechargestheBESSlessfrequentlyduetoreduceddischarging. Asaresult,inthisscenario,theagentrechargesthe\nBESSatcheaperpricescomparedtothepreviousscenario. ToshowtheperformanceofthelearnedDSACagentsina\nreal-lifecase,thelearnedagentsaretestedusingdatafromMarch31,2022. AsFig.8shows,thereisonemajorpeakin\ntheimbalancepricefrom11:00to13:15andonemajorvalleyfrom13:30to17:00onthisday. Bothagentsproperly\nrespondtotheseprices: theagentwithoutthecycleconstraintreactstoroughlyallfluctuationsintheimbalanceprice,\nevensmallones(suchasthepricefluctuationbetween4:30and6:00,orbetween20and21:30). However,another\nagentmostlyfocusesonmoresignificantfluctuationstolimitthenumberofchargingcycles.\nFigure7: Theprojectionofthelearnedpolicyintherisk-neutralwithcycleconstraintscenarioforDSAC.\n(a) (b)"
  },
  {
    "id": 1163,
    "text": "agentmostlyfocusesonmoresignificantfluctuationstolimitthenumberofchargingcycles.\nFigure7: Theprojectionofthelearnedpolicyintherisk-neutralwithcycleconstraintscenarioforDSAC.\n(a) (b)\nFigure8: TheperformanceofthetrainedagentbytheDSACmethodonMarch31,2022(a)withoutand(b)with\nconsideringcycleconstraint.\n5.4 ArbitrageStrategywithRiskManagement(Q3)\nToanswerQ3,wetraintheDSACagentwithoutthecycleconstraintconsiderationforvaryingβ values. Resultsin\nTable3showsthattherisk-averseagentwithβ =3experiencesa54.8%reductionintheaveragedailyprofitcompared\ntotherisk-neutralagent,butgiventhatitavoidsriskybehavior,wenoteahigherprofitpercycle. Figure9illustratesthe\ndifferencebetweenthelearnedcriticnetworkforthefullyrisk-averseandrisk-neutralagents. Thelearnedcriticnetwork\nforthefullyrisk-averseagentisnarrowerduetoapplyingtheriskmeasurefunction(VaR)insteadoftheexpectation.\nAlsotheVaRvaluesalignwiththisobservation: VaRvaluesfortherisk-neutralandfullyrisk-aversecriticnetworksare"
  },
  {
    "id": 1164,
    "text": "AlsotheVaRvaluesalignwiththisobservation: VaRvaluesfortherisk-neutralandfullyrisk-aversecriticnetworksare\nequalto−589.2Cand−240.5C,respectively. Theprobabilitydistributionofthehourlyprofitfortestdataisshownin\nFig.10. BasedonFig.10,therisk-averseagentsuccessfullyhedgesagainsttheuncertaintyintheimbalancepriceand\nmitigatesthetailofthehourlyprofitdistribution.1 TheVaRvalueofeachdistributionisprovidedinTable3.\n1Notethatboththeleft-andright-tailsarereduced,althoughfromtheriskperspectiveespeciallythelower(negative)return\nvaluesshouldbeavoided.\n12"
  },
  {
    "id": 1165,
    "text": "Table3: EvaluationofDSACmethodonthetestsetintherisk-sensitivescenario(β =3).\nProfit Cycles Proportionalprofit\nRiskaversion VaRvalue\n(C/day) (perday) (C/cycle)\nβ =0 1148.5 3.6 314.1 −71\nβ =0.3 796.7 2 399 −48.5\nβ =1 593.9 1.25 474.6 −32.5\nβ =3 518.9 1 518.9 −24.7\nFigure9: Thelearnedcriticnetworkfortherisk-neutral(β =0)andrisk-averse(β =3)agents.\nFigure10: Theprobabilitydistributionofhourlyprofitwithandwithouttherisk.\nFigure 11 shows the learned risk-averse policy when β = 3. Compared to Fig. 4, we note that the idle area gets\nsignificantlylarger: theagentdoesnotdischargethebatterywhentheSoCislow. Inthisway,theagentmakessurethat\nthebatteryhasalwaysenoughenergytoinjectintothegridwhenthepriceishigh. Moreover,thereisanobservable\nchangeinthechargethresholdthatcanbejustifiedbyFig.12. Thechargethresholdfortherisk-neutralagentranges\nbetween0and400 C/MWh. However,Fig.12indicatesthatwithinthisrange,theactualpriceissignificantlyuncertain"
  },
  {
    "id": 1166,
    "text": "between0and400 C/MWh. However,Fig.12indicatesthatwithinthisrange,theactualpriceissignificantlyuncertain\nandthechanceofchargingbatteryatapricelargerthantheforecastedvalueishigh. Tomitigatethisrisk,therisk-averse\nagentlearnsalowerchargethreshold. Therisk-averseagentchargesthebatteryatcheaperpricestominimizetherisk\nofchargingatahighpriceresultingfrominaccuratepricepredictions.\n13"
  },
  {
    "id": 1167,
    "text": "Figure11: TheprojectionofthelearnedpolicyusingDSACfortherisk-averseagent\nFigure12: Actualimbalancepricevs. forecastedimbalanceprice\n6 Conclusion\nIn thispaper, aDRL-based control frameworkfor BESS was proposedto obtain energy arbitrage strategies in the\nimbalancesettlementmechanism. Intheproposedcontrolframework,inadditiontoconsideringaconstraintonthe\ndailynumberofcycles,thedegreeofrisktakinginthelearnedarbitragestrategycanbeadjustedbasedontherisk\npreferenceofBRPs. Toevaluatetheperformanceoftheproposedcontrolframework,twostate-of-the-artRLmethods,\ni.e.,DQNandSAC,andtheirdistributionalvariantshavebeenimplemented. TheresultsfortheBelgianimbalance\npricein2022showedthattheDSACmethodoutperformsothermethods(i.e.,boththenon-distributionalbaselinesas\nwellasDDQN)inallexperiments. DSACimprovestheaveragedailyprofitintheexperimentwithoutcycleconstraint\nby53.1%andintheexperimentwithcycleconstraintby43.9%,respectively,comparedtothe(worstperforming)DQN"
  },
  {
    "id": 1168,
    "text": "wellasDDQN)inallexperiments. DSACimprovestheaveragedailyprofitintheexperimentwithoutcycleconstraint\nby53.1%andintheexperimentwithcycleconstraintby43.9%,respectively,comparedtothe(worstperforming)DQN\nmethod. ThedominanceofSACoverDQNintermsofdataefficiencyandmitigatingQ-valueoverestimation,stem\nfromreplacingthemaxoperatorintheBellmanequationwiththeexpectationoperator. Moreover,thedistributional\nmethodsexhibitbetterperformancethanthestandardRLmethodsbecausetheyestimatethefullprobabilitydistribution\nofreturnsratherthantheexpectationofreturns,andtheyresolveinstabilityintheBellmanoptimalityoperator.\nInafirstexperiment,withoutconsideringcycleconstraints,wenotedthattheDSACagentlearnedasmoothandrational\npolicy: itlearnedtochargethebatterywhenthepriceisverycheap(withinthelower7%quantile),dischargewhenthe\npriceisveryexpensive(withintheupper5%quantile),andtaketheactionbasedontheSoCforpricesinbetween. Ina"
  },
  {
    "id": 1169,
    "text": "priceisveryexpensive(withintheupper5%quantile),andtaketheactionbasedontheSoCforpricesinbetween. Ina\nsecondexperiment,includingthecycleconstraints,thecycle-awarearbitragestrategyexpectedlyshowedalarger‘idle’\nactionareacomparedtothecasewithoutcycleconstraints,effectivelyleadingtoalowernumberofcyclesused. The\ntrainedcycle-awareagenttendedtorespondonlytomajorpeaksandvalleysintheimbalancepriceduetothelimited\nnumberofcycles,whilethecycle-unawareagentreactedtoalmostallfluctuationsintheimbalanceprice. Ourstudyof\nrisk-sensitiveagentsshowedthattherisk-aversearbitragestrategiesmakethedistributionofhourlyprofitnarrowerand\nmitigatethetailofthedistribution. Indeed,therisk-averseagentchargesthebatteryatlowerpricestomitigatetherisk\nassociatedwithinaccuratepriceforecastsandavoidincurringhigherchargingcosts.\n14"
  },
  {
    "id": 1170,
    "text": "Finally, we note that in this paper, the day-ahead schedule for the battery was set to zero. In future research, the\nproposedcontrolframeworkwillbegeneralizedbytakingintoaccountenergyarbitragebetweentheday-aheadmarket\nandtheimbalancesettlementmechanism. Studyingtheeffectofconsideringacontinuousactionspaceinsteadofa\ndiscreteoneformsanothernextsteptotake.\nA ComparingDQNwithFQI\nTheFQImethod[50]isanotherwidelyusedvalue-basedmethod. In[13],FQIisusedtoobtaina15-minute-based\narbitragestrategyintheimbalancesettlementmechanism. Inthissection,asmallexperimentiscarriedouttocompare\ntheperformanceoftheDQNandFQImethods. Inthisexperiment,themethodsaretrainedonthefirstninedaysof\nFebruaryandevaluatedonFebruary10,2022. ThearchitectureoftheneuralnetworkusedintheFQImethodisthe\nsameasthatoftheDQNmethod. Theexperiencereplaybuffersize,numberofiterations,andnumberofepisodesare\n16384,400,and500,respectively. InaccordancewithFig.13,bothmethodsperformalmostsimilarly. However,the"
  },
  {
    "id": 1171,
    "text": "sameasthatoftheDQNmethod. Theexperiencereplaybuffersize,numberofiterations,andnumberofepisodesare\n16384,400,and500,respectively. InaccordancewithFig.13,bothmethodsperformalmostsimilarly. However,the\nruntimeoftheFQImethodisroughly5timesgreaterthanthatoftheDQNmethodandevengetsworsebyincreasing\ntheexperiencereplaysizeandthenumberofepisodes. ThereasonforthelongerruntimeforFQIisitsnumberof\niterations: ineachepisode,theQnetworkistrainedforthementionednumberofiterations. Thus,theFQImethodis\ninappropriateforobtainingthearbitragestrategy.\nFigure13: ThelearningprocessoftheDQNandFQImethodsforthesmallexperiment.\nReferences\n[1] CounciloftheEuropeanUnionEuropeanParliament.“Regulation(EU)2021/1119oftheEuropeanParliament\nandoftheCouncilof30June2021establishingtheframeworkforachievingclimateneutralityandamending\nRegulations(EC)No401/2009and(EU)2018/1999(‘EuropeanClimateLaw’)”.In:(2021).\n[2] InternationalRenewableEnergyAgency.“RENEWABLECAPACITYSTATISTICS2023”.In:IRENA(2023)."
  },
  {
    "id": 1172,
    "text": "Regulations(EC)No401/2009and(EU)2018/1999(‘EuropeanClimateLaw’)”.In:(2021).\n[2] InternationalRenewableEnergyAgency.“RENEWABLECAPACITYSTATISTICS2023”.In:IRENA(2023).\nURL:https://www.irena.org/Publications/2023/Mar/Renewable-capacity-statistics-2023.\n[3] JérémieBottieauetal.“Very-Short-TermProbabilisticForecastingforaRisk-AwareParticipationintheSingle\nPriceImbalanceSettlement”.In:IEEETransactionsonPowerSystems35(2Mar.2020),pp.1218–1230.\n[4] European Network of Transmission System Operators for Electricity. “Explanatory document to all TSOs’\nproposaltofurtherspecifyandharmoniseimbalancesettlementinaccordancewithArticle52(2)ofCommission\nRegulation(EU)2017/2195of23November2017,establishingaguidelineonelectricitybalancing”.In:(2018).\n[5] YinguoYangetal.“Lifecycleeconomicviabilityanalysisofbatterystorageinelectricitymarket”.In:Journal\nofEnergyStorage70(Oct.2023),p.107800.\n[6] AmelieKruppetal.“Operatingstrategyoptimizationconsideringbatteryagingforasectorcouplingsystem"
  },
  {
    "id": 1173,
    "text": "ofEnergyStorage70(Oct.2023),p.107800.\n[6] AmelieKruppetal.“Operatingstrategyoptimizationconsideringbatteryagingforasectorcouplingsystem\nprovidingfrequencycontainmentreserve”.In:JournalofEnergyStorage68(Sept.2023),p.107787.\n[7] KinzaFidaetal.“Optimalbatteryenergystoragesystemdeploymentfromperspectivesofprivateinvestorsand\nsystemoperatorsforenhancingpowersystemreliability”.In:JournalofEnergyStorage69(2023),p.107882.\n15"
  },
  {
    "id": 1174,
    "text": "[8] ChixinXiaoetal.“Multi-perioddatadrivencontrolstrategyforreal-timemanagementofenergystoragesin\nvirtualpowerplantsintegratedwithpowergrid”.In:InternationalJournalofElectricalPower&EnergySystems\n118(June2020),p.105747.\n[9] RubenSmetsetal.“StrategicImplicitBalancingwithEnergyStorageSystemsviaStochasticModelPredictive\nControl”.In:IEEETransactionsonEnergyMarkets,PolicyandRegulation(Apr.2023),pp.1–14.\n[10] SadeghVejdanandSantiagoGrijalva.“TheValueofReal-TimeEnergyArbitragewithEnergyStorageSystems”.\nIn:IEEEPowerandEnergySocietyGeneralMeeting2018-August(Dec.2018).\n[11] DheepakKrishnamurthyetal.“EnergyStorageArbitrageUnderDay-AheadandReal-TimePriceUncertainty”.\nIn:IEEETransactionsonPowerSystems33(1Apr.2017),pp.84–93.\n[12] AlirezaAkbari-Dibavar,KazemZare,andSayyadNojavan.“Ahybridstochastic-robustoptimizationapproach\nforenergystoragearbitrageinday-aheadandreal-timemarkets”.In:SustainableCitiesandSociety49(Aug.\n2019),p.101600."
  },
  {
    "id": 1175,
    "text": "forenergystoragearbitrageinday-aheadandreal-timemarkets”.In:SustainableCitiesandSociety49(Aug.\n2019),p.101600.\n[13] Jesus Lago et al. “Optimal Control Strategies for Seasonal Thermal Energy Storage Systems with Market\nInteraction”.In:IEEETransactionsonControlSystemsTechnology29(Sept.2021),pp.1891–1906.\n[14] Ming-HuaLinetal.“Areviewofpiecewiselinearizationmethods”.In:MathematicalproblemsinEngineering\n(2013).\n[15] JonasEngels,BertClaessens,andGeertDeconinck.“Combinedstochasticoptimizationoffrequencycontrol\nandself-consumptionwithabattery”.In:IEEETransactionsonSmartGrid10.2(2017),pp.1971–1981.\n[16] XianqiuZhaoetal.“Day-aheaddispatchofnovelbatterychargingandswappingstationbasedondistributionally\nrobustoptimization”.In:JournalofEnergyStorage63(2023),p.107080.\n[17] BryanEllis,ChrisWhite,andLukasSwan.“Degradationoflithium-ionbatteriesthataresimultaneouslyservicing\nenergyarbitrageandfrequencyregulationmarkets”.In:JournalofEnergyStorage66(2023),p.107409."
  },
  {
    "id": 1176,
    "text": "[17] BryanEllis,ChrisWhite,andLukasSwan.“Degradationoflithium-ionbatteriesthataresimultaneouslyservicing\nenergyarbitrageandfrequencyregulationmarkets”.In:JournalofEnergyStorage66(2023),p.107409.\n[18] MohammadRezaAnsari,MostafaYaghtin,andMostafaKazemi.“Abi-levelapproachforparticipationofhybrid\ntransmissionoperatingcompaniesintheday-aheadmarket,consideringenergystoragesystems”.In:Journalof\nEnergyStorage61(2023),p.106765.\n[19] IoannisBoukasetal.“Adeepreinforcementlearningframeworkforcontinuousintradaymarketbidding”.In:\nMachineLearning110(2021),pp.2335–2387.\n[20] YunyangZou,YanXu,andCuoZhang.“ARisk-AverseAdaptiveStochasticOptimizationMethodforTransactive\nEnergyManagementofaMulti-EnergyMicrogrid”.In:IEEETransactionsonSustainableEnergy14(2023),\npp.1599–1611.\n[21] FrederikRuelensetal.“Sequentialdecision-makingstrategyforademandresponseaggregatorinatwo-settlement\nelectricitymarket”.In:(2016),pp.1229–1235."
  },
  {
    "id": 1177,
    "text": "pp.1599–1611.\n[21] FrederikRuelensetal.“Sequentialdecision-makingstrategyforademandresponseaggregatorinatwo-settlement\nelectricitymarket”.In:(2016),pp.1229–1235.\n[22] TomBrijsetal.“Quantifyingelectricitystoragearbitrageopportunitiesinshort-termelectricitymarketsinthe\nCWEregion”.In:JournalofEnergyStorage25(2019),p.100899.\n[23] MihalyDolanyietal.“CapturingElectricityMarketDynamicsinStrategicMarketParticipationusingNeural\nNetworkConstrainedOptimization”.In:IEEETransactionsonPowerSystems(2023).\n[24] LucianBus¸oniuetal.“Reinforcementlearningforcontrol:Performance,stability,anddeepapproximators”.In:\nAnnualReviewsinControl46(Jan.2018),pp.8–28.\n[25] DaweiQiuetal.“Reinforcementlearningforelectricvehicleapplicationsinpowersystems:Acriticalreview”.\nIn:RenewableandSustainableEnergyReviews173(2023),p.113052.\n[26] NasrinSadeghianpourhamami,JohannesDeleu,andChrisDevelder.“Definitionandevaluationofmodel-free"
  },
  {
    "id": 1178,
    "text": "In:RenewableandSustainableEnergyReviews173(2023),p.113052.\n[26] NasrinSadeghianpourhamami,JohannesDeleu,andChrisDevelder.“Definitionandevaluationofmodel-free\ncoordinationofelectricalvehiclechargingwithreinforcementlearning”.In:IEEETransactionsonSmartGrid\n11(12019),pp.203–214.\n[27] SJSultanuddinetal.“Developmentofimprovedreinforcementlearningsmartchargingstrategyforelectric\nvehiclefleet”.In:JournalofEnergyStorage64(2023),p.106987.\n[28] Zeyang Li et al. “Reinforcement learning-based demand response strategy for thermal energy storage air-\nconditioningsystemconsideringroomtemperatureandhumiditysetpoints”.In:JournalofEnergyStorage72\n(2023),p.108742.\n[29] AhmedHYakoutetal.“Improvedreinforcementlearningstrategyofenergystorageunitsforfrequencycontrol\nofhybridpowersystems”.In:JournalofEnergyStorage72(2023),p.108248.\n[30] DiCaoetal.“Reinforcementlearninganditsapplicationsinmodernpowerandenergysystems:Areview”.In:\nJournalofmodernpowersystemsandcleanenergy8(62020),pp.1029–1042."
  },
  {
    "id": 1179,
    "text": "[30] DiCaoetal.“Reinforcementlearninganditsapplicationsinmodernpowerandenergysystems:Areview”.In:\nJournalofmodernpowersystemsandcleanenergy8(62020),pp.1029–1042.\n[31] VolodymyrMnihetal.“Human-levelcontrolthroughdeepreinforcementlearning”.In:Nature518(75402015),\npp.529–533.\n16"
  },
  {
    "id": 1180,
    "text": "[32] Tuomas Haarnoja et al. “Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a\nstochasticactor”.In:(2018),pp.1861–1870.\n[33] JensBaetensetal.“ImbalancepricingmethodologyinBelgium:Implicationsforindustrialconsumers”.In:\n(2020).\n[34] JesusLagoetal.“Amarketframeworkforgridbalancingsupportthroughimbalancestrading”.In:Renewable\nandSustainableEnergyReviews137(2021),p.110467.\n[35] BehzadVatandoustetal.“IntegratedForecastingandSchedulingofImplicitDemandResponseinBalancing\nMarketsUsingInverseOptimization”.In:(2023),pp.1–6.\n[36] IGMarnerisetal.“OptimalParticipationofRESaggregatorsinenergyandancillaryservicesmarkets”.In:\nIEEETransactionsonIndustryApplications59(12022),pp.232–243.\n[37] Takuji Matsumoto, Derek Bunn, and Yuji Yamada. “Mitigation of the inefficiency in imbalance settlement\ndesignsusingday-aheadprices”.In:IEEETransactionsonPowerSystems37(52021),pp.3333–3345.\n[38] RichardSSuttonandAndrewGBarto.Reinforcementlearning:Anintroduction.MITPress,2018."
  },
  {
    "id": 1181,
    "text": "designsusingday-aheadprices”.In:IEEETransactionsonPowerSystems37(52021),pp.3333–3345.\n[38] RichardSSuttonandAndrewGBarto.Reinforcementlearning:Anintroduction.MITPress,2018.\n[39] JonasEngels,BertClaessens,andGeertDeconinck.“Techno-economicanalysisandoptimalcontrolofbattery\nstorageforfrequencycontrolservices,appliedtotheGermanmarket”.In:AppliedEnergy242(2019),pp.1036–\n1049.\n[40] ChengkeZhouetal.“ModelingofthecostofEVbatterywearduetoV2Gapplicationinpowersystems”.In:\nIEEETransactionsonEnergyConversion26(42011),pp.1041–1050.\n[41] YuHu,MiguelArmada,andMaríaJesúsSánchez.“Potentialutilizationofbatteryenergystoragesystems(BESS)\ninthemajorEuropeanelectricitymarkets”.In:AppliedEnergy322(Sept.2022),p.119512.\n[42] NitishSrivastavaetal.“Dropout:asimplewaytopreventneuralnetworksfromoverfitting”.In:TheJournalof\nMachineLearningResearch15(12014),pp.1929–1958.\n[43] TimothyPLillicrapetal.“Continuouscontrolwithdeepreinforcementlearning”.In:(2015)."
  },
  {
    "id": 1182,
    "text": "MachineLearningResearch15(12014),pp.1929–1958.\n[43] TimothyPLillicrapetal.“Continuouscontrolwithdeepreinforcementlearning”.In:(2015).\n[44] MarcGBellemare,WillDabney,andRémiMunos.“Adistributionalperspectiveonreinforcementlearning”.In:\n(2017),pp.449–458.\n[45] JingliangDuanetal.“Distributionalsoftactor-critic:Off-policyreinforcementlearningforaddressingvalue\nestimationerrors”.In:IEEEtransactionsonneuralnetworksandlearningsystems33(112021),pp.6584–6598.\n[46] ThibautThéateandDamienErnst.“Risk-sensitivepolicywithdistributionalreinforcementlearning”.In:Algo-\nrithms16(72023),p.325.\n[47] XiaotengMaetal.“Dsac:Distributionalsoftactorcriticforrisk-sensitivereinforcementlearning”.In:arXiv\npreprintarXiv:2004.14547(2020).\n[48] EliaDataDownloadPage.Accessed: 2023-12-23. URL: https://www.elia.be/en/grid-data/data-\ndownload-page?csrt=783739960382611489.\n[49] Elia.EndUserDocumentation\"1-minutepublications\"onElia.bewebsite.Accessed:2023-12-23.2019.URL:"
  },
  {
    "id": 1183,
    "text": "download-page?csrt=783739960382611489.\n[49] Elia.EndUserDocumentation\"1-minutepublications\"onElia.bewebsite.Accessed:2023-12-23.2019.URL:\nhttps://www.elia.be/-/media/project/elia/elia-site/grid-data/balancing/20190827%5C_\nend-user-documentation-elia1-minute%20publications.pdf.\n[50] MartinRiedmiller.“NeuralfittedQiteration–firstexperienceswithadataefficientneuralreinforcementlearning\nmethod”.In:(2005),pp.317–328.\n17"
  },
  {
    "id": 1184,
    "text": "QAOA on Hamiltonian Cycle problem\nZhuoyang Ye, UCLA Physics and Astronomy, yezhuoyang98@g.ucla.edu\nJanuary 2, 2024\nAbstract\nI use QAOA to solve the Hamiltonian Circle problem. First, inspired by Lucas [8], I define\nthe QUBO form of Hamiltonian Cycle an transform it to a quantum circuit by embedding the\nproblemofnverticestoanencodingof(n−1)2 qubits. Then,Icalcluatethespectrumofthecost\nhamiltonianforbothtrianglecaseandsquarecaseandjustifymydefinition. Ialsowriteapython\nprogram to generate the cost hamiltonian automatically for finding the hamiltonian cycle in an\narbitrarygraph. Itestthecorrectessofthehamailtonianby analyze their energy spectrums.\nSince the (n−1)2 embedding limit my simulation of graph size to be less than 5, I decide to test\nthecorrectness,onlyforsmallandsimplegraphinthisproject. IimplementtheQAOAalgorithm\nusing qiskit and run the simulation for the triangle case and the square case, which are easy to"
  },
  {
    "id": 1185,
    "text": "thecorrectness,onlyforsmallandsimplegraphinthisproject. IimplementtheQAOAalgorithm\nusing qiskit and run the simulation for the triangle case and the square case, which are easy to\ntest the correctness, both with and without noise. A very interesting result I got is that for the\nsquare case, the QAOA get much better result on a noisy simulator than a noiseless\nsimulator! The explanation for this phenomena require further investigation, perhaps quantum\nnoise can actually be helpful, rather than harmful in the annealing algorithms. I also use two\ndifferent kinds of mixer, R mixer and R circuit to run the simulation. It turns out that R\nx y x\nmixer performs much better than R mixer in this problem.\ny\n1 Introduction\nQAOA, first introduce in 2014 [5], is one of the the most famous and widely studied in NISQ era [9]\n[2] of quantum computing. In 2020, Google AI quantum implement the QAOA on a real Sycamore"
  },
  {
    "id": 1186,
    "text": "QAOA, first introduce in 2014 [5], is one of the the most famous and widely studied in NISQ era [9]\n[2] of quantum computing. In 2020, Google AI quantum implement the QAOA on a real Sycamore\nsuperconducting qubit quantum processor [6], where they, for the first time, got a non-trivial result\nusing QAOA to solve maxcut problem, where the graph has the same topology as the real Hardware\nGrid. Recently, a group from Harvard used Rydberg atom arrays with up to 289 qubits in two spatial\ndimensions,andexperimentallyinvestigatequantumalgorithmsforsolvingthemaximumindependent\nset problem[4].\nDespitealltheeffortandprogress,whetherQAOAhasanadvantageinsolvingclassicalintractable\nproblem , especially NP-complete problem, is still an open question.\n2 Theory of QAOA\nQAOAisatypicalvariationalquantumalgorithmthatcouldsolvecombinatorialoptimizationproblem.\nThe initial idea of QAOA comes from quantum adabatic theorem. Consider a time dependent"
  },
  {
    "id": 1187,
    "text": "2 Theory of QAOA\nQAOAisatypicalvariationalquantumalgorithmthatcouldsolvecombinatorialoptimizationproblem.\nThe initial idea of QAOA comes from quantum adabatic theorem. Consider a time dependent\nhamiltonian that evolve slowly with time. The initial hamiltonian is Hˆ and the final one after\nM\nevolution time T is Hˆ . The adabitic theorem tells us that if we set the initial state as the ground\nC\nstate of Hˆ , the final state will also be the ground state of Hˆ .\nM C\nThe theorem has a potential implementation, when we can easily prepare the ground state of Hˆ ,\nC\nand encode the solution of a hard problem we want to solve to be the ground state of Hˆ . We can\nM\nchoose the Hˆ , to be Pauli X gates on all qubits, the ground state of which, is simply |+⟩⊗n, which\nM\ncan be easily prepared in a quantum computer by a row of Hadmard gate on the initial |0⟩⊗n.\nThe time dependent Hamiltonian can be expressed as\nHˆ(t)=f(t)Hˆ +g(t)Hˆ (1)\nC M"
  },
  {
    "id": 1188,
    "text": "M\ncan be easily prepared in a quantum computer by a row of Hadmard gate on the initial |0⟩⊗n.\nThe time dependent Hamiltonian can be expressed as\nHˆ(t)=f(t)Hˆ +g(t)Hˆ (1)\nC M\nThe two function f(t) and g(t) changes slowly in time. An example for such f(t) g(t) is f(t) = t/T\nand g(t)=1−t/T. The unitary of such slowly varying hamiltonian is\nUˆ(t)=e−i(cid:82)\n0\ntdτHˆ(τ)\n(2)\n1\n3202\nceD\n62\n]TE.sc[\n1v71000.1042:viXra"
  },
  {
    "id": 1189,
    "text": "can be simulated in a quantum computer, by Trotterization:\nr−1 r−1\nUˆ(t)≈ (cid:89) exp[−iHˆ(k∆τ)∆τ]= (cid:89) exp[−if(k∆τ)Hˆ ∆τ]exp[−ig(k∆τ)Hˆ ∆τ] (3)\nC M\nk=0 k=1\nNext, let’s talk about how to find suitable Hˆ with regard to the problem we want to solve and\nC\nhow to embed the solution to its ground state.\nThe most formal way to describe the classical problem that QAOA want to solve is define a set of\nboolean functions, {C ,α=0,1,··· ,m}, each one of these function define a condition to be satisfied\nα\ngiven the specific problem, or mathematically as a mapping: C : {0,1}n → {0,1}. The input of\nα\nthe boolean function represent an “assignment” to the given problem and the output is whether the\nassignment belong to the solution space or not.\nThe optimization version of the problem, when there are ,can be stated in the function below,\nformally as:\nm\n(cid:88)\nC(z)= C (z) (4)\nα\nα=1\nWhenC(z)=m, alloftheconditionsoftheproblemaresatisfied, otherwisesomeofthemarenot."
  },
  {
    "id": 1190,
    "text": "formally as:\nm\n(cid:88)\nC(z)= C (z) (4)\nα\nα=1\nWhenC(z)=m, alloftheconditionsoftheproblemaresatisfied, otherwisesomeofthemarenot.\nOur goal is to maximized the value of C(z).\nThe way to construct the circuit of QAOA, is to make alternating layers of mixer and cost circuit\nthatcouldpossiblysimulatetheadiabaticevolutionofquantumhamiltonianineuqationEquation(3).\nHowever, ifwewantto getaccuratesolution, wehave to chooseinfinitlysmall∆τ inEquation(3),\nwhich is unacceptable for a quantum computer in the real application. Thus, in QAOA algorithm,\nwe only construct a fixed number of layers, and assign each layer with a parameter to be optimized,\nunder the hope that after such optimization, the hamiltonian of the quantum computer approximate\nthe Trotterized adiabitic unitary very well.\nThe optimization algorithm itself, utilize the similar idea from training a neural network. The\nparameters are changed every time we execute the circuit and measured the result. People also"
  },
  {
    "id": 1191,
    "text": "The optimization algorithm itself, utilize the similar idea from training a neural network. The\nparameters are changed every time we execute the circuit and measured the result. People also\ncall this Variational Quantum Eigenvalue (VQE) algorithms [11]. The same kind of methods have\ndemonstrated it’s potential in solving eigenstate and energy for chemical molecule. A very recent\nresultisGooglerunVQEon12qubitsonSycamore[10]andgetthegroundstateenergyforhydrogen\nchains.\n2.1 Structure of Cost circuit\nFirst,wehavetodesignthecosthamiltonianHˆ withregardtothegivenproblem. Allofthepossible\nC\nsolutionsforembeddedas|x⟩shouldbeaneigenstateofHˆ ,whoseeigenvaluecontaintheinformation\nC\nof the cost function:\nHˆ |x⟩=C(x)|x⟩ (5)\nC\nThe unitary for the cost circuit, with parameter γ , is defined as\nU C\n(γ)=e−iγHˆ\nC =\n(cid:89) e−iγwjkZˆ jZˆ\nk (6)\nj<k\nThe circuit, can be implemented by two CNOT gate and a R(Z) gate\n2.2 Structure of Mixer circuit\nThe mixer hamiltonion, is choosen to be\nHˆ = (cid:88) Xˆ (7)"
  },
  {
    "id": 1192,
    "text": "C =\n(cid:89) e−iγwjkZˆ jZˆ\nk (6)\nj<k\nThe circuit, can be implemented by two CNOT gate and a R(Z) gate\n2.2 Structure of Mixer circuit\nThe mixer hamiltonion, is choosen to be\nHˆ = (cid:88) Xˆ (7)\nM j\nj∈V\nThe unitary for the mixer part, with parameter β, is defined as\nU (β)=e−iβB = (cid:89) e−iβXˆ j (8)\nM\nj\nThe circuit, can be implemented by a R gate.\nX\nWe can also choose another mixer hamiltonion, such as Grover Mixer [1],\n2"
  },
  {
    "id": 1193,
    "text": "2.3 Structure of the QAOA circuit\nJustasmostofthecircutistructureofquantumalgorithm,thereshouldbearowofHadamardgatead\nthefrontofthecircuitthatconvert|0⟩⊗n to|+⟩n. Andthen,weaddpalternatinglayersofCostcircuit\nand Mixer circuit. The two set of parameters, are denoted as γ = (γ ,··· ,γ ) and β = (β ,··· ,β ).\n1 p 1 p\nThe\n|γ,β⟩=U (β )U (γ )...U (β )U (γ )|+⟩⊗n (9)\nM p C p M 1 C 1\nAnd the measured cost function of the equation equation (4).\n⟨C⟩=⟨γ,β|Hˆ |γ,β⟩ (10)\nC\n2.4 Optimization algorithm\nWe can simply use a classical optimizer to optimize all the parameters.\n2.5 Steps of the algorithm\n1. Define a cost Hamiltonion Hˆ given the problem. The eigen state with the highest eigen energy\nC\nof Hˆ should be the exact solution to the optimization problem.\nC\n2. Initialize the state in |s⟩.\n|s⟩=|+⟩⊗n = √ 1 (cid:88) |x⟩ (11)\n2n\nx∈{0,1}n\n|s⟩hereisactuallytheeigenstateofthehighesteigenstateofthemixerhamiltonianH defined\nM\nin Equation (7)"
  },
  {
    "id": 1194,
    "text": "C\n2. Initialize the state in |s⟩.\n|s⟩=|+⟩⊗n = √ 1 (cid:88) |x⟩ (11)\n2n\nx∈{0,1}n\n|s⟩hereisactuallytheeigenstateofthehighesteigenstateofthemixerhamiltonianH defined\nM\nin Equation (7)\n3. Choose the number of layer p. Make p alternating pair of mixer and cost circuit.\n4. Initialize 2p parameters⃗γ =(γ ,γ ,··· ,γ ) and β⃗ =(β ,β ,··· ,β ) such that γ ,β ∈{0,2π}\n1 2 p 1 2 p i k\n5. Calculate the cost by measuring repeatedly.\nF (⃗γ,β⃗)=⟨ψ (⃗γ,β⃗)|H |ψ (⃗γ,β⃗)⟩ (12)\np p C p\n6. UseaclassicalalgorithmtooptimizetheparamterbymaximizetheexpectationvalueinEquation\n(12).\n(⃗γ∗,β⃗∗)=argmaxF (⃗γ,β⃗) (13)\np\n⃗γ,β⃗\n2.6 MAX-CUT\nI use MAX cut problem first to test the QAOA circuit. Which is the most commonly used algorithm\nto benchmark the behavior of QAOA.\nThe input to the MAXCUT is a graph G = (V,E). V is the set of vertices, E is the set of edges.\nAssume that there is a weight w assigned to each of the edge (i,j)∈E. We want to find the largest\nij"
  },
  {
    "id": 1195,
    "text": "The input to the MAXCUT is a graph G = (V,E). V is the set of vertices, E is the set of edges.\nAssume that there is a weight w assigned to each of the edge (i,j)∈E. We want to find the largest\nij\ncut in graph G, which is a subset of the vertices whose “cut” with the rest of the vertices are the\nmaximum. The “cut” is defined as the sum of all the weight between a vertex in the subset and a\nvertex that is not in the subset. Any possible assignment can be represented by a set of 0,1. So we\nuse x to represent the assignment for vertex i. x =1 if and only if x is assigned to the subset.\ni i i\n|V|\n(cid:88)\nC(x)= w x (1−x ) (14)\ni,j i j\ni,j=1\nThe correspondence between the x in Equation (11) with the Pauli Z gate used in our Cost\ni\nHamiltonian is:\n1\nx → (1−Z ) (15)\ni 2 i\n3"
  },
  {
    "id": 1196,
    "text": "For example, for the cost function of MAX-CUT defined in Equation (11), the corresponding cost\nhamiltonian is\n|V| |V|\n(cid:88) 1 1 (cid:88)\nH = w (1−Z )Z = w (Z −Z Z ) (16)\nC i,j4 i j 4 i,j j i j\ni,j=1 i,j=1\n3 Classical NP-complete problem\nIn 1971, [3] Cook first proved that the boolean satisfiability problem(3-SAT) is NP-complete, which is\nalsocalledtheCook-Levintheorem. In1972,Karp[7]usedCook’sresultandfirstintroduce21famous\nNP complete problem. In 2014, Lucas [8] first discussed how to map all of the 21 NP complete to\nQuadratic Unconstrained Binary Optimization problems(QUBO) in polynomial time, which suddenly\nraise people’s attention because this open the door for using the quantum computer to solve NP-\ncomplete problem.\nOne of the most notorious NP complete problem is the Hamiltonian Circle problem. I will try to\nsolve the problem using QAOA in this small project.\n3.1 Hamiltonian Circle problem"
  },
  {
    "id": 1197,
    "text": "complete problem.\nOne of the most notorious NP complete problem is the Hamiltonian Circle problem. I will try to\nsolve the problem using QAOA in this small project.\n3.1 Hamiltonian Circle problem\nThe input for the Hamiltonian Circle problem is a graph G =(V,E). Suppose |G|=n. Our goal is to\nfind a cycle to travel thorough all vertices exactly once. I use the QUBO form given by Lucas [8] for\nthe Hamiltonian Circle as follows:\nn n n n n−1\n(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)\nH =A (1− x )2+A (1− x )2+A [ x x +x x ] (17)\nv,j v,j u,j v,j+1 u,n v,1\nv=1 j=1 j=1 v=1 (uv)∈/E j=1\nx is the encoding of whether the vertex v is at the jth position of the Circle. The first part of\nv,j\nH requires that every vertex can be only assigned to one position in the Circle, which we call vertex\nuniqueness term. The second part is the constraint that every position in the Circle of length N\nwe find is only assigned to one vertex, which we call edge uniqueness term. The final term is"
  },
  {
    "id": 1198,
    "text": "uniqueness term. The second part is the constraint that every position in the Circle of length N\nwe find is only assigned to one vertex, which we call edge uniqueness term. The final term is\nthe penalty for the two consecutive vertices in the Circle are actually not connected in the original\ngraph, which we call edge validity term. Thus, it is obvious that the minimal value of H, given any\nassignment x =f(v,j), is 0, when such assignment represent a hamiltonian Circle.\nv,j\nTo construct the circuit, we can also use the following substitution:\n1\nx → (1−Z ) (18)\nv,j 2 i,j\nThe encoding of a given graph requires (n−1)×(n−1) qubits.\nFirst, we calculate a very simple case, a triangle, as illustrate in Figure 1.\nFigure 1: The sketch of a triangle which we want to find a hamiltonian circle.\nIn this simple example, we can write the full embedding in details:\n4"
  },
  {
    "id": 1199,
    "text": "x ≡1 We always fix the position of the first vertex to be 1.\n1,1\nx ≡x ≡x ≡x ≡0 The impossible assignment when the first vertex is fixed.\n1,2 1,3 2,1 3,1\nx =1 Vertex 2 is at the second position. (19)\n2,2\nx =1 Vertex 2 is at the third position.\n2,3\nx =1 Vertex 2 is at the third position.\n3,3\nBy the above definition, we can see that 2×2 qubits are enough for defining the hamiltonian for\nhamiltonian circle.\nNow we can derive the concrete form of the hamiltonian defined in Equation 20, where we set the\nconstant A=1.\n3 3 3 3 2\n(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)\nH = (1− x )2+ (1− x )2+[ x x + x + x ] (20)\nv,j v,j u,j v,j+1 u,3 u,2\nv=2 j=2 j=2 v=2 (uv)∈/Ej=2 (u1)∈/E (1u)∈/E\nWe expand the three terms seperately\n1.The vertex uniqueness term:\n3 3\n(cid:88) (cid:88)\nH = (1− x )2\n1 v,j\nv=2 j=2\n=(1−x −x )2+(1−x −x )2\n2,2 2,3 3,2 3,3\n=2+x2 +x2 +x2 +x2 −2x −2x +2x x −2x −2x +2x x\n2,2 2,3 3,2 3,3 2,2 2,3 2,2 2,3 3,2 3,3 3,2 3,3"
  },
  {
    "id": 1200,
    "text": "3 3\n(cid:88) (cid:88)\nH = (1− x )2\n1 v,j\nv=2 j=2\n=(1−x −x )2+(1−x −x )2\n2,2 2,3 3,2 3,3\n=2+x2 +x2 +x2 +x2 −2x −2x +2x x −2x −2x +2x x\n2,2 2,3 3,2 3,3 2,2 2,3 2,2 2,3 3,2 3,3 3,2 3,3\nWe can simply use the substitution rule x → 1(1−Z )\ni,j 2 i,j\n3 3\n(cid:88) (cid:88)\nH = (1− x )2\n1 v,j\nv=2 j=2\n1 1 1 1\n⇒( Z + Z )2+( Z + Z )2\n2 2,2 2 2,3 2 3,2 2 3,3\n1\n= [I+I+2Z Z +I+I+2Z Z ]\n4 2,2 2,3 3,2 3,3\n1 1\n=I+ Z Z + Z Z\n2 2,2 2,3 2 3,2 3,3\n2.The edge uniqueness term:\n3 3\n(cid:88) (cid:88)\nH = (1− x )2\n2 v,j\nj=2 v=2\n=(1−x −x )2+(1−x −x )2\n2,2 3,2 2,3 3,3\nWe also use the substitution rule x → 1(1−Z )\ni,j 2 i,j\nH =(1−x −x )2+(1−x −x )2\n2 2,2 3,2 2,3 3,3\n1 1 1 1\n⇒( Z + Z )2+( Z + Z )2\n2 2,2 2 3,2 2 2,3 2 3,3\n1\n= (I+I+2Z Z +I+I+2Z Z )\n4 2,2 3,2 2,3 3,3\n1 1\n=I+ Z Z + Z Z\n2 2,2 3,2 2 2,3 3,3\n5"
  },
  {
    "id": 1201,
    "text": "3.The edge validity term:\n2\n(cid:88) (cid:88) (cid:88) (cid:88)\nH = x x + x + x\n3 u,j v,j+1 u,3 u,2\n(uv)∈/Ej=2 (u1)∈/E (1u)∈/E\nNotice the the triangle is actually a completely connected graph, so H =0. Finally, we have\n3\nH =H +H +H\nC 1 2 3\n1 1 1 1\n=2I+ Z Z + Z Z + Z Z + Z Z\n2 2,2 2,3 2 3,2 3,3 2 2,2 3,2 2 2,3 3,3\nSince I and constant factor don’t affect the eigen energy and eigen state, we can rewrite H as\nC\nH =Z Z +Z Z +Z Z +Z Z\nC 2,2 2,3 3,2 3,3 2,2 3,2 2,3 3,3\nFinally, we can construct the circuit for the cost hamiltonian. The circuit has four qubits, each\nrepresent:\n1. Q represent x .\n1 2,2\n2. Q represent x .\n2 2,3\n3. Q represent x .\n3 3,2\n4. Q represent x .\n4 3,3\nSince the correct result must be either (x = 1,x = 0,x = 0,x = 1), which represent the\n2,2 2,3 3,2 3,3\nhamiltonian cycle (1 → 2 → 3 → 1) or (x = 0,x = 1,x = 1,x = 0), which represent the\n2,2 2,3 3,2 3,3\nhamiltonian cycle (1 → 3 → 2 → 1). The ground state for this solution must in dirac notation be"
  },
  {
    "id": 1202,
    "text": "hamiltonian cycle (1 → 2 → 3 → 1) or (x = 0,x = 1,x = 1,x = 0), which represent the\n2,2 2,3 3,2 3,3\nhamiltonian cycle (1 → 3 → 2 → 1). The ground state for this solution must in dirac notation be\neither |1001⟩ or |0110⟩. We can check the correctness. We can write our H as:\nC\nH =Z Z +Z Z +Z Z +Z Z (21)\nC 1 2 3 4 1 3 2 4\nLet’s check the correctness of the above statement by diagonalization:\nimport numpy as np\nfrom scipy.linalg import eigh\nfrom functools import reduce\nimport matplotlib.pyplot as plt\n# Pauli Z matrix\npauli_z = np.array([[1, 0], [0, -1]])\n# Function to create a matrix representation of Z_k gate on k-th qubit\ndef z_k_matrix(k, total_qubits):\nI = np.eye(2) # Identity matrix\nmatrices = [pauli_z if i == k else I for i in range(total_qubits)]\nreturn reduce(np.kron, matrices)\n# Terms for the Hamiltonian H_C\nterms_direct = [\n(1, [0, 1]), # Z1Z2\n(1, [2, 3]), # Z3Z4\n(1, [0, 2]), # Z1Z3\n(1, [1, 3]) # Z2Z4\n]\n# Total number of qubits for the Hamiltonian\n6"
  },
  {
    "id": 1203,
    "text": "new_total_qubits = 4\n# Construct the Hamiltonian matrix directly\nH_direct = np.zeros((2**new_total_qubits, 2**new_total_qubits))\n# Add each term directly to the Hamiltonian\nfor coeff, qubits in terms_direct:\nterm = np.eye(2**new_total_qubits)\nfor qubit in qubits:\nterm = np.dot(term, z_k_matrix(qubit, new_total_qubits))\nH_direct += coeff * term\n# Calculate eigenvalues and eigenvectors of the Hamiltonian directly\nnew_eigenvalues_direct, new_eigenvectors_direct = eigh(H_direct)\n# Plot the energy spectrum with annotations\nplt.figure(figsize=(12, 8))\nprevious_eigenvalue = None\noffset_multiplier = 0\nfor i in range(2**new_total_qubits):\neigenvalue = new_eigenvalues_direct[i]\nmax_amplitude_index = np.argmax(np.abs(new_eigenvectors_direct[:, i]))\ndirac_state = \"|{0:04b}>\".format(max_amplitude_index)\nif previous_eigenvalue == eigenvalue:\noffset_multiplier += 1\nelse:\noffset_multiplier = 0\nhorizontal_position = + offset_multiplier * 0.088"
  },
  {
    "id": 1204,
    "text": "dirac_state = \"|{0:04b}>\".format(max_amplitude_index)\nif previous_eigenvalue == eigenvalue:\noffset_multiplier += 1\nelse:\noffset_multiplier = 0\nhorizontal_position = + offset_multiplier * 0.088\nplt.hlines(eigenvalue, 0, 1, colors=’b’, linestyles=’solid’)\nplt.text(horizontal_position, eigenvalue, dirac_state, fontsize=12,\nverticalalignment=’center’)\nprevious_eigenvalue = eigenvalue\nplt.xlabel(’State Index’,fontsize=20)\nplt.ylabel(’Energy’,fontsize=20)\nplt.title(’Energy Spectrum and Corresponding Eigenstates\n(Dirac Notation)’,fontsize=20)\nplt.xticks([])\nplt.grid(True)\nplt.savefig(\"SpectrumTriangle.png\")\nplt.show()\nThe spectrum of the hamiltonian calculated above is computed and shown in Figure 2. There is a\nlarge energy gap, between the solution state and the non-solution state.\n7"
  },
  {
    "id": 1205,
    "text": "Figure 2: The sketch of the cost function for the triangle. The lowest energy is −4, with eigenstates\n|0110⟩ and |1001⟩, which are the exact solution for hamiltonian cycle for a triangle. The spectrum\njustify the correctness of our definition.\nAnother example we is a square , as illustrate in Figure 3:\nFigure 3: The sketch of a rectangle from where we want to find a hamiltonian circle.\nIn this simple example, we can also write the full embedding in details:\n8"
  },
  {
    "id": 1206,
    "text": "x ≡1 We always fix the position of the first vertex to be 1.\n1,1\nx ≡x ≡x ≡x ≡x ≡x ≡0 The impossible assignment when the first vertex is fixed.\n1,2 1,3 1,4 2,1 3,1 4,1\nx =1 Vertex 2 is at the second position.\n2,2\nx =1 Vertex 2 is at the third position.\n2,3\nx =1 Vertex 2 is at the fourth position.\n2,4\nx =1 Vertex 3 is at the second position.\n3,2\nx =1 Vertex 2 is at the third position.\n3,3\nx =1 Vertex 2 is at the fourth position.\n3,4\nx =1 Vertex 2 is at the third position.\n4,2\nx =1 Vertex 2 is at the third position.\n4,3\nx =1 Vertex 2 is at the third position.\n4,4\n(22)\nWe can see that 3⊗3=9 for defining the hamiltonian for hamiltonian circle.\nNow we can derive the concrete form of the hamiltonian defined in Equation 20, where we set the\nconstant A=1.\n4 4 4 4 4\n(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)\nH = (1− x )2+ (1− x )2+ x x (23)\nv,j v,j u,j v,j+1\nv=2 j=2 j=2 v=2 (uv)∈/Ej=2\nWe expand the four terms seperately\n1.The vertex uniqueness term:\n4 4\n(cid:88) (cid:88)"
  },
  {
    "id": 1207,
    "text": "H = (1− x )2+ (1− x )2+ x x (23)\nv,j v,j u,j v,j+1\nv=2 j=2 j=2 v=2 (uv)∈/Ej=2\nWe expand the four terms seperately\n1.The vertex uniqueness term:\n4 4\n(cid:88) (cid:88)\nH = (1− x )2\n1 v,j\nv=2 j=2\n=(1−x −x −x )2+(1−x −x −x )2+(1−x −x −x )2\n2,2 2,3 2,4 3,2 3,3 3,4 4,2 4,3 4,4\nWe can simply use the substitution rule x → 1(1−Z )\ni,j 2 i,j\n4 4\n(cid:88) (cid:88)\nH = (1− x )2\n1 v,j\nv=2 j=2\nI 1 1 1 I 1 1 1 I 1 1 1\n=(− + Z + Z + Z )2+(− + Z + Z + Z )2+(− + Z + Z + Z )2\n2 2 2,2 2 2,3 2 2,4 2 2 3,2 2 3,3 2 3,4 2 2 4,2 2 4,3 2 4,4\n1\n= [I+Z2 +Z2 +Z2 −2Z −2Z −2Z +2Z Z +2Z Z +2Z Z +\n4 2,2 2,3 2,4 2,2 2,3 2,4 2,2 2,3 2,2 2,4 2,3 2,4\nI+Z2 +Z2 +Z2 −2Z −2Z −2Z +2Z Z +2Z Z +2Z Z +\n3,2 3,3 3,4 3,2 3,3 3,4 3,2 3,3 3,2 3,4 3,3 3,4\nI+Z2 +Z2 +Z2 −2Z −2Z −2Z +2Z Z +2Z Z +2Z Z ]\n4,2 4,3 4,4 4,2 4,3 4,4 4,2 4,3 4,2 4,4 4,3 4,4\n1\n= [12I−2Z −2Z −2Z +2Z Z +2Z Z +2Z Z\n4 2,2 2,3 2,4 2,2 2,3 2,2 2,4 2,3 2,4\n−2Z −2Z −2Z +2Z Z +2Z Z +2Z Z +\n3,2 3,3 3,4 3,2 3,3 3,2 3,4 3,3 3,4\n−2Z −2Z −2Z +2Z Z +2Z Z +2Z Z ]"
  },
  {
    "id": 1208,
    "text": "1\n= [12I−2Z −2Z −2Z +2Z Z +2Z Z +2Z Z\n4 2,2 2,3 2,4 2,2 2,3 2,2 2,4 2,3 2,4\n−2Z −2Z −2Z +2Z Z +2Z Z +2Z Z +\n3,2 3,3 3,4 3,2 3,3 3,2 3,4 3,3 3,4\n−2Z −2Z −2Z +2Z Z +2Z Z +2Z Z ]\n4,2 4,3 4,4 4,2 4,3 4,2 4,4 4,3 4,4\nFinally, after removing I and the constant factor:\nH =−Z −Z −Z +Z Z +Z Z +Z Z\n1 2,2 2,3 2,4 2,2 2,3 2,2 2,4 2,3 2,4\n−Z −Z −Z +Z Z +Z Z +Z Z + (24)\n3,2 3,3 3,4 3,2 3,3 3,2 3,4 3,3 3,4\n−Z −Z −Z +Z Z +Z Z +Z Z\n4,2 4,3 4,4 4,2 4,3 4,2 4,4 4,3 4,4\n9"
  },
  {
    "id": 1209,
    "text": "2.The edge uniqueness term:\n4 4\n(cid:88) (cid:88)\nH = (1− x )2\n2 v,j\nj=2 v=2\n=(1−x −x −x )2+(1−x −x )2\n3,2 3,2 4,2 2,2 2,3\nWe also use the substitution rule x → 1(1−Z )\ni,j 2 i,j\nH =(1−x −x −x )2+(1−x −x −x )2+(1−x −x −x )2\n2 2,2 3,2 4,2 2,3 3,3 4,3 2,4 3,4 4,4\nLikewise, we can use the substitution rules:\nWe also also use the substitution rule x → 1(1−Z ), and finally get the same kind of format\ni,j 2 i,j\nas H :\n1\nH =−Z −Z −Z +Z Z +Z Z +Z Z\n2 2,2 3,2 4,2 2,2 3,2 2,2 4,2 3,2 4,2\n−Z −Z −Z +Z Z +Z Z +Z Z + (25)\n2,3 3,3 4,3 2,3 3,3 2,3 4,3 3,3 4,3\n−Z −Z −Z +Z Z +Z Z +Z Z\n2,4 3,4 4,4 2,4 3,4 2,4 4,4 3,4 4,4\n3.The edge validity term:\n3\n(cid:88) (cid:88) (cid:88) (cid:88)\nH = x x + x + x\n3 u,j v,j+1 u,4 u,2\n(uv)∈/Ej=2 (u1)∈/E (1u)∈/E\nDifferent from the triangle case, there are two pairs of vertices not connected with each other:\n(X ,X ) and (X ,X ). Which means that our cost function will punish the assignment which try to\n1 3 2 4\nfind a path with X ,X or X ,X adjacent with each other."
  },
  {
    "id": 1210,
    "text": "(X ,X ) and (X ,X ). Which means that our cost function will punish the assignment which try to\n1 3 2 4\nfind a path with X ,X or X ,X adjacent with each other.\n1 3 2 4\nH =x x +x x +x x +x x +x +x\n3 2,2 4,3 2,3 4,4 4,2 2,3 4,3 2,4 3,4 3,2\nAfter substitution, the edge validity term becomes:\nH =(I−Z )(I−Z )+(I−Z )(I−Z )+(I−Z )(I−Z )\n3 2,2 4,3 2,3 4,4 4,2 2,3\n+(I−Z )(I−Z )+2(I−Z )+2(I−Z )\n4,3 2,4 3,4 3,2\n=−Z −Z +Z Z −Z −Z +Z Z −Z −\n2,2 4,3 2,2 4,3 2,3 4,4 2,3 4,4 4,2\nZ +Z Z −Z −Z +Z Z −2Z −2Z\n2,3 4,2 2,3 4,3 2,4 4,3 2,4 3,4 3,2\n=−Z −2Z −2Z −Z −Z −Z −2Z −2Z\n2,2 4,3 2,3 4,4 4,2 2,4 3,4 3,2\n+Z Z +Z Z +Z Z +Z Z\n2,2 4,3 2,3 4,4 4,2 2,3 4,3 2,4\nH =H +H +H\nC 1 2 3\n=−Z −Z −Z +Z Z +Z Z +Z Z\n2,2 2,3 2,4 2,2 2,3 2,2 2,4 2,3 2,4\n−Z −Z −Z +Z Z +Z Z +Z Z +\n3,2 3,3 3,4 3,2 3,3 3,2 3,4 3,3 3,4\n−Z −Z −Z +Z Z +Z Z +Z Z\n4,2 4,3 4,4 4,2 4,3 4,2 4,4 4,3 4,4\n10"
  },
  {
    "id": 1211,
    "text": "H =−Z −Z −Z +Z Z +Z Z +Z Z\nC 2,2 2,3 2,4 2,2 2,3 2,2 2,4 2,3 2,4\n−Z −Z −Z +Z Z +Z Z +Z Z +\n3,2 3,3 3,4 3,2 3,3 3,2 3,4 3,3 3,4\n−Z −Z −Z +Z Z +Z Z +Z Z\n4,2 4,3 4,4 4,2 4,3 4,2 4,4 4,3 4,4\n−Z −Z −Z +Z Z +Z Z +Z Z\n2,2 3,2 4,2 2,2 3,2 2,2 4,2 3,2 4,2\n−Z −Z −Z +Z Z +Z Z +Z Z +\n2,3 3,3 4,3 2,3 3,3 2,3 4,3 3,3 4,3\n−Z −Z −Z +Z Z +Z Z +Z Z\n2,4 3,4 4,4 2,4 3,4 2,4 4,4 3,4 4,4\n−Z −2Z −2Z −Z −Z −Z −2Z −2Z (26)\n2,2 4,3 2,3 4,4 4,2 2,4 3,4 3,2\n+Z Z +Z Z +Z Z +Z Z\n2,2 4,3 2,3 4,4 4,2 2,3 4,3 2,4\n=Z Z +Z Z +Z Z +Z Z +Z Z −3Z\n2,2 2,3 2,2 2,4 2,2 3,2 2,2 4,2 2,2 4,3 2,2\n+Z Z +Z Z +Z Z +Z Z +Z Z −4Z\n2,3 2,4 2,3 3,3 2,3 4,2 2,3 4,3 2,3 4,4 2,3\n+Z Z +Z Z +Z Z −3Z +Z Z +Z Z +Z Z −4Z\n2,4 3,4 2,4 4,3 2,4 4,4 2,4 3,2 3,3 3,2 3,4 3,2 4,2 3,2\n+Z Z +Z Z −2Z +Z Z −4Z\n3,3 3,4 3,3 4,3 3,3 3,4 4,4 3,4\n+Z Z +Z Z −3Z +Z Z −4Z −3Z\n4,2 4,3 4,2 4,4 4,2 4,3 4,4 4,3 4,4\nFinally, we can construct the circuit for the cost hamiltonian. The circuit has four qubits, each\nrepresent:\n1. Q represent x\n1 2,2\n2. Q represent x =1"
  },
  {
    "id": 1212,
    "text": "4,2 4,3 4,2 4,4 4,2 4,3 4,4 4,3 4,4\nFinally, we can construct the circuit for the cost hamiltonian. The circuit has four qubits, each\nrepresent:\n1. Q represent x\n1 2,2\n2. Q represent x =1\n2 2,3\n3. Q represent x =1\n3 2,4\n4. Q represent x =1\n4 3,2\n5. Q represent x =1\n5 3,3\n6. Q represent x =1\n6 3,4\n7. Q represent x =1\n7 4,2\n8. Q represent x =1\n8 4,3\n9. Q represent x =1\n9 4,4\nThis format should be more compact while still clearly representing the polynomial.\nThe final hamiltonian is:\nH =Z Z +Z Z +Z Z +Z Z +Z Z −3Z +Z Z +Z Z +Z Z\nC 1 2 1 3 1 4 1 7 1 8 1 2 3 2 5 2 7\n+Z Z +Z Z −4Z +Z Z +Z Z +Z Z −3Z +Z Z +Z Z +Z Z\n2 8 2 9 2 3 6 3 8 3 9 3 4 5 4 6 4 7\n−4Z +Z Z +Z Z −2Z +Z Z −4Z +Z Z +Z Z −3Z +Z Z −4Z −3Z\n4 5 6 5 8 5 6 9 6 7 8 7 9 7 8 9 8 9\nSince the correct result 1 must be either (1,0,0,0,1,0,0,0,1), which represent the hamiltonian\ncycle (1 → 2 → 3 →→ 4 → 1) or (0,0,1,0,1,0,1,0,0), which represent the hamiltonian cycle (1 →"
  },
  {
    "id": 1213,
    "text": "Since the correct result 1 must be either (1,0,0,0,1,0,0,0,1), which represent the hamiltonian\ncycle (1 → 2 → 3 →→ 4 → 1) or (0,0,1,0,1,0,1,0,0), which represent the hamiltonian cycle (1 →\n4 → 3 → 2 → 1). The ground state for this solution must in dirac notation be either |100010001⟩ or\n|001010100⟩. We can check the correctness by calculating the eigen value and eigen energies of the\nabove equation:\nimport numpy as np\nfrom scipy.linalg import eigh\nfrom functools import reduce\n# Define the Pauli Z matrix\npauli_z = np.array([[1, 0], [0, -1]])\n1The benefit to use a cycle to be the example is that there are only two solution, clockwise and counterclockwise,\nandthuswecaneasilycheckthecorrectnessofourhamiltonian\n11"
  },
  {
    "id": 1214,
    "text": "# Function to create a matrix representation of Z_k gate on k-th qubit\ndef z_k_matrix(k, total_qubits):\nI = np.eye(2) # Identity matrix\nmatrices = [pauli_z if i == k else I for i in range(total_qubits)]\nreturn reduce(np.kron, matrices)\n# Total number of qubits\ntotal_qubits = 9\n# Construct the Hamiltonian matrix\nH = np.zeros((2**total_qubits, 2**total_qubits))\n# Define the terms of the Hamiltonian (coefficients and qubit indices)\nterms = [\n(1, [0, 1]), (1, [0, 2]), (1, [0, 3]), (1, [0, 6]), (1, [0, 7]),\n(-3, [0]), (1, [1, 2]), (1, [1, 4]), (1, [1, 6]), (1, [1, 7]),\n(1, [1, 8]), (-4, [1]), (1, [2, 5]), (1, [2, 7]), (1, [2, 8]),\n(-3, [2]), (1, [3, 4]), (1, [3, 5]), (1, [3, 6]), (-4, [3]),\n(1, [4, 5]), (1, [4, 7]), (-2, [4]), (1, [5, 8]), (-4, [5]),\n(1, [6, 7]), (1, [6, 8]), (-3, [6]), (1, [7, 8]), (-4, [7]),\n(-3, [8])\n]\n# Add each term to the Hamiltonian\nfor coeff, qubits in terms:\nif len(qubits) == 1:\nH += coeff * z_k_matrix(qubits[0], total_qubits)\nelse:"
  },
  {
    "id": 1215,
    "text": "(-3, [8])\n]\n# Add each term to the Hamiltonian\nfor coeff, qubits in terms:\nif len(qubits) == 1:\nH += coeff * z_k_matrix(qubits[0], total_qubits)\nelse:\nterm = z_k_matrix(qubits[0], total_qubits)\nfor qubit in qubits[1:]:\nterm = np.dot(term, z_k_matrix(qubit, total_qubits))\nH += coeff * term\n# Calculate eigenvalues and eigenvectors\neigenvalues, eigenvectors = eigh(H)\n# Extract the five lowest eigenvalues and their corresponding eigenstates\nlowest_five_eigenvalues = eigenvalues[:5]\nlowest_five_eigenstates = eigenvectors[:, :5]\n# Convert the eigenstates to Dirac notation\nlowest_five_eigenstates_dirac = []\nfor i in range(5):\nmax_amplitude_index = np.argmax(np.abs(lowest_five_eigenstates[:, i]))\ndirac_state = \"|{0:09b}>\".format(max_amplitude_index)\nlowest_five_eigenstates_dirac.append(dirac_state)\n# Display the results\nprint(\"Lowest Five Eigenvalues:\", lowest_five_eigenvalues)\nprint(\"Corresponding Eigenstates in Dirac Notation:\",\nlowest_five_eigenstates_dirac)\n12"
  },
  {
    "id": 1216,
    "text": "Figure 4: The output has proved the correctness of our hamiltonian, because the eigen states with\nthe lowest energies are |001010100⟩ and |100010001⟩, which are the exact solution for the hamiltonian\npath of a square.\nFigure5: Plottheenergyspectrumofthecosthamiltonian. Theminimumenergyis-20,andtheeigen\nstates are |001010100⟩ and |100010001⟩. The energy spectrum justify the design of our hamiltonian\nfor hamiltonian cycle because there is a huge energy gap between the solution we want and the other\nsolutions.\n4 Simulation using Qiskit\n4.1 How do I scale the problem up\nThe most difficult part in implementation is how to embed an arbitrary graph automatically\nto cost hamiltonian in QAOA?. I write a function, the input is the graph, the output is the the\nparameter for the hamiltonian, in a python dictionary.\nfrom sympy import symbols, Sum, IndexedBase, simplify\nfrom sympy.abc import n, v, j, u\n# Define symbolic variables\nx = IndexedBase(’x’)"
  },
  {
    "id": 1217,
    "text": "parameter for the hamiltonian, in a python dictionary.\nfrom sympy import symbols, Sum, IndexedBase, simplify\nfrom sympy.abc import n, v, j, u\n# Define symbolic variables\nx = IndexedBase(’x’)\n# Function to represent the vertex uniqueness term of the Hamiltonian\ndef vertex_uniqueness_term(n):\n13"
  },
  {
    "id": 1218,
    "text": "return Sum((1 - Sum(x[v, j], (j, 2, n)))**2, (v, 2, n))\n# Function to represent the edge uniqueness term of the Hamiltonian\ndef edge_uniqueness_term(n):\nreturn Sum((1 - Sum(x[v, j], (v, 2, n)))**2, (j, 2, n))\n# Function to represent the edge validity term\n# of the Hamiltonian for a given graph\ndef edge_validity_term(graph, n):\nvalidity_term = 0\nfor u in range(2,n+1):\nedge=(u,1)\nif not edge in graph:\nvalidity_term +=x[u,n]\nfor v in range(2,n+1):\nu, v = edge\nif not edge in graph:\nvalidity_term += Sum(x[u, j] * x[v, j+1], (j, 1, n-1))\nreturn validity_term\n# Combine the terms to form the complete Hamiltonian\ndef hamiltonian(graph, n):\nH = vertex_uniqueness_term(n) +\nedge_uniqueness_term(n) + edge_validity_term(graph, n)\nreturn simplify(H)\ndef apply_substitution_to_hamiltonian(H, n):\nZ = IndexedBase(’Z’)\nH_substituted = H\nfor v in range(2, n+1):\nfor j in range(2, n+1):\nz_index = (v-2)*(n-1) + j-1 # Corrected index calculation\nif z_index > 0:\nH_substituted ="
  },
  {
    "id": 1219,
    "text": "Z = IndexedBase(’Z’)\nH_substituted = H\nfor v in range(2, n+1):\nfor j in range(2, n+1):\nz_index = (v-2)*(n-1) + j-1 # Corrected index calculation\nif z_index > 0:\nH_substituted =\nH_substituted.subs(x[v, j], 1/2 * (1 - Z[z_index]))\nelse:\nH_substituted = H_substituted.subs(x[v, j], 0)\nreturn simplify(H_substituted)\ndef expand_and_simplify_hamiltonian(H,n):\nZ = IndexedBase(’Z’)\nH_expanded = H.expand()\n# Apply the simplification rule Z_k^2 = I\nfor k in range(1, (n-1)**2+1): # Assuming up to 8 qubits for this example\nH_expanded = H_expanded.subs(Z[k]**2, 0)\nreturn simplify(H_expanded)\ndef hamiltonian_to_string_list(H, n):\n\"\"\"\nConvert the expanded Hamiltonian to a\nlist of strings with corresponding coefficients.\nEach string represents a term in the Hamiltonian,\nwith ’Z’ at positions corresponding to qubits involved in the term.\n14"
  },
  {
    "id": 1220,
    "text": "For example, ’ZZI’ represents Z_1 Z_2.\n:param H: The expanded Hamiltonian expression\n:param n: Number of qubits\n:return: List of tuples (string, coefficient)\n\"\"\"\nZ = IndexedBase(’Z’)\nterms = []\n# Iterate over each term in the Hamiltonian expression\nfor term in H.as_ordered_terms():\n# Initialize a string with ’I’s for each qubit\nterm_string = [’I’] * n\ncoeff = H.coeff(term) # Extract the coefficient of the term\n# Check for the presence of Z operators in the term\nfor k in range(1, n+1):\nif term.has(Z[k]):\nterm_string[k-1] = ’Z’\n# Join the term string and append it with its coefficient to the list\nterms.append((’’.join(term_string), coeff))\nreturn terms\nn=3\n# Example: Hamiltonian for a triangle graph\ntriangle_graph = [(1, 2), (2, 3), (3, 1), (2, 1), (3, 2), (3, 1)]\nH_triangle = hamiltonian(triangle_graph, n)\nprint(f\"Polynomial H is {H_triangle}\")\n# Apply the substitution rule to the\n# Hamiltonian for a triangle graph (n = 3)"
  },
  {
    "id": 1221,
    "text": "H_triangle = hamiltonian(triangle_graph, n)\nprint(f\"Polynomial H is {H_triangle}\")\n# Apply the substitution rule to the\n# Hamiltonian for a triangle graph (n = 3)\nH_triangle_substituted = apply_substitution_to_hamiltonian(H_triangle, n)\nprint(f\"After Substitute wo Z is {H_triangle_substituted}\")\nH_final=expand_and_simplify_hamiltonian(H_triangle_substituted,n)\nprint(f\"After simplification {H_final}\")\n# Convert the final Hamiltonian for the\n# triangle graph to string list representation\nhamiltonian_string_list = hamiltonian_to_string_list(H_final, (n-1)**2)\nprint(f\"Final result {hamiltonian_string_list}\")\nFigure 6: The output after I execute the above code, which calculate the final cost hamiltonian of a\ntriangle according to Equation 20. The output is just the same as what I’ve calculated by hand.\nThe output in Figure 6 is exactly the form of hamiltonian that I calculated by hand based on"
  },
  {
    "id": 1222,
    "text": "triangle according to Equation 20. The output is just the same as what I’ve calculated by hand.\nThe output in Figure 6 is exactly the form of hamiltonian that I calculated by hand based on\nEquation 20. With the code above, I can generate the cost hamiltonian for hamiltonian cycle problem\nfor an arbitrary graph. Qiskit has already implement the next step to compile the dictionary to the\ncircuit.\n15"
  },
  {
    "id": 1223,
    "text": "4.2 Simulation of QAOA for Hamiltonian cycle of a triangle.\nFirst,IrunthesimulationofQAOAalgorithmonthesimplestcase: Whenthegraphisatriangle! The\nbenefitofdoingthisisthatthecircuitisreallysimpleanditiseasyforustotestthecorrectness. The\ncompiled circuit, when the repetition number is two, is shown in Figure 7. After a row of hadamard\ngate,therearefourpairsofZZ gate,asdefinedandcalculatedinEquation21,whichisthecostcircuit.\nThe mixer part, is chosen as a row of R gate, with the same rotation angle. Finally, we measure\nx\nthe result and get the output. Since our goal is to minimize the energy of the cost hamiltonian with\nrespect to the output state, we have to to calculate the cost value2 and use a classical optimizer to\nminimize the energy. We use the ”COBYLA” method of scipy.optimize to to the optimization.\nFigure 7: The QAOA circuit for hamiltonian cycle on a triangle that I used in the experiment. There\nare four qubits and 2 total repetitions of mixer and cost circuit."
  },
  {
    "id": 1224,
    "text": "Figure 7: The QAOA circuit for hamiltonian cycle on a triangle that I used in the experiment. There\nare four qubits and 2 total repetitions of mixer and cost circuit.\nFigure 8: The QAOA circuit block demonstration. There are four block of hamiltonian, each with its\nown parameter.\n2In classcal simulation, the cost is easy to calculate, because we only need to the the matrix vector\ncalculation:⟨ψ|H|ψ⟩. However, in a real quantum computer, to get ⟨ψ|H|ψ⟩ is much harder. Generally speaking,\nwehavetodivideH intodifferentPauliGate,andmeasuretheexpectationofeachpauligatebysampling. Finally,add\ntheenergyofeachpauligate.\n16"
  },
  {
    "id": 1225,
    "text": "Figure 9: The qubit mapping to the FakeManilaV2 after transpilation.\nFigure10: TheresultofQAOAsolvinghamiltoniancycle. Thefinalprobabilityofmeasurement,after\noptimizing the parameters. The result match with our previous expectation.\nFigure 11: The result of running QAOA for hamiltonian cycle on a triangle on a fake noisy quantum\nchip. We use FakeManilaV2 to the simulation. Although the success probability drp a little bit, we\nwill still get correct result, with hight probability.\n17"
  },
  {
    "id": 1226,
    "text": "Figure12: ComparethenoiselessresultinFigure10resultandthenoisyresultFigure11ofatriangle\ncase.\n4.3 Simulation of QAOA for Hamiltonian cycle of a Square\nFigure 13: The experimental result of QAOA for hamiltonian cycle without noise when the graph is a\nsquare. I use red color to highlight the two exact solution |001010100⟩ and |100010001⟩.\n4.4 Result of different Mixers\nThechoiceofmixercanbeessentialinthetrainingandoptimizationprocessofQAOA.Intheprevious,\nI used the default R mixer. In this section, I also use R mixer to run the QAOA and compare the\nx y\nnew result with the previous one.\nInFigure17,werunthesamenoiselesssimulationforhamiltoniancycleonatriangle,andcompare\nthe result of R mixer and R mixer. I also simulate for the same comparison when the graph is a\nx y\nsquare, which is plotted in Figure 18. From the result in Figure 17 and Figure 18, it’s clear that R\nx\nmixer is much better than R mixer.\ny\n18"
  },
  {
    "id": 1227,
    "text": "Figure 14: The qubit mapping to the FakeAlmadenV2 that we use for solving the hamiltonian path\nproblem on a square.\nFigure 15: The experimental result of QAOA for hamiltonian cycle with noise. Amazingly, the result\nactually is much better than without noise!\n19\n\nFigure 16: Compare the result of noiseless simulation in Figure 13 and noisy simulation Figure 15\ntogether. The blue color denotes the result of simulation on a noiseless simulator and the green color\ndenotes the result of a noisy simulator. I highlight the correct result,|001010100⟩ and |100010001⟩ in\nredforthenoiselessresultandinpinkforthenoisyresult. Thefinalprobabilityofgettingtheaccurate\nresult is much higher in the FakeAlmadenV2 simulator!\nFigure17: ComparetheQAOAforhamiltoniancycleonatriangleresultbetweentwodifferentmixers.\nThe two outstanding red bars represent the measurement of the correct solution by circuit with R\nx\nmixer, while two tiny pinks bars are the result of simulation by circuit with R mixer.\ny\n20"
  },
  {
    "id": 1228,
    "text": "Figure 18: Compare the QAOA for hamiltonian cycle on a square result between two different mixers.\nThe two outstanding red bars represent the measurement of the correct solution by circuit with R\nx\nmixer, while two tiny pinks bars are the result of simulation by circuit with R mixer.\ny\n21"
  },
  {
    "id": 1229,
    "text": "5 Code for my simulation\n# General imports\nimport numpy as np\n# Pre-defined ansatz circuit, operator class and visualization tools\nfrom qiskit.circuit.library import QAOAAnsatz\nfrom qiskit.quantum_info import SparsePauliOp\nfrom qiskit.visualization import plot_distribution\nfrom qiskit.providers.fake_provider import FakeManilaV2\n# Qiskit Runtime\nfrom qiskit_ibm_runtime import QiskitRuntimeService\nfrom qiskit_ibm_runtime import Estimator, Sampler, Session, Options\nfrom qiskit.providers.fake_provider import FakeManilaV2\n# SciPy minimizer routine\nfrom scipy.optimize import minimize\nfrom qiskit.primitives import Estimator, Sampler\noptions = Options()\noptions.transpilation.skip_transpilation = False\noptions.execution.shots = 10000\nestimator = Estimator(options={\"shots\": int(1e4)})\nsampler = Sampler(options={\"shots\": int(1e4)})\nfrom sympy import symbols, Sum, IndexedBase, simplify\nfrom sympy.abc import n, v, j, u\n# Define symbolic variables\nx = IndexedBase(’x’)"
  },
  {
    "id": 1230,
    "text": "sampler = Sampler(options={\"shots\": int(1e4)})\nfrom sympy import symbols, Sum, IndexedBase, simplify\nfrom sympy.abc import n, v, j, u\n# Define symbolic variables\nx = IndexedBase(’x’)\n# Function to represent the vertex uniqueness term of the Hamiltonian\ndef vertex_uniqueness_term(n):\nreturn Sum((1 - Sum(x[v, j], (j, 2, n)))**2, (v, 2, n))\n# Function to represent the edge uniqueness term of the Hamiltonian\ndef edge_uniqueness_term(n):\nreturn Sum((1 - Sum(x[v, j], (v, 2, n)))**2, (j, 2, n))\n# Function to represent the edge validity\n# term of the Hamiltonian for a given graph\ndef edge_validity_term(graph, n):\nvalidity_term = 0\nfor u in range(2,n+1):\nedge=(u,1)\nif not edge in graph:\nvalidity_term +=x[u,n]\nfor v in range(2,n+1):\nu, v = edge\nif not edge in graph:\nvalidity_term += Sum(x[u, j] * x[v, j+1], (j, 1, n-1))\nreturn validity_term\n# Combine the terms to form the complete Hamiltonian\ndef hamiltonian(graph, n):\nH = 1.5*vertex_uniqueness_term(n) +"
  },
  {
    "id": 1231,
    "text": "validity_term += Sum(x[u, j] * x[v, j+1], (j, 1, n-1))\nreturn validity_term\n# Combine the terms to form the complete Hamiltonian\ndef hamiltonian(graph, n):\nH = 1.5*vertex_uniqueness_term(n) +\nedge_uniqueness_term(n) + edge_validity_term(graph, n)\nreturn simplify(H)\n22"
  },
  {
    "id": 1232,
    "text": "def apply_substitution_to_hamiltonian(H, n):\nZ = IndexedBase(’Z’)\nH_substituted = H\nfor v in range(2, n+1):\nfor j in range(2, n+1):\nz_index = (v-2)*(n-1) + j-1 # Corrected index calculation\nif z_index > 0:\nH_substituted =\nH_substituted.subs(x[v, j], 1/2 * (1 - Z[z_index]))\nelse:\nH_substituted = H_substituted.subs(x[v, j], 0)\nreturn simplify(H_substituted)\ndef expand_and_simplify_hamiltonian(H,n):\nZ = IndexedBase(’Z’)\nH_expanded = H.expand()\n# Apply the simplification rule Z_k^2 = I\nfor k in range(1, (n-1)**2+1):\n# Assuming up to 8 qubits for this example\nH_expanded = H_expanded.subs(Z[k]**2, 0)\nreturn simplify(H_expanded)\ndef hamiltonian_to_string_list(H, n):\n\"\"\"\nConvert the expanded Hamiltonian to a list of\nstrings with corresponding coefficients.\nEach string represents a term in the Hamiltonian,\nwith ’Z’ at positions corresponding to qubits involved in the term.\nFor example, ’ZZI’ represents Z_1 Z_2.\n:param H: The expanded Hamiltonian expression\n:param n: Number of qubits"
  },
  {
    "id": 1233,
    "text": "with ’Z’ at positions corresponding to qubits involved in the term.\nFor example, ’ZZI’ represents Z_1 Z_2.\n:param H: The expanded Hamiltonian expression\n:param n: Number of qubits\n:return: List of tuples (string, coefficient)\n\"\"\"\nZ = IndexedBase(’Z’)\nterms = []\n# Iterate over each term in the Hamiltonian expression\nfor term in H.as_ordered_terms():\n# Initialize a string with ’I’s for each qubit\nterm_string = [’I’] * n\ncoeff = H.coeff(term) # Extract the coefficient of the term\nfindZ=False\n# Check for the presence of Z operators in the term\nfor k in range(1, n+1):\nif term.has(Z[k]):\nfindZ=True\nterm_string[k-1] = ’Z’\nif not findZ:\ncontinue\n# Join the term string and append it with its coefficient to the list\nterms.append((’’.join(term_string), coeff))\n23"
  },
  {
    "id": 1234,
    "text": "return terms\nn=4\n# Example: Hamiltonian for a triangle graph\ntriangle_graph = [(1, 2), (2, 3), (3, 4),(4,1), (2, 1), (3, 2),(4,3),(1,4)]\nH_triangle = hamiltonian(triangle_graph, n)\nprint(f\"Polynomial H is {H_triangle}\")\n# Apply the substitution rule to the Hamiltonian for a triangle graph (n = 3)\nH_triangle_substituted = apply_substitution_to_hamiltonian(H_triangle, n)\nprint(f\"After Substitute wo Z is {H_triangle_substituted}\")\nH_final=expand_and_simplify_hamiltonian(H_triangle_substituted,n)\nprint(f\"After simplification {H_final}\")\n# Convert the final Hamiltonian for the triangle\n# graph to string list representation\nhamiltonian_string_list = hamiltonian_to_string_list(H_final, (n-1)**2)\nprint(f\"Final result {hamiltonian_string_list}\")\nfrom qiskit.providers.fake_provider import FakeAlmadenV2\n# Get a fake backend from the fake provider\nbackend = FakeAlmadenV2()\nfrom qiskit.transpiler import PassManager\nfrom qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager"
  },
  {
    "id": 1235,
    "text": "# Get a fake backend from the fake provider\nbackend = FakeAlmadenV2()\nfrom qiskit.transpiler import PassManager\nfrom qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\nfrom qiskit_ibm_runtime.transpiler.passes.scheduling import (\nALAPScheduleAnalysis,\nPadDynamicalDecoupling,\n)\nfrom qiskit.circuit.library import XGate\ntarget = backend.target\npm = generate_preset_pass_manager(target=target, optimization_level=3)\nansatz_ibm = pm.run(ansatz)\nhamiltonian_string_list = [\n(’ZZIIIIIII’, 1), (’ZIZIIIIII’, 1), (’ZIIZIIIII’, 1), (’ZIIIIIZII’, 1),\n(’ZIIIIIIZI’, 1), (’ZIIIIIIII’, -3), (’IZZIIIIII’, 1), (’IZIIZIIII’, 1),\n(’IZIIIIZII’, 1), (’IZIIIIIZI’, 1), (’IZIIIIIIZ’, 1), (’IZIIIIIII’, -4),\n(’IIZIIZIII’, 1), (’IIZIIIIZI’, 1), (’IIZIIIIIZ’, 1), (’IIZIIIIII’, -3),\n(’IIIZZIIII’, 1), (’IIIZIZIII’, 1), (’IIIZIIZII’, 1), (’IIIZIIIII’, -4),\n(’IIIIZZIII’, 1), (’IIIIZIIZI’, 1), (’IIIIZIIII’, -2), (’IIIIIZIIZ’, 1),"
  },
  {
    "id": 1236,
    "text": "(’IIIZZIIII’, 1), (’IIIZIZIII’, 1), (’IIIZIIZII’, 1), (’IIIZIIIII’, -4),\n(’IIIIZZIII’, 1), (’IIIIZIIZI’, 1), (’IIIIZIIII’, -2), (’IIIIIZIIZ’, 1),\n(’IIIIIZIII’, -4), (’IIIIIIZZI’, 1), (’IIIIIIZIZ’, 1), (’IIIIIIZII’, -3),\n(’IIIIIIIZZ’, 1), (’IIIIIIIZI’, -4), (’IIIIIIIIZ’, -3)\n]\n# Problem to Hamiltonian operator\nhamiltonian = SparsePauliOp.from_list(hamiltonian_string_list)\n# QAOA ansatz circuit\nansatz = QAOAAnsatz(hamiltonian, reps=8)\nansatz.decompose(reps=8).draw(output=\"mpl\", style=\"iqp\")\n24"
  },
  {
    "id": 1237,
    "text": "def cost_func(params, ansatz, hamiltonian, estimator):\n\"\"\"Return estimate of energy from estimator\nParameters:\nparams (ndarray): Array of ansatz parameters\nansatz (QuantumCircuit): Parameterized ansatz circuit\nhamiltonian (SparsePauliOp): Operator representation of Hamiltonian\nestimator (Estimator): Estimator primitive instance\nReturns:\nfloat: Energy estimate\n\"\"\"\ncost = estimator.\nrun(ansatz, hamiltonian, parameter_values=params).\nresult().values[0]\nreturn cost\nx0 = 2 * np.pi * np.random.rand(ansatz_ibm.num_parameters)\nres = minimize(cost_func, x0, args=(ansatz, hamiltonian, estimator),\nmethod=\"COBYLA\",options={’disp’: True})\n# Assign solution parameters to ansatz\nqc = ansatz.assign_parameters(res.x)\n# Add measurements to our circuit\nqc.measure_all()\n# Sample ansatz at optimal parameters\nsamp_dist = sampler.run(qc).result().quasi_dists[0]\n# Close the session since we are now done with it\nsession.close()\n6 Conclusion"
  },
  {
    "id": 1238,
    "text": "qc.measure_all()\n# Sample ansatz at optimal parameters\nsamp_dist = sampler.run(qc).result().quasi_dists[0]\n# Close the session since we are now done with it\nsession.close()\n6 Conclusion\nI got many interesting result in this project. First and foremost, this is the first time I test that the\nQAOA really works, for solving the NP complete problem such as hamiltonian cycle path problem.\nHowever, since the embedding require (n−1)2 qubits, I can only simulate up to no more than n=6.\nIn this problem, I choose n = 3,4 and run the simulation on the simplest case: A triangle and\na square. In both cases, I analyze the energy spectrum of the cost hamiltonian beforehand, and\ndidn’t start the simulation of QAOA until I’m convinced that the cost hamiltonian is correct. This\nstep actually benefit me a lot in understanding the behavior of QAOA. One important thing is that\nonce you know what exactly the minimum energy is, you can check the quality of QAOA parameter"
  },
  {
    "id": 1239,
    "text": "step actually benefit me a lot in understanding the behavior of QAOA. One important thing is that\nonce you know what exactly the minimum energy is, you can check the quality of QAOA parameter\noptimizer,bycomparingthecostgivenbyQAOAcircuitwiththeminimumenergy. Anotherinteresting\nobservation in the spectrum plotted in Figure 2 and Figure 5 is that in both cases there is a large\nenough energy gap between the solution space the the non-solution space. I highly doubt\nthat, such energy gap is crutial for the success and time complexity of QAOA. There is no doubt that\nwhen the structure of the graph get more complicated, the spectrum can also get complicated, and\nthusitbecomesharderforanoptimizertotellwhetherwearegettingclosertogroundstateornot. On\nthe other hand, if we could find a better general way of embedding Hamiltonian cycle, such that the\nsolution space has a large gap between the non-solution space, than I would be much more confident"
  },
  {
    "id": 1240,
    "text": "the other hand, if we could find a better general way of embedding Hamiltonian cycle, such that the\nsolution space has a large gap between the non-solution space, than I would be much more confident\nthat we can use QAOA to get the accurate solution state.\nOne thing one can never neglect is the role of quantum noise. Intuitively, when we add more noise\nintothecircuit,ouralgorithmwillonlygetworseresult. Theexperimentsoftrianglecase,inFigure12,\nisconsistentwiththisintuition. However,inthesquarecase,theresultofrunningQAOAismuch\nbetter than on a noiseless simulator! Does that mean, we don’t need error correction at all for\nQAOA? Because noise seems to be a resource that benefit our optimization and annealing process,\n25"
  },
  {
    "id": 1241,
    "text": "ratherthanaharmfulfactor! Theideaofutilizationquantumnoise,issocrazybutattracting. Maybe\nI will explore this possiblity someday in the future.\n26"
  },
  {
    "id": 1242,
    "text": "References\n[1] AndreasBartschiandStephanEidenbenz.“GroverMixersforQAOA:ShiftingComplexityfrom\nMixerDesigntoStatePreparation”.In:2020 IEEE International Conferenceon Quantum Com-\nputing and Engineering (QCE). IEEE, Oct. 2020. doi: 10.1109/qce49297.2020.00020. url:\nhttp://dx.doi.org/10.1109/QCE49297.2020.00020.\n[2] Kostas Blekos et al. A Review on Quantum Approximate Optimization Algorithm and its Vari-\nants. 2023. arXiv: 2306.09198 [quant-ph].\n[3] Stephen A. Cook. “The Complexity of Theorem-Proving Procedures”. In: Proceedings of the\nThird Annual ACM Symposium on Theory of Computing. STOC ’71. Shaker Heights, Ohio,\nUSA: Association for Computing Machinery, 1971, pp. 151–158. isbn: 9781450374644. doi: 10.\n1145/800157.805047. url: https://doi.org/10.1145/800157.805047.\n[4] S.Ebadietal.“QuantumoptimizationofmaximumindependentsetusingRydbergatomarrays”.\nIn: Science 376.6598 (2022), pp. 1209–1215. doi: 10.1126/science.abo6587. eprint: https:"
  },
  {
    "id": 1243,
    "text": "[4] S.Ebadietal.“QuantumoptimizationofmaximumindependentsetusingRydbergatomarrays”.\nIn: Science 376.6598 (2022), pp. 1209–1215. doi: 10.1126/science.abo6587. eprint: https:\n//www.science.org/doi/pdf/10.1126/science.abo6587. url: https://www.science.org/\ndoi/abs/10.1126/science.abo6587.\n[5] Edward Farhi, Jeffrey Goldstone, and Sam Gutmann. A Quantum Approximate Optimization\nAlgorithm. 2014. arXiv: 1411.4028 [quant-ph].\n[6] Matthew P. Harrigan et al. “Quantum approximate optimization of non-planar graph problems\non a planar superconducting processor”. In: Nature Physics 17.3 (Feb. 2021), pp. 332–336. issn:\n1745-2481. doi: 10.1038/s41567-020-01105-y. url: http://dx.doi.org/10.1038/s41567-\n020-01105-y.\n[7] RichardKarp.“ReducibilityAmongCombinatorialProblems”.In:vol.40.Jan.1972,pp.85–103.\nisbn: 978-3-540-68274-5. doi: 10.1007/978-3-540-68279-0_8.\n[8] Andrew Lucas. “Ising formulations of many NP problems”. In: Frontiers in Physics 2 (2014)."
  },
  {
    "id": 1244,
    "text": "isbn: 978-3-540-68274-5. doi: 10.1007/978-3-540-68279-0_8.\n[8] Andrew Lucas. “Ising formulations of many NP problems”. In: Frontiers in Physics 2 (2014).\nissn: 2296-424X. doi: 10.3389/fphy.2014.00005. url: http://dx.doi.org/10.3389/fphy.\n2014.00005.\n[9] JohnPreskill.“QuantumComputingintheNISQeraandbeyond”.In:Quantum 2(Aug.2018),\np. 79. issn: 2521-327X. doi: 10.22331/q-2018-08-06-79. url: http://dx.doi.org/10.\n22331/q-2018-08-06-79.\n[10] Google AI Quantum et al. “Hartree-Fock on a superconducting qubit quantum computer”. In:\nScience 369.6507 (2020), pp. 1084–1089. doi: 10.1126/science.abb9811. eprint: https://\nwww.science.org/doi/pdf/10.1126/science.abb9811. url: https://www.science.org/\ndoi/abs/10.1126/science.abb9811.\n[11] Jules Tilly et al. “The Variational Quantum Eigensolver: A review of methods and best prac-\ntices”. In: Physics Reports 986 (Nov. 2022), pp. 1–128. issn: 0370-1573. doi: 10.1016/j.\nphysrep.2022.08.003. url: http://dx.doi.org/10.1016/j.physrep.2022.08.003.\n27"
  },
  {
    "id": 1245,
    "text": "CombiningBayesianreconstructionentropywithmaximumentropymethodforanalytic\ncontinuationsofmatrix-valuedGreen’sfunctions\nSonglin Yang,1 Liang Du,1,∗ and Li Huang1,†\n1CollegeofPhysicsandTechnology, GuangxiNormalUniversity, Guilin, Guangxi541004, China\n(Dated:January2,2024)\nTheBayesianreconstructionentropyisconsideredanalternativetotheShannon-Jaynesentropy,asitdoes\nnot exhibit the asymptotic flatness characteristic of the Shannon-Jaynes entropy and obeys the scale invari-\nance. Itiscommonlyutilizedinconjunctionwiththemaximumentropymethodtoderivespectralfunctions\nfromEuclideantimecorrelatorsproducedbylatticeQCDsimulations. Thisstudyexpandstheapplicationof\nthe Bayesian reconstruction entropy to the reconstruction of spectral functions for Matsubara or imaginary-\ntime Green’s functions in quantum many-body physics. Furthermore, it extends the Bayesian reconstruction\nentropy to implement the positive-negative entropy algorithm, enabling the analytic continuations of matrix-"
  },
  {
    "id": 1246,
    "text": "entropy to implement the positive-negative entropy algorithm, enabling the analytic continuations of matrix-\nvaluedGreen’sfunctionsonanelement-wisemanner. Boththediagonalandoff-diagonalcomponentsofthe\nmatrix-valuedGreen’sfunctionsaretreatedequally. Benchmarkresultsfortheanalyticcontinuationsofsyn-\ntheticGreen’sfunctionsindicatethattheBayesianreconstructionentropy,whencombinedwiththepreblurtrick,\ndemonstratescomparableperformancetotheShannon-Jaynesentropy. Notably,itexhibitsgreaterresilienceto\nnoisesintheinputdata,particularlywhenthenoiselevelismoderate.\nI. INTRODUCTION its own advantages and disadvantages. There is no doubt\nthat the MaxEnt method is particularly popular due to its\nIn quantum many-body physics, single-particle Green’s ability to maintain a balance between computational effi-\nfunction holds significant importance [1, 2]. It is typically ciency and accuracy [18–20]. Several open source toolkits,"
  },
  {
    "id": 1247,
    "text": "function holds significant importance [1, 2]. It is typically ciency and accuracy [18–20]. Several open source toolkits,\ncomputed using numerical methods such as quantum Monte such as ΩMAXENT [53], QMEM [54], MAXENT by Kraberger et\nCarlo [3–5] and functional renormalization group [6, 7], or al.[55], MAXENTbyLevyetal.[56], ALF[57], ACFlow[58],\nmany-body perturbative techniques like random phase ap- eDMFT[59], andana cont[60], havebeendevelopedtoim-\nproximation[8,9]andGWapproximation[10–13],withinthe plementthismethod.\nframeworkofimaginarytimeorMatsubarafrequencyformu- The MaxEnt method is rooted in Bayesian inference [61].\nlation. This quantity provides valuable insights into the dy- Initially, the spectral function A(ω) is constrained to be non-\nnamicresponseofthesystem. However,itcannotbedirectly negativeandisinterpretedasaprobabilitydistribution. Sub-\ncomparedtoreal-frequencyexperimentalobservables. There- sequently,theMaxEntmethodendeavorstoidentifythemost"
  },
  {
    "id": 1248,
    "text": "comparedtoreal-frequencyexperimentalobservables. There- sequently,theMaxEntmethodendeavorstoidentifythemost\nfore, it is necessary to convert the calculated values to real probable spectrum by minimizing a functional Q[A], which\nfrequencies,whichpresentsthefundamentalchallengeofthe comprises the misfit functional L[A] and the entropic term\nanalyticcontinuationproblem. Intheory,theretardedGreen’s S[A] [see Eq. (10)]. Here, L[A] quantifies the difference be-\nfunctionGR(ω)andthespectralfunctionA(ω)canbederived tween the input and reconstructed Green’s function, while\nfrom the imaginary-time Green’s function G(τ) or Matsub- S[A] is employed to regulate the spectrum [18]. The Max-\naraGreen’sfunctionG(iω )byperforminganinverseLaplace Entmethodiswell-establishedforthediagonalcomponentsof\nn\ntransformation[1,2]. Duetotheill-conditionednatureofthe matrix-valued Green’s functions, as the corresponding spec-"
  },
  {
    "id": 1249,
    "text": "n\ntransformation[1,2]. Duetotheill-conditionednatureofthe matrix-valued Green’s functions, as the corresponding spec-\ntransformationkernel,solvingitdirectlyisnearlyimpossible. tralfunctionsarenon-negative(forfermionicsystems). How-\nIn fact, the output for this inverse problem is highly sensi- ever, the non-negativity of the spectral functions cannot be\ntivetotheinput. EvenminorfluctuationsornoisesinG(τ)or assured for the off-diagonal components of matrix-valued\nG(iω ) can result in nonsensical changes for A(ω) orGR(ω), Green’sfunctions. Asaresult,conventionalMaxEntmethod\nn\nmakinganalyticcontinuationsextremelyunstable. failsinthissituation.\nIn recent decades, numerous techniques have been devel- Currently,thereisagrowingemphasisonobtainingthefull\noped to address the challenges associated with analytic con- matrix representation of the Green’s function (or the corre-\ntinuationproblems. ThesemethodsincludePade´ approxima- spondingself-energyfunction)ontherealaxis[54,62]. This"
  },
  {
    "id": 1250,
    "text": "tinuationproblems. ThesemethodsincludePade´ approxima- spondingself-energyfunction)ontherealaxis[54,62]. This\ntion(PA)[14–17],maximumentropymethod(MaxEnt)[18– is necessary for calculating lattice quantities of interest in\n20],stochasticanalyticcontinuation(SAC)[21–30],stochas- Dyson’s equation [59]. Therefore, there is a strong demand\ntic optimization method (SOM) [31–35], stochastic pole ex- for a reliable method to perform analytic continuation of the\npansion (SPX) [36, 37], sparse modeling (SpM) [38, 39], entireGreen’sfunctionmatrix[41–43]. Tomeetthisrequire-\nNevanlinna analytical continuation (NAC) [40, 41], causal ment,severalnovelapproacheshavebeensuggestedinrecent\nprojections [42], Prony fits [43–45], and machine learning years as enhancements to the conventional MaxEnt method.\nassistant approaches [46–52], and so on. Each method has The three most significant advancements are as follows: (1)\nAuxiliaryGreen’sfunctionalgorithm. Itispossibletocreate"
  },
  {
    "id": 1251,
    "text": "assistant approaches [46–52], and so on. Each method has The three most significant advancements are as follows: (1)\nAuxiliaryGreen’sfunctionalgorithm. Itispossibletocreate\nan auxiliary Green’s function by combining the off-diagonal\nanddiagonalelementsofthematrix-valuedGreen’sfunction\n∗liangdu@gxnu.edu.cn toensurethepositivityoftheauxiliaryspectrum[63–68].The\n†lihuang.dmft@gmail.com\nanalytic continuation of the auxiliary Green’s function can\n3202\nceD\n72\n]tal-peh[\n1v81000.1042:viXra"
  },
  {
    "id": 1252,
    "text": "2\nthen be carried out using the standard MaxEnt method. (2) introduces the spectral representation of the single-particle\nPositive-negative entropy algorithm. Kraberger et al. pro- Green’s function. Furthermore, it explains the fundamental\nposedthattheoff-diagonalspectralfunctionscanbeseenasa formalisms of the MaxEnt method, the Bayesian reconstruc-\nsubtractionoftwoartificialpositivefunctions. Theyextended tionentropy,andtheprincipleofthepositive-negativeentropy\nthe entropic term to relax the non-negativity constraint pre- algorithm. SectionIIIisdedicatedtovariousbenchmarkex-\nsentedinthestandardMaxEntmethod[55,60]. Thisenables amples, including analytic continuation results for synthetic\nsimultaneoustreatmentofboththediagonalandoff-diagonal single-bandGreen’sfunctionsandmulti-orbitalmatrix-valued\nelements, thereby restoring crucial constraints on the mathe- Green’sfunctions. ThespectralfunctionsobtainedbyS and\nSJ"
  },
  {
    "id": 1253,
    "text": "elements, thereby restoring crucial constraints on the mathe- Green’sfunctions. ThespectralfunctionsobtainedbyS and\nSJ\nmatical properties of the resulting spectral functions, includ- S are compared with the exact spectra. In Section IV, we\nBR\ningpositivesemi-definitenessandHermiticity. (3)Maximum further examine the preblur trick and the auxiliary Green’s\nquantum entropy algorithm. Recently, Sim and Han gener- function algorithm for S and S . The robustness of S\nSJ BR BR\nalized the MaxEnt method by reformulating it with quantum with respect to noisy Matsubara data is discussed and com-\nrelativeentropy,maintainingtheBayesianprobabilisticinter- pared with that of S . A concise summary is presented in\nSJ\npretation[18,61]. Thematrix-valuedGreen’sfunctionisdi- Section V. Finally, Appendix A introduces the goodness-of-\nrectlycontinuedasasingleobjectwithoutanyfurtherapprox- fit functional. The detailed mathematical derivations for S\nSJ"
  },
  {
    "id": 1254,
    "text": "rectlycontinuedasasingleobjectwithoutanyfurtherapprox- fit functional. The detailed mathematical derivations for S\nSJ\nimationorad-hoctreatment[54]. andS areprovidedinAppendicesBandC,respectively.\nBR\nAlthough various algorithms have been proposed to en-\nhance the usefulness of the MaxEnt method [53–55], further\nimprovementsinalgorithmsandimplementationsarealways II. METHOD\nbeneficial. As mentioned above, the essence of the MaxEnt\nmethodliesinthedefinitionoftheentropictermS[A]. Typ- A. Spectralrepresentationofsingle-particleGreen’sfunction\nically, it takes the form of Shannon-Jaynes entropy (dubbed\nS SJ )intherealmofquantummany-bodyphysics[69]. How- It is well known that the single-particle imaginary-time\never, alternative forms such as the Tikhonov regulator [70], Green’sfunctionG(τ)canbedefinedasfollows:\nthe positive-negative entropy [55], and the quantum relative\nentropy[54]arealsoacceptable. Itisworthnotingthatinthe G(τ)=⟨T d(τ)d†(0)⟩, (1)\nτ"
  },
  {
    "id": 1255,
    "text": "the positive-negative entropy [55], and the quantum relative\nentropy[54]arealsoacceptable. Itisworthnotingthatinthe G(τ)=⟨T d(τ)d†(0)⟩, (1)\nτ\ncontextoflatticeQCDsimulations,theextractionofspectral\nfunctionsfromEuclideantimecorrelatorsisofparticularim- whereτisimaginarytime,T meansthetime-orderedopera-\nτ\nportant. TheMaxEntmethodisalsotheprimarytoolforthe tor, d (d†)denotestheannihilation(creation)operator[1,2].\nanalytic continuation of lattice QCD data [71]. Burnier and GivenG(τ), Matsubara Green’s functionG(iω ) can be con-\nn\nRothkopf introduced an enhanced dimensionless prior distri- structedviadirectFouriertransformation:\nbution,knownastheBayesianreconstructionentropy(dubbed\n(cid:90) β\nh\nS\na\nB\nv\nR\ni\n)\nor\n[7\nc\n2\no\n,\nm\n7\np\n3\na\n]\nr\n,\ned\nwh\nto\nic\nS\nh de\n[\nm\n69\no\n]\nn\n.\nst\nT\nra\nh\nt\ne\ne\ny\ns\nd\ns\ni\nu\ns\np\nc\ne\no\nr\nv\ni\ne\no\nr\nr\ned\nas\nt\ny\nh\nm\nat\np\nb\nto\ny\nti\nu\nc\nsi\nb\nn\ne\ng\n- G(iω\nn\n)= dτe−iωnτG(τ), (2)\nSJ 0\nS inconjunctionwiththeMaxEntmethod,asignificantim-\nBR"
  },
  {
    "id": 1256,
    "text": "[\nm\n69\no\n]\nn\n.\nst\nT\nra\nh\nt\ne\ne\ny\ns\nd\ns\ni\nu\ns\np\nc\ne\no\nr\nv\ni\ne\no\nr\nr\ned\nas\nt\ny\nh\nm\nat\np\nb\nto\ny\nti\nu\nc\nsi\nb\nn\ne\ng\n- G(iω\nn\n)= dτe−iωnτG(τ), (2)\nSJ 0\nS inconjunctionwiththeMaxEntmethod,asignificantim-\nBR\nprovementinthereconstructionofspectralfunctionsforEu- where β is the inverse temperature of the system (β ≡ 1/T),\nclideantimecorrelatorscanbeachieved.Theylatterextended and ω n is the Matsubara frequency. Note that ω n is equal to\nS to support Bayesian inference of non-positive spectral\n(2n+1)π/βforfermionsand2nπ/βforbosons(nisaninteger).\nBR\nfunctionsinquantumfieldtheory[74]. Buttothebestofour Let us assume that the spectral function of the single-\nknowledge, the application of S has been limited to post- particle Green’s function is A(ω), then the spectral represen-\nBR\nprocessing for lattice QCDdata thus far. Hence, some ques- tationsofG(τ)andG(iω n )read:\ntionsnaturallyarise.HoweffectiveisS BR forsolvinganalytic (cid:90) +∞"
  },
  {
    "id": 1257,
    "text": "BR\nprocessing for lattice QCDdata thus far. Hence, some ques- tationsofG(τ)andG(iω n )read:\ntionsnaturallyarise.HoweffectiveisS BR forsolvinganalytic (cid:90) +∞\ncontinuationproblemsinquantummany-bodysimulations?Is G(τ)= K(τ,ω)A(ω)dω, (3)\nittrulysuperiortoS SJ ? Keepingthesequestionsinmind,the −∞\nprimary objective of this study is to broaden the application\nand\nscopeofS . WeatfirstverifywhetherS canhandleMat-\nBR BR\nsubara(orimaginary-time)Green’sfunctions. Then,wegen- (cid:90) +∞\neralize S BR to implement the positive-negative entropy algo- G(iω n )= K(iω n ,ω)A(ω)dω, (4)\nrithm[55]foranalyticcontinuationsofmatrix-valuedGreen’s −∞\nfunctions.ThesimulatedresultsindicatethatS BR worksquite respectively.Here,K(τ,ω)andK(iω n ,ω)arecalledthekernel\nwell in transforming imaginary-time or Matsubara Green’s\nfunctions. Theirdefinitionsareasfollows:\nfunctiondatatorealfrequency,nomatterwhethertheGreen’s\nfunctionisamatrixornot. WefindthatS hasatendencyto e−τω\nBR K(τ,ω)= , (5)"
  },
  {
    "id": 1258,
    "text": "functions. Theirdefinitionsareasfollows:\nfunctiondatatorealfrequency,nomatterwhethertheGreen’s\nfunctionisamatrixornot. WefindthatS hasatendencyto e−τω\nBR K(τ,ω)= , (5)\nsharpen the peaks in spectralfunctions, but this shortcoming 1±e−βω\ncanbelargelyovercamebythepreblurtrick[55,60].Overall,\nand\ntheperformanceofS iscomparabletothatofS [69]for\nBR SJ\ntheexamplesinvolved.\n1\nK(iω ,ω)= . (6)\nThe rest of this paper is organized as follows. Section II n iω −ω\nn"
  },
  {
    "id": 1259,
    "text": "3\nIn the right-hand side of Eq. (5), + is for fermionic correla- C. Bayesianreconstructionentropy\ntors and - is for bosonic correlators [18]. In the subsequent\ndiscussion,ourfocuswillbeonfermioniccorrelators. The entropy S is also known as the Kullback-Leibler dis-\nWe observe that Eqs. (3) and (4) are classified as Fred- tance sometimes. In principle, there are multiple choices for\nholmintegralequationsofthefirstkind. WhenA(ω)isgiven, it. PerhapsS isthemostfrequentlyusedinquantummany-\nSJ\nit is relatively straightforward to compute the corresponding bodyphysicsandcondensedmatterphysics[18,69]. Itreads:\nG(τ)andG(iω )bynumericalintegrationmethods. However,\nn\n(cid:90) (cid:34) (cid:32) (cid:33)(cid:35)\nthe inverse problem of deducing A(ω) from G(τ) or G(iω ) A(ω)\nn S [A]= dω A(ω)−D(ω)−A(ω)log . (12)\nis challenging due to the ill-conditioned nature of the kernel SJ D(ω)\nfunctionK. Tobemorespecific,theconditionnumberofKis"
  },
  {
    "id": 1260,
    "text": "n S [A]= dω A(ω)−D(ω)−A(ω)log . (12)\nis challenging due to the ill-conditioned nature of the kernel SJ D(ω)\nfunctionK. Tobemorespecific,theconditionnumberofKis\nverylargebecauseoftheexponentialdecayofKwithωandτ. Here,D(ω)iscalledthedefaultmodel,providingtheessential\nConsequently,directinversionofKbecomesimpracticalfrom featuresofspectra. Usually D(ω)isaconstantoraGaussian\nanumericalstandpoint[19]. Furthermore,theG(τ)orG(iω ) function, but it can be determined by making use of high-\nn\ndata obtained from finite temperature quantum Monte Carlo frequency behavior of input data as well [53]. If both A(ω)\nsimulationsisnotfreefromerrors[3–5],furthercomplicating andD(ω)havethesamenormalization,Eq.(12)reducesto:\nthesolutionofEqs.(3)and(4).\n(cid:90) (cid:32) (cid:33)\nA(ω)\nS [A]=− dωA(ω)log . (13)\nSJ D(ω)\nB. Maximumentropymethod\nTheS introducedbyBurnierandRothkopf[72,73]isdom-\nBR\ninantinhigh-energyphysicsandparticlephysics. Itreads:\nThecornerstoneoftheMaxEntmethodisBayes’theorem."
  },
  {
    "id": 1261,
    "text": "SJ D(ω)\nB. Maximumentropymethod\nTheS introducedbyBurnierandRothkopf[72,73]isdom-\nBR\ninantinhigh-energyphysicsandparticlephysics. Itreads:\nThecornerstoneoftheMaxEntmethodisBayes’theorem.\nLet us treat the input Green’s function G and the spectrum (cid:90) (cid:34) (cid:32) (cid:33)(cid:35)\nA(ω) A(ω)\nAastwoevents. AccordingtoBayes’theorem, theposterior S [A]= dω 1− +log . (14)\nprobabilityP[A|G]canbecalculatedby:\nBR D(ω) D(ω)\nP[G|A]P[A] Note that S is constructed from four axioms [18]. While\nP[A|G]= , (7) SJ\nP[G] for S\nBR\n, two of these axioms are replaced [72]. First of all,\nthescaleinvarianceisincorporatedinS . ItmeansthatS\nwhere P[G|A] is the likelihood function, P[A] is the prior BR BR\nonly depends on the ratio between A and D. The integrand\nprobability,andP[G]istheevidence[61]. P[G|A]isassumed\ntobeindirectproportiontoe−L[A],wherethemisfitfunctional inEq.(14)isdimensionless,suchthatthechoiceofunitsfor\nAandDwillnotchangetheresultofthespectralreconstruc-\nL[A]reads:"
  },
  {
    "id": 1262,
    "text": "tobeindirectproportiontoe−L[A],wherethemisfitfunctional inEq.(14)isdimensionless,suchthatthechoiceofunitsfor\nAandDwillnotchangetheresultofthespectralreconstruc-\nL[A]reads:\ntion. Second, S favors choosing smooth spectrum. The\n1 1(cid:88)N (cid:32) G −G˜ [A] (cid:33)2 spectrathatdevia B te R betweentwoadjacentfrequencies,ω and\nL[A]= χ2[A]= i i . (8) 1\n2 2 i=1 σ i ω 2 ,shouldbepenalized.\nHere,Nisthenumberofinputdatapoints,σdenotesthestan-\ndard deviation ofG,G˜[A] is the reconstructed Green’s func- D. Positive-negativeentropy\ntionviaEqs.(3)and(4),andχ2[A]iscalledthegoodness-of-\nfitfunctional(seeAppendixAformoredetails). Ontheother\nTheformulasdiscussedaboveareonlycorrectforpositive\nhand, P[A] is supposed to be in direct proportion to eαS[A],\ndefinitespectrawithafinitenorm,i.e.,\nwhereαisaregulationparameterandS isentropy. Sincethe\n(cid:90)\nevidence P[G] can be viewed as a normalization constant, it\ndωA(ω)>0. (15)\nisignoredinwhatfollows. Thus,"
  },
  {
    "id": 1263,
    "text": "whereαisaregulationparameterandS isentropy. Sincethe\n(cid:90)\nevidence P[G] can be viewed as a normalization constant, it\ndωA(ω)>0. (15)\nisignoredinwhatfollows. Thus,\nP[A|G]∝eQ, (9) However, the norm could be zero for off-diagonal elements\nwhereQisafunctionalofA[18]. Itisdefinedasfollows: ofmatrix-valuedGreen’sfunctionsduetothefermionicanti-\ncommutationrelation[1,42]:\n1\nQ[A]=αS[A]−L[A]=αS[A]− χ2[A]. (10)\n(cid:90)\n2\ndωA(ω)=0. (16)\nThebasicideaoftheMaxEntmethodistoidentifytheoptimal\nspectrum Aˆ that maximizes the posterior probability P[A|G]\n(or equivalently Q[A]). In essence, our goal is to determine This suggests that the spectral function is not positive defi-\nthemostfavorableAˆthatsatisfiesthefollowingequation: niteanymore. TheequationsforS\nSJ\nandS\nBR\n[i.e.,Eqs.(12)\nand (14)] should be adapted to this new circumstance. The\n∂Q(cid:12)\n(cid:12) (cid:12) =0. (11) positive-negativeentropyalgorithmproposedbyKrabergeret\n(cid:12)\n∂A A=Aˆ"
  },
  {
    "id": 1264,
    "text": "SJ\nandS\nBR\n[i.e.,Eqs.(12)\nand (14)] should be adapted to this new circumstance. The\n∂Q(cid:12)\n(cid:12) (cid:12) =0. (11) positive-negativeentropyalgorithmproposedbyKrabergeret\n(cid:12)\n∂A A=Aˆ\nal.isagracefulsolutiontothisproblem[55,60].Theyrewrite\nEq.(11)canbeeasilysolvedbythestandardNewton’salgo- the off-diagonal spectral function A(ω) as the subtraction of\nrithm[75]. InAppendicesA,BandC,alltermsintheright- twopositivedefinitespectra:\nhandsideofEq.(10)areelaboratedindetail. Wealsoexplain\nhowtosolveEq.(11)efficiently. A(ω)= A + (ω)−A−(ω). (17)"
  },
  {
    "id": 1265,
    "text": "4\nHere A+(ω) and A−(ω) are independent, but have the same S±[A] (or S[A+,A−]) is called the positive-negative entropy,\nnorm. Thentheresultingentropycanbesplitintotwoparts: whichwasfirstusedfortheanalysisofNMRspectra[76,77].\nTheexpressionsofpositive-negativeentropyforS andS\nSJ BR\nS±[A]=S[A + ,A−]=S[A + ]+S[A−]. (18) areasfollows:\nS\nS\n±\nJ\n[A]=\n(cid:90)\ndω\n  √\nA2+4D2−2D−Alog\n  √ A2+\n2\n4\nD\nD2+A    \n, (19)\nand\nS\nB\n±\nR\n[A]=\n(cid:90)\ndω\n \n2−\n√ A2+\nD\nD2+D\n+log\n  √ A2+\n2D\nD2+D    \n. (20)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n4 2 0 2 4\nx\n)x(s\ns (x)=1−x+logx, (23)\nBR\nsBR\n√\ns B±R √ x2+4+x\nsSJ s±(x)= x2+4−2−xlog , (24)\nSJ 2\ns S±J\n√\n√ x2+1+1\ns± (x)=2−( x2+1+1)+log . (25)\nBR 2\nTheyarevisualizedinFigure1.\nClearly, all the entropy densities are strictly convex and\nnon-positive (i.e., s ≤ 0 and s± ≤ 0). The ordinary en-\ntropy density s(x) is just defined for positive x. s (x) be-\nBR\ncomes maximal at x = 1, and exhibits a similar quadratic"
  },
  {
    "id": 1266,
    "text": "non-positive (i.e., s ≤ 0 and s± ≤ 0). The ordinary en-\ntropy density s(x) is just defined for positive x. s (x) be-\nBR\ncomes maximal at x = 1, and exhibits a similar quadratic\nbehavior around its maximum as s (x). In the case x → 0 SJ\n(A≪ D), s (x)isnotsuppressed,while s (x)→−1. Thus,\nBR SJ\ns (x) avoids the asymptotic flatness inherent in s (x) [72].\nBR SJ\nThepositive-negativeentropydensity s±(x)isvalidforany x\nFIG.1.Comparisonofentropydensitiesfordifferententropicterms.\n(x∈R). Both s± (x)and s±(x)areevenfunctions. Theyalso\nTheentropyS isrelatedtotheentropydensitysviaEq.(21).Forthe BR SJ\npositive-negativeentropyS±,D+=D−=Disassumed. exhibitquadraticbehaviorsaroundx=0,ands±\nBR\n(x)≥ s±\nSJ\n(x).\nIn the limit of α → ∞, the goodness-of-fit functional χ2[A]\nhas negligible weight, and the entropic term αS[A] becomes\nThedefaultmodelsforA+andA−areD+andD−,respectively. dominant[53].Thus,themaximizationofQ[A](equivalently,"
  },
  {
    "id": 1267,
    "text": "has negligible weight, and the entropic term αS[A] becomes\nThedefaultmodelsforA+andA−areD+andD−,respectively. dominant[53].Thus,themaximizationofQ[A](equivalently,\nToderiveEqs.(19)and(20),D+ = D− = Disassumed. Fora the maximization of S[A]) yields Aˆ = D (at x = 1) for con-\ndetailedderivation,pleaseseeAppendicesBandC.Similarto ventional entropy, in contrast to Aˆ = 0 (at x = 0) for the\nS [A],S± [A]alsoexhibitsscaleinvariance.Inotherwords, positive-negativeentropy[55].\nBR BR\nitdependsonA/Donly.\nIII. RESULTS\nE. Entropydensity\nA. Computationalsetups\nTheintegrandintheexpressionforentropyS isreferredas\nentropydensitys,i.e., When the conventional MaxEnt method is combined with\n(cid:90) S SJ (orS S ± J )[69], itiscalledtheMaxEnt-SJmethod. Onthe\nS[A]= dωs[A(ω)]. (21) other hand, a combination of the MaxEnt method with S BR\n(orS± )[72–74]iscalledtheMaxEnt-BRmethod.Weimple-\nBR\nmentedbothmethodsintheopensourceanalyticcontinuation\nNext,wewouldliketomakeadetailedcomparisonaboutthe"
  },
  {
    "id": 1268,
    "text": "(orS± )[72–74]iscalledtheMaxEnt-BRmethod.Weimple-\nBR\nmentedbothmethodsintheopensourceanalyticcontinuation\nNext,wewouldliketomakeadetailedcomparisonaboutthe\nentropy densities of different entropic terms. Supposed that toolkit,namelyACFlow[58]. Allthebenchmarkcalculations\nx = A/D, the expressions for various entropy densities are involved in this paper were done by this toolkit. The simu-\nlatedresultsfortheMaxEnt-SJandMaxEnt-BRmethodswill\ncollectedasfollows:\nbepresentedinthissection. Next,wewillexplainthecompu-\ns (x)= x−1−xlogx, (22) tationaldetails.\nSJ"
  },
  {
    "id": 1269,
    "text": "5\n(a) (b) (c)\n(d) (e) (f)\nFIG.2. Analyticcontinuationsofsingle-bandGreen’sfunctions. (a)-(c)Forsingleoff-centeredpeakcase. (d)-(f)FortwoGaussianpeaks\nwithagapcase.Inpanels(a)and(d),thecalculatedandexactspectraarecompared.Inpanels(b)and(e),thelog (χ2)−log (α)curvesare\n10 10\nplotted.Theinvertedtrianglesymbolsindicatetheoptimalαparameters,whicharedeterminedbytheχ2-kinkalgorithm[53].Thedeviations\nofthereproducedGreen’sfunctionfromtherawonesareshowninpanels(c)and(f). Notethatinpanel(c),RisafunctionofMatsubara\nfrequency.However,Risτ-dependentinpanel(c).\nWealwaysstartfromanexactspectrum A(ω), whichcon- whereG˜ isevaluatedbyAˆviaEq.(3)orEq.(4).\nsistsofoneormoreGaussian-likepeaks. Then A(ω)isused\nto construct the imaginary-time Green’s function G(τ) via\nEq.(3),ortheMatsubaraGreen’sfunctionG(iω )viaEq.(4). B. Single-bandGreen’sfunctions\nn\nHere, for the sake of simplicity, the kernel function K is as-"
  },
  {
    "id": 1270,
    "text": "Eq.(3),ortheMatsubaraGreen’sfunctionG(iω )viaEq.(4). B. Single-bandGreen’sfunctions\nn\nHere, for the sake of simplicity, the kernel function K is as-\nsumed to be fermionic. Since the realistic Green’s function At first, the analytic continuations of single-band Green’s\ndata from finite temperature quantum Monte Carlo simula- functionsareexamined. Theexactspectralfunctionsarecon-\ntions is usually noisy, multiplicative Gaussian noise will be structedbyasuperpositionofsomeGaussianpeaks. Wecon-\nmanuallyaddedtothecleanG(τ)orG(iω n )toimitatethissit- sidertworepresentativespectra. (i)Singleoff-centeredpeak.\nuation. Thefollowingformulaisadopted: Thespectralfunctionis:\nG\nnoisy\n=G\nclean\n[1+δNC(0,1)], (26) (cid:34)\n(ω−ϵ)2\n(cid:35)\nA(ω)=exp − , (28)\nwhereδdenotesthenoiseleveloftheinputdataandNCisthe 2Γ2\ncomplex-valued normal Gaussian noise [42]. Unless stated\notherwise, the standard deviation of G is fixed to 10−4. The where ϵ = 0.5, Γ = 1.0, δ = 10−4, σ = 10−4, and β = 20.0."
  },
  {
    "id": 1271,
    "text": "complex-valued normal Gaussian noise [42]. Unless stated\notherwise, the standard deviation of G is fixed to 10−4. The where ϵ = 0.5, Γ = 1.0, δ = 10−4, σ = 10−4, and β = 20.0.\nnumbers of input data forG(τ) andG(iω ) are 1000 and 50, TheinputdataisMatsubaraGreen’sfunction,whichcontains\nn\nrespectively. DuringtheMaxEntsimulations,theχ2-kinkal- 50 Matsubara frequencies (N = 50). The blur parameter for\ngorithm[53]isusedtodeterminetheregulationparameterα. theMaxEnt-BRmethodis0.45. (ii)TwoGaussianpeakswith\nTheBryanalgorithm[78]isalsotested. Italwaysgivessim- agap. Thespectralfunctionis:\nilar results, which will not be presented in this paper. Once\n \nS\nto\nB\ns\nR\nm\na\no\nn\no\nd\nth\nS\nt\nB ±\nh\nR\nes\na\np\nr\ne\ne\nc\nc\ntr\nh\na\no\n.\ns\nT\nen\nh\n,\ne\nt\nb\nh\nl\ne\nur\np\np\nr\na\ne\nr\nb\na\nl\nm\nur\net\nt\ne\nri\nr\nc\ni\nk\nsa\n[5\nd\n5\nju\n,\ns\n6\nte\n0\nd\n]\nc\nis\nas\nu\ne\ns\nb\ned\ny\nA(ω) = √\n2\nf\nπ\n1\nΓ\nexp  − (ω\n2\n−\nΓ\nϵ\n2\n1 )2\n1 1\ncase.Finally,thereconstructedspectrumiscomparedwiththe  \nt\nt\nr\nif\nu\ny\ne\nt\ns\nh\no\ne\nlu\nd\nt\ne\nio\nv\nn\nia\n."
  },
  {
    "id": 1272,
    "text": ",\ns\n6\nte\n0\nd\n]\nc\nis\nas\nu\ne\ns\nb\ned\ny\nA(ω) = √\n2\nf\nπ\n1\nΓ\nexp  − (ω\n2\n−\nΓ\nϵ\n2\n1 )2\n1 1\ncase.Finally,thereconstructedspectrumiscomparedwiththe  \nt\nt\nr\nif\nu\ny\ne\nt\ns\nh\no\ne\nlu\nd\nt\ne\nio\nv\nn\nia\n.\nti\nW\non\ne\no\na\nf\nls\nt\no\nhe\nad\nre\no\nc\np\no\nt\nn\nt\ns\nh\nt\ne\nru\nf\nc\no\nt\nl\ne\nl\nd\now\nG\ni\nr\nn\ne\ng\nen\nq\n’\nu\ns\na\nf\nn\nu\nt\nn\nit\nc\ny\nti\nt\no\no\nn\nq\nf\nu\nro\nan\nm\n- + √\n2\nf\nπ\n2\nΓ 2\nexp  − (ω\n2\n−\nΓ\nϵ\n2 2\n2 )2 , (29)\ntheinputone:\nwhere f = f =1.0,ϵ =−ϵ =2.0,Γ =Γ =0.5,δ=10−4,\nR i =100 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ∆ G G i i (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) =100 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) G i − G G˜ i i [Aˆ] (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) , (27) σ Gr = een 1 ’ 0 s − 1 f 3 u , n a c n 2 t d io β n, = w 5 h . i 0 c 1 . h T c h o e n 2 t s a y in n s th 1 e 0 ti 0 c 0 1 d i a m ta ag i 2 s in im ar a y g t i i n m a e ry s - l t i i c m e e s"
  },
  {
    "id": 1273,
    "text": "6\n(a) (b) (c)\nA A A\n11 22 12\n(d) (e) (f)\nA A A\n11 22 12\n(g) (h) (i)\nA A A\n11 22 12\nFIG. 3. Analytic continuations of two-band matrix-valued Green’s functions. For analytic continuations for the diagonal elements of the\nGreen’sfunction, S andS areusedinthecalculations. Whilefortheoff-diagonalelementsoftheGreen’sfunctions, S± andS± are\nSJ BR SJ BR\nemployed.IntheMaxEnt-BRsimulations,thepreblurtrickisalwaysapplied,andtheblurparameterbisfixedto0.2.Thedefaultmodelsfor\noff-diagonalcalculationsareconstructedbyA andA .Seemaintextformoredetails.(a)-(c)θ=0.1;(d)-(f)θ=0.5;(g)-(i)θ=0.9.\n11 22\n(N = 1000). TheblurparameterfortheMaxEnt-BRmethod tendstofitthenoiseinG(τ)orG(iω ).(iii)Information-fitting\nn\nis0.30. region. χ2increaseswiththeincrementofα. Itiscomparable\nThe analytic continuation results are shown in Fig. 2. For withαS. Theoptimalαparameterislocatedatthecrossover\ncase (i), it is clear that the exact spectrum and Matsubara betweenthenoise-fittingregionandtheinformation-fittingre-"
  },
  {
    "id": 1274,
    "text": "case (i), it is clear that the exact spectrum and Matsubara betweenthenoise-fittingregionandtheinformation-fittingre-\nGreen’sfunctionarewellreproducedbyboththeMaxEnt-SJ gion.ForS andS ,theirχ2(α)curvesalmostoverlapatthe\nBR SJ\nmethod and the MaxEnt-BR method [see Fig. 2(a) and (c)]. defaultmodelregionandthenoise-fittingregion,butdifferat\nFor case (ii), the MaxEnt-SJ method works quite well. The the information-fitting region. It indicates that the optimal α\nMaxEnt-BRmethodcanresolvethegapandthepositionsof parametersforS arelargerthanthoseforS .\nSJ BR\nthe two peaks accurately. However, it slightly overestimates\ntheir heights. We also observe that the difference between\nG(τ) andG˜(τ) becomes relatively apparent in the vicinity of\nτ = β/2 [see Fig. 2(d) and (f)]. Figure 2(b) and (e) plot C. Matrix-valuedGreen’sfunctions\nlog (χ2)asafunctionoflog (α). Theplotscanbesplitinto\n10 10\nthreedistinctregions[53]: (i)Defaultmodelregion(α→∞). Next, the analytic continuations of matrix-valued Green’s"
  },
  {
    "id": 1275,
    "text": "log (χ2)asafunctionoflog (α). Theplotscanbesplitinto\n10 10\nthreedistinctregions[53]: (i)Defaultmodelregion(α→∞). Next, the analytic continuations of matrix-valued Green’s\nχ2 isrelativelyflatandgoestoitsglobalmaximum. Theen- functionsareexamined. Weconsideratwo-bandmodel. The\ntropictermαS ismuchlargerthanχ2. Theobtainedspectrum initialspectralfunctionisadiagonalmatrix:\nA(ω)resemblesthedefaultmodel D(ω). (ii)Noise-fittingre-\n(cid:34) (cid:35)\ngion (α → 0). χ2 approaches its global minimum, but it is A′ (ω) 0\nA′(ω)= 11 . (30)\nlarger than αS. In this region, the calculated spectrum A(ω) 0 A′ (ω)\n22"
  },
  {
    "id": 1276,
    "text": "7\nHereA′ (ω)andA′ (ω)areconstructedbyusingEq.(29).For\n11 22 1.0\nA′ (ω),theparametersare: f = f = 0.5,ϵ = 1.0,ϵ = 2.0,\n11 1 2 1 2\nΓ = 0.20, and Γ = 0.70. For A′ (ω), the parameters are:\n1 2 22\nf = f =0.5,ϵ =−1.0,ϵ =−2.1,Γ =0.25,andΓ =0.60. 0.8\n1 2 1 2 1 2\nThe true spectral function is a general matrix with non-zero\noff-diagonalcomponents. Itreads:\n0.6\n(cid:34) (cid:35)\nA (ω) A (ω)\nA(ω)= 11 12 . (31)\nA (ω) A (ω)\n21 22\n0.4\nThediagonalmatrixA′(ω)isrotatedbyarotationmatrixRto\ngenerateA(ω). TherotationmatrixRisdefinedasfollows:\n0.2\n(cid:34) (cid:35)\ncosθ sinθ\nR= , (32)\n−sinθ cosθ\n0.0\n6 4 2 0 2 4 6\nω\nwhere θ denotes the rotation angle. In the present work, we\nconsiderthreedifferentrotationangles. Theyare: (i)θ =0.1,\n(ii) θ = 0.5, and (iii) θ = 0.9. Then A(ω) is used to con-\nstruct the matrix-valued Green’s function G(iω ) by using\nn\nEq. (4). The essential parameters are: δ = 0.0, σ = 10−4,\nandβ = 40.0. Theinputdatacontains50Matsubarafrequen-"
  },
  {
    "id": 1277,
    "text": "struct the matrix-valued Green’s function G(iω ) by using\nn\nEq. (4). The essential parameters are: δ = 0.0, σ = 10−4,\nandβ = 40.0. Theinputdatacontains50Matsubarafrequen-\ncies. For analytic continuations of the diagonal elements of\nG(iω ), the default models are Gaussian-like. However, for\nn\ntheoff-diagonalelements,thedefaultmodelsarequitediffer-\n√\nent. Theyareevaluatedby D (ω) = A (ω)A (ω),where\n12 11 22\nA (ω)andA (ω)arethecalculatedspectralfunctionsfordi-\n11 22\nagonal elements [55, 60]. This means that we have to carry\nout analytic continuations for the diagonal elements at first,\nandthenusethediagonalspectratopreparethedefaultmod-\nelsfortheoff-diagonalelements.FortheMaxEnt-BRmethod,\ntheprebluralgorithmisusedtosmooththespectra. Theblur\nparameterisfixedto0.2.\nThe simulated results are illustrated in Fig. 3. For the\ndiagonal spectral spectral functions (A and A ), both the\n11 22\nMaxEnt-SJ and MaxEnt-BR methods can accurately resolve"
  },
  {
    "id": 1278,
    "text": "parameterisfixedto0.2.\nThe simulated results are illustrated in Fig. 3. For the\ndiagonal spectral spectral functions (A and A ), both the\n11 22\nMaxEnt-SJ and MaxEnt-BR methods can accurately resolve\nthe peaks that are close to the Fermi level. However, for the\nhigh-energy peaks, the MaxEnt-BR method tends to overes-\ntimate their heights and yield sharper peaks [see the peaks\naroundω=2.0inFig.3(a),(d),and(g)]. Fortheoff-diagonal\nspectralfunctions,only A isshown,since A isequivalent\n12 21\nto A . We observe that the major features of A are well\n12 12\ncaptured by both the MaxEnt-SJ and MaxEnt-BR methods.\nUndoubtedly, the MaxEnt-SJ method works quite well in all\ncases. The MaxEnt-BR method exhibits good performance\nwhentherotationangleissmall[θ = 0.1,seeFig.3(c)]. But\nsomewigglesemergearoundω=±2.0whentherotationan-\ngle is large [θ = 0.5 and θ = 0.9, see Fig. 3(f) and (i)]. We\ntestmorerotationangles(0.0 < θ < 2.0). Itseemsthatthese\nwiggleswon’tbeenhancedunlesstheblurparameterbisde-\ncreased."
  },
  {
    "id": 1279,
    "text": "gle is large [θ = 0.5 and θ = 0.9, see Fig. 3(f) and (i)]. We\ntestmorerotationangles(0.0 < θ < 2.0). Itseemsthatthese\nwiggleswon’tbeenhancedunlesstheblurparameterbisde-\ncreased.\nIV. DISCUSSIONS\nIn previous section, analytic continuations for single-band\nGreen’sfunctionsandmatrix-valuedGreen’sfunctionsbyus-\ning the MaxEnt-BR method have been demonstrated. How-\never,therearestillsomeimportantissuesthatneedtobeclar-\n)ω(A\nb=0.0\nb=0.1\nb=0.2\nb=0.3\nb=0.4\nExact\nFIG.4. Testofthepreblurtrick. Asingle-bandGreen’sfunctionis\ntreated. TheexactspectralfunctionisgeneratedbyusingEq.(29).\nThemodelparametersarethesameasthoseusedinSec.IIIB.Only\ntheanalyticcontinuationresultsobtainedbytheMaxEnt-BRmethod\nareshown.\n0.10\n0.05\n0.00\n0.05\n0.10\n6 4 2 0 2 4 6\nω\n)ω(A\nb=0.0\nb=0.1\nb=0.2\nExact\nFIG. 5. Test of the preblur trick. A matrix-valued Green’s func-\ntionistreatedviathepositive-negativeentropyapproach.Theinitial\nspectralfunctionisgeneratedbyusingEq.(29). Themodelparam-"
  },
  {
    "id": 1280,
    "text": "b=0.2\nExact\nFIG. 5. Test of the preblur trick. A matrix-valued Green’s func-\ntionistreatedviathepositive-negativeentropyapproach.Theinitial\nspectralfunctionisgeneratedbyusingEq.(29). Themodelparam-\netersarethesameasthoseusedinSec.IIIC.Therotationangleθis\n0.1. Onlytheoff-diagonalspectralfunctions(A )obtainedbythe\n12\nMaxEnt-BRmethodareshown.\nified. Inthissection,wewouldliketofurtherdiscussthepre-\nbluralgorithm,theauxiliaryGreen’salgorithm,andthenoise\ntolerancefortheMaxEnt-BRmethod.\nA. Effectofpreblur\nThebenchmarkresultsshowninSectionIIIimplythatthe\nMaxEnt-BR method has a tendency to generate sharp peaks\norsmallfluctuationsinthehigh-energyregionsofthespectra.\nTheprebluralgorithmishelpfultoalleviatethisphenomenon.\nInthecontextofanalyticcontinuation, theprebluralgorithm"
  },
  {
    "id": 1281,
    "text": "8\n(a) (c) (e)\n(b) (d) (f)\nFIG.6. Analyticcontinuationsofmatrix-valuedGreen’sfunctionsviatheauxiliaryGreen’sfunctionapproach. Themodelparametersare\nthesameasthoseusedinSec.IIIC.FortheMaxEnt-BRsimulations,thepreblurtrickisalwaysapplied,andtheblurparameterbisfixedto\n0.2. Onlytheresultsfortheoff-diagonalelementsarepresented(A ). TheupperpanelsshowthespectralfunctionsfortheauxiliaryGreen’s\n12\nfunctions[seeEq.(35)]. Thelowerpanelsshowtheoff-diagonalspectralfunctionsevaluatedbyEq.(36). (a)-(b)θ = 0.1; (c)-(d)θ = 0.5;\n(e)-(f)θ=0.9.\nwasintroducedbyKrabergeretal.[55]. Thekernelfunction 0.2. If b is further increased, it is difficult to obtain a stable\nK(iω ,ω) [see Eq. (6)] is “blurred” by using the following solution. Here, we should emphasize that these unphysical\nn\nexpression: featuresseeninthespectraarenotuniquefortheMaxEnt-BR\nmethod. Actually, a similar tendency was already observed\n(cid:90) ∞"
  },
  {
    "id": 1282,
    "text": "n\nexpression: featuresseeninthespectraarenotuniquefortheMaxEnt-BR\nmethod. Actually, a similar tendency was already observed\n(cid:90) ∞\nK (iω ,ω)= dω′ K(iω ,ω′)g (ω−ω′). (33) in the early applications of the MaxEnt-SJ method in image\nb n n b\nprocessing tasks. In order to address this problem, Skilling\n−∞\nsuggested that the spectrum can be expressed as a Gaussian\nHere, K b (iω n ,ω) is the blurred kernel, which is then used in convolution: A=g ⋆h,wherehisa“hidden”function[79].\nEq.(8)toevaluatetheχ2-term. g (ω)isaGaussianfunction: b\nb Thentheentropyisevaluatedfromh(ω),insteadof A(ω). In\nfact,Skilling’sapproachisequivalenttothepreblurtrick.\nexp(−ω2/2b2)\ng (ω)= √ , (34)\nb\n2πb\nB. AuxiliaryGreen’sfunctions\nwherebistheblurparameter.\nIn Figures 4 and 5, we analyze the effects of the preblur\nAs stated before, in addition to the positive-negative en-\ntrick for two typical scenarios: (i) Positive spectral function\ntropy method, the auxiliary Green’s algorithm can be used"
  },
  {
    "id": 1283,
    "text": "As stated before, in addition to the positive-negative en-\ntrick for two typical scenarios: (i) Positive spectral function\ntropy method, the auxiliary Green’s algorithm can be used\nwith two separate Gaussian peaks. (ii) Complicated spec-\nto continue the off-diagonal elements of the Green’s func-\ntralfunctionforoff-diagonalGreen’sfunction. Notethatthe\ntions [63–68]. Its idea is quite simple. At first, an auxiliary\nmodel parameters for generating the exact spectra are taken\nGreen’sfunctionisconstructedasfollows:\nfromSectionsIIIBandIIIC,respectively.Itisevidentthatthe\nMaxEnt-BRmethodwithoutthepreblurtrick(b=0)hastrou- G (iω )=G (iω )+G (iω )+2G (iω ). (35)\naux n 11 n 22 n 12 n\nble resolving the spectra accurately. It usually favors sharp\npeaks(seeFig.4). Sometimesitmayleadtoundesirablearti- Supposedthat A aux (ω)isthecorrespondingspectralfunction\nfacts,suchasthesidepeaksaroundω = ±1.5inFig.5. The forG aux (iω n ). ItiseasytoprovethatA aux (ω)ispositivesemi-"
  },
  {
    "id": 1284,
    "text": "facts,suchasthesidepeaksaroundω = ±1.5inFig.5. The forG aux (iω n ). ItiseasytoprovethatA aux (ω)ispositivesemi-\npreblur algorithm can remedy this problem to some extent. definiteness. So,itissafetoperformanalyticcontinuationfor\nThe major peaks are smoothed, and the artificial side peaks G aux (iω n )byusingthetraditionalMaxEnt-SJorMaxEnt-BR\nare suppressed upon increasing b. But it’s not the case that method. Next, the off-diagonal spectral function A 12 (ω) can\nbigger is always better. There is an optimal b. For case (i), beevaluatedasfollows:\nthe optimal b is 0.3. A larger b (b > 0.3) will destroy the A (ω)−A (ω)−A (ω)\ngap, inducing a metallic state. For case (ii), the optimal b is A 12 (ω)= aux 11 22 . (36)\n2"
  },
  {
    "id": 1285,
    "text": "9\n(a) (b) (c)\n(d) (e) (f)\n(g) (h)\nFIG.7. RobustnessoftheMaxEntmethodwithrespecttothenoisyMatsubaradata. Herewetreattheoff-diagonalelements(G )ofthe\n12\ntwo-bandmatrix-valuedGreen’sfunctionsonlyviathepositive-negativeentropymethod.Themodelparametersarethesameasthoseusedin\nSec.IIIC.Therotationangleθis0.5. TheblurparameterfortheMaxEnt-BRmethodisfixedto0.2. (a)-(g)Off-diagonalspectralfunctions\n(A )extractedfromnoisyMatsubaradata.δdenotesthenoiseleveloftheinputdata.Itvariesfrom10−8(tinynoise)to10−2(largenoise).(h)\n12\nIntegratedrealaxiserrorasafunctionofδ.Itmeasuresthedeviationsfromtheexactspectralfunction,seeEq.(37).\nThis algorithm is not restricted to the particle-hole symmet- θ = 0.5andθ = 0.9. Whentherotationangleismoderateor\nric case, as assumed in Ref. [64]. Here, we employ the two- large,thespectraobtainedbytheMaxEnt-SJmethodarewell\nband’s example presented in Section IIIC again to test the consistent with the exact solutions. By using the MaxEnt-"
  },
  {
    "id": 1286,
    "text": "band’s example presented in Section IIIC again to test the consistent with the exact solutions. By using the MaxEnt-\ncombination of the auxiliary Green’s function algorithm and BR method, though the fluctuations in high-energy regions\nthe MaxEnt-BR method (and the MaxEnt-SJ method). The aregreatlysuppressed,smalldeviationsfromtheexactspectra\nmodel and computational parameters are kept, and the ana- stillexist. Overall,theauxiliaryGreen’sfunctionalgorithmis\nlyticcontinuationresultsareplottedinFig.6. inferiortothepositive-negativeentropymethodintheexam-\nplesstudied. Especiallywhentherotationangleissmall,the\nThreerotationanglesareconsideredinthesimulations. We auxiliaryGreen’sfunctionalgorithmusuallyfails,irrespective\nfind the auxiliary Green’s function algorithm is numerically ofwhichformofentropyisadopted.\nunstable.(i)θ=0.1.Whentherotationangleissmall,theam-\nplitude(absolutevalue)ofA (ω)issmall. BoththeMaxEnt-\n12\nSJmethodandtheMaxEnt-BRmethodcanresolvethemajor"
  },
  {
    "id": 1287,
    "text": "unstable.(i)θ=0.1.Whentherotationangleissmall,theam-\nplitude(absolutevalue)ofA (ω)issmall. BoththeMaxEnt-\n12\nSJmethodandtheMaxEnt-BRmethodcanresolvethemajor\npeaks around ω = ±1.0. But they will produce apparent os- C. Robustnesswithrespecttonoisyinputdata\ncillations at higher energies. Increasing the b parameter fur-\nther could lead to wrong peaks. It seems that these unphys- Analytic continuation is commonly used on noisy Monte\nical features are likely due to the superposition of errors in Carlo data. The precision of the Monte Carlo data, which\nA (ω),A (ω),andA (ω).TheperformanceoftheMaxEnt- strongly depends on the sampling algorithm and the estima-\naux 11 22\nBRmethodisworsethanthatoftheMaxEnt-SJmethod. (ii) tor used, is rarely better than 10−5 [3–5]. The distributions"
  },
  {
    "id": 1288,
    "text": "10\nof inherent errors in the Monte Carlo data are often Gaus- functions (such as optical conductivity and spin susceptibil-\nsian. This poses a strict requirement for the noise tolerance ity) [80, 81], and frequency-dependent transport coefficients\nof the analytic continuation methods. Previous studies have withnon-positivespectralweight[65,66],etc. Furtherinves-\ndemonstrated that the MaxEnt-SJ method is robust to noise. tigationsarehighlydesirable.\nHere,wewouldliketoexaminetherobustnessoftheMaxEnt- Finally,theMaxEnt-BRmethod,togetherwiththeMaxEnt-\nBRmethodwithrespecttonoisyMatsubaradata. Werecon- SJmethod,hasbeenintegratedintotheopensourcesoftware\nsidertheanalyticcontinuationoftheoff-diagonalelementsof package ACFlow [58], which may be useful for the analytic\nmatrix-valued Green’s functions. As mentioned above, the continuationcommunity.\nsyntheticMatsubaradataforthiscaseisassumedtobenoise-\nless(δ = 0.0). NowthenoiseismanuallyaddedbyEq.(26)"
  },
  {
    "id": 1289,
    "text": "matrix-valued Green’s functions. As mentioned above, the continuationcommunity.\nsyntheticMatsubaradataforthiscaseisassumedtobenoise-\nless(δ = 0.0). NowthenoiseismanuallyaddedbyEq.(26)\nandthenoiselevelδischangedfrom10−8 to10−2. Theother\nACKNOWLEDGMENTS\ncomputationalparametersarethesameasthoseusedinSec-\ntion IIIC. In order to estimate the sensitivity of the analytic\ncontinuationmethodtothenoisydata,anewquantity,namely This work is supported by the National Natural Science\nintegratedrealaxiserror,isintroduced: FoundationofChina(No.12274380andNo.11934020),and\ntheCentralGovernmentGuidanceFundsforLocalScientific\n(cid:90)\n(cid:12) (cid:12) andTechnologicalDevelopment(No.GUIKEZY22096024).\nerr(δ)= dω (cid:12) (cid:12)A(ω)−Aˆ\nδ\n(ω)(cid:12) (cid:12). (37)\nHere, Aˆ δ (ω)meansthereconstructed(optimal)spectralfunc- AppendixA:Goodness-of-fitfunctional\ntionunderthegivennoiselevelδ.\nInFigure7(a)-(g),theconvergenceofthespectralfunctions\nGiven the spectral function A(ω), the Matsubara Green’s"
  },
  {
    "id": 1290,
    "text": "tionunderthegivennoiselevelδ.\nInFigure7(a)-(g),theconvergenceofthespectralfunctions\nGiven the spectral function A(ω), the Matsubara Green’s\nbytheMaxEnt-BRmethodforsimulatedGaussianerrorswith functionG˜[A] could be reconstructed by using the following\nvaryingmagnitudeisshown. Whenδ=10−2,themainpeaks\nequation(the˜symbolisusedtodistinguishthereconstructed\nat ω ≈ ±1.0 are roughly reproduced, while the side peaks\nGreen’sfunctionfromtheinputGreen’sfunction):\nat ω ≈ 2.0 are completely smeared out. When δ = 10−3,\nthemajorcharacteristicsoftheoff-diagonalspectrumaresuc- (cid:90)\ncessfully captured. As the simulated errors decrease further G˜ n [A]= dωK(iω n ,ω)A(ω), (A1)\n(δ < 10−3), theMaxEnt-BRmethodrapidlyconvergestothe\nexact solution. For comparison, the results for the MaxEnt-\nwhere K(iω ,ω) denotes the kernel function, and n is the\nn\nSJ method are also presented. It is evident that the MaxEnt-\nindex for Matsubara frequency. For the sake of simplicity,"
  },
  {
    "id": 1291,
    "text": "where K(iω ,ω) denotes the kernel function, and n is the\nn\nSJ method are also presented. It is evident that the MaxEnt-\nindex for Matsubara frequency. For the sake of simplicity,\nSJ method is less robust than the MaxEnt-BR method when\nEq.(A1)isreformulatedintoitsdiscretizationform:\nthe noise level is moderate. It fails to resolve the satellite\npeaks around ω = ±2.0. Only when δ ≤ 10−6, the MaxEnt- (cid:88)\nG˜ [A]= K A ∆ , (A2)\nSJ method can recover the exact spectrum. Figure 7(h) ex- n nm m m\nhibitstheintegratederrorerr(δ). When10−6 < δ < 10−3,the m\nMaxEnt-BRmethodexhibitsbetterrobustnesswithrespectto where K ≡ K(iω ,ω ), A ≡ A(ω), and ∆ means the\nnm n m i i i\nnoisethantheMaxEnt-SJmethod.\nweightofthemeshatrealaxis.\nThegoodness-of-fitfunctionalχ2[A]measuresthedistance\nbetween the input Green’s function G and the reconstructed\nV. CONCLUDINGREMARKS Green’sfunctionG˜[A]. Itsexpressionisasfollows:"
  },
  {
    "id": 1292,
    "text": "Thegoodness-of-fitfunctionalχ2[A]measuresthedistance\nbetween the input Green’s function G and the reconstructed\nV. CONCLUDINGREMARKS Green’sfunctionG˜[A]. Itsexpressionisasfollows:\naly I t n ic su co m n m tin ar u y a , ti w o e ns ex o t f e i n m d a t g h i e n a a p ry p - l t i i c m at e io G n r s e c e o n p ’s e f o u f n S ct B io R n t s o a a n n d - χ2[A]= (cid:88)N (G n −G˜ n [A])2 (A3)\nσ2\nMatsubaraGreen’sfunctionsinquantummany-bodyphysics n=1 n\nand condense matter physics. It is further generalized to the\nHerewejustassumethatthereare N datapoints,andσ de-\nform of positive-negative entropy to support analytic contin- n\nnotesthestandardderivative(i.e.,theerrorbar)ofG . Substi-\nuation of matrix-valued Green’s function, in which the posi- n\ntutingEq.(A2)intoEq.(A3),thenwearrive:\ntive semi-definiteness of the spectral function is broken. We\ndemonstratethattheMaxEnt-BRmethod,inconjunctionwith"
  },
  {
    "id": 1293,
    "text": "tutingEq.(A2)intoEq.(A3),thenwearrive:\ntive semi-definiteness of the spectral function is broken. We\ndemonstratethattheMaxEnt-BRmethod,inconjunctionwith\ntheprebluralgorithmandthepositive-negativeentropyalgo- χ2[A]= (cid:88)N (G n − (cid:80) m K nm A m ∆ m )2 . (A4)\nrithm,iscapableofcapturingtheprimaryfeaturesofthediag- σ2\nonalandoff-diagonalspectralfunctions,eveninthepresence\nn=1 n\nof moderate levels of noise. Overall, its performance is on\nThefirstderivativeofχ2[A]withrespecttoA reads:\ni\nparwiththatoftheMaxEnt-SJmethodintheexamplesstud-\nied. Possible applications of the MaxEnt-BR method in the  \nf\nf\nu\nu\nt\nn\nu\nc\nr\nt\ne\nio\ni\nn\nn\ns\nc\na\nlu\nn\nd\nd\ne\nse\na\nl\nn\nf-\na\ne\nl\nn\ny\ne\nti\nr\nc\ngy\nco\nfu\nn\nn\nti\nc\nn\nt\nu\nio\na\nn\nti\ns\no\n[\nn\n6\ns\n4\no\n,6\nf\n8\na\n]\nn\n,\no\nb\nm\nos\na\no\nl\nn\no\ni\nu\nc\ns\nre\nG\ns\nr\np\ne\no\ne\nn\nn\ns\n’\ne\ns\n∂χ\n∂\n2\nA\n[A\ni\n]\n=2\n(cid:88)\nn\nN\n=1\nK\nσ\nni\n2 n\n∆\ni\n (cid:88)\nm\nK\nnm\nA\nm\n∆\nm\n−G\nn\n\n. (A5)"
  },
  {
    "id": 1294,
    "text": "11\nAppendixB:Shannon-Jaynesentropy Itisactually:\n∂S [A] 1∂χ2[A]\nInthisappendix,thetechnicaldetailsforS SJ ,S S ± J ,andthe α ∂ S A J − 2 ∂A =0. (B9)\nMaxEnt-SJmethodarereviewed. Mostoftheequationspre- i i\nsentedinthisappendixhavebeenderivedinRefs.[18],[55],\nSubstitutingEqs.(B3)and(A5)intotheaboveequation:\nand[60]. Werepeatthemathematicalderivationheresothat\nthispaperisself-contained.\nα∆\ni\nlog\n(cid:32)\nD\nA\ni\n(cid:33)\n+\n(cid:88)N K\nσ\nni\n2\n∆\ni\n  (cid:88)\nK\nnm\nA\nm\n∆\nm\n−G\nn\n \n=0. (B10)\ni n=1 n m\n1. Entropy\nEliminating∆:\ni\nS\nThe\n[A\nS\n]\nha\n=\nnn\n(cid:90)\non\nd\n-\nω\nJay\n(cid:34) A\nne\n(ω\ns\n)\nen\n−\ntr\nD\nop\n(ω\ny\n)\nis\n−\nd\nA\nefi\n(ω\nne\n)\nd\nlo\na\ng\ns\n(cid:32)\nf\nA\nol\n(\nl\nω\now\n) (cid:33)\ns\n(cid:35)\n:\n, (B1)\nαlog\n(cid:32)\nD\nA\ni\ni\n(cid:33)\n+\n(cid:88)\nn\nN\n=1\nK\nσ\nn\n2 n\ni\n  (cid:88)\nm\nK\nnm\nA\nm\n∆\nm\n−G\nn\n \n=0. (B11)\nSJ D(ω)\nSubstitutingEqs.(B6)and(B5)intoEq.(B11):\nThediscretizationformofEq.(B1)is: (cid:88)\nα V u +\n(cid:34) (cid:32) (cid:33)(cid:35) im m\n(cid:88) A"
  },
  {
    "id": 1295,
    "text": "m\nK\nnm\nA\nm\n∆\nm\n−G\nn\n \n=0. (B11)\nSJ D(ω)\nSubstitutingEqs.(B6)and(B5)intoEq.(B11):\nThediscretizationformofEq.(B1)is: (cid:88)\nα V u +\n(cid:34) (cid:32) (cid:33)(cid:35) im m\n(cid:88) A\nS [A]= ∆ A −D −A log i . (B2) m\nThefirstderi\nS\nv\nJ\nativeof\ni\nS SJ [\ni\nA]\ni\nwith\ni\nrespe\ni\ncttoA\nD\ni\ni\nreads:\n(cid:88)\nn\nN\n=1 σ\n1\n2 n\n(cid:88)\nm\nU\nnm\nξ\nm\nV\nim\n  (cid:88)\nl\nK\nnl\nA\nl\n∆\nl\n−G\nn\n \n=0.(B12)\n(cid:32) (cid:33) (cid:80)\n∂S SJ [A] =−∆ log A i . (B3) Now m V im can be removed from the two terms in the left-\n∂A i D handside. Finallyweget:\ni i\n \n2. Parameterizationofspectralfunction\nαu\nm\n+\n(cid:88)N\nσ\n1\n2\nξ\nm\nU\nnm\n (cid:88)\nK\nnl\nA\nl\n∆\nl\n−G\nn\n\n=0. (B13)\nn=1 n l\nA singular value decomposition can be readily performed SinceA dependson{u }aswell,Eq.(B13)isactuallyanon-\nl m\nforthekernelfunctionK: linear equation about {u }. In the ACFlow package [58], the\nm\nNewton’smethodisadoptedtosolveit. Sowehavetoevalu-\nK =UξVT, (B4)\natethefollowingtwovariablesandpassthemtotheNewton’s"
  },
  {
    "id": 1296,
    "text": "m\nNewton’smethodisadoptedtosolveit. Sowehavetoevalu-\nK =UξVT, (B4)\natethefollowingtwovariablesandpassthemtotheNewton’s\nalgorithm[75]:\nwhere U and V are column-orthogonal matrices and ξ is the\nvectorofsingularvalues. So,thematrixelementofK reads:  \nK =\n(cid:88)\nU ξ V . (B5)\nf\nm\n=αu\nm\n+ξ\nm\n(cid:88)N\nσ\n1\n2\nU\nnm\n (cid:88)\nK\nnl\nA\nl\n∆\nl\n−G\nn\n\n, (B14)\nni nm m im n=1 n l\nm\nThecolumnsofVcanbeunderstoodasbasisfunctionsforthe\n∂f (cid:88)N 1 (cid:88)\nspectralfunction. Thatistosaytheyspanaso-calledsingular J = m =αδ +ξ U K A∆V , (B15)\nvaluespace. Andinthesingularvaluespacethespectrumcan mi ∂u i mi m n=1 σ2 n nm l nl l l li\nbeparameterizedthrough:\nwhere J canbeconsideredasaJacobianmatrix. Notethat\n  mi\nA\nl\n= D\nl\nexp\n (cid:88)\nV\nlm\nu\nm\n\n. (B6)\nEq\nS\n.\ni\n(\nn\nB\nc\n7\ne\n)\nt\ni\nh\ns\ne\na\nc\np\na\np\nl\nl\nc\ni\nu\ned\nla\nt\nt\no\nio\nd\nn\ne\ns\nr\nf\niv\no\ne\nr\nE\nf\nq.\na\n(\nn\nB\nd\n1\nJ\n5).\narequitecomplicated\nm mi\nm and time-consuming, the Einstein summation technology is"
  },
  {
    "id": 1297,
    "text": "Eq\nS\n.\ni\n(\nn\nB\nc\n7\ne\n)\nt\ni\nh\ns\ne\na\nc\np\na\np\nl\nl\nc\ni\nu\ned\nla\nt\nt\no\nio\nd\nn\ne\ns\nr\nf\niv\no\ne\nr\nE\nf\nq.\na\n(\nn\nB\nd\n1\nJ\n5).\narequitecomplicated\nm mi\nm and time-consuming, the Einstein summation technology is\nItiseasytoprovethat: usedto improvethecomputational efficiency. Thefollowing\nthree variables should be precomputed and stored during the\n \n∂\n∂\nA\nu\nl = D\nl\nexp  (cid:88) V\nlm\nu\nm\n V\nli\n= A\nl\nV\nli\n. (B7) initializationstage:\ni m (cid:88)N\n1\nB = ξ U G , (B16)\nm σ2 m nm n\nn=1 n\n3. Newton’smethod\n(cid:88) 1\nthe\nN\ns\ne\nta\nx\nt\nt\ni\n,\no\nw\nna\ne\nry\nw\nc\nil\no\nl\nn\nd\nd\ne\ni\nr\nti\ni\no\nv\nn\ne\n[\nth\nse\ne\ne\ne\na\nq\nl\nu\nso\nat\nE\nio\nq\nn\n.\ns\n(1\nfo\n1\nr\n)]\n{\n:\nu m }. Let us recall W ml =\npn\nσ2\nn\nU nm ξ m U np ξ p V lp ∆ l D l , (B17)\n∂Q[A]\n∂A i =0. (B8) W mli =W ml V li . (B18)"
  },
  {
    "id": 1298,
    "text": "12\nClearly,thethreevariablesonlydependontheinputGreen’s However, the ordinary MaxEnt method is only rigorous for\nfunction G, the default model D, and the singular value de- non-negative spectrum [18]. In order to remedy this prob-\ncompositionofthekernelfunctionK. Now f and J canbe lem, one could imagine that the off-diagonal spectral func-\nm mi\nreformulatedintermsofthem: tions originate from a subtraction of two artificial positive\n(cid:88) functions[55],\nf =αu + W w −B , (B19)\nm m ml l m\nl A= A +−A−, (B22)\n(cid:88)\nJ mi =αδ mi + W mli w l , (B20) A− = A +−A. (B23)\nl\nAssuming the independence of A+ and A−, the resulting en-\nwhere\ntropyS [A+,A−]isthesumoftherespectiveentropies:\n  SJ\nw\nl\n=exp\n (cid:88)\nV\nlm\nu\nm\n\n. (B21)\nS [A + ,A−]=\n(cid:90)\ndω\n(cid:34)\nA +−D−A + log\n(cid:32) A+(cid:33)(cid:35)\n+\nm SJ D\n(cid:90) (cid:34) (cid:32) A− (cid:33)(cid:35)\ndω A−−D−A−log . (B24)\n4. Positive-negativeentropy D"
  },
  {
    "id": 1299,
    "text": "\n. (B21)\nS [A + ,A−]=\n(cid:90)\ndω\n(cid:34)\nA +−D−A + log\n(cid:32) A+(cid:33)(cid:35)\n+\nm SJ D\n(cid:90) (cid:34) (cid:32) A− (cid:33)(cid:35)\ndω A−−D−A−log . (B24)\n4. Positive-negativeentropy D\nFor matrix-valued Green’s function, the spectral functions Thus, S [A+,A−] is called the positive-negative entropy in\nSJ\noftheoff-diagonalcomponentscouldexhibitnegativeweight. theliterature[55,60]. WeatfirsteliminateA−\n(cid:90) (cid:34) (cid:32) A+(cid:33)(cid:35) (cid:90) (cid:34) (cid:32) A+−A (cid:33)(cid:35)\nS [A,A + ]= dω A +−D−A + log + dω (A +−A)−D−(A +−A)log , (B25)\nSJ D D\nandwrite: SubstitutingEqs.(B31)and(B32)intoEq.(B24)toeliminate\nA+andA−,weobtain:\n1\nQ[A,A + ]=αS [A,A + ]− χ2[A]. (B26)\nSJ 2 (cid:90)  √\nSincewearesearchingforamaximumofQwithrespecttoA\nS\nSJ\n[A + ,A−] = dω\n\nA2+4D2−2D\nandA+,wealsoapply: √ \n∂Q[A,A+] = ∂S SJ [A,A+] =0, (B27) − Alog\n A2+\n2\n4\nD\nD2+A \n. (B33)\n∂A+ ∂A+\nItsdiscretizationformbecomes:"
  },
  {
    "id": 1300,
    "text": "S\nSJ\n[A + ,A−] = dω\n\nA2+4D2−2D\nandA+,wealsoapply: √ \n∂Q[A,A+] = ∂S SJ [A,A+] =0, (B27) − Alog\n A2+\n2\n4\nD\nD2+A \n. (B33)\n∂A+ ∂A+\nItsdiscretizationformbecomes:\ntoeliminateA+. Thefollowingequationsshowsomeinterme-\n\ndiatestep\n(cid:90)\ns:\n(cid:34) (cid:32) A+(cid:33) (cid:32) A+−A (cid:33)(cid:35)\nS\nSJ\n[A + ,A−] =\n(cid:88) (cid:113)\nA2\nm\n+4D2\nm\n−2D\nm\ndω log +log =0, (B28) m\n(cid:112) \nD D\n− A\nm\nlog\n A2\nm\n+\n2\n4\nD\nD2\nm\n+A\nm\n \n∆\nm\n.(B34)\n(cid:32) A+(cid:33) (cid:32) A+−A (cid:33) m\nlog +log =0, (B29) Note that Eq. (B33) is exactly the same with Eq. (19).\nD D\nS [A+,A−]isjustS±[A]. ThefirstderivativeofS [A+,A−]\nSJ SJ SJ\nwithrespecttoA is:\nA+(A+−A) i\nD2 =1. (B30) ∂S SJ [A+,A−] =−∆ log (cid:32) A+ i (cid:33) . (B35)\n∂A i D\nFinally,weobtain: i i\n√\nA2+4D2+A\nA + = , (B31) 5. Parameterizationforpositive-negativespectralfunctions\n2\n√ AccordingtoEq.(B31)andEq.(B32),wefind:\nA2+4D2−A\nA− = . (B32) A + A− = D2. (B36)\n2"
  },
  {
    "id": 1301,
    "text": "13\nInspiredbyEq.(B6),itisnaturallytoparameterizeA+andA−\n2. Parameterizationofspectralfunction\ninthesingularvaluespaceaswell:\n  TheoriginalparameterizationofA[seeEq.(B6)]cannotbe\nA +\ni\n= D\ni\nexp\n (cid:88)\nV\nim\nu\nm\n\n, (B37) usedhere. AnewparameterizationschemeforAisnecessary.\nm Assumedthat\nA− i = D i exp   − (cid:88) V im u m   . (B38) D 1 i − A 1 i = (cid:88) m V im u m , (C4)\nm\nthenwehave:\nHence,\nA i = D i exp   (cid:88) V im u m   −D i exp   − (cid:88) V im u m   (B39) A l = 1−D l (cid:80) D m l V lm u m . (C5)\nm m\nSo,thefirstderivativeofA withrespecttou reads:\nl i\n(cid:32) (cid:33)\n1 ∂A\nA i = D i w i − w i . (B40) ∂u i l = A l A l V li . (C6)\n∂A+\n∂u l = A + l V li . (B41) 3. Newton’smethod\ni\n∂A− Next, wewouldliketoderive f m and J mi fortheBayesian\nl =−A−V . (B42) reconstructionentropy. SubstitutingEq.(C3)intoEq.(B9):\n∂u l li\ni\n∂ ∂ u A =(A + l +A− l )V li = D l (cid:32) w l + w 1 (cid:33) V li . (B43) α∆ i\n(cid:32)\nD\n1\ni − A\n1"
  },
  {
    "id": 1302,
    "text": "l =−A−V . (B42) reconstructionentropy. SubstitutingEq.(C3)intoEq.(B9):\n∂u l li\ni\n∂ ∂ u A =(A + l +A− l )V li = D l (cid:32) w l + w 1 (cid:33) V li . (B43) α∆ i\n(cid:32)\nD\n1\ni − A\n1\ni\n(cid:33)\n+\n(cid:88)\nn\nN\n=1\nK\nσ ni 2 n\n∆\ni\n  (cid:88)\nm K nm A m ∆ m −G n\n \n=0. (C7)\ni l\nEliminating∆:\nByusingtheEinsteinsummationnotationsasdefinedinSec- i\ntionB3,wei\nf\nm\nm\nm\n=\ne\nα\nd\nu\nia\nm\nte\n+\nly\n(cid:88)\nobt\nW\nai\nm\nn\nl\n:\n(cid:32) w l − w 1 (cid:33) −B m , (B44) α\n(cid:32)\nD\n1\ni − A\n1\ni\n(cid:33)\n+\n(cid:88)\nn\nN\n=1\nK\nσ n 2 n i\n  (cid:88)\nm K nm A m ∆ m −G n\n \n=0. (C8)\nl\nl\nSubstitutingEq.(C4)intoEq.(C8):\n(cid:32) (cid:33)\n(cid:88) 1 (cid:88)\nJ =αδ + W w + . (B45) α V u +\nmi mi mli l w im m\nl l m\n \nAppendixC:Bayesianreconstructionentropy\n(cid:88)\nn\nN\n=1 σ\n1\n2 n\n(cid:88)\nm\nU\nnm\nξ\nm\nV\nim\n (cid:88)\nl\nK\nnl\nA\nl\n∆\nl\n−G\nn\n\n=0. (C9)\n(cid:80)\nInthisappendix,thetechnicaldetailsforS ,S± ,andthe Eliminating m V im again:\nBR BR\nMaxEnt-BRmethodarediscussed.  \nαu\nm\n+\n(cid:88)N"
  },
  {
    "id": 1303,
    "text": "V\nim\n (cid:88)\nl\nK\nnl\nA\nl\n∆\nl\n−G\nn\n\n=0. (C9)\n(cid:80)\nInthisappendix,thetechnicaldetailsforS ,S± ,andthe Eliminating m V im again:\nBR BR\nMaxEnt-BRmethodarediscussed.  \nαu\nm\n+\n(cid:88)N\nσ\n1\n2\nξ\nm\nU\nnm\n (cid:88)\nK\nnl\nA\nl\n∆\nl\n−G\nn\n\n=0. (C10)\nn=1 n l\n1. Entropy\nClearly,Eq.(C10)isthesamewithEq.(B13).Thus,theequa-\nTheBayesianreconstructionentropyreads: tionsfor f m andJ mi areasfollows:\n(cid:90) (cid:34) (cid:32) (cid:33)(cid:35)  \nS BR [A]= dω 1− D A( ( ω ω ) ) +log D A( ( ω ω ) ) . (C1) f m =αu m +ξ m (cid:88)N σ 1 2 U nm  (cid:88) K nl A l ∆ l −G n  , (C11)\nn=1 n l\nItsdiscretizationformis:\n(cid:34) (cid:32) (cid:33)(cid:35)\nS BR [A]= (cid:88) i ∆ i 1− D A i i +log D A i i . (C2) J mi = ∂ ∂u f m i =αδ mi +ξ m (cid:88) n N =1 σ 1 2 n U nm (cid:88) l K nl A l A l ∆ l V li . (C12)\nItsfirstderivativewithrespecttoA is:\ni\nNote that Eq. (C11) is the same with Eq. (B14). However,\n(cid:32) (cid:33)\n∂S [A] 1 1 Eq.(C12)differsfromEq.(B15)byafactorA inthesecond"
  },
  {
    "id": 1304,
    "text": "ItsfirstderivativewithrespecttoA is:\ni\nNote that Eq. (C11) is the same with Eq. (B14). However,\n(cid:32) (cid:33)\n∂S [A] 1 1 Eq.(C12)differsfromEq.(B15)byafactorA inthesecond\nBR =−∆ − . (C3) l\n∂A i D A term of the right-hand side. To derive Eq. (C12), Eq. (C6)\ni i i"
  },
  {
    "id": 1305,
    "text": "14\nis used. Now the Einstein summation notation is adopted to +\n(cid:90)\ndω\n(cid:34)\n1−\nA−\n+log\n(cid:32) A− (cid:33)(cid:35)\n. (C16)\nsimplifythecalculationsof f andJ again: D D\nm mi\n(cid:88) SinceA= A+−A−,Eq.(C16)canbetransformedinto:\nf =αu + W w −B , (C13)\nm m ml l m\nl (cid:90) (cid:34) A+ (cid:32) A+(cid:33)(cid:35)\nS [A,A + ] = dω 1− +log\nBR D D\n(cid:90) (cid:34) A+−A (cid:32) A+−A (cid:33)(cid:35)\nJ mi =αδ mi + (cid:88) W mli D l w l w l , (C14) + dω 1− D +log D (C. 17)\nl\nThenweshouldeliminateA+inEq.(C17). Because\nwhere\n∂S [A,A+] (cid:90) (cid:32) 1 1 2 (cid:33)\nBR =0= dω + − , (C18)\n1 ∂A+ A+ A+−A D\nw l = 1−D (cid:80) V u . (C15)\nl m lm m weimmediatelyget:\n√\nHere, the definitionsof W ml , W mli , and B m couldbe foundin A2+D2+D+A\nSectionB3. A + = , (C19)\n2\nand\n√\n4. Positive-negativeentropy A− =\nA2+D2+D−A\n. (C20)\n2\nNext,wewouldliketogeneralizetheBayesianreconstruc- FromthedefinitionsofA+andA−,itiseasilytoprove:\ntionentropytorealizethepositive-negativeentropyalgorithm"
  },
  {
    "id": 1306,
    "text": "A2+D2+D−A\n. (C20)\n2\nNext,wewouldliketogeneralizetheBayesianreconstruc- FromthedefinitionsofA+andA−,itiseasilytoprove:\ntionentropytorealizethepositive-negativeentropyalgorithm\ntosupporttheanalyticcontinuationofmatrix-valuedGreen’s 2A + A− =(A ++A−)D, (C21)\nfunction[55]. Thepositive-negativeentropyfortheBayesian\nreconstructionentropyisdefinedasfollows: and\n√\n(cid:90) (cid:34) A+ (cid:32) A+(cid:33)(cid:35) A ++A− = A2+D2+D. (C22)\nS [A + ,A−] = dω 1− +log\nBR D D\nWewouldliketoexpressS [A+,A−]intermsofA:\nBR\n(cid:90) (cid:34) A+ A− (cid:32) A+A− (cid:33)(cid:35)\nS [A + ,A−]= dω 2− − +log . (C23)\nBR D D D2\nSubstitutingEq.(C21)intoEq.(C23):\n(cid:90) (cid:34) A++A− (cid:32) A++A− (cid:33)(cid:35)\nS [A + ,A−]= dω 2− +log . (C24)\nBR D 2D\nSubstitutingEq.(C22)intoEq.(C24):\nS\nBR\n[A + ,A−]=\n(cid:90)\ndω\n \n2−\n√ A2+\nD\nD2+D\n+log\n  √ A2+\n2D\nD2+D    \n. (C25)\nItsdiscretizationformreads:\n (cid:112) (cid:112) \nS\nBR\n[A + ,A−]=\n(cid:88)\n2−\nA2\nm\n+\nD\nD2\nm\n+D\nm +log\n A2"
  },
  {
    "id": 1307,
    "text": "2−\n√ A2+\nD\nD2+D\n+log\n  √ A2+\n2D\nD2+D    \n. (C25)\nItsdiscretizationformreads:\n (cid:112) (cid:112) \nS\nBR\n[A + ,A−]=\n(cid:88)\n2−\nA2\nm\n+\nD\nD2\nm\n+D\nm +log\n A2\nm\n+\n2D\nD2\nm\n+D\nm\n \n∆\nm\n. (C26)\nm m m\nNote that Eq. (C25) is exactly the same with Eq. (20). S [A+,A−]withrespecttoA is:\nBR i\nS [A+,A−] is just S± [A]. Then the first derivative of\nBR BR ∂S[A+,A−] (cid:32) 1 1 (cid:33)\n=−∆ − (C27)\n∂A i D A+\ni i i"
  },
  {
    "id": 1308,
    "text": "15\nThisequationisverysimilartoEq.(C3). firstderivativesofA+andA−withrespecttou are:\nl l i\n∂A+\n5. Parameterizationforpositive-negativespectralfunctions ∂u l = A + l A + l V li , (C32)\ni\nNowweassumethat: and\nD\nA + l = 1−D\nl\n(cid:80)\nm\nl V\nlm\nu\nm\n, (C28) ∂\n∂\nA\nu\n− l =−A−\nl\nA−\nl\nV\nli\n. (C33)\ni\nand\nThefirstderivativeofA withrespecttou is:\nl i\nD\nA− l = 1+D l (cid:80) m l V lm u m . (C29) ∂A l = A + A + V +A−A−V . (C34)\n∂u l l li l l li\ni\nWealsointroduce:\n1 Aftersomesimplealgebra,weeasilyget:\nw + l = 1−D l (cid:80) m V lm u m , (C30) f =αu + (cid:88) W (w +−w−)−B , (C35)\nm m ml l l m\nand\nl\n1\nw− l = 1+D (cid:80) V u . (C31) (cid:88)\nl m lm m J =αδ + W D(w + w ++w−w−). (C36)\nmi mi mli l l l l l\nSoA+ = Dw+,A− = Dw−,andA = D(w+−w−).Itiseasyto l\nl l l l l l l l l l\nverifythatthedefinitionsof A+ and A− obeyEq.(C21). The\nl l\n[1] JohnW.NegeleandHenriOrland,QuantumMany-ParticleSys- [12] GiovanniOnida,LuciaReining, andAngelRubio,“Electronic"
  },
  {
    "id": 1309,
    "text": "l l l l l l l l l l\nverifythatthedefinitionsof A+ and A− obeyEq.(C21). The\nl l\n[1] JohnW.NegeleandHenriOrland,QuantumMany-ParticleSys- [12] GiovanniOnida,LuciaReining, andAngelRubio,“Electronic\ntems(PerseusBooks,1998). excitations: density-functional versus many-body Green’s-\n[2] Piers Coleman, Introduction to Many-Body Physics (Cam- functionapproaches,”Rev.Mod.Phys.74,601–659(2002).\nbridgeUniversityPress,2016). [13] JanGukelberger,LiHuang, andPhilippWerner,“Onthedan-\n[3] Emanuel Gull, Andrew J. Millis, Alexander I. Lichtenstein, gersofpartialdiagrammaticsummations: Benchmarksforthe\nAlexey N. Rubtsov, Matthias Troyer, and Philipp Werner, two-dimensionalHubbardmodelintheweak-couplingregime,”\n“Continuous-timeMonteCarlomethodsforquantumimpurity Phys.Rev.B91,235114(2015).\nmodels,”Rev.Mod.Phys.83,349–404(2011). [14] K. S. D. Beach, R. J. Gooding, and F. Marsiglio, “Reliable"
  },
  {
    "id": 1310,
    "text": "“Continuous-timeMonteCarlomethodsforquantumimpurity Phys.Rev.B91,235114(2015).\nmodels,”Rev.Mod.Phys.83,349–404(2011). [14] K. S. D. Beach, R. J. Gooding, and F. Marsiglio, “Reliable\n[4] W. M. C. Foulkes, L. Mitas, R. J. Needs, and G. Rajagopal, Pade´ analyticalcontinuationmethodbasedonahigh-accuracy\n“QuantumMonteCarlosimulationsofsolids,”Rev.Mod.Phys. symboliccomputationalgorithm,”Phys.Rev.B61,5147–5157\n73,33–83(2001). (2000).\n[5] James Gubernatis, Naoki Kawashima, and Philipp Werner, [15] Johan Scho¨tt, Inka L. M. Locht, Elin Lundin, Oscar Gråna¨s,\nQuantumMonteCarloMethods:AlgorithmsforLatticeModels OlleEriksson, andIgorDiMarco,“Analyticcontinuationby\n(CambridgeUniversityPress,2016). averagingPade´approximants,”Phys.Rev.B93,075104(2016).\n[6] Walter Metzner, Manfred Salmhofer, Carsten Honerkamp, [16] H.J.VidbergandJ.W.Serene,“SolvingtheEliashbergequa-\nVolkerMeden, andKurtScho¨nhammer,“Functionalrenormal- tionsbymeansofN-pointPade´ approximants,”J.LowTemp."
  },
  {
    "id": 1311,
    "text": "VolkerMeden, andKurtScho¨nhammer,“Functionalrenormal- tionsbymeansofN-pointPade´ approximants,”J.LowTemp.\nization group approach to correlated fermion systems,” Rev. Phys.29,179–192(1977).\nMod.Phys.84,299–352(2012). [17] GergelyMarko´,UrkoReinosa, andZsoltSze´p,“Pade´approx-\n[7] W.HankeC.PlattandR.Thomale,“Functionalrenormalization imantsandanalyticcontinuationofEuclideanΦ-derivableap-\ngroupformulti-orbitalFermisurfaceinstabilities,”Adv.Phys. proximations,”Phys.Rev.D96,036002(2017).\n62,453–562(2013). [18] MarkJarrellandJ.E.Gubernatis,“Bayesianinferenceandthe\n[8] JudithHarlandGeorgKresse,“AccurateBulkPropertiesfrom analyticcontinuationofimaginary-timequantumMonteCarlo\nApproximate Many-Body Techniques,” Phys. Rev. Lett. 103, data,”Phys.Rep.269,133–195(1996).\n056401(2009). [19] J. E. Gubernatis, Mark Jarrell, R. N. Silver, and D. S. Sivia,\n[9] GuoP.Chen,VamseeK.Voora,MatthewM.Agee,SreeGanesh “Quantum Monte Carlo simulations and maximum entropy:"
  },
  {
    "id": 1312,
    "text": "056401(2009). [19] J. E. Gubernatis, Mark Jarrell, R. N. Silver, and D. S. Sivia,\n[9] GuoP.Chen,VamseeK.Voora,MatthewM.Agee,SreeGanesh “Quantum Monte Carlo simulations and maximum entropy:\nBalasubramani, andFilippFurche,“Random-PhaseApproxi- Dynamicsfromimaginary-timedata,”Phys.Rev.B44,6011–\nmationMethods,”Annu.Rev.Phys.Chem.68,421–445(2017). 6029(1991).\n[10] Lars Hedin, “New Method for Calculating the One-Particle [20] R. N. Silver, D. S. Sivia, and J. E. Gubernatis, “Maximum-\nGreen’s Function with Application to the Electron-Gas Prob- entropy method for analytic continuation of quantum Monte\nlem,”Phys.Rev.139,A796–A823(1965). Carlodata,”Phys.Rev.B41,2380–2389(1990).\n[11] F Aryasetiawan and O Gunnarsson, “The GW method,” Rep. [21] AndersW.Sandvik,“Stochasticmethodforanalyticcontinua-\nProg.Phys.61,237(1998). tionofquantumMonteCarlodata,”Phys.Rev.B57,10287–\n10290(1998)."
  },
  {
    "id": 1313,
    "text": "16\n[22] AndersW.Sandvik,“Constrainedsamplingmethodforanalytic [42] ZhenHuang,EmanuelGull, andLinLin,“Robustanalyticcon-\ncontinuation,”Phys.Rev.E94,063308(2016). tinuationofGreen’sfunctionsviaprojection, poleestimation,\n[23] HuiShaoandAndersW.Sandvik,“Progressonstochastican- andsemidefiniterelaxation,”Phys.Rev.B107,075151(2023).\nalyticcontinuationofquantumMonteCarlodata,”Phys.Rep. [43] Lei Zhang and Emanuel Gull, “Minimal Pole Representation\n1003,1–88(2023). andControlledAnalyticContinuationofMatsubaraResponse\n[24] K. S. D. Beach, “Identifying the maximum entropy method Functions,” (2023),arXiv:2312.10576[cond-mat.str-el].\nasaspeciallimitofstochasticanalyticcontinuation,” (2004), [44] LexingYing,“PoleRecoveryFromNoisyDataonImaginary\narXiv:0403055[cond-mat.str-el]. Axis,”J.Sci.Comput.92,107(2022).\n[25] Sebastian Fuchs, Thomas Pruschke, and Mark Jarrell, “Ana- [45] Lexing Ying, “Analytic continuation from limited noisy Mat-"
  },
  {
    "id": 1314,
    "text": "arXiv:0403055[cond-mat.str-el]. Axis,”J.Sci.Comput.92,107(2022).\n[25] Sebastian Fuchs, Thomas Pruschke, and Mark Jarrell, “Ana- [45] Lexing Ying, “Analytic continuation from limited noisy Mat-\nlyticcontinuationofquantumMonteCarlodatabystochastic subaradata,”J.Comput.Phys.469,111549(2022).\nanalyticalinference,”Phys.Rev.E81,056701(2010). [46] RomainFournier,LeiWang,OlegV.Yazyev, andQuanSheng\n[26] K.VafayiandO.Gunnarsson,“Analyticalcontinuationofspec- Wu,“ArtificialNeuralNetworkApproachtotheAnalyticCon-\ntraldatafromimaginarytimeaxistorealfrequencyaxisusing tinuationProblem,”Phys.Rev.Lett.124,056401(2020).\nstatisticalsampling,”Phys.Rev.B76,035115(2007). [47] Hongkee Yoon, Jae-Hoon Sim, and Myung Joon Han, “Ana-\n[27] David R. Reichman and Eran Rabani, “Analytic continuation lytic continuation via domain knowledge free machine learn-\naveragespectrummethodforquantumliquids,”J.Chem.Phys. ing,”Phys.Rev.B98,245101(2018)."
  },
  {
    "id": 1315,
    "text": "averagespectrummethodforquantumliquids,”J.Chem.Phys. ing,”Phys.Rev.B98,245101(2018).\n131,054502(2009). [48] Dongchen Huang and Yi-feng Yang, “Learned optimizers for\n[28] Khaldoon Ghanem and Erik Koch, “Extending the average analyticcontinuation,”Phys.Rev.B105,075112(2022).\nspectrummethod:Gridpointsamplinganddensityaveraging,” [49] RongZhang,MaximilianE.Merkel,SophieBeck, andClaude\nPhys.Rev.B102,035114(2020). Ederer, “Training biases in machine learning for the analytic\n[29] KhaldoonGhanemandErikKoch,“Averagespectrummethod continuationofquantummany-bodyGreen’sfunctions,”Phys.\nfor analytic continuation: Efficient blocked-mode sampling Rev.Res.4,043082(2022).\nanddependenceonthediscretizationgrid,”Phys.Rev.B101, [50] Juan Yao, Ce Wang, Zhiyuan Yao, and Hui Zhai, “Noise\n085111(2020). enhanced neural networks for analytic continuation,” Mach.\n[30] OlavF.Syljuåsen,“Usingtheaveragespectrummethodtoex- Learn.:Sci.Technol.3,025010(2022)."
  },
  {
    "id": 1316,
    "text": "085111(2020). enhanced neural networks for analytic continuation,” Mach.\n[30] OlavF.Syljuåsen,“Usingtheaveragespectrummethodtoex- Learn.:Sci.Technol.3,025010(2022).\ntractdynamicsfromquantumMonteCarlosimulations,”Phys. [51] Louis-Franc¸oisArsenault,RichardNeuberg,LaurenAHannah,\nRev.B78,174429(2008). andAndrewJMillis,“Projectedregressionmethodforsolving\n[31] A. S. Mishchenko, N. V. Prokof’ev, A. Sakamoto, and B. V. Fredholmintegralequationsarisingintheanalyticcontinuation\nSvistunov,“DiagrammaticquantumMonteCarlostudyofthe problemofquantumphysics,”Inv.Prob.33,115007(2017).\nFro¨hlichpolaron,”Phys.Rev.B62,6317–6336(2000). [52] Nathan S. Nichols, Paul Sokol, and Adrian Del Maestro,\n[32] F.Bao,Y.Tang,M.Summers,G.Zhang,C.Webster,V.Scarola, “Parameter-freedifferentialevolutionalgorithmfortheanalytic\nandT.A.Maier,“Fastandefficientstochasticoptimizationfor continuation of imaginary time correlation functions,” Phys.\nanalyticcontinuation,”Phys.Rev.B94,125149(2016). Rev.E106,025312(2022)."
  },
  {
    "id": 1317,
    "text": "andT.A.Maier,“Fastandefficientstochasticoptimizationfor continuation of imaginary time correlation functions,” Phys.\nanalyticcontinuation,”Phys.Rev.B94,125149(2016). Rev.E106,025312(2022).\n[33] Olga Goulko, Andrey S. Mishchenko, Lode Pollet, Nikolay [53] Dominic Bergeron and A.-M. S. Tremblay, “Algorithms for\nProkof’ev, and Boris Svistunov, “Numerical analytic contin- optimizedmaximumentropyanddiagnostictoolsforanalytic\nuation: Answers to well-posed questions,” Phys. Rev. B 95, continuation,”Phys.Rev.E94,023303(2016).\n014102(2017). [54] Jae-HoonSimandMyungJoonHan,“Maximumquantumen-\n[34] IgorKrivenkoandMalteHarland,“TRIQS/SOM:Implementa- tropymethod,”Phys.Rev.B98,205102(2018).\ntionofthestochasticoptimizationmethodforanalyticcontinu- [55] GernotJ.Kraberger,RobertTriebl,ManuelZingl, andMarkus\nation,”Comput.Phys.Commun.239,166–183(2019). Aichhorn,“Maximumentropyformalismfortheanalyticcon-"
  },
  {
    "id": 1318,
    "text": "ation,”Comput.Phys.Commun.239,166–183(2019). Aichhorn,“Maximumentropyformalismfortheanalyticcon-\n[35] IgorKrivenkoandAndreyS.Mishchenko,“TRIQS/SOM2.0: tinuationofmatrix-valuedGreen’sfunctions,”Phys.Rev.B96,\nImplementation of the stochastic optimization with consistent 155128(2017).\nconstraintsforanalyticcontinuation,”Comput.Phys.Commun. [56] RyanLevy, J.P.F.LeBlanc, andEmanuelGull,“Implementa-\n280,108491(2022). tionofthemaximumentropymethodforanalyticcontinuation,”\n[36] Li Huang and Shuang Liang, “Stochastic pole expansion Comput.Phys.Commun.215,149–155(2017).\nmethodforanalyticcontinuationoftheGreen’sfunction,”Phys. [57] F. F. Assaad, M. Bercx, F. Goth, A. Go¨tz, J. S. Hofmann,\nRev.B108,235143(2023). E. Huffman, Z. Liu, F. Parisen Toldin, J. S. E. Portela, and\n[37] Li Huang and Shuang Liang, “Reconstructing lattice QCD J.Schwab,“TheALF(AlgorithmsforLatticeFermions)project"
  },
  {
    "id": 1319,
    "text": "Rev.B108,235143(2023). E. Huffman, Z. Liu, F. Parisen Toldin, J. S. E. Portela, and\n[37] Li Huang and Shuang Liang, “Reconstructing lattice QCD J.Schwab,“TheALF(AlgorithmsforLatticeFermions)project\nspectral functions with stochastic pole expansion and Nevan- release 2.0. Documentation for the auxiliary-field quantum\nlinna analytic continuation,” (2023), arXiv:2309.11114 [hep- MonteCarlocode,”SciPostPhys.Codebases,1(2022).\nlat]. [58] LiHuang,“ACFlow: Anopensourcetoolkitforanalyticcon-\n[38] Junya Otsuki, Masayuki Ohzeki, Hiroshi Shinaoka, and tinuationofquantumMonteCarlodata,”Comput.Phys.Com-\nKazuyoshi Yoshimi, “Sparse modeling approach to analytical mun.292,108863(2023).\ncontinuation of imaginary-time quantum monte carlo data,” [59] KristjanHaule,Chuck-HouYee, andKyooKim,“Dynamical\nPhys.Rev.E95,061302(2017). mean-fieldtheorywithinthefull-potentialmethods:Electronic\n[39] Yuichi Motoyama, Kazuyoshi Yoshimi, and Junya Otsuki, structureofCeIrIn ,CeCoIn ,andCeRhIn ,”Phys.Rev.B81,\n5 5 5"
  },
  {
    "id": 1320,
    "text": "[39] Yuichi Motoyama, Kazuyoshi Yoshimi, and Junya Otsuki, structureofCeIrIn ,CeCoIn ,andCeRhIn ,”Phys.Rev.B81,\n5 5 5\n“Robustanalyticcontinuationcombiningtheadvantagesofthe 195107(2010).\nsparsemodelingapproachandthepade´ approximation,”Phys. [60] Josef Kaufmann and Karsten Held, “ana cont: Python pack-\nRev.B105,035139(2022). age for analytic continuation,” Comput. Phys. Commun. 282,\n[40] JianiFei,Chia-NanYeh, andEmanuelGull,“NevanlinnaAn- 108519(2023).\nalyticalContinuation,”Phys.Rev.Lett.126,056402(2021). [61] AthanasiosPapoulis,ProbabilityandStatistics(PrenticeHall,\n[41] Jiani Fei, Chia-Nan Yeh, Dominika Zgid, and Emanuel 1989).\nGull, “Analytical continuation of matrix-valued functions: [62] M.DeRaychaudhury, E.Pavarini, andO.K.Andersen,“Or-\nCarathe´odoryformalism,”Phys.Rev.B104,165111(2021). bitalFluctuationsintheDifferentPhasesofLaVO andYVO ,”\n3 3"
  },
  {
    "id": 1321,
    "text": "17\nPhys.Rev.Lett.99,126402(2007). FieldTheories,”Phys.Rev.Lett.111,182003(2013).\n[63] JanM.TomczakandSilkeBiermann,“Effectivebandstructure [73] AlexanderRothkopf,“Bayesianinferenceofreal-timedynam-\nof correlated materials: the case of VO ,” J. Phys.: Condens. icsfromlatticeQCD,”Front.Phys.10,1028995(2022).\n2\nMatter19,365206(2007). [74] AlexanderRothkopf,“Bayesianinferenceofnonpositivespec-\n[64] E.GullandA.J.Millis,“Pairingglueinthetwo-dimensional tralfunctionsinquantumfieldtheory,”Phys.Rev.D95,056016\nHubbardmodel,”Phys.Rev.B90,041110(2014). (2017).\n[65] A. Reymbaut, D. Bergeron, and A.-M. S. Tremblay, “Maxi- [75] JorgeNocedalandStephenJ.Wright,NumericalOptimization\nmumentropyanalyticcontinuationforspectralfunctionswith (SpringerNewYork,NY,2006).\nnonpositivespectralweight,”Phys.Rev.B92,060509(2015). [76] E. D. Laue, J. Skilling, and J. Staunton, “Maximum entropy\n[66] A.Reymbaut,A.-M.Gagnon,D.Bergeron, andA.-M.S.Trem- reconstructionofspectracontainingantiphasepeaks,”J.Magn."
  },
  {
    "id": 1322,
    "text": "[66] A.Reymbaut,A.-M.Gagnon,D.Bergeron, andA.-M.S.Trem- reconstructionofspectracontainingantiphasepeaks,”J.Magn.\nblay, “Maximum entropy analytic continuation for frequency- Reson.63,418–424(1985).\ndependent transport coefficients with nonpositive spectral [77] Sibusiso Sibisi, “Quantified Maxent: An NMR Application,”\nweight,”Phys.Rev.B95,121104(2017). inMaximumEntropyandBayesianMethods,editedbyPaulF.\n[67] EmanuelGull,OlivierParcollet, andAndrewJ.Millis,“Super- Fouge`re(SpringerNetherlands,Dordrecht,1990)pp.351–358.\nconductivityandthePseudogapintheTwo-DimensionalHub- [78] R.K.Bryan,“Maximumentropyanalysisofoversampleddata\nbardModel,”Phys.Rev.Lett.110,216405(2013). problems,”Eur.Biophys.J.18,165–174(1990).\n[68] Changming Yue and Philipp Werner, “Maximum entropy [79] JSkilling,“FundamentalsofMaxEntindataanalysis,”inMax-\nanalytic continuation of anomalous self-energies,” (2023), imum Entropy in Action: A Collection Of Expository Essays"
  },
  {
    "id": 1323,
    "text": "analytic continuation of anomalous self-energies,” (2023), imum Entropy in Action: A Collection Of Expository Essays\narXiv:2303.16888[cond-mat.supr-con]. (OxfordUniversityPress,1991).\n[69] E.T.Jaynes,“InformationTheoryandStatisticalMechanics,” [80] O.Gunnarsson,M.W.Haverkort, andG.Sangiovanni,“Ana-\nPhys.Rev.106,620–630(1957). lyticalcontinuationofimaginaryaxisdataforopticalconduc-\n[70] A.N.Tikhonov,A.V.Goncharsky,V.V.Stepanov, andA.G. tivity,”Phys.Rev.B82,165125(2010).\nYagola,NumericalMethodsfortheSolutionofIll-PosedProb- [81] J.Scho¨tt,E.G.C.P.vanLoon,I.L.M.Locht,M.I.Katsnelson,\nlems(SpringerNetherlands,1995). andI.DiMarco,“Comparisonbetweenmethodsofanalytical\n[71] M.Asakawa,Y.Nakahara, andT.Hatsuda,“Maximumentropy continuation for bosonic functions,” Phys. Rev. B 94, 245140\nanalysis of the spectral functions in lattice QCD,” Prog. Part. (2016).\nNucl.Phys.46,459–508(2001).\n[72] YannisBurnierandAlexanderRothkopf,“BayesianApproach"
  },
  {
    "id": 1324,
    "text": "analysis of the spectral functions in lattice QCD,” Prog. Part. (2016).\nNucl.Phys.46,459–508(2001).\n[72] YannisBurnierandAlexanderRothkopf,“BayesianApproach\nto Spectral Function Reconstruction for Euclidean Quantum"
  },
  {
    "id": 1325,
    "text": "4202\nnuJ\n81\n]hp-tnauq[\n2v91000.1042:viXra\nSymbolic, numeric and quantum computation of\nHartree-Fock equation\nIchio Kikuchi1, Akihito Kikuchi2∗\n1\nInternationales Forschungszentrumfu¨r Quantentechnik\n2\nInternational Research Center for Quantum Technology, Tokyo\nJune 19, 2024\nAbstract\nIn this article, we discuss how a kind of hybrid computation, which\nemploys symbolic, numeric, classic, and quantum algorithms, allows us to\nconduct Hartree-Fock electronic structure computation of molecules. In\nthe proposed algorithm, we replace the Hartree-Fock equations with a set\nof equations composed of multivariate polynomials. We transform those\npolynomials to the corresponding Gr¨obner bases, and then we investigate\nthe corresponding quotient ring, wherein the orbital energies, the LCAO\ncoefficients, or the atomic coordinates are represented by the variables in\nthe ring. In this quotient ring, the variables generate the transformation"
  },
  {
    "id": 1326,
    "text": "coefficients, or the atomic coordinates are represented by the variables in\nthe ring. In this quotient ring, the variables generate the transformation\nmatrices that represent the multiplication with the monomial bases, and\nthe eigenvalues of those matrices compose the roots of the equation. The\nquantumphaseestimation(QPE)algorithmenablesustorecordthoseroots\nin the quantum states, which would be used in the input data for more\nadvanced and more accurate quantum computations.\n1 Introduction\nThere is a symbolic-numeric method of quantum chemistry [1], whereby the\ncomputations are carried out in the following way:\n• The molecular integrals are represented by the polynomial approximation\nof analytic formulas, which are computed symbolically if we use analytic\natomic bases, such as Gaussian-Type or Slater-Type orbitals (GTO or\n∗\nakihito kikuchi@gakushikai.jp (Thecorresponding author; a visiting researcher in IFQT)\n1"
  },
  {
    "id": 1327,
    "text": "STO) [2]. Those formulas are the analytic functions of several variables,\nnamely, those of orbital exponents and atomic coordinates. By Tailor\nexpansion with respect to those variables, the molecular integrals are ap-\nproximated by polynomials.\n• The total energy is a polynomial composed of molecular integrals and the\nundetermined coefficients of LCAO. The ortho-normalization conditions\nare similarly treated.\n• We compose the objective function from the total energy and the ortho-\nnormalization condition with the Lagrange multipliers which represent the\norbital energies.\n• By symbolic differentiation, we obtain a system of polynomial equations\nthat gives the optima.\n• To get the roots of the system of polynomial equations, we apply sev-\neral methods of computer algebra, where Gro¨bner bases and the primary\nideal decomposition play central roles in getting the quantum eigenstates\n[3,4,5,6,7,8]. Namely, wecomposeanidealI fromthegiven polynomials"
  },
  {
    "id": 1328,
    "text": "ideal decomposition play central roles in getting the quantum eigenstates\n[3,4,5,6,7,8]. Namely, wecomposeanidealI fromthegiven polynomials\nand transform them into another system that has a more suitable form for\nroot-finding [9, 10, 11, 12, 13, 11]. The ideal representing a Hartree-Fock\nequation could be decomposed into several subsystems described by pri-\nmary ideals. Each primary ideal would represent one solution set, namely,\none quantum state if the decomposition is executed to the full.\n• Up to now, we have reported the results of several simple molecules, using\nSTO and n-GTO models ( [1] and [14]). In those works, the adopted\nalgorithms are classical, not quantum. One might use the term Molecular\nAlgebraic Geometry to refer to this algebraic computational scheme for\nmolecular orbital theory.\nThe algebraic method described above could relate to the quantum algo-\nrithms, and the theme of the present study is to demonstrate it. This article"
  },
  {
    "id": 1329,
    "text": "molecular orbital theory.\nThe algebraic method described above could relate to the quantum algo-\nrithms, and the theme of the present study is to demonstrate it. This article\nis structured as follows. First, we show the computational step whereby the\nclassical symbolic computation prepares the eigenvalue problem that gives the\nroots of the given equation. Second, we show how the quantum algorithm could\nsolve the problem. Then we discuss several points that should be treated with\ncare.\n2"
  },
  {
    "id": 1330,
    "text": "2 Computational process\nIn this section, we incarnate the algorithms for symbolic-numeric and classical-\nquantum computation in quantum chemistry. The computational process is\ncomposed of two phases. The first phase uses the symbolic-numeric classical\nalgorithm andconverts theHartree-Fock equationsintoarepresentation suitable\nfor quantum computation. Thesecond phaseuses the data generated in the first\nphase and computes the roots of the Hartree-Fock equations. We describe the\nalgorithm in each phase, using two examples.\n2.1 Phase 1: symbolic-numeric classical algorithm\n2.1.1 Tools for symbolic, numeric, and classical computation\nWe solve the set of polynomial equations through the computational steps ex-\nplained in [15].\n• LetI beanidealmadeofmultivariatepolynomials(f ,f ,...,f )inR[x ,x ,...,x ].\n1 2 t 1 2 n\nOnce the Gro¨bner for the ideal I is computed, it is an easy task to repre-\nsent any element in R[x ,x ,...,x ]/I uniquely as the linear combination\n1 2 n"
  },
  {
    "id": 1331,
    "text": "1 2 t 1 2 n\nOnce the Gro¨bner for the ideal I is computed, it is an easy task to repre-\nsent any element in R[x ,x ,...,x ]/I uniquely as the linear combination\n1 2 n\nof the monomial basis of the quotient ring.\n• Letx¯ ,x¯ ,...,x¯ betherepresentativesofx ,x ,...,x inR[x ,x ,...,x ]/I.\n1 2 2 1 2 n 1 2 n\nAdditionally, let b be a vector that is composed of the representatives of\nthe monomial basis of the quotient ring.\n• For any i, the multiplication x¯ b is represented by\ni\n·\nx¯ b = b M (1)\ni\n· ·\nxi\nwith a transformation matrix M . The entries of the matrix are numbers,\nxi\nbut not symbols.\n• As M M = M M , those transformation matrices share common\nxi\n·\nxj xj\n·\nxi\neigenvectors v j = 1,...,M , where M is size of the monomial basis b.\nj\n{ | }\n• Let us consider the eigenvalue problems, defined as follows,\nξ¯(j)\nv = v M (2)\ni j j · xi\nfor i = 1,...,n and j = 1,...,M. Those equations are solved numerically,\nand the eigenvalues give the common zeros of the polynomials included in\n3"
  },
  {
    "id": 1332,
    "text": "the ideal I. Namely, the eigenvalues give the roots of the set of polynomial\nequations defined by\nf (x ,x ,...,x )= f (x ,x ,...,x ) = ... = f (x ,x ,...,x ) = 0 (3)\n1 1 2 n 2 1 2 n t 1 2 n\nin such a way that\n(x ,x ,...,x )= (ξ¯(j) ,ξ¯(j) ,...,ξ¯(j)) (4)\n1 2 n 1 2 n\nfor j = 1,...,M. Note that if eigenvectors v for one M is obtained,\nj j i\n{ }\nthe other components of the roots are computed by\n(v M ,v )\nξ¯(j)\n=\nj\n·\nxi j\n. (5)\ni (v ,v )\nj j\nThe root-finding of a system of polynomial equations is replaced by a set\nof eigenvalue problems, which could be solved by quantum algorithms. We put\nthe eigenvectors v into a set of quantum states v , and the computa-\nj j j\n{ } {| i}\ntional steps are carried out by a quantum circuit, which conducts the following\ntransformation:\nv Ancilla Ancilla Ancilla v ξ¯(j) ξ¯(j) ξ¯(j) (6)\n| j i| 1 i| 2 i···| n i → | j i| 1 i| 2 i···| n i\nwhere the eigenvalues of M (i = 1,...,n) for v are recorded in ancilla qubits\nxi j"
  },
  {
    "id": 1333,
    "text": "transformation:\nv Ancilla Ancilla Ancilla v ξ¯(j) ξ¯(j) ξ¯(j) (6)\n| j i| 1 i| 2 i···| n i → | j i| 1 i| 2 i···| n i\nwhere the eigenvalues of M (i = 1,...,n) for v are recorded in ancilla qubits\nxi j\nthrough a successive application of quantum phase estimation.\n2.1.2 Computation for a simple toy model\nLet us compute a simple toy model, where the secular equation is given by\nV(x,y) 1 x x\n− = e (7)\n1 V(x,y)! y! y!\n−\nalong with the normalization condition x2+y2 = 1. The variables (x,y) are the\namplitudes of the wavefunction and e is the orbital energy. V(x,y) is the on-site\npotential that is the function of the amplitude of the wavefunction. We assume\nthat the roots are real.\nThe polynomial ideal that represents the secular equation is given by\nI = (xV(x,y) y ex,yV(x,y) x ey,x2+y2 1) (8)\n− − − − −\nIn the case of V(x,y) = 0, the Gro¨bner basis is given by\nI = (e2 1,2y2 1,x+ye) (9)\nstd\n− −\n4"
  },
  {
    "id": 1334,
    "text": "The roots of the set of polynomial equations are given by\n1 1 1 1\n(x,y,e) = , , 1 , , ,1 (10)\n±√2 ±√2 − ±√2 ∓√2\n(cid:18) (cid:19) (cid:18) (cid:19)\nThe entries in the quotient ring Q(x,y,e)/I are the linear combinations of\nthe monomial basis b = (b[0],b[1],b[2],b[3]):\nb[0] = ye (11)\nb[1] = y (12)\nb[2] = e (13)\nb[3] = 1 (14)\nIn the quotient ring, the multiplications of the entries of the basis b by x, y,\nandearerepresentedbythetransformationmatrices: b p = b m for p= x,y,e.\np\n· ·\n0 0 0 1\n−\n 0 0 1 0 \nm = − (15)\nx\n0 0.5 0 0\n \n−\n \n 0.5 0 0 0 \n− \n \n0 0 1 0\n 0 0 0 1\nm = (16)\ny\n0.5 0 0 0\n \n \n 0 0.5 0 0\n \n \n0 1 0\n1 0 0 0\nm = (17)\ne\n0 0 0 1\n \n \n0 0 1 0\n \n \nFor the above example, the related properties are given as follows.\nv (vm ,v) (vm ,v) (vm ,v)\nx y e\n( 1 , 1 , 1,1) 1 1 1\n \n−√2 √2 − √2 √2 −\n( 1 , 1 , 1,1) 1 1 1 (18)\n √2 −√2 − −√2 −√2 − \n \n ( 1 , 1 ,1,1) 1 1 1 \n √2 √2 −√2 √2 \n \n( 1 , 1 ,1,1) 1 1 1 \n −√2 −√2 √2 −√2 \n "
  },
  {
    "id": 1335,
    "text": "x y e\n( 1 , 1 , 1,1) 1 1 1\n \n−√2 √2 − √2 √2 −\n( 1 , 1 , 1,1) 1 1 1 (18)\n √2 −√2 − −√2 −√2 − \n \n ( 1 , 1 ,1,1) 1 1 1 \n √2 √2 −√2 √2 \n \n( 1 , 1 ,1,1) 1 1 1 \n −√2 −√2 √2 −√2 \n \nThe data in Table 18 covers all the solutions of the secular equation.\n5"
  },
  {
    "id": 1336,
    "text": "2.2 Computation for Hartree-Fock model\nIn this section, the restricted Hartree-Fock computation of a realistic molecule\nHeH+ isusedasanexample. Thismoleculeisthesimplestheteronuclearmolecule\nand is used as a benchmark problem for the solving of the Hartree-Fock model\n[2].\nAt first, we compute the total energy functional of the RHF model of the\nmolecule through STO-3g basis set [16]. The analytic formulas of the molecular\nintegrals are computed and substituted into the formula of the energy, namely,\nthe objective function. The total energy functional is a function of the LCAO\ncoefficients(x,y),theorbitalenergye,andtheinteratomicdistanceR,asdefined\nin the following.\n1\nE = ihi + ([iijj] [ij ji]) (19)\nHF\nh | | i 2 | − |\ni ij\nX X\nihi = dx χ (x )h(r )χ (x ) (20)\n1 ∗i 1 1 j 1\nh | | i\nZ\n1\n[ij kl] = dx dx χ (x )χ (x ) χ (x )χ (x ) (21)\n|\n1 2 ∗i 1 j 1\nr\n∗i 2 j 2\n12\nZ\nχ (x) = (xφ (r R )+yφ (r R ))σ (22)\ni 1s,He He 1s,H H i\n− −\nσ :spin function (23)\ni\n3\nφSTO 3G(r) = c(i)exp( z(i)r2) (24)\n1s − −"
  },
  {
    "id": 1337,
    "text": "|\n1 2 ∗i 1 j 1\nr\n∗i 2 j 2\n12\nZ\nχ (x) = (xφ (r R )+yφ (r R ))σ (22)\ni 1s,He He 1s,H H i\n− −\nσ :spin function (23)\ni\n3\nφSTO 3G(r) = c(i)exp( z(i)r2) (24)\n1s − −\ni=1\nX\nFor the computation of HeH+, we use two spin orbitals.\ni = α,β (25)\nThe total energy in the restricted Hartree-Fock model is given by\n1 Z Z\nh(r) = 2 He H (26)\n−2∇ −\n|\nr\n−\nRHe| −−\n|\nr\n−\nRH\n|\n6"
  },
  {
    "id": 1338,
    "text": "R = R R ,Z = 2,Z = 1 (27)\nHe H He H\n| − |\nZ Z\nHe H\nE (x,y,e,R) = E (x,y,R) e ( χ χ 1)+ (28)\ntot HF i j\n− h | i− R\ni\nX\n• ThetotalenergyfunctionalisconvertedtoapolynomialthroughtheTaylor\nexpansion with respect to the atomic distance R. The expansion is carried\nout at the center R = 1.5.\n0\n• The numerical coefficients in the total energy are approximated by frac-\ntional numbers so that the objective function, multiplied by the powers\nof ten, is given by a polynomial with integer coefficients. To this end, we\nsimply approximate the numerical coefficient C by rounding 10nC to the\nnearest integer N and get N /10n. We multiply the polynomial by 10n\nc c\nand get the objective function.\n• A set of polynomial equations is derived by the partial differentiation with\nrespect to (x,y) and e so that the roots of those equations give the optima\nof the objective function. For the sake of simplicity, we do not carry out\nthe optimization for R. Instead, we replace ∂Ω with 100R 146 so that\n∂R −"
  },
  {
    "id": 1339,
    "text": "of the objective function. For the sake of simplicity, we do not carry out\nthe optimization for R. Instead, we replace ∂Ω with 100R 146 so that\n∂R −\nthe interatomic distance is fixed at R = 1.46.\n• Weapplyalgebra. WeusetheringQ[x,y,e](aringoverthefieldofrational\nnumbersQ)withthedegree reverse lexicographic monomial ordering, such\nthat x > y > e. The generators of the set of polynomial equations form an\nideal I. We compute the Gro¨bner basis of I, by which the quotient ring\nQ[x,y,e]/I is defined. In this quotient ring, the monomial basis and the\ntransformation matrices representing the operation of x, y, and e over b\nare computed.\n• As the transformation matrices are numerical data, we then use classical\nor quantum methods to compute eigenvalues.\nThe objective function f is computed as\nobj\n7"
  },
  {
    "id": 1340,
    "text": "OBJ=281*R**5*x**3*y + 1119*R**5*x**2*y**2 + 164*R**5*x**2 +\n֒→ 533*R**5*x*y**3 - 901*R**5*x*y - 70*R**5*y**2 - 1756*R**5 -\n֒→ 2892*R**4*x**3*y - 9431*R**4*x**2*y**2 - 2273*R**4*x**2 -\n֒→ 5040*R**4*x*y**3 + 8552*R**4*x*y + 712*R**4*y**2 + 15802*R**4\n֒→ + 11305*R**3*x**3*y + 29175*R**3*x**2*y**2 + 12477*R**3*x**2 +\n֒→ 18393*R**3*x*y**3 - 30849*R**3*x*y - 1877*R**3*y**2 -\n֒→ 59260*R**3 - 15964*R**2*x**3*y - 32038*R**2*x**2*y**2 -\n֒→ 35996*R**2*x**2 - 27890*R**2*x*y**3 + 37012*R**2*x*y -\n֒→ 3516*R**2*y**2 + 118518*R**2 - 12479*R*x**3*y -\n֒→ 18807*R*x**2*y**2 + 58692*R*x**2 + 1281*R*x*y**3 + 52833*R*x*y\n֒→ + 28135*R*y**2 - 133334*R - 2*e*(114*R**5*x*y - 1281*R**4*x*y\n֒→ + 5600*R**3*x*y - 10194*R**2*x*y + 115*R*x*y + 10000*x**2 +\n֒→ 18221*x*y + 10000*y**2 - 10000) + 13071*x**4 + 45874*x**3*y +\n֒→ 59634*x**2*y**2 - 91649*x**2 + 32206*x*y**3 - 146963*x*y +\n֒→ 7746*y**4 - 65195*y**2 + 79999;\nThe ideal that gives the optima of the objective function is composed of the\nfollowing components:"
  },
  {
    "id": 1341,
    "text": "֒→ 59634*x**2*y**2 - 91649*x**2 + 32206*x*y**3 - 146963*x*y +\n֒→ 7746*y**4 - 65195*y**2 + 79999;\nThe ideal that gives the optima of the objective function is composed of the\nfollowing components:\n∂f ∂f ∂f ∂f\nobj obj obj obj\nI = , , , (29)\n∂x ∂y ∂e ∂R\n(cid:18) (cid:19)\nTo save thecomputational cost, the atomic distance R is fixed, andI is modified\nas\n∂f ∂f ∂f\nobj obj obj\nI = , , ,100R 146 (30)\n∂x ∂y ∂e −\n(cid:18) (cid:19)\nThequotientringQ[x,y,e]/I hasthemonomialbasisb = (y2,xe,ye,e2,x,y,e,1),\nand the transformation matrices (m , m , and m ) for three variables (x,y, e)\nx y e\nare obtained.\nLet us inspect the computed result.\nAs a reference, the result of the Hartree-Fock computation by the standard\nself-consistent method is shown in Table 1.\nx y e\nSTO-3G 0.801918 0.336800 -1.597448\nTable 1: The result of the Hartree-Fock computation of HeH+ by the standard\nself-consistent method with STO-3g basis set, at the interatomic distance R =\n1.4632.\n8"
  },
  {
    "id": 1342,
    "text": "The solutions obtained from the symbolic-numeric method are shown in Ta-\nble 2. We use the normalized right eigenvectors φ) and compute the expectation\nvalues for mT and mT. The solutions at the third and fourth rows correspond\nx e\nto the ground state in the reference data. Those results are quantitatively satis-\nfactory in giving the electronic structure of the molecule, although there is a bit\nof deviation from the reference data. The cause of the deviation is that we have\napproximated the objective function as a polynomial with integer coefficients af-\nter the Taylor expansion, and as a result, this rough approximation dropped the\nsubtle features of the numeric data used in the standard self-consistent method.\ni Eig (φ x φ ) (φ y φ ) (φ eφ )\ni i i i i i\n| | | | | |\n1 -1.114772 0.604062 -1.114772 -0.537546\n2 1.114772 -0.604062 1.114772 -0.537546\n3 -0.337484 -0.801308 -0.337484 -1.600455\n4 0.337484 0.801308 0.337484 -1.600455"
  },
  {
    "id": 1343,
    "text": "i i i i i i\n| | | | | |\n1 -1.114772 0.604062 -1.114772 -0.537546\n2 1.114772 -0.604062 1.114772 -0.537546\n3 -0.337484 -0.801308 -0.337484 -1.600455\n4 0.337484 0.801308 0.337484 -1.600455\nTable 2: The result of the Hartree-Fock computation of HeH+ by the symbolic\nnumeric method. We used four normalized right eigenvectors φ ),..., φ ) of mT\n1 4 y\n| |\nthat have real eigenvalues (which are given as Eig in the table) and computed\n(φ mT φ ) for j = x,y,e. The third and the fourth solutions give the ground\ni | j | i\nstate.\n2.3 Phase 2: quantum computation\n2.3.1 Tools for quantum computation\nNowwehaverestatedthegivenquestionasaneigenvalueproblem,andweantici-\npatetheapplication ofquantumphaseestimation togettheeigenvalues. There-\nmainingquestionisthattheQPEisnotapplieddirectly,sincethetransformation\nmatrices m are not Hermitian, and the time-evolution operator exp( iTm ) is\np p\n−\nnot unitary. To settle this issue, we use the block-encoding, by which any com-"
  },
  {
    "id": 1344,
    "text": "matrices m are not Hermitian, and the time-evolution operator exp( iTm ) is\np p\n−\nnot unitary. To settle this issue, we use the block-encoding, by which any com-\nplex matrix can be embedded in the diagonal part of certain unitary matrices.\nSeveralalgorithms enableustoconducttheblock-encodinganddesignthequan-\ntum circuits [17, 18, 19].\nThe block encoding of an n-qubit operator A is formally defined as follows:\nA˜= 0 a I U 0 a I (31)\n⊗ n ⊗ n\nh | ⊗ | i ⊗\n(cid:0) (cid:1) (cid:0) (cid:1)\nIn the above, A˜ = αA, for which the factor α is chosen in such a way that\n9"
  },
  {
    "id": 1345,
    "text": "A˜ 1 for all i and j. U is a unitary matrix operating on a+n qubits, and\nij\n| | ≤\nits action on the qubits is given by\nU 0 a φ = 0 a A˜φ + 1 A˜φ 2 σ (32)\n⊗ ⊗ ⊥\n| i ⊗| i | i ⊗ | i −k | ik | i\nq\n(cid:0) (cid:1)\nwith\n0 a I σ = 0 (33)\n⊗ n ⊥\nh | ⊗ | i\n(cid:0) (cid:1)\nand\nσ = 1 (34)\n⊥\nk| ik\nArepetition of partialmeasurementsof theancilla qubitsyields 0 a withprob-\n⊗\n| i\nability A˜φ 2, and the circuit gives rise to k A˜ | φ i .\nk | i} A˜φ\nk | ik\nFor simplicity, let us assume that α = 1 and A 1 for all i and j. In this\nij\n| |≤\ncase, the matrix query operation O is defined by\nA\nO 0 i j = a + 1 a 2 i j (35)\nA ij ij\n| i| i| i −| | | i| i\n(cid:18) q (cid:19)\nwhere i and j are n-qubit computational basis states. The unitary represen-\n| i | i\ntation of O is given by\nA\nc s\n00 00\n−\nc s\n 01 01 \n−\n... ...\n \n \n c s \nO A =   N − 1,N − 1 − N − 1,N − 1   (36)\ns 00 c 00 \n \n s c \n 01 01 \n\n\n... ... \n\n \n \n s c \n N 1,N 1 N 1,N 1 \n− − − −\n "
  },
  {
    "id": 1346,
    "text": "A\nc s\n00 00\n−\nc s\n 01 01 \n−\n... ...\n \n \n c s \nO A =   N − 1,N − 1 − N − 1,N − 1   (36)\ns 00 c 00 \n \n s c \n 01 01 \n\n\n... ... \n\n \n \n s c \n N 1,N 1 N 1,N 1 \n− − − −\n \nwhere c = cos(θ ), s = sin(θ ), and θ = arccos(a ). Keep in mind that the\nij ij ij ij ij ij\nindices of c and S are given by n-qubit computational basis states.\nij ij\nThe quantum circuit that embodies the block encoding is defined by\nU = (I H n I )(I SWAP)O (I H n I ) (37)\nA 1 ⊗ n 1 A 1 ⊗ n\n⊗ ⊗ ⊗ ⊗ ⊗\nwhere I and I means the identity operations; SWAP is the swap gate; H is\n1 n\nthe Hadamard gate. After algebra, one obtains\n1\n0 0 n iU 0 0 n j = a (38)\nh |h | ⊗ h | A | i| i ⊗ | i 2n ij\n10"
  },
  {
    "id": 1347,
    "text": "This relation means that, if the n+1 ancilla qubits are measured as the zero-\nAφ\nstate, the signal register, which is initialized by φ , returns | i .\n| i Aφ\nk ik\nIf A > 1 for some i and j, we must replace A with αA, using a scale\nij\nk k\nfactor α such that α < 1. It increases the complexity of the quantum circuit\n| |\nfor the QPE. If we use the block encoding of U2k for a unitary U with α < 1,\nduring the QPE, the controlled U2k yields\n1\n0\n+αei2kλ\n1 ψ (39)\n√2 | i | i ⊗| i\n(cid:16) (cid:17)\nTo record the eigenvalue λ in the bit string, however, the state at the left qubit\nshould be given by\n0\n+ei2kλ\n1 (40)\n| i | i\nTo get the latter state, we prepare U αI, and we apply (X I)U αI(X I) though\n⊗ ⊗\nthe controlled gate operation. Then we get\nα\n0\n+ei2kλ\n1 (41)\n√2 | i | i\n(cid:16) (cid:17)\nIn(41),αcouldbeneglected onaccountofthenormalization oftheoutputstate.\nThe problem in the above construction is that the naive design of the quan-"
  },
  {
    "id": 1348,
    "text": "α\n0\n+ei2kλ\n1 (41)\n√2 | i | i\n(cid:16) (cid:17)\nIn(41),αcouldbeneglected onaccountofthenormalization oftheoutputstate.\nThe problem in the above construction is that the naive design of the quan-\ntumcircuittoconducttheoperation O requirestoo manynumbersof R gates,\nA y\nwhich causes worse complexity than the classical case. To avoid it, the FABLE\nalgorithm uses Gray codes to designate the operations on the ancilla qubits so\nthat this algorithm achieves improved scaling with respect to the number of the\nR gates [19].\ny\n2.3.2 The quantum steps for the simple toy model and the Hartree-\nFock computation\nIn this section, the accuracy of block encodings for the simple examples (the\nsimple toy model and the Hartree-Fock computation for HeH+ are investigated.\nThose models are equally given in Q[x,y,e] (in the ring with three variables),\nand they are studied together.\nUsing the FABLE algorithm, we construct the block encoding of the unitary"
  },
  {
    "id": 1349,
    "text": "Those models are equally given in Q[x,y,e] (in the ring with three variables),\nand they are studied together.\nUsing the FABLE algorithm, we construct the block encoding of the unitary\noperator A. InTables 3and4, theexpectation values of theblock encodingform\nof unitary operators (φAφ) and (φ exp( iA)φ) for A = mT,mT, and mT are\nx y e\n| | | − |\nshown, respectively, for the two examples. They are computed by numerical lin-\near algebra with a suitable choice of φ. Furthermore, those values are compared\nto (φO φ), where O = O is obtained by the FABLE algorithm.\nexp exp exp( iM)\n| | −\n11"
  },
  {
    "id": 1350,
    "text": "The block encodings by the FABLE algorithm are quantitatively accurate for\nrepresenting the corresponding evolution of non-unitary A.\ni M (φ M φ ) (φ exp( √ 1MT)φ ) (φO φ )\ni i i i exp i\n| | | − − | | |\n1 m 0.707107 0.760245-0.649637j 0.760245-0.649637j\nx\n1 m 0.707107 0.760245-0.649637j 0.760245-0.649637j\ny\n1 m -1.000000 0.540302+0.841471j 0.540302+0.841471j\ne\n2 m 0.707107 0.760245-0.649637j 0.760245-0.649637j\nx\n2 m -0.707107 0.760245+0.649637j 0.760245+0.649637j\ny\n2 m 1.000000 0.540302-0.841471j 0.540302-0.841471j\ne\nTable 3: The expectation values of the unitary operators (φ exp( √ 1A)φ )\ni i\n| − − |\nfor A = mT,mT, and mT in the simple toy model. The table contains the result\nx y e\nfor two different solutions, distinguished by two different eigenvectors (φ and\n1\nφ ), which correspond to the solutions for e = 1 and e = 1, respectively. The\n2\n−\neigenvectors φ ) are computed from the analytic formula given in the previous\ni\n|"
  },
  {
    "id": 1351,
    "text": "1\nφ ), which correspond to the solutions for e = 1 and e = 1, respectively. The\n2\n−\neigenvectors φ ) are computed from the analytic formula given in the previous\ni\n|\nsection. The symbol j in the table means the imaginary unit number √ 1,\n−\ni M (φ MT φ ) (φ exp( √ 1MT)φ ) (φ O φ )\ni i i i i exp i\n| | | − − | | |\n1 m 0.604062 0.823035-0.567990j 0.823035-0.567990j\nx\n1 m -1.114772 0.440383+0.897810j 0.440383+0.897810j\ny\n1 m -0.537546 0.858968+0.512030j 0.858968+0.512030j\ne\n2 m -0.604062 0.823035+0.567990j 0.823035+0.567990j\nx\n2 m 1.114772 0.440383-0.897810j 0.440383-0.897810j\ny\n2 m -0.537546 0.858968+0.512030j 0.858968+0.512030j\ne\n3 m -0.801308 0.695768+0.718267j 0.695768+0.718267j\nx\n3 m -0.337484 0.943591+0.331114j 0.943591+0.331114j\ny\n3 m -1.600455 -0.029654+0.999560j -0.029654+0.999560j\ne\n4 m 0.801308 0.695768-0.718267j 0.695768-0.718267j\nx\n4 m 0.337484 0.943591-0.331114j 0.943591-0.331114j\ny\n4 m -1.600455 -0.029654+0.999560j -0.029654+0.999560j\ne"
  },
  {
    "id": 1352,
    "text": "e\n4 m 0.801308 0.695768-0.718267j 0.695768-0.718267j\nx\n4 m 0.337484 0.943591-0.331114j 0.943591-0.331114j\ny\n4 m -1.600455 -0.029654+0.999560j -0.029654+0.999560j\ne\nTable 4: The expectation values of the unitary operators (φ exp( √ 1A)φ )\ni i\n| − − |\nforA = mT,mT,andmT intheHartree-Fock computation. Forthecomputation\nx y e\nof expectation values, we used four eigenvectors ( φ i = 1,...,4 ) of mT, which\ni y\n{ | }\nhave real eigenvalues.\n12"
  },
  {
    "id": 1353,
    "text": "In the computations presented here, we used the eigenvectors that were ana-\nlytically derived or computed by the eigenvalue solver. In theHartree-Fock case,\nwe cast off the eigenvectors with complex eigenvalues since those useless vectors\nare easily detected by classical computations. However, in quantum computa-\ntions, it is not so easy to examine the state vectors in the quantum circuit. In\nthe next section, we discuss how to carry out the state preparation properly.\n3 Discussion\nIn this section, we discuss several points that should be treated with care.\n3.1 The difficulty of quantum algorithm concerning complex-\nvalued solutions\nThe existence of complex roots of the given system of polynomial equations is\nan obstacle to full-fledged quantum computation in the current problem setting.\nThe standard quantum phase estimation is applied to the Hermitian operators\nwhich have real eigenvalues only. Let λ be an eigenvalue, which is represented\nin the following way:\nλ j j j\n1 2 n"
  },
  {
    "id": 1354,
    "text": "The standard quantum phase estimation is applied to the Hermitian operators\nwhich have real eigenvalues only. Let λ be an eigenvalue, which is represented\nin the following way:\nλ j j j\n1 2 n\n= + + + (42)\n2π 21 22 ··· 2n\nIn the intermediate stage of the computation by the QPE, the quantum state\nvectors are generated and transformed as follows.\n1\nH\n0 φ ( 0 + 1 ) φ (43)\n| i| i −→ √2 | i | i | i\n1 ( 0 + 1 ) φ Λ(U2n−1 ) 1 0 +ei(2π)jn 1 φ (44)\n√2 | i | i | i −−−−−−→ √2 | i | i | i\n(cid:16) (cid:17)\n1\n0 +ei(2π)jn 1 φ H j φ (45)\nn\n√2 | i | i | i −→ | i| i\n(cid:16) (cid:17)\nHowever, if the eigenvalue is given by λ+√ 1ν, the quantum circuit yields\n−\n1\n0 +exp( 2k 2πν)ei(2π)jn 1 φ , (46)\n√2 | i − · | i | i\n(cid:16) (cid:17)\nfrom which, the integer j cannot be extracted at j φ by the Hadamard\nn n\n| i| i\ntransformation.\nSeveral approaches tackle this problem [20, 17, 21].\n13"
  },
  {
    "id": 1355,
    "text": "• The algorithm in [20] generates the state of the form of (46), estimates\nthe factor exp( 2k(2π)ν) by projective measurements on theindex qubit\n| − |\nin the basis 1 1, rotates the quantum state to cancel that factor, and\n| ih |\nobtains the wanted form of (45).\n• The algorithm in [17] similarly estimates exp( 2k(2π)ν) by measure-\n| − |\nments and then obtains the phase part of the eigenvalue.\n• The algorithm in [21] prepares the initial state vector in such a way that\nψ = β E E¯ (47)\ninit j j j\n| i | i| i\nX\nwhere E and E¯ are the eigenvectors of a matrix M, and they have\nj j\n| i | i\nthe conjugated eigenvalues λ +iµ and λ iµ , respectively. The time\ni j i j\n−\nevolution using M I +I M yields\n⊗ ⊗\ne2πiλj∆T E E¯ (48)\nj j\n| i| i\nIn addition, the time evolution using i(M I +I M) yields\n⊗ ⊗\ne2πiµj∆T E E¯ (49)\nj j\n| i| i\nThus, the real and imaginary parts of the eigenvalues are recorded sepa-\nrately in two ancillae:\nλ µ E E¯ (50)\nj j j j\n| i| i| i| i"
  },
  {
    "id": 1356,
    "text": "⊗ ⊗\ne2πiµj∆T E E¯ (49)\nj j\n| i| i\nThus, the real and imaginary parts of the eigenvalues are recorded sepa-\nrately in two ancillae:\nλ µ E E¯ (50)\nj j j j\n| i| i| i| i\nAny of those approaches increases the complexity of the quantum circuits.\nThe former two approaches require additional measurements to determine the\ncomplex amplitude Indeed, before applying those methods, we should prepare a\nparticular eigenvector that has a complex eigenvalue λ+iµ. If not, the measure-\nments do not report correctly exp( 2kµ). The third approach needs a special\n| − |\npreparation of the initial state in which conjugate states are paired.\nThe occurrence of complex eigenvalues is related to the question of how to\nprepare good initial states for the QPE. If we could use classical algorithms, it\nwould be easy to get rid of complex eigenvalues. We prepare the randomized\ninitial state vector and project out the components that give rise to complex"
  },
  {
    "id": 1357,
    "text": "would be easy to get rid of complex eigenvalues. We prepare the randomized\ninitial state vector and project out the components that give rise to complex\neigenvalues. On the other hand, it is laborious to detect complex solutions only\nby quantum algorithms. Regarding this issue, there are several ways of filtering\nout eigenvalues before applying the quantum phase estimations for Hermitian\noperators [22, 23, 24]. In pity, to the best of our knowledge, those methods are\n14"
  },
  {
    "id": 1358,
    "text": "not applied in the removal of complex eigenvalues, since the existing filtering\nmethods make use of convenient properties of Hermitian matrices which always\nhave real eigenvalues. Moreover, those methods are composed to prepare the\nground state, namely, the lowest eigenvalue. Meanwhile, the request of the\npresent study is to obtain all real eigenvalues. However, the following measures\nwould do that task.\n• The inverse power iteration yields the eigenvectors that have the eigenval-\nues closest to the given λ. In the present work, we choose real λ, and we\nuse the parallel character of quantum computation.\n• To this end, we apply the method with (A λI) 1, solving\n−\n−\n0 (A λI) 0 x\nk 1\n− = − (51)\n(AT λI) 0 ! x k! 0 !\n−\nby somequantumlinear systemsolver. We startfrom x = β , repeat\n0 init\n| i | i\nthe computation, and after a suitable number of iterations, obtain the\ndesired quantum states x .\nk\n| i\nThe initial state preparation goes as follows:"
  },
  {
    "id": 1359,
    "text": "0 init\n| i | i\nthe computation, and after a suitable number of iterations, obtain the\ndesired quantum states x .\nk\n| i\nThe initial state preparation goes as follows:\n• Prepare the initial state: β β e . This state is implicitly\ns| i → s| i| s i\ngiven by C v e , where v are the eigenvectors of A, and e\ns j j | j i| s i P { j } Pj s\nis the index to the sampling points for λ in the inverse power method.\nP P\n• Apply the inverse power method:\nC (A e ) N v e (52)\nj s − j s\n− | i| i\ns j\nXX\nwith a sufficiently large N. Then we get\nD v (l) e (53)\nl\n|\nes\ni|\ns\ni\ns l\nXX\nwhere v (l) are the eigenvectors that have the eigenvalue closest to e .\n{\nes\n}\nl s\n• Similarly, doubly applying the inverse power method to β β by (A\n′\n| i| i −\n(λ+iµ)I) 1 (A (λ iµ)I) 1, we get the state C E Eˆ .\n− − j j j\n⊗ − − | i| i\nP\n15"
  },
  {
    "id": 1360,
    "text": "By the measures prescribed above, in general cases, we record the real eigen-\nvalues in the state vector as follows:\nλˆ(e ) e (54)\ns s\n| i| i\nwhere λˆ(e ) is the bit string representation of the eigenvalue closest to e . Note\ns s\nthat one label e shall catch exactly one real eigenvalue. In exceptional cases,\ns\n| i\nhowever, we would prepare an initial state composed of two conjugated eigen-\nvectors that have the eigenvalues λ iµ. The initial state vector is given by\n±\nψ = (p λ+iµ +q λ iµ ) e . (55)\ns\n| i | i | − i | i\nForsuchstatevectors,theQPEcannotobtaintheeigenvaluesasin(54). Instead,\nthe result of the QPE is given by\nC k e (56)\nk s\n| i| i\ns k\nXX\nwhere k = k k k . In this case, the label e is connected to the noisy\n0 1 N s\n| i | ··· i | i\nsuperposition of the states k . If such a result is measured for the label e ,\nk s\n{| i} | i\nit means that the corresponding eigenvalues are complex. We should discard\nthem since complex eigenvalues are meaningless in our problem setting. In the"
  },
  {
    "id": 1361,
    "text": "k s\n{| i} | i\nit means that the corresponding eigenvalues are complex. We should discard\nthem since complex eigenvalues are meaningless in our problem setting. In the\ninverse power iteration, we could use λ √ 1δ (with a shift by small positive\n− −\nδ), instead of genuine real λ, so that we shake off the eigenstate that has the\neigenvalue withthepositiveimaginary part,whichcausestheunboundedgrowth\nof the amplitude in the time evolution. There is another rare exceptional case\nwhere two different state vectors are involved. This case shall happen that the\nsamplingpointe islocatedexactlyinthemiddleoftwoadjacentrealeigenvalues\ns\nlambda and λ . The measurement also gives the noisy superposition of k .\n1 2 k\n{| i}\nHowever, such a circumstance almost surely does not take place, and a small\nshift of the sampling point e e +δ shall avoid it.\ns s\n→\nIf the quantum algorithms for the state preparation do not work well, we"
  },
  {
    "id": 1362,
    "text": "shift of the sampling point e e +δ shall avoid it.\ns s\n→\nIf the quantum algorithms for the state preparation do not work well, we\nare obliged to use one diagonalization of one of the transformation matrices by\nclassical algorithms so that we get rid of the right eigenvectors with complex-\nvalued solutions. The initial state vector could be chosen as an arbitrary linear\ncombination of the right eigenvectors with real eigenvalues. It is an expediency,\nbut it has its merit, as we shall discuss later in Section 3.3.\nNote that if we calculate the eigenenergy of QUBO models by the present\nmethod, the set of polynomial equations is given by\nC x +C x x + +C x x e = 0 (57)\n1 i 2 i1 i2\n···\nn i1··· in\n−\nX\ni i X1,i2 i1X ,...,in\n16"
  },
  {
    "id": 1363,
    "text": "and\nx2 x = 0 for i= 1,...,n. (58)\ni i\n−\nThis kind of equation is without complex solutions, thanks to the restriction of\nthe ranges of x , which is explicitly written by the polynomials. It follows\ni i\n{ }\nthat, if we construct a set of polynomial equations of the Hartree-Fock model as\na QUBO one, there is no problem concerning the complex eigenvalues, although\nthis construction increases the number of qubits and the cost of symbolic com-\nputations.\n3.2 The choice of basis vectors in the eigenvalue problems\nNote that there is an ambiguity in the choice of the basis vectors. In the toy\nmodelcase, the matrix m has two eigenvectors for eigenvalue 1, which are given\ne\nby\n1 1\nv = , , 1,1 (59)\n1\n−√2 √2 −\n(cid:18) (cid:19)\n1 1\nv = , , 1,1 (60)\n2\n√2 −√2 −\n(cid:18) (cid:19)\nWe could choose the basis vectors differently, such as\n1\nw = (v +v )= (0,0, 1,1) (61)\n1 1 2\n2 −\n1\nw = (v v )= ( 1,1,0,0) (62)\n2 1 2\n√2 − −\nHowever, w and w are not suitable choices in the present problem, for they are\n1 2"
  },
  {
    "id": 1364,
    "text": "1\nw = (v +v )= (0,0, 1,1) (61)\n1 1 2\n2 −\n1\nw = (v v )= ( 1,1,0,0) (62)\n2 1 2\n√2 − −\nHowever, w and w are not suitable choices in the present problem, for they are\n1 2\nnot represented by the monomial basis vector b = (ye,y,e,1). Indeed, they are\nnot the eigenvectors of m or m .\nx y\nw m =( 1/2,1/2,0,0) (63)\n1 x\n−\nw m = (0,0,1, 1) (64)\n2 x\n−\nw m = ( 1/2,1/2,0,0) (65)\n1 y\n−\nw m = (0,0, 1,1) (66)\n2 y\n−\nTherefore, if there is a degeneracy of the eigenvalues, we should make the basis\nvectors for the corresponding subspace in such a way that all the basis vectors\nare potentially given by the monomial basis vector b.\n17"
  },
  {
    "id": 1365,
    "text": "3.3 The merit of the quantum algorithm\nTo see the superiority of the quantum algorithm over the classical one, let us\nconsider the following circumstances. Let m be the list of transformation\ni i\n{ }\nmatrices, and assume that mT has two eigenvectors (v and v ) with a common\n1 1 2\neigenvalue E :\nv\nmT v = E v ,mT v = E v . (67)\n1 1 v 1 1 2 v 2\nThese two vectors are not necessarily the eigenvectors of the other m if they are\ni\nnot suitably prepared, as pointed out in Section 3.2.\nIn the classical algorithm, we prepare the eigenvectors of the other mT, say\ni\nmT, by the generalized eigenvalue problem:\n2\n(v mT v ) (v mT v ) (v v ) (v v ) c j 0\n1 | 2| 1 1 | 2| 2 E 1 | 1 1 | 2 1 = (68)\n\" (v 2 | mT 2| v 1 ) (v 2 | mT 2| v 2 )! − j (v 2 | v 1 ) (v 2 | v 2 )!# c j 2 ! 0!\nFrom this equation, we get two eigenvectors\nj j\nw = c v +c v for j = 1,2 (69)\nj 1 1 2 2\nand two corresponding eigenvalues E and E .\nw1 w2\nOntheotherhand,thistaskofsolvingtheeigenvalueproblemcanbeskipped"
  },
  {
    "id": 1366,
    "text": "j j\nw = c v +c v for j = 1,2 (69)\nj 1 1 2 2\nand two corresponding eigenvalues E and E .\nw1 w2\nOntheotherhand,thistaskofsolvingtheeigenvalueproblemcanbeskipped\nin quantum algorithms. To see this, let us use the initial state vector (combined\nwith ancilla qubits) defined by\nψ = (p v +q v )Ancilla Ancilla Ancilla (70)\n1 2 1 2 N\n| i | i | i | i| i···| i\n= (s w +t w )Ancilla Ancilla Ancilla .\n1 2 1 2 N\n| i | i | i| i···| i\nIn the above, p and q would randomly be chosen. Consequently, s and t are de-\ntermined. Thenlet us apply theQPE by mT andrecord the phaseat Ancilla .\n1 | 1 i\nWe get\nψ = (s w +t w )E Ancilla Ancilla (71)\n1 2 v 2 N\n| i | i | i | i| i···| i\nNext, apply the QPE by mT and record the phase at Ancilla . We record the\n2 | 2 i\ncorresponding phases at Ancilla and get\n2\n| i\nψ = s w E E Ancilla (72)\n| i |\n1\ni|\nv\ni|\nw1i···| N\ni\n+t w E E Ancilla\n|\n2\ni|\nv\ni|\nw2i···| N\ni\nIfE = E ,thetwodata(E ,E )and(E ,E )in ψ aredistinguished,and\nw1\n6\nw2 v w1 v w2\n| i"
  },
  {
    "id": 1367,
    "text": "2\n| i\nψ = s w E E Ancilla (72)\n| i |\n1\ni|\nv\ni|\nw1i···| N\ni\n+t w E E Ancilla\n|\n2\ni|\nv\ni|\nw2i···| N\ni\nIfE = E ,thetwodata(E ,E )and(E ,E )in ψ aredistinguished,and\nw1\n6\nw2 v w1 v w2\n| i\nthey are the parts of two distinct roots of the given set of polynomial equations.\n18"
  },
  {
    "id": 1368,
    "text": "If E = E , we successively apply the QPE using mT,...,mT . Then we finally\nw1 w2 3 N\nget the distinct roots of the form (E v ,E w ,E w ′,...,E w( ′···′ ) ). Each of the roots is\nrecorded in one of the orthonormalized bases in the output state and measured\ndistinctly one from the other since the orthogonality is guaranteed by the bit\nstring representation of the ancilla.\n3.4 On the enormous complexity concerning symbolic compu-\ntation\nAnother obstacle is the enormous complexity in the computation of Gro¨bner\nbasis, which scales with the number of variables (n) and the maximal degree of\nthe input polynomials (d). If the primitive algorithm (as was initially proposed)\nisapplied,thecomplexityisdoublyexponentialinnfortheworstcase. However,\ndetailed inspections have revealed the following fact [25].\n• Let (f ,...,f ) be a system of homogeneous polynomials in k[x ,...,x ]\n1 m 1 n\nwhere k is an arbitrary field. (A homogeneous polynomial is composed of"
  },
  {
    "id": 1369,
    "text": "• Let (f ,...,f ) be a system of homogeneous polynomials in k[x ,...,x ]\n1 m 1 n\nwhere k is an arbitrary field. (A homogeneous polynomial is composed of\nnonzero monomials, all of which have the same degree.)\n• The number of operations to compute a Gro¨bner basis of the ideal I =\n(f ,...,f ) for a graded monomial ordering up to degree D scales with\n1 m\nω\nn+D 1\nO mD − as D (73)\nD ! ! → ∞\nwhere ω is the exponent of matrix multiplication over k. Namely, ω is the\nsmallest constant such that two N N matrices could be multiplied by\n×\nperforming O(Nω+ǫ) arithmetic operations for every ǫ > 0.\nIn this estimation, the bound D for a full Gro¨bner basis is not yet given.\nHowever, it could be estimated under a certain assumption, and the conclusion\nis that the complexity is simply exponential in n, thanks to the assumption that\nthe polynomials are homogeneous [25]. As any system of polynomials can be\ntransformed into this form by adding a variable and homogenizing, it means"
  },
  {
    "id": 1370,
    "text": "the polynomials are homogeneous [25]. As any system of polynomials can be\ntransformed into this form by adding a variable and homogenizing, it means\nthat the doubly exponential complexity could be avoided.\nNote that the estimation of the complexity is carried out for the worst cases,\nmeanwhile, the actual computations often finish with much lower computational\ncosts. Moreover, the algorithmic improvements are successful in facilitating the\ncomputation. Currently, the F5 algorithm is regarded as the most effective one\n[26]. The complexity of this algorithm was studied in [25]. The formula of\n19"
  },
  {
    "id": 1371,
    "text": "the complexity is given in a refined style that reflects the special feature of the\nalgorithm, although it is still exponential in n.\nThe complexity in computing Gro¨bner basis, however, would be mitigated\nby quantum algorithms. The computational steps of the Gro¨bner basis are as\nfollows [3, 4, 5, 6, 8, 7]\n• Input: F = (f ,...,f ); Output: the Gro¨bner basis G for F\n1 m\n• G := F\n• For every pair of polynomials (f ,f ) in F, compute the s-polynomial,\ni j\nwhich is defined by\na a\nij ij\nS(f ,f ) = f f . (74)\ni j i j\ng − g\nj i\nIn the above, g (resp. g ) the leading term of f (resp. f ) in the given\ni j i j\nmonomial ordering, and a is the least common multiple of g and g .\nij i j\n• ReduceS(f ,f ). By thedivisionalgorithm, it isrepresented as S(f ,f ) =\ni j i j\nc f +r, and the residual r is the result. If r = 0, add r to G.\nl l l 6\n• R P epeat the computation of s-polynomials and the reduction until the ex-\ntension of G terminates.\n• Return G."
  },
  {
    "id": 1372,
    "text": "i j i j\nc f +r, and the residual r is the result. If r = 0, add r to G.\nl l l 6\n• R P epeat the computation of s-polynomials and the reduction until the ex-\ntension of G terminates.\n• Return G.\nThis algorithm is essentially a Gaussian elimination that carries out the re-\nduction of rows in a matrix that holds the coefficients in the system of polyno-\nmials [27, 26]. The difficulties in conducting that task are as follows.\n• InthereductionofS(f ,f ), ithappensthatmanypairsofthepolynomials\ni j\nreduceto zero, beingcompletely useless to theconstruction of theGro¨bner\nbasis. It is necessary to detect the unnecessary pairs beforehand, and the\ntrials to improve the efficiency of the algorithm are intensively carried out\nregarding this issue.\n• The size of the Gaussian elimination would vary, indeed increase, during\nthecomputation. Thecomputationofs-polynomialsshallincreasethetotal\nnumber of the maximum degree of the polynomials in G. The expanding"
  },
  {
    "id": 1373,
    "text": "thecomputation. Thecomputationofs-polynomialsshallincreasethetotal\nnumber of the maximum degree of the polynomials in G. The expanding\nmatrix requires ever-increasing usage of a vast amount of memory.\n20"
  },
  {
    "id": 1374,
    "text": "To reduce the computational costs enumerated above, the quantum algo-\nrithms would be hopeful choices. First, the qubits could encompass a vast set\nof quantum states that describe a set of data with enormous size. They could\nembrace the incessant increase of polynomial data during the computation of\nGro¨bner basis. Second, the quantum algorithms are efficient in linear computa-\ntion and searching for unconstructed data. The HHL algorithm would facilitate\nthe computation of Gaussian elimination. Moreover, theGrover database search\nalgorithm would be useful in detecting the terms in polynomials that should be\neliminated in the reduction.\n4 Conclusion\nThe main aim of this paper is to illustrate a computational scheme of quantum\ncomputation that enables us to carry out the Hartree-Fock computation, in the\nsense that the computation shall realize the optimization of molecular orbitals\ncomposed of atomic bases. The proposed computational scheme uses algebraic"
  },
  {
    "id": 1375,
    "text": "sense that the computation shall realize the optimization of molecular orbitals\ncomposed of atomic bases. The proposed computational scheme uses algebraic\ntechniques to reform the Hartree-Fock equations into a set of eigenvalue prob-\nlems, wherein the eigenvalues give the LCAO coefficients, the orbital energy,\nand if necessary, the optimized atomic coordinates. The eigenvalue problems\ncould be solved by the quantum phase estimation through the block-encoding\ntechnique for non-Hermitian or non-unitary operators. The computed results\nare recorded in quantum states, which shall be used for more complicated quan-\ntum computations with the aid of quantum RAM. There are several unsettled\npoints in the present work. The first is the occurrence of complex-valued eigen-\nvalues in the eigenvalue problem, which is caused by the potential occurrence\nof complex-valued solutions of the Hartree-Fock equation which is treated as a"
  },
  {
    "id": 1376,
    "text": "values in the eigenvalue problem, which is caused by the potential occurrence\nof complex-valued solutions of the Hartree-Fock equation which is treated as a\nsystem of polynomial equations. For the sound application of the QPE,this sort\nof eigenstates should be removed by any means – if it is algorithmically difficult,\nthe quantum device should remove them. The second is the possibly enormous\ncomplexity of symbolic computation. However, the required symbolic computa-\ntions are Gauss eliminations, which would be facilitated by quantum algorithms\never proposed.\nReferences\n[1] Akihito Kikuchi. An approach to first principles electronic structure com-\nputation by symbolic-numeric computation. QScience Connect, 2013:14,\n21"
  },
  {
    "id": 1377,
    "text": "2013. http://dx.doi.org/10.5339/connect.2013.14.\n[2] Attila Szabo and Neil S Ostlund. Modern quantum chemistry: introduction\nto advanced electronic structure theory. Courier Corporation, 2012.\n[3] Wolfram Decker and Christoph Lossen. Computing in algebraic geometry,\nvolume 16 of algorithms and computation in mathematics, 2006.\n[4] Frank Sottile. From Enumerative Geometry to Solving Systems of Polyno-\nmial Equations. In Computations in algebraic geometry with Macaulay 2,\npages 101–130. Springer, 2001.\n[5] David A Cox, John Little, and Donal O’shea. Using algebraic geometry,\nvolume 185. Springer Science & Business Media, 2006.\n[6] David Cox,JohnLittle, andDonalOShea. Ideals, varieties, and algorithms:\nan introduction to computational algebraic geometry and commutative alge-\nbra. Springer Science & Business Media, 2013.\n[7] Viviana Ene and Ju¨rgen Herzog. Gr¨obner Bases in Commutative Algebra,\nvolume 130. American Mathematical Soc., 2011."
  },
  {
    "id": 1378,
    "text": "bra. Springer Science & Business Media, 2013.\n[7] Viviana Ene and Ju¨rgen Herzog. Gr¨obner Bases in Commutative Algebra,\nvolume 130. American Mathematical Soc., 2011.\n[8] David Eisenbud. Commutative Algebra: with a view toward algebraic geom-\netry, volume 150. Springer Science & Business Media, 2013.\n[9] Patrizia Gianni, Barry Trager, and Gail Zacharias. Gro¨bner bases and pri-\nmarydecompositionofpolynomialideals.JournalofSymbolic Computation,\n6(2-3):149–167, 1988.\n[10] David Eisenbud, Craig Huneke, and Wolmer Vasconcelos. Direct meth-\nods for primary decomposition. Inventiones mathematicae, 110(1):207–235,\n1992.\n[11] H Michael M¨oller. On decomposing systems of polynomial equations with\nfinitely many solutions. Applicable Algebra in Engineering, Communication\nand Computing, 4(4):217–230, 1993.\n[12] T Shimoyama and K Yokoyama. Localization and primary decomposition\nof polynomial ideals. Journal of Symbolic Computation, 22:247–277, 1996."
  },
  {
    "id": 1379,
    "text": "and Computing, 4(4):217–230, 1993.\n[12] T Shimoyama and K Yokoyama. Localization and primary decomposition\nof polynomial ideals. Journal of Symbolic Computation, 22:247–277, 1996.\n[13] Daniel Lazard. Solvingzero-dimensional algebraic systems. Journal of sym-\nbolic computation, 13(2):117–131, 1992.\n22"
  },
  {
    "id": 1380,
    "text": "[14] Ichio Kikuchi and Akihito Kikuchi. Ab-initio quantum chemistry through\ncomputer algebra–a challenge beyond born-oppenheimer theory. OSF\nPreprints, 2022. https://osf.io/preprints/qd8nz/.\n[15] Frank Sottile. From enumerative geometry tosolvingsystems ofpolynomial\nequations. In Computations in algebraic geometry with Macaulay 2, pages\n101–129. Springer, 2002.\n[16] S Francis Boys. Electronic wave functions-i. a general method of calcula-\ntion for the stationary states of any molecular system. Proceedings of the\nRoyal Society of London. Series A. Mathematical and Physical Sciences,\n200(1063):542–554, 1950.\n[17] Anmer Daskin, Ananth Grama, and Sabre Kais. A universal quantum cir-\ncuit scheme for findingcomplex eigenvalues. Quantum information process-\ning, 13:333–353, 2014.\n[18] Guang Hao Low and Isaac L Chuang. Hamiltonian simulation by qubitiza-\ntion. Quantum, 3:163, 2019.\n[19] Daan Camps and Roel Van Beeumen. Fable: Fast approximate quantum"
  },
  {
    "id": 1381,
    "text": "ing, 13:333–353, 2014.\n[18] Guang Hao Low and Isaac L Chuang. Hamiltonian simulation by qubitiza-\ntion. Quantum, 3:163, 2019.\n[19] Daan Camps and Roel Van Beeumen. Fable: Fast approximate quantum\ncircuits for block-encodings. In 2022 IEEE International Conference on\nQuantum Computing and Engineering (QCE), pages 104–113. IEEE, 2022.\n[20] HefengWang,Lian-AoWu,Yu-xiLiu,andFrancoNori.Measurement-based\nquantum phase estimation algorithm for finding eigenvalues of non-unitary\nmatrices. Physical Review A, 82(6):062303, 2010.\n[21] Changpeng Shao. Computing eigenvalues of diagonalizable matrices on a\nquantumcomputer. ACM Transactions on Quantum Computing, 3(4):1–20,\n2022.\n[22] David Poulin and Pawel Wocjan. Preparing ground states of quantum\nmany-body systems on a quantum computer. Physical review letters,\n102(13):130503, 2009.\n[23] Yimin Ge, JordiTura, andJ Ignacio Cirac. Faster groundstate preparation\nand high-precision ground energy estimation with fewer qubits. Journal of"
  },
  {
    "id": 1382,
    "text": "102(13):130503, 2009.\n[23] Yimin Ge, JordiTura, andJ Ignacio Cirac. Faster groundstate preparation\nand high-precision ground energy estimation with fewer qubits. Journal of\nMathematical Physics, 60(2), 2019.\n[24] Lin Lin and Yu Tong. Optimal polynomial based quantum eigenstate filter-\ning with application to solving quantum linear systems. Quantum, 4:361,\n2020.\n23"
  },
  {
    "id": 1383,
    "text": "[25] Magali Bardet, Jean-Charles Faug`ere, and Bruno Salvy. On the complexity\nof the f5 gro¨bner basis algorithm. Journal of Symbolic Computation, 70:49–\n70, 2015.\n[26] Jean Charles Faug`ere. A new efficient algorithm for computing Gro¨bner\nbases without reduction to zero (F 5). In Proceedings of the 2002 inter-\nnational symposium on Symbolic and algebraic computation, pages 75–83.\nACM, 2002.\n[27] Jean-Charles Faugere. A new efficient algorithm for computing gro¨bner\nbases (F4). Journal of pure and applied algebra, 139(1-3):61–88, 1999.\n24"
  },
  {
    "id": 1384,
    "text": "SHENNONGALPHA: AN AI-DRIVEN SHARING AND\nCOLLABORATION PLATFORM FOR INTELLIGENT CURATION,\nACQUISITION, AND TRANSLATION OF NATURAL MEDICINAL\nMATERIAL KNOWLEDGE\nYANGZijie1,2,4,5,6,YINYongjing3,6,KONGChaojun2,4,6,CHITiange2,4,6,TAOWufan2,4,6,(cid:12),ZHANG\nYue3,5,6,(cid:12),andXUTian2,4,5,6,7,(cid:12)\n1FudanUniversity,Shanghai,China\n2KeyLaboratoryofGrowthRegulationandTranslationResearchofZhejiangProvince,SchoolofLifeSciences,\nWestlakeUniversity,Hangzhou,Zhejiang,China\n3SchoolofEngineering,WestlakeUniversity,Hangzhou,Zhejiang,China\n4WestlakeLaboratoryofLifeSciencesandBiomedicine,Hangzhou,Zhejiang,China.\n5ResearchCenterforIndustriesoftheFuture,WestlakeUniversity,Hangzhou,Zhejiang,China\n6ShennongProgram,WestlakeUniversity,Hangzhou,Zhejiang,China\n7ChangpingLaboratory,Beijing,China\n(cid:12)Correspondingauthors\n{yangzijie, taowufan, zhangyue, xutian}@westlake.edu.cn\nABSTRACT\nNaturalMedicinalMaterials(NMMs)havealonghistoryofglobalclinicalapplicationsandawealth"
  },
  {
    "id": 1385,
    "text": "(cid:12)Correspondingauthors\n{yangzijie, taowufan, zhangyue, xutian}@westlake.edu.cn\nABSTRACT\nNaturalMedicinalMaterials(NMMs)havealonghistoryofglobalclinicalapplicationsandawealth\nofrecordsandknowledge. AlthoughNMMsareamajorsourcefordrugdiscoveryandclinicalap-\nplication,theutilizationandsharingofNMMknowledgefacecrucialchallenges,includingthestan-\ndardizeddescriptionofcriticalinformation,efficientcurationandacquisition,andlanguagebarriers.\nTo address these, we developed ShennongAlpha, an AI-driven sharing and collaboration platform\nfor intelligent knowledge curation, acquisition, and translation. For standardized knowledge cu-\nration, the platform introduced a Systematic Nomenclature to enable accurate differentiation and\nidentification of NMMs. More than fourteen thousand Chinese NMMs have been curated into the\nplatform along with their knowledge. Furthermore, the platform pioneered chat-based knowledge"
  },
  {
    "id": 1386,
    "text": "identification of NMMs. More than fourteen thousand Chinese NMMs have been curated into the\nplatform along with their knowledge. Furthermore, the platform pioneered chat-based knowledge\nacquisition,standardizedmachinetranslation,andcollaborativeknowledgeupdating. Together,our\nstudy represents the first major advance in leveraging AI to empower NMM knowledge sharing,\nwhich not only marks a novel application of AI for Science, but also will significantly benefit the\nglobalbiomedical,pharmaceutical,physician,andpatientcommunities.\n1 Main toserveasaprolificsourceofnoveldrugleads[4,5]. In\nfact, around50%oftheFDA-approveddrugsarenatural\nproducts or related molecules [1]. Furthermore, NMMs\nNatural Medicinal Materials (NMMs) have been a rich havebeendirectlyusedfortherapeuticpurposesthrough-\nreservoir of therapeutic agents [1]. Their importance is outhumanhistoryandcontinuetoplayasignificantrole"
  },
  {
    "id": 1387,
    "text": "reservoir of therapeutic agents [1]. Their importance is outhumanhistoryandcontinuetoplayasignificantrole\nhighlighted by the diversity and biological relevance of in different societies. The broad clinical application of\nthecompoundstheyproduce[2,3]. Thecompoundsiso- NMMshasbeendocumentedinvariouscountriesandre-\nlated from NMMs or their derivatives have been instru- gions,includingChina[6],Japan[7],SouthKorea[8],In-\nmentalinthetreatmentofvariouspathologicalconditions, dia[9],Iran[10],Europe[11,12],theAmericas[13,14],\nranging from infectious diseases to cancer, and continue\n4202\nyaM\n61\n]IA.sc[\n2v02000.1042:viXra"
  },
  {
    "id": 1388,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nAfrica[15,16],andtheArabregion[17,18]. Forexam- forms like ChatGPT also exacerbate these inaccuracies,\nple, Chinese historical records alone documented nearly asdemonstratedbytheirincorrectassertionthatEphedra\ntenthousandNMMsandtheirapplications[19]. Thisvast sinicaisthesolespeciesoriginforEphedra(Fig.S2).The\nrepositoryofknowledgehasproventobevaluableindis- rootcauseoftheseissuesliesinthecurrentabsenceofa\ncoveringnewtherapies[20–22].Therefore,theglobaluti- standardized and systematic approach to accurately dif-\nlization, sharing, and collaboration of NMM knowledge ferentiateandidentifyeachuniqueNMMwiththeinfor-\nare crucial for advancing biomedical and pharmacologi- mation regarding its species origin, medicinal part, spe-\ncalresearchandapplications. cialdescription, andprocessingmethod, andtocorrectly"
  },
  {
    "id": 1389,
    "text": "are crucial for advancing biomedical and pharmacologi- mation regarding its species origin, medicinal part, spe-\ncalresearchandapplications. cialdescription, andprocessingmethod, andtocorrectly\ncatalog its related knowledge. Given the vast number\nHowever,thegreatpotentialofNMMshasnotbeenfully\nof NMMs, along with their extensive and yet dispersed\nexplored despite their diverse chemical reservoirs, rich\nrecords, it is, therefore, a daunting task to curate the re-\nhistoryofhumanusage,andvastamountsofaccumulated\nlated information using traditional approaches and to fa-\nrecords and knowledge. The biomedical, pharmaceuti-\ncilitateefficientknowledgeacquisition.\ncal,physician,andpatientcommunitiesfaceseveralchal-\nlengesinutilizingandsharingNMMknowledge(Fig.1a). ThelackofamultilingualplatformforNMMspresentsa\nAtthecenterofthechallengesistheconspicuousabsence significantbarrierfortheglobalbiomedicalandpharma-"
  },
  {
    "id": 1390,
    "text": "lengesinutilizingandsharingNMMknowledge(Fig.1a). ThelackofamultilingualplatformforNMMspresentsa\nAtthecenterofthechallengesistheconspicuousabsence significantbarrierfortheglobalbiomedicalandpharma-\nof a standardized and systematic nomenclature capable ceuticalcommunities, aswellasphysiciansandpatients.\nof accurately differentiating and identifying each NMM. Theabsenceofastandardizedandsystematicnomencla-\nThis magnifies the difficulty of efficiently procuring ac- ture also impedes the global sharing and dissemination\ncurate and reliable knowledge about NMMs. The en- of knowledge about NMMs. This issue is evident when\nriched information and records for NMMs further pose translatingtheChinesesentenceregardingtheNMMen-\nchallengesforcurationandacquisition. Finally,language try “Ma Huang” (麻黄是一种天然药材) via Google\nbarriers also hinder the global dissemination and utiliza- Translate. Theresultingtranslationof“Ephedraisanatu-\ntionofNMMknowledge. ralmedicine”impliesasingleplantratherthanthemulti-"
  },
  {
    "id": 1391,
    "text": "barriers also hinder the global dissemination and utiliza- Translate. Theresultingtranslationof“Ephedraisanatu-\ntionofNMMknowledge. ralmedicine”impliesasingleplantratherthanthemulti-\nspecies intricacies of “Ma Huang” (Fig. S3). Such mis-\nThe problem of lacking standardized and systematic\ntranslationscouldmisguideEnglish-speakinguserstouse\nnomenclature for NMMs can be exemplified by the dis-\n“Ephedra”forfurtherknowledgesearchesandpotentially\ncovery of the anti-malaria drug artemisinin, which was\nlead them to the aforementioned misleading Wikipedia\nawardedthe2015NobelPrizeinPhysiologyorMedicine.\npage.\nThis discovery emerged from historically documented\nmalaria-treating NMM with the Chinese name “Qing Tosystematicallytacklethesechallenges,wehavedevel-\nHao”. However, this name could refer to at least six oped an AI-driven, web-based sharing platform named\ndifferent plants in the genus Artemisia. The artemisinin ShennongAlpha for intelligent curation, acquisition, and"
  },
  {
    "id": 1392,
    "text": "different plants in the genus Artemisia. The artemisinin ShennongAlpha for intelligent curation, acquisition, and\nmolecule was eventually found only in Artemisia annua translation of NMM knowledge (Fig. 1). The platform\nwith the Chinese name of “Huang Hua Hao”, but not in is also an open, collaborative, and evolving hub, which\nfiveotherplants, includingthemodern-daybotanicplant enables the global community to participate and con-\nwiththeChinesenameof“QingHao”(Artemisiacaruifo- tributetoNMMknowledge. Asthefirstmajoradvancein\nlia) [21]. Although the Chinese Pharmacopoeia (2020 leveragingAItoempowerNMMknowledgesharing,the\nedition)nowspecifiesArtemisiaannuaasthesoleNMM ShennongAlphaplatformcurrentlycontainsknowledgeof\nsuitableformedicinaluse[23],overthree-quartersofthe morethan14,000ChineseNMMs. Bypresentinganex-\nNMMsinthecurrenteditionstillhaveambiguousnames emplarycaseinthisnovelAIforSciencefield,ourstudy"
  },
  {
    "id": 1393,
    "text": "suitableformedicinaluse[23],overthree-quartersofthe morethan14,000ChineseNMMs. Bypresentinganex-\nNMMsinthecurrenteditionstillhaveambiguousnames emplarycaseinthisnovelAIforSciencefield,ourstudy\n(Table S1). For example, the NMM “Ma Huang” (also hasnotonlypropelledAImethodologiesandapplications\nknown as “Ephedra” or “Ephedrae Herba”) corresponds but also will significantly accelerate the NMM research\ntothreespecies: Ephedrasinica,Ephedraintermedia,or and applications for the global biomedical, pharmaceuti-\nEphedra equisetina (Fig. 1c). The same problem exists cal,andphysicianandpatientcommunities.\nfor NMMs in other countries. In the Indian Pharma-\ncopoeia,forinstance,thename“Acacia”pertainstomul-\n2 Results\ntiple species within the Acacia genus [24]. Similar am-\nbiguity problems also exist regarding the descriptions of\n2.1 SystematicNomenclaturedefinesunique\nthepartsusedformedicinalpurposesandthepreparation\nidentitiesandcriticalinformationforNMMs\nprocessesofNMMs(TableS1)."
  },
  {
    "id": 1394,
    "text": "2.1 SystematicNomenclaturedefinesunique\nthepartsusedformedicinalpurposesandthepreparation\nidentitiesandcriticalinformationforNMMs\nprocessesofNMMs(TableS1).\nTheabsenceofastandardizedandsystematicnomencla- In order to accurately differentiate and identify each\nturefordefiningNMMsfurthercomplicatesboththecu- NMM, we introduced a Systematic Nomenclature to de-\nration of related information and the acquisition of reli- fineuniqueidentitiesandcriticalinformationforNMMs\nable and consistent knowledge about these NMMs. For (SupplementarytextCandD).TheSystematicNomen-\nexample,Wikipediaoftenprovidesmisleadingentriesfor clature specifies each NMM with a Systematic Name, a\nNMMs. The entry for “Ephedra” or “Ma Huang” mis- GenericName,andauniqueNMMID(Fig.2). TheSys-\nleadingly describes “Ephedra as a medicinal preparation tematicNameisanauthoritativedesignationforNMMsto\nfromtheplantEphedrasinica”(Fig.S1).Chat-basedplat- ensureacademicaccuracy. Itcomprisesfourcomponents:"
  },
  {
    "id": 1395,
    "text": "fromtheplantEphedrasinica”(Fig.S1).Chat-basedplat- ensureacademicaccuracy. Itcomprisesfourcomponents:\nI.theLatinnamethatdefinesthespeciesoriginofNMM;\n2"
  },
  {
    "id": 1396,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nII. the part of the organism used for medicinal purposes; ferentiationandidentification. NMMsoftenexistinthree\nIII. the special description providing information about types (Fig. 2b): Raw NMMs, Agricultural NMMs, and\ntheinitialpreparationattheproductionsiteandotherspe- Processed NMMs. Raw NMMs are unprocessed, repre-\ncificcharacteristics;andIV.theprocessingmethodtopro- sentingNMMsintheirnaturalstate. AgriculturalNMMs\nduce the NMM for medicinal use (Fig. 2a, Fig. S4). In- are derived from Raw NMMs through initial prepara-\nformationforthespecialdescription(III)andtheprocess- tion at the production site and are not intended for di-\ning method (IV) is included when this critical informa- rect medicinal use. Agricultural NMMs are often regu-\ntionisnecessary. TheGenericNamefollowsacommonly lated as agricultural products. On the other hand, Pro-"
  },
  {
    "id": 1397,
    "text": "tionisnecessary. TheGenericNamefollowsacommonly lated as agricultural products. On the other hand, Pro-\nused traditional name, facilitating everyday communica- cessedNMMsarefurtherprocessedusingspecificmeth-\ntionandaidingphysiciansinprescribing. TheuniqueID odsformedicinaluse. Theseprocessingmethodsorpro-\nisa4-digitbase-36codewiththeprefix“NMM-”,provid- cedures often change the molecular nature and composi-\ning a unique identity for each NMM on the digital plat- tion of the NMMs, which are critical for medicinal effi-\nform. cacy. Processed NMMs are typically regulated as phar-\nmaceuticals. In countries like China [23], Agricultural\nFor instance, the Systematic Name for “Jing Pian Jiang\nand Processed NMMs are subject to different regulatory\nHuang” (净片姜黄) is “Curcuma wenyujin Rhizome\nframeworks due to their distinct applications in agricul-\nFreshly-sliced Cleaned” [25] (Fig. 2c). This indicates\nture and pharmaceuticals. Historically, the information"
  },
  {
    "id": 1398,
    "text": "frameworks due to their distinct applications in agricul-\nFreshly-sliced Cleaned” [25] (Fig. 2c). This indicates\nture and pharmaceuticals. Historically, the information\nthat the NMM “Jing Pian Jiang Huang” is derived from\nfortheinitialpreparationandprocessingmethodisnota\nthespeciesCurcumawenyujin,whichutilizestherhizome\npart of the identities of NMMs. Due to this, consumers\nas its medicinal part, and is freshly sliced at the pro-\noftenmistakenlypurchaseAgriculturalNMMsinsteadof\nductionsiteandthenfurtherprocessedbyspecificclean-\nthe necessary medicinal-grade Processed NMMs. This\ning methods for medicinal use. The Generic Name uses\noversight also results in regulatory challenges for indus-\nits traditional Chinese name, “Jing-pian-jiang-huang”.\ntryandgovernment(SupplementarytextCandD).\nThe assigned unique ID is “NMM-0016”. Another ex-\nample of a Systematic Name is “Ephedra sinica Stem- To implement the Systematic Nomenclature for a large"
  },
  {
    "id": 1399,
    "text": "tryandgovernment(SupplementarytextCandD).\nThe assigned unique ID is “NMM-0016”. Another ex-\nample of a Systematic Name is “Ephedra sinica Stem- To implement the Systematic Nomenclature for a large\nherbaceous Segmented and Aquafried-honey” for “Mi numberofNMMsfortheresearcherandconsumercom-\nCao Ma Huang” (蜜草麻黄) [26] (Fig. 2a). It contains munities, we developed an algorithm and its online web\nthespeciesorigin,medicinalpart,andprocessingmethod, application,ShennongName,forinteractiveutility(Fig.3,\nbutnocriticalinformationforthespecialdescription. SupplementarymethodE,https://shennongalpha.\nwestlake.edu.cn/name). Researchers and users can\nFor the species origin, some NMMs come from more\ncontribute new NMM entries or modify existing ones\nthan one species. In this case, the Systematic Name\nthrough ShennongName, which will be integrated into\nwill incorporate the Latin names of all pertinent species\nthe AI platform after review (Fig. 3 8, also see below)."
  },
  {
    "id": 1400,
    "text": "through ShennongName, which will be integrated into\nwill incorporate the Latin names of all pertinent species\nthe AI platform after review (Fig. 3 8, also see below).\nto ensure clarity. For example, the Systematic Name\nTheSystematicNomenclature,accompaniedbytheShen-\nfor “Ma Huang” (麻黄) includes the species of Ephedra\nnongName algorithm and application, provides a stan-\nequisetina, Ephedra intermedia, and Ephedra sinica\ndardized, unambiguous, and efficient platform for differ-\n(“Ephedra equisetina vel intermedia vel sinica Stem-\nentiatingandidentifyingNMMs.\nherbaceous”)[27]. If,inthehistoricalrecord,thespecies\norigin for an NMM is not clear, the term “unspecified”\nwill be included in the Systematic Name. For exam-\n2.2 ShennongAlpha: anAI-drivensharingand\nple, “Pu Gong Ying” (蒲公英) in the Chinese Pharma-\ncollaborationplatformforintelligentcuration,\ncopoeia(2020edition)isdescribedasanNMMusingits\nacquisition,andtranslationofNMMknowledge\nwholeplant. However,itcouldoriginatefromanyspecies"
  },
  {
    "id": 1401,
    "text": "collaborationplatformforintelligentcuration,\ncopoeia(2020edition)isdescribedasanNMMusingits\nacquisition,andtranslationofNMMknowledge\nwholeplant. However,itcouldoriginatefromanyspecies\nwithintheTaraxacumgenus,whichincludesthousandsof\nFollowing Systematic Nomenclature, we embarked on a\nspecies[28].Itisunclearwhichsingleormultiplespecies\ncomprehensiveprocesstocollectandcurateNMMknowl-\nwithin Taraxacum have the medicinal effect and can be\nedge. Theinitialphaseoftheendeavordescribedherein-\nattributed to “Pu Gong Ying”. Therefore, we designate\nvolvedgatheringatotalof14,256ChineseNMMentries\ntheSystematicNamefor“PuGongYing”as“Taraxacum\n(Table S2), as they are the largest collection of NMMs.\nunspecified Herb” [29]. For the medicinal part, differ-\nThe resources include the 2020 and 2015 editions of the\nent parts of the same species can potentially be used as\nChinesePharmacopeia[23,32],andtheChineseMedic-\nNMMs with varying medicinal properties. Therefore, it"
  },
  {
    "id": 1402,
    "text": "ent parts of the same species can potentially be used as\nChinesePharmacopeia[23,32],andtheChineseMedic-\nNMMs with varying medicinal properties. Therefore, it\ninal Information Platform [33]. Using this knowledge,\nisnecessarytospecifytheexactpartusedformedicinein\nwe applied the Systematic Nomenclature and Shennong-\ntheSystematicNametoensureclarity. Forexample,both\nNametocomputetherelevantinformationandgenerated\nthe herbaceous stem and the root of Ephedra sinica can\nthe Systematic Name, Generic Name, and ID for each\nbe used medicinally; hence, their Systematic Names are\nNMM. Meanwhile, this collected knowledge was digi-\ndesignatedas“EphedrasinicaStem-herbaceous”[30]and\ntalized, structured, reviewed, and curated into an NMM\n“EphedrasinicaRoot”[31],respectively.\nKnowledgeBase(Fig.S5). TosharethisvaluableKnowl-\nThe information for local production and processing edgeBasewithworldwideusersandfacilitateitsutiliza-"
  },
  {
    "id": 1403,
    "text": "“EphedrasinicaRoot”[31],respectively.\nKnowledgeBase(Fig.S5). TosharethisvaluableKnowl-\nThe information for local production and processing edgeBasewithworldwideusersandfacilitateitsutiliza-\nmethods for medicinal use is also critical for NMM dif- tionandcollaboration,wedevelopedanAI-poweredweb\nplatformnamed“ShennongAlpha”fortheefficientandin-\n3"
  },
  {
    "id": 1404,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\ntelligent curation, acquisition, and translation of related https://shennongalpha.westlake.edu.cn/\nNMMknowledge. knowledge/<nmm-id>\nShennongAlpha, named after the pioneer of Chinese For instance, “Ma Huang” is assigned the NMM ID:\nmedicine, is an advanced AI platform characterized by “NMM-0006”,leadingtotheknowledgepage:\nits comprehensive integration. It combines Shennong-\nhttps://shennongalpha.westlake.edu.cn/\nName with a hexa-domain modular system to establish\nknowledge/nmm-0006\nits foundational architecture. This architecture encom-\npassesacustom-designedNMMWeb,aKnowledgeBase, BesidesaccessinginformationviadirectURLs,userscan\na Search Engine, and a Large Language Model (LLM) leverage a search function. This feature is prominently\nsystem, alongwithanapplicationforchat-basedhuman- availablewithinasearchboxonthehomepage(Fig.4 1)"
  },
  {
    "id": 1405,
    "text": "system, alongwithanapplicationforchat-basedhuman- availablewithinasearchboxonthehomepage(Fig.4 1)\nmachine interaction and an application for standardized oratthetopnavigationbarofeveryotherpage(Fig.4 2),\nmachinetranslation(Fig.1b). allpoweredbytheShennongAlphaSearchEngine. Upon\nexecuting a search, results are presented on a dedicated\nWe believe that the cornerstone of an excellent AI plat-\nsearch page, where each entry contains pertinent details,\nform lies in its user-friendliness and accessibility. With\nincludingtheNMMID,SystematicName,GenericName,\nthis in mind, ShennongAlpha has been crafted to ensure\nandbriefintroductoryabstract(Fig.4 4). Thisinforma-\nthatevenuserswithnobackgroundinAI,programming,\ntionaidsusersindiscerningtherelevanceoftheirsearch\nor algorithms can easily navigate and utilize the plat-\nresults.\nform. In the following sections, we will illustrate how\nuserscaneffortlesslyandintelligentlycurate,acquire,and Knowledge pages within ShennongAlpha utilize a hier-"
  },
  {
    "id": 1406,
    "text": "results.\nform. In the following sections, we will illustrate how\nuserscaneffortlesslyandintelligentlycurate,acquire,and Knowledge pages within ShennongAlpha utilize a hier-\ntranslate NMM-related knowledge through the straight- archical tree structure, with content organized into se-\nforward process of accessing and harnessing the power quenced sections and allowing for nested subsections\nofShennongAlphaviatheWeb. Thesophisticatedtechni- (Fig. 4 5, 6). The “Table of Contents” in the sidebar\ncaldetails,implementations,andalgorithmsthatunderpin navigationfacilitatesusersinswiftlypinpointingandnav-\nShennongAlpha are discussed extensively in the “Meth- igatingtodistinctknowledgesections(Fig.4 7).\nods” and “Supplementary information” sections, show-\nIn our bid to ensure knowledge accessibility across lan-\ncasing the substantial work and expertise that have gone\nguage barriers, we designed a new text format known\nintodevelopingsuchanaccessibleyetadvancedplatform."
  },
  {
    "id": 1407,
    "text": "casing the substantial work and expertise that have gone\nguage barriers, we designed a new text format known\nintodevelopingsuchanaccessibleyetadvancedplatform.\nas Multilingual Markdown (MLMD) (Supplementary\nmethod F) for curating multilingual NMM knowledge.\nThisapproachallowsShennongAlphatouniversallysup-\n2.3 UsersaccessthecuratedNMMknowledgeon port four distinct language display modes: Chinese-\nShennongAlpha English (zh-en), English-Chinese (en-zh), Chinese (zh),\nandEnglish(en)(Fig.S6). Userscaneffortlesslyswitch\nbetween these modes using the language button (Fig. 4\nTo allow users to access and interact with the curated\n8).Thefunctionalitiesofthesemodesaredetailedbelow:\nNMM knowledge easily, we launched the ShennongAl-\npha Web (https://shennongalpha.westlake.edu. • zh-en: The Platform interface is in Chinese, with\ncn). TheWebservesastheprimaryportalforuserstoen- the Knowledge Base content rendered in a bilingual"
  },
  {
    "id": 1408,
    "text": "pha Web (https://shennongalpha.westlake.edu. • zh-en: The Platform interface is in Chinese, with\ncn). TheWebservesastheprimaryportalforuserstoen- the Knowledge Base content rendered in a bilingual\ngage with ShennongAlpha, and it is distinguished by its Chinese-Englishformat.\nmodern and user-centric web design, allowing a consis- • en-zh: The Platform interface is in English, with\ntentlysuperioruserexperienceregardlessofdevicescreen the Knowledge Base content rendered in a bilingual\ndimensions (Fig. 1c). The Web was crafted to be bilin- English-Chineseformat.\ngual, toggling seamlessly between English and Chinese, • zh: Both the Platform interface and the Knowledge\nthereby catering to both Chinese and international users BasecontentareexclusivelyinChinese.\n(Fig. 1d, e). The bilingual feature of ShennongAlpha, • en: Both the Platform interface and the Knowledge\nfor the first time, enables the international biomedical, BasecontentareexclusivelyinEnglish."
  },
  {
    "id": 1409,
    "text": "(Fig. 1d, e). The bilingual feature of ShennongAlpha, • en: Both the Platform interface and the Knowledge\nfor the first time, enables the international biomedical, BasecontentareexclusivelyinEnglish.\npharmaceutical,physician,andpatientcommunitiestoac-\nFurthermore,withinknowledgepages,usersareafforded\ncesstheknowledgeofChineseNMMs,asthisknowledge\nthe option to save (Fig. 4 9), cite (Fig. 4 11), or down-\nhas largely been limited to the local community in the\nload (Fig. 4 12) the content of the page. The download-\npast. ThestandardizedandstructuredKnowledgeBasein\nablecontentisstructuredinJSONformat,enablingusers\nShennongAlphaiscriticalforcorrectlyunderstandingand\ntoprogrammaticallyutilizetheoriginalNMMknowledge\nutilizingNMMknowledge. Therefore,ShennongAlphais\nforadvancedprocessingandapplications.\nparticularlysignificantforfutureresearchandapplication\nofNMMs.\nToaccesstheNMMknowledgethroughtheShennongAl- 2.4 UserscontributeNMMknowledgethrough"
  },
  {
    "id": 1410,
    "text": "foradvancedprocessingandapplications.\nparticularlysignificantforfutureresearchandapplication\nofNMMs.\nToaccesstheNMMknowledgethroughtheShennongAl- 2.4 UserscontributeNMMknowledgethrough\nphaWeb,userscaneasilybrowsedifferentNMMentries ShennongAlpha\nthrough distinct knowledge pages. The URLs of these\npagesarestructuredbyuniqueNMMIDs,asexemplified Unlike traditional knowledge platforms, ShennongAlpha\nbelow: isuniquelydesignedasanopenandcollaborativehub. It\n4"
  },
  {
    "id": 1411,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\ninvites global users to contribute their invaluable knowl- It’snoteworthythatthisansweraptlyhighlightsthemulti-\nedge and findings about NMMs. On every knowledge species character of “Ma Huang”, distinguishing it from\npage, users can find a prominent “Leave your name and theresponsegivenbyGPT-4(Fig.S2).\nknowledge!” button (Fig. 4 13). This feature encourages\nFurthermore, the Chat’s interface has been specially de-\nusers to submit new information or propose modifica-\nsignedtoenhancetheinterpretabilityofresponses.Search\ntions to existing content. Each section of NMM knowl-\nresults areembedded withinthe Chat’sanswers ina col-\nedge,suchas“Abstract”,“ChinesePharmacopoeia”,and\nlapsible format, allowing users to expand these sections\n“MLMDEncyclopedia”isaccessibleforenhancementor\nfor deeper insights into the background knowledge sup-"
  },
  {
    "id": 1412,
    "text": "lapsible format, allowing users to expand these sections\n“MLMDEncyclopedia”isaccessibleforenhancementor\nfor deeper insights into the background knowledge sup-\namendmentthroughthe“EditContent”button(Fig.4 14).\nportingtheanswers(Fig.5 9).\nTheseuser-generatedmodificationswillbeintegratedinto\ntheKnowledgeBaseuponreviewandapprovalbyShen- Moreover, leveraging the advanced intelligence of the\nnongAlpha. This feature will continually advance the LLM system, the Chat supports interactions in multiple\nKnowledgeBasebyleveragingandempoweringthecom- languages. Users can specify their preferred response\nmunity. style,whetherinadifferentlanguageoraparticularstruc-\ntural format (Fig. S9). This adaptability enhances the\nFurthermore, the “Show Edit History” button (Fig. 4 15)\noverall user experience, broadens knowledge accessibil-\nprovides a tracking method and a transparent view of all\nity,andpromotesglobalknowledgesharing.\nhistorical modifications. Notably, once a user’s contri-"
  },
  {
    "id": 1413,
    "text": "provides a tracking method and a transparent view of all\nity,andpromotesglobalknowledgesharing.\nhistorical modifications. Notably, once a user’s contri-\nbution is included in the Knowledge Base, it will be ac-\nknowledged. Their names and avatars are prominently 2.6 Standardizedandinterpretabletranslationof\ndisplayedunderthe“Contributors”section(Fig.4 16),not NMMknowledgeonShennongAlpha\nonly as a token of appreciation but also as a badge of\nhonor for their collaborative efforts. By pooling the col- Another important factor in NMM research and applica-\nlective expertise of the global NMM community, Shen- tions is language barriers, as much of the accumulated\nnongAlpha will rapidly evolve, driven by shared knowl- experience and knowledge is often recorded in a mono-\nedge, collaborative spirit, and global community dedica- lingual format. Therefore, a key feature of the Shen-\ntion. nongAlpha platform is an AI-based standardized and in-\nterpretable Translate application for NMM knowledge"
  },
  {
    "id": 1414,
    "text": "tion. nongAlpha platform is an AI-based standardized and in-\nterpretable Translate application for NMM knowledge\n(ShennongTranslate,Fig.6). TheTranslateapplicationis\n2.5 Chat-basedNMMknowledgeacquisitionon\nunderpinnedbyanAItranslationalgorithmthatwedevel-\nShennongAlpha\noped(Neural MachineTranslationbased onCoreference\nInadditiontothestructuredknowledgepagesforacom- Primary Term, NMT-CPT, Fig. S10). It addresses lan-\nprehensive overview, ShennongAlpha also heralds a ma- guagebarriersandalsopreventssemanticdriftcausedby\njor shift in NMM knowledge acquisition with the in- non-standardizedtranslations.\ntroduction of its AI-powered Chat application (Shen-\nUserscanaccesstheTranslateviathefollowingURLon\nnongChat, Fig. 5). By integrating the advanced LLM\ntheShennongAlphaWeb:\nsystem into the Chat application, users are empowered\nwith chat-based acquisition for NMM knowledge. Users https://shennongalpha.westlake.edu.cn/\ncan pose their questions to the Chat in natural language, translate"
  },
  {
    "id": 1415,
    "text": "with chat-based acquisition for NMM knowledge. Users https://shennongalpha.westlake.edu.cn/\ncan pose their questions to the Chat in natural language, translate\neliminating the need for a technical background or spe-\nThe Translate upholds consistency in translations from\ncific search syntax. Leveraging the power of the Search\nChinese to English or vice versa. For example, a user\nEngine, the Chat automatically searches the Knowledge\nsubmitsthefollowingtexttobetranslated(Fig.6 2):\nBaseinreal-timeandarticulatesthegatheredknowledge\nintoanaturallanguageresponsefortheuser(Fig.S8). 麻黄是一种天然药材。\nUsers can access the Chat via the following URL on the\nShennongAlphaWeb: Then, the Translate automatically searches the Knowl-\nedge Base for the related NMM entities in the text, re-\nhttps://shennongalpha.westlake.edu.cn/chat\ntrieves their standardized translations, and generates the\nTheChatsupportsmulti-turnconversations.Forexample, final translation for the sentence to ensure it is stan-"
  },
  {
    "id": 1416,
    "text": "trieves their standardized translations, and generates the\nTheChatsupportsmulti-turnconversations.Forexample, final translation for the sentence to ensure it is stan-\nauserposesthefollowingquestion(Fig.5 7): dardized and interpretable for both language and NMM\nWhat is the species origin of Ma Huang?\nknowledgerequirements(Fig.6 5):\nEphedra equisetina vel intermedia vel\nThen, the Chat interprets the user’s intent, automatically sinica Stem-herbaceous (NMM-0006,\nsearchestheKnowledgeBase,andprovidesthefollowing Ma-huang) is a kind of Natural\nresponse(Fig.5 8): Medicinal Material.\nThe species origin of Ma Huang is\nEphedra equisetina, Ephedra intermedia, Here, “麻黄” is translated using its standardized NMM\nor Ephedra sinica.\nnomenclature: “Ephedra equisetina vel intermedia vel\n5"
  },
  {
    "id": 1417,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nsinicaStem-herbaceous(NMM-0006,Ma-huang)”. Simi- Chemical Abstracts Service registry number [36], the\nlarly,“天然药材”incorporatesauser-definedglossaryas PubChemCompoundIDnumber[37]).\n“NaturalMedicinalMaterial”.\nInthisstudy,wehaveintroducedaSystematicNomencla-\nUniquely, within the Translate’s interface, distinct color tureforNMMs.TheSystematicNamedescribesanNMM\nhighlights are applied to both the standardized transla- by its species origin, medicinal part, special description,\ntion “Ephedra equisetina vel intermedia vel sinica Stem- andprocessingmethod. TheGenericNameprovidescon-\nherbaceous (NMM-ooo6, Ma-huang)” (Fig. 6 6) and venience for NMM usage and also observes local tradi-\nthe user-defined glossary “Natural Medicinal Material” tion. The NMM ID facilitates use on digital platforms."
  },
  {
    "id": 1418,
    "text": "the user-defined glossary “Natural Medicinal Material” tion. The NMM ID facilitates use on digital platforms.\n(Fig. 6 7). This offers users instant clarity on special- Therefore, for the first time, the Systematic Nomencla-\nized terms, thus enhancing the standardization and inter- ture provides a standardized system that uniquely dif-\npretability of the translation results. Even more signif- ferentiates and identifies each NMM. Using the Sys-\nicantly, the Translate automatically identifies the NMM tematic Nomenclature and its companion ShengnonAl-\nassociatedwiththestandardizedtranslation,enablingac- phaplatform(seebelow),wehaveassignedidentitiesfor\ncesstoitsknowledgepageonShennongAlpha.Bymerely more than 14,000 Chinese NMMs and curated their re-\nhovering over the standardized translation term, users latedknowledgeintheKnowledgeBase. TheSystematic\nare presented with a bilingual summary tooltip (adapt- NomenclaturecouldalsobeappliedtoadditionalNMMs"
  },
  {
    "id": 1419,
    "text": "are presented with a bilingual summary tooltip (adapt- NomenclaturecouldalsobeappliedtoadditionalNMMs\nable based on the user’s language display preference on fromotherregionsoftheworld.\nShennongAlpha) (Fig. 6 6’). Clicking the term within\nTo systematically empower NMM knowledge sharing,\nthe tooltip directly navigates to its detailed knowledge\nwe have developed the AI-driven platform ShengnonAl-\npage. This interactive approach not only provides users\npha. First, ShennongAlpha automates the implementa-\nwithstandardizedtranslationsbutalsofostersfurtherex-\ntion of the Systematic Nomenclature for NMMs through\nplorationofrelatedknowledge,therebyenhancingunder-\nthe tailored algorithm and application. Second, to ad-\nstanding. It extends the traditional role of the Translate\ndress language barriers, all knowledge entries in Shen-\nbeyond mere translation, transforming it into an applica-\nnongAlpha are curated bilingually in English and Chi-\ntionforknowledgeexploration,learning,andsharing."
  },
  {
    "id": 1420,
    "text": "beyond mere translation, transforming it into an applica-\nnongAlpha are curated bilingually in English and Chi-\ntionforknowledgeexploration,learning,andsharing.\nnese,makingthemaccessibletobothChineseandglobal\nusers. This is supported by our Translate application,\nwhichprovidesstandardizedandinterpretabletranslation\n3 Discussion for the cross-lingual sharing of NMM knowledge. Al-\nthough the current platform primarily supports English\nand/or Chinese interface displays and bidirectional text\nGlobal knowledge sharing is particularly important for translation, the MLMD syntax and the NMT-CPT algo-\nspecialized fields such as NMMs, which are tradition- rithmallowtheplatformtobeeasilyexpandedformulti-\nallyrichwithhistoricalrecordsdocumentedinlocaltexts lingualpresentationandtranslationintootherlanguages.\nand formats. The emergence of large language models Third,byleveragingtheLLM,ShennongAlphaalsooffers"
  },
  {
    "id": 1421,
    "text": "and formats. The emergence of large language models Third,byleveragingtheLLM,ShennongAlphaalsooffers\n(LLMs) offers unique opportunities to facilitate knowl- the Chat application for NMM knowledge acquisition.\nedge sharing. However, it also raises concerns that in- Its conversational interface enables users to obtain stan-\ncorrectinformationcouldbespreadbyadvancedAItech- dardized NMM knowledge in an interactive, chat-based\nnologies at unprecedented rates. Furthermore, LLMs manner. Fourth,ShennongAlphahasintroducedaunique\ntrainedwithmisinformationcouldundermineknowledge collaborationmechanismforupdatingNMMknowledge.\nintegrityinspecializedfields. Therefore, itisurgentthat By encouraging global users to contribute their valuable\nknowledge in such fields be correctly curated and trans- new information and findings about NMMs, the Knowl-\nlated,therebyenablingintelligentacquisitionandfurther edge Base within ShennongAlpha will continue to grow."
  },
  {
    "id": 1422,
    "text": "lated,therebyenablingintelligentacquisitionandfurther edge Base within ShennongAlpha will continue to grow.\ndisseminationthroughAImethods. This represents not only a mechanism for sharing new\nknowledgebutalsofosterscollaborationwithintheglobal\nA critical issue for sharing NMM knowledge and pro-\nNMM community. Finally, despite its sophisticated ar-\nmoting research and applications is the lack of standard-\nchitecture,algorithms,andimplementation,ShennongAl-\nized and systematic nomenclature. This deficiency ham-\npha offers an integrated, user-friendly interface through\npersaccuratedifferentiationandidentificationofNMMs,\ntheShennongAlphaWeb. Thisdesignskillfullyconceals\nand obstructs effective communication and collaboration\nthe underlying complexities, making it easily accessible\nwithin the field. In comparison, the pharmaceutical sec-\nforusers.\ntor has established systematic nomenclature for chemi-"
  },
  {
    "id": 1423,
    "text": "the underlying complexities, making it easily accessible\nwithin the field. In comparison, the pharmaceutical sec-\nforusers.\ntor has established systematic nomenclature for chemi-\ncal drugs to ensure accuracy and consistency in chemi- In summary, ShennongAlpha epitomizes the exciting fu-\ncal identification and research. Specifically, the chemi- sionofAIwithNMMknowledgesharinginthenovelAI\ncalsystematicnamingsystemincludesasystematicname for Science field. ShennongAlpha’s hexa-domain modu-\nfor accurately describing the chemical structure of each lar architecture (Fig. 1b) also offers a model for knowl-\ndrug(theInternationalUnionofPureandAppliedChem- edge sharing in other specialized fields. ShennongAl-\nistry nomenclature [34]); a generic name for convenient pha also provides the world’s largest NMM Knowledge\nusage(theInternationalNonproprietaryNames[35]);and Base with standardized and curated knowledge, serving\na unique identification number for digital platforms (the\n6"
  },
  {
    "id": 1424,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nas a unique resource for NMM researchers, physicians, ing SNNMM, Text, Knowledge, Glossary, and Relation\npatients, and users, as well as for future LLM training. (Fig. S5). This organization follows a thorough process\nWe believe that as AI increasingly emerges as a primary ofcollection,structuring,standardization,andreview,all\nknowledge provider in scientific research [38], our plat- carriedoutbyourShennongAlphateam.\nformwillsignificantlypromoteglobalknowledgesharing\nandcollaborationintheNMMfield,ultimatelybenefiting\n4.4 ShennongAlphaWeb\nglobalhealthandhumanwell-being.\nTheShennongAlphaWebservesastheprincipalinterface\n4 Methods foruserswiththeShennongAlpha, situatedwithinShen-\nnongAlpha’s1stlayer,thewebanduserinteractionlayer\n(Fig.1b).\n4.1 SystematicNomenclatureforNMMswithits"
  },
  {
    "id": 1425,
    "text": "4 Methods foruserswiththeShennongAlpha, situatedwithinShen-\nnongAlpha’s1stlayer,thewebanduserinteractionlayer\n(Fig.1b).\n4.1 SystematicNomenclatureforNMMswithits\nalgorithmandapplication Embracing a responsive design approach, ShennongAl-\npha Web assures users of a uniform and superior user\nDetailed rules for Systematic Nomenclature for NMMs\nexperience on screens of any dimension. ShennongAl-\n(SNNMM) can be found in the Supplementary text C\npha Web is built on TypeScript, employing Next.js as\nandD.\nits principal framework. It amalgamates the Shennon-\nSystematic Nomenclature for NMMs Algorithm (SNN- gAlpha Knowledge Base and Search Engine. Addition-\nMMA)iscodedinPython, withitsalgorithmstringently ally,itincorporatesapplicationssuchasShennongName,\nfollowingtherulesofSNNMM.Itisopen-source,andthe ShennongChat,andShennongTranslate. ShennongAlpha\nsource code is available on GitHub: https://github. takesadvantageofamicroservicesarchitecture,facilitat-"
  },
  {
    "id": 1426,
    "text": "source code is available on GitHub: https://github. takesadvantageofamicroservicesarchitecture,facilitat-\ncom/shennong-program/shennongname. ing the decoupling of diverse service units for enhanced\nmanageabilityandextensibility. Thebackendforapplica-\nThe associated Python package for SNNMMA is pub-\ntionslikeShennongName,ShennongChat,andShennong-\nlished and released on PyPI: https://pypi.org/\nTranslateisdevelopedusingPython,leveragingtheFlask\nproject/shennongname.\norFastAPIframework. Theseapplicationsinterfacewith\nFor a concise overview of the data structure and al- ShennongAlpha through APIs that adhere to RESTful\ngorithms of SNNMMA, refer to the Supplementary standards,fosteringoverallsystemstabilityandmaintain-\nmethodE. ability. Fordeployment,ShennongAlphaexploitsDocker\nforcontainerorchestrationandDockerComposetechnol-\nogy for integrating and managing multiple containerized\n4.2 MultilingualMarkdown\napplications.\nMultilingual Markdown (MLMD) is a newly designed"
  },
  {
    "id": 1427,
    "text": "forcontainerorchestrationandDockerComposetechnol-\nogy for integrating and managing multiple containerized\n4.2 MultilingualMarkdown\napplications.\nMultilingual Markdown (MLMD) is a newly designed\nlightweightmarkuplanguageexplicitlytailoredforman- 4.5 ShennongAlphaSearchEngine\nagingmultilingualtext.\nShennongAlpha Search Engine (ShennongSearch)\nTheessentialsyntaxofMLMDcanbefoundintheSup-\n(Fig. S7) is the core of knowledge retrieval in the Shen-\nplementarymethodF.\nnongAlpha, located in the 4th layer referred to as the\nTheMLMDHTMLparserisbuiltusingTypeScript. Itis searchenginelayer(Fig.1b).\nopen-source,withthesourcecodeaccessibleonGitHub:\nShennongSearch can be seen as a data highway within\nhttps://github.com/shennong-program/mlmd.\nthe ShennongAlpha. To accommodate a wide array of\nThe associated TypeScript package is published and re- queries and guarantee efficient and precise information\nleased on npm: https://www.npmjs.com/package/ retrieval, ShennongSearch is customized and designed"
  },
  {
    "id": 1428,
    "text": "leased on npm: https://www.npmjs.com/package/ retrieval, ShennongSearch is customized and designed\nmlmd. with three advanced search methods: Coreference-based\nGraphSearch(CGS)(Fig.S7a),vectorsearch(Fig.S7b)\n4.3 ShennongAlphaKnowledgeBase and full-text search (Fig. S7c). Each search method\npossesses distinct strengths and can be employed indi-\nShennongAlpha Knowledge Base (ShennongKB) repre- vidually or collectively to yield accurate and thorough\nsentsthecoreknowledgebasewithintheShennongAlpha search results, thereby enhancing the overall usability of\nandresidesinthe5thlayerknownastheknowledgebase ShennongSearch. Thethreeadvancedsearchmethodsof\nlayer(Fig.1b). ShennongSearch are primarily applied in the knowledge\nsearch process of ShennongChat. In the publicly acces-\nShennongKB,builtontheMongoDBdocument-oriented\nsibleShennongChat, knowledgesearchesapply theCGS\ndatabase,integratesandorganizesadiversedataset. The\nmethod.\ncurrent version of ShennongKB predominantly sources"
  },
  {
    "id": 1429,
    "text": "sibleShennongChat, knowledgesearchesapply theCGS\ndatabase,integratesandorganizesadiversedataset. The\nmethod.\ncurrent version of ShennongKB predominantly sources\nits data from the 2020 [23] and 2015 [32] editions of Coreference-basedGraphSearch. TheCGSalgorithm\nthe Chinese Pharmacopoeia, and the Chinese Medicinal (detailed in Supplementary method G), designed and\nInformation Platform [33]. This NMM-associated data developedbythisproject,centersondispatchingrelation-\nis organized into several interrelated collections, includ- ships between different terms within the relation collec-\n7"
  },
  {
    "id": 1430,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\ntionofShennongKB,automaticallyconstructingaCoref- 4.6 ShennongAlphaLargeLanguageModel\nerence Primary Term Graph (CPTG) (Fig. S7a). CPTG\nessentially operates as a directed acyclic graph. Given Situated within the 3rd layer, the Artificial Intelligence\na term/named entity, graph search within CPTG locates layer, ShennongAlpha Large Language Model (Shen-\nitscorrespondingPrimaryTerm. ThisPrimaryTermthen nongLLM) acts as the pivotal AI hub for the Shennon-\nallows querying its corresponding standard information gAlpha (Fig. 1b). Built on a foundation of LLMs, the\nwithinothercollectionsinShennongKB(suchasKnowl- hub is designed for adaptability, supporting both open-\nedge or Translation). This method is especially useful source and proprietary language models like Llama 2/3"
  },
  {
    "id": 1431,
    "text": "edge or Translation). This method is especially useful source and proprietary language models like Llama 2/3\nwhen standardizing queries. We construct the CPTG [41],GPT-3.5/4/4o[42],Gemini,Claude,ERNIE,Qwen,\nusing the Python networkx package [39]. The asso- amongothers. Thisversatilityisaccessibleeitherthrough\nciated Python implementation for CGS is available on localdeploymentorAPIcalls.\nGitHub (https://github.com/shennong-program/\npycgs)orPyPI(https://pypi.org/project/pycgs). 4.7 ShennongAlaphChatapplication\nVector search. The method utilizes a vector-based\nAcquiringknowledgethroughdirectchatwithLLMsher-\napproach, transforming search queries and documents\nalds a transformative paradigm in information retrieval.\nwithin ShennongKB into vector embeddings (Fig. S7b).\nHowever,LLMsoftenproduceinaccurateorspuriousout-\nA unique strength of this method lies in its ability to\nputs,especiallyinunfamiliardomains[43].Giventhefact\naccept entire sentences or paragraphs as search input."
  },
  {
    "id": 1432,
    "text": "A unique strength of this method lies in its ability to\nputs,especiallyinunfamiliardomains[43].Giventhefact\naccept entire sentences or paragraphs as search input.\nthatNMMknowledgehadnotbeenstandardizedpriorto\nBy converting this text into vector space through em-\nthis study, the standardized knowledge we curated was\nbedding, the semantic essence is captured within a vec-\nunavailable for training LLMs. Therefore, even state-of-\ntor representation. Subsequently, by evaluating the sim-\nthe-art LLMs like GPT-4 tend to provide misleading re-\nilarity between the query text vector embedding and the\nsponses(Fig.S2). Recognizingthisconstraint, weadopt\narchivedtextvectorembeddingswithinShennongKB,in-\nthe retrieval augmented generation (RAG) methodology\nformation that is semantically relevant to the query text\n[44, 45] in the design of the ShennongAlaph Chat appli-\ncan be swiftly identified and retrieved. We employ Ope-\ncation(ShennongChat),substantiallyenhancingtheaccu-"
  },
  {
    "id": 1433,
    "text": "[44, 45] in the design of the ShennongAlaph Chat appli-\ncan be swiftly identified and retrieved. We employ Ope-\ncation(ShennongChat),substantiallyenhancingtheaccu-\nnAI’s embedding model for text vectorization: text-\nracyofLLMresponses.\nembedding-3-small. Each text within ShennongKB is\npreprocessedwithtext-embedding-3-small, generat- ShennongChat utilizes the prompt chain technique [46,\ning a 1536-dimensional vector embedding. These em- 47], a method that uses a step-by-step process to ensure\nbeddings and their original texts are then stored within theLLM’sresponsestouserqueriesarebasedonretrieved\nShennongKB. Upon each new search, the query text is standardizedNMMknowledge. Thistechniqueenhances\nprocessedwithtext-embedding-3-smalltogeneratea the relevance and reliability of the knowledge acquired\nnewvectorembedding. Then,thesearchengineusesco- duringchats.\nsinesimilaritytomeasurethesimilaritybetweenthisnew\nFor illustrative purposes, let’s examine the user query"
  },
  {
    "id": 1434,
    "text": "newvectorembedding. Then,thesearchengineusesco- duringchats.\nsinesimilaritytomeasurethesimilaritybetweenthisnew\nFor illustrative purposes, let’s examine the user query\nembeddingandthevectorembeddingsarchivedinShen-\nmentionedinthe“Results”section(Fig.5andS8):\nnongKB, returning the text corresponding to the most\nsimilar vector embedding as the search result. Shen- What is the species origin of Ma Huang?\nnongSearch utilizes MongoDB’s $vectorSearch oper-\natorforitsvectorsearchfunctionality. Uponreceivingaquery,ShennongChatprocessesitusing\nShennongLLM to assess if the current chat history con-\nFull-textsearch. Thismethodisrootedinfull-textfuzzy\ntains sufficient knowledge to respond. If it does, Shen-\nsearching,usingatokenizationandinvertedindexprocess\nnongLLM will directly answer the user’s question. In\n(Fig. S7c). Operating by breaking down documents and\ncaseswheremoreinformationisneeded,ShennongSearch\nsearch queries in ShennongKB into individual words or"
  },
  {
    "id": 1435,
    "text": "(Fig. S7c). Operating by breaking down documents and\ncaseswheremoreinformationisneeded,ShennongSearch\nsearch queries in ShennongKB into individual words or\nis activated to conduct a knowledge search. This pro-\n“tokens”, this method allows efficient searching and re-\ncessinvolvesdispatchingShennongSearchtoretrieverel-\ntrievalofpertinentinformation,accommodatingapproxi-\nevant standardized knowledge from ShennongKB. From\nmatematches,variationsinphrasing,typos,misspellings,\nthissearch,weglean:\noralternatespellings. Thismethodnotablyenhancesthe\nsystem’sflexibilityanduser-friendlinessbyleveragingthe\nspecies_origins: Ephedra equisetina,\ntokenizationandinvertedindex. ShennongSearchutilizes\nor, Ephedra intermedia, or, Ephedra\nMongoDB’s $text and $search operators for its full-\nsinica\ntext search functionality. Documents within MongoDB\naretokenizedusingJieba[40], afterwhichafull-textin-\nThiscontextualinformationisthenamalgamatedwiththe\nvertedindexisconstructed. Duringqueries,thetexttobe"
  },
  {
    "id": 1436,
    "text": "aretokenizedusingJieba[40], afterwhichafull-textin-\nThiscontextualinformationisthenamalgamatedwiththe\nvertedindexisconstructed. Duringqueries,thetexttobe\nuser’squeryandsubmittedtoShennongLLMtoformulate\nsearchedisalsotokenizedusingJiebabeforethefull-text\nanaccurateresponse:\nsearchisexecuted.\nThe species origin of Ma Huang is\nEphedra equisetina, Ephedra intermedia,\nor Ephedra sinica.\n8"
  },
  {
    "id": 1437,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n4.8 ShennongAlaphTranslateapplication In the MLMD syntax, the term “麻黄” is translated\nas “[[nmm-0006 | Ephedra equisetina vel intermedia\nTo enhance the quality of standardized translations, par- vel sinica Stem-herbaceous (NMM-ooo6, Ma-huang)]]”.\nticularly in the field of NMMs, capturing the original Here, “nmm-0006” (i.e., the “xxx” part of the annota-\ntext’sessenceandemployingstandardizedterminologyin tion)signifiesthePrimaryTerm(orNMMID)fortheterm\nthetargetlanguageisimperative.Thisway,wecanensure “麻黄”,asdeterminedthroughaShennongSearch’sCGS\nacoherentlinguisticandterminologicalframework,facil- query within ShennongKB. Meanwhile, “Ephedra equi-\nitating standardized scholarly communication. However, setinavelintermediavelsinicaStem-herbaceous(NMM-"
  },
  {
    "id": 1438,
    "text": "itating standardized scholarly communication. However, setinavelintermediavelsinicaStem-herbaceous(NMM-\ndespitetheconsiderableadvancementsinNeuralMachine ooo6,Ma-huang)”(i.e.,the“yyy”part)representsitsstan-\nTranslation (NMT) technologies [48–50], current NMT dardized translation in the target language. For transla-\nalgorithms fallshort of fulfillingour specialized require- tions in the user-customized dictionary, as they do not\nments. To tackle this limitation, we introduced a new needtoapplyaShennongSearchquerytofindthePrimary\ntranslationalgorithmnamed“NeuralMachineTranslation Terms, the annotation adopts the form “[[yyy]]” without\nBasedonCoreferencePrimaryTerm(NMT-CPT)”. the“xxx”component.\nAt the heart of the NMT-CPT is a dual-purpose strategy. By forwarding the user’s text to be translated, the trans-\nWhenausersubmitsatranslationrequestviaShennong- lation direction, and the formatted dictionary to Shen-"
  },
  {
    "id": 1439,
    "text": "WhenausersubmitsatranslationrequestviaShennong- lation direction, and the formatted dictionary to Shen-\nTranslate, the system proactively identifies standardized nongLLM, we obtain the following translation with\ntermswithinthetextrecordedinShennongKB,referenc- MLMDsyntax:\ning their Primary Terms and standardized translations in\nthe target language. Subsequently, using the specialized [[nmm-0006 | Ephedra equisetina vel\nannotation syntax of MLMD, ShennongTranslate, pow- intermedia vel sinica Stem-herbaceous\neredbyShennongLLM,producesatranslationthatincor- (NMM-ooo6, Ma-huang)]] is a kind of\nporatesthesestandardizedtranslations. [[Natural Medicinal Material]].\nTo illustrate, let’s examine the translation request men-\ntionedinthe“Results”section(Fig.6andS10): ShennongTranslate further parses this MLMD-styled\ntranslationtoprovidefunctionalitiesfortermhighlighting\nText for translation:\nandtooltips.\n麻黄是一种天然药材。\nTranslation direction: ShennongTranslateutilizesthejiebapackage[40]forto-"
  },
  {
    "id": 1440,
    "text": "translationtoprovidefunctionalitiesfortermhighlighting\nText for translation:\nandtooltips.\n麻黄是一种天然药材。\nTranslation direction: ShennongTranslateutilizesthejiebapackage[40]forto-\nzh -> en kenization and leverages the LangChain framework [46]\nUser-customized dictionary: toconstructtranslationprompttemplates. Thisapproach\n天然药材 -> Natural Medicinal Material guaranteesthattranslationsadheretoastrictstructureand\nconsistent formatting. By employing a few-shot learn-\ning technique [51], the system can generalize to the new\nOnceShennongTranslatereceivesthetranslationrequest,\ntranslationsyntaxwithoutrequiringextensivemodelfine-\nit tokenizes the input text and utilizes ShennongSearch’s\ntuning.\nCGS to identify potential NMMs mentioned within the\ntextandsearchoutthecorrespondingPrimaryTerms. In\nthis case, the term “麻黄” is detected, associated with\n5 Dataavailability\nthecorrespondingPrimaryTerm“nmm-0006”. Basedon\nthePrimaryTerm(“nmm-0006”)andthetargetlanguage"
  },
  {
    "id": 1441,
    "text": "this case, the term “麻黄” is detected, associated with\n5 Dataavailability\nthecorrespondingPrimaryTerm“nmm-0006”. Basedon\nthePrimaryTerm(“nmm-0006”)andthetargetlanguage\ncode (“en”), we can probe the ShennongKB’s glossary Data from ShennongKB, as well as the web and appli-\ncollection, ultimately obtaining its standardized transla- cations of the ShennongAlpha, can be accessed through\ntioninthe“NMMSN(NMMID,NMMGN)”format: the ShennongAlpha Web (https://shennongalpha.\nwestlake.edu.cn).\nEphedra equisetina vel intermedia vel\nsinica Stem-herbaceous (NMM-ooo6,\nMa-huang)\n6 Codeavailability\nThese standardized and user-customized translations are\nThe SNNMMA and ShennongName suite can\nthen formatted according to the unique MLMD syntax\nused in the NMT-CPT algorithm: “[[xxx | yyy]]”. After\nbe obtained on GitHub (https://github.com/\nshennong-program/shennongname). The relevant\nprocessing,thedictionaryisformattedasfollows:\nPythonpackagehasbeenpublishedandreleasedonPyPI"
  },
  {
    "id": 1442,
    "text": "be obtained on GitHub (https://github.com/\nshennong-program/shennongname). The relevant\nprocessing,thedictionaryisformattedasfollows:\nPythonpackagehasbeenpublishedandreleasedonPyPI\n麻黄 -> [[nmm-0006 | Ephedra equisetina (https://pypi.org/project/shennongname).\nvel intermedia vel sinica\nThe MLMD parser can be obtained on GitHub (https:\nStem-herbaceous (NMM-ooo6, Ma-huang)]]\n//github.com/shennong-program/mlmd). The rele-\n天然药材 -> [[Natural Medicinal\nvantTypeScriptpackagehasbeenpublishedandreleased\nMaterial]]\nonnpm(https://www.npmjs.com/package/mlmd).\n9"
  },
  {
    "id": 1443,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nThe Python implementation of CGS can be obtained on 2. Lautie, E., Russo, O., Ducrot, P. & Boutin, J. A.\nGitHub (https://github.com/shennong-program/ Unravelingplantnaturalchemicaldiversityfordrug\npycgs)orPyPI(https://pypi.org/project/pycgs). discovery purposes. Frontiers in pharmacology 11,\n397(2020).\n3. Chen,Y.,GarciadeLomana,M.,Friedrich,N.-O.&\n7 Contributions\nKirchmair,J.Characterizationofthechemicalspace\nof known and readily obtainable natural products.\nYANG Zijie, ZHANG Yue, and XU Tian conceived and\nJournal of chemical information and modeling 58,\ndesigned the study. YANG Zijie, TAO Wufan, ZHANG\n1518–1532(2018).\nYue, and XU Tian designed the SNNMM. YANG Zi-\n4. Harvey, A. L., Edrada-Ebel, R. & Quinn, R. J. The\njie, YIN Yongjing, KONG Chaojun, and CHI Tiange\nre-emergenceofnaturalproductsfordrugdiscovery"
  },
  {
    "id": 1444,
    "text": "Yue, and XU Tian designed the SNNMM. YANG Zi-\n4. Harvey, A. L., Edrada-Ebel, R. & Quinn, R. J. The\njie, YIN Yongjing, KONG Chaojun, and CHI Tiange\nre-emergenceofnaturalproductsfordrugdiscovery\ncollected, structured, and reviewed information about\ninthegenomicsera.Naturereviewsdrugdiscovery\nNMMs. YANG Zijie designed and completed the archi-\n14,111–129(2015).\ntecture,syntax,andalgorithmsrelatedtoShennongAlpha.\n5. Atanasov, A. G., Zotchev, S. B., Dirsch, V. M. &\nYANG Zijie and YIN Yongjing completed the develop-\nSupuran, C. T. Natural products in drug discovery:\nment and deployment of ShennongAlpha. TAO Wufan,\nadvances and opportunities. Nature reviews Drug\nZHANGYue,andXUTiansupervisedthestudy. YANG\ndiscovery20,200–216(2021).\nZijie,YINYongjing,TAOWufan,ZHANGYue,andXU\nTianwrotethepaper. 6. The State Council Information Office of the\nPeople’s Republic of China. Traditional Chinese\nMedicineinChina2016.http://english.scio.\n8 Acknowledgements\ngov.cn/2017-01/17/content_40621689.htm."
  },
  {
    "id": 1445,
    "text": "People’s Republic of China. Traditional Chinese\nMedicineinChina2016.http://english.scio.\n8 Acknowledgements\ngov.cn/2017-01/17/content_40621689.htm.\n7. Motoo, Y., Seki, T. & Tsutani, K. Traditional\nWe appreciate XU Anlong (Beijing University of Chi- Japanese medicine, Kampo: its history and current\nnese Medicine), CHEN Kaixian (Chinese Academy of status. Chinese Journal of Integrative Medicine 17,\nSciences)foradvisingonthisstudy;YANGDong,CONG 85–87(2011).\nPeikuanforparticipatinginthediscussionsofthisstudy;\n8. Park,H.-L.etal.TraditionalmedicineinChina,Ko-\nYU Ziyan for assisting in deploying the GPT-3.5/4 API;\nrea, and Japan: a brief introduction and compari-\nWU Kaile, CHEN Jienan, ZHOU Zhaozhao for assist-\nson.Evidence-basedcomplementaryandalternative\ning in the development of the ShennongAlpha Web; YU\nmedicine2012(2012).\nQian,WANGWeicheng,ZHANGHongjiforassistingin\n9. Sen, S. & Chakraborty, R. Revival, modernization\nthe artistic design of the ShennongAlpha Web; WU Qi-"
  },
  {
    "id": 1446,
    "text": "medicine2012(2012).\nQian,WANGWeicheng,ZHANGHongjiforassistingin\n9. Sen, S. & Chakraborty, R. Revival, modernization\nthe artistic design of the ShennongAlpha Web; WU Qi-\nandintegrationofIndiantraditionalherbalmedicine\nuping, YANG Xusheng, ZHANG Lin, WU Qiaozhi, XU\nin clinical practice: Importance, challenges and\nShuangyan for participating in review of NMM knowl-\nfuture. Journal of traditional and complementary\nedge.\nmedicine7,234–244(2017).\nThis work was supported by a grant from the National 10. Naghizadeh,A.etal.UNaProd:auniversalnatural\nNatural Science Foundation of China (U21A20201);\nproductdatabaseforMateriaMedicaofIraniantra-\ngrants from the Department of Science and Technol- ditional medicine. Evidence-based complementary\nogy of Zhejiang Province (2020E10027, 2021ZY1019, andalternativemedicine2020(2020).\n2022ZY1005); theResearchProgramNo. 202208011of\n11. Leonti, M. & Verpoorte, R. Traditional Mediter-\nWestlake Laboratory of Life Sciences and Biomedicine;"
  },
  {
    "id": 1447,
    "text": "2022ZY1005); theResearchProgramNo. 202208011of\n11. Leonti, M. & Verpoorte, R. Traditional Mediter-\nWestlake Laboratory of Life Sciences and Biomedicine;\nranean and European herbal medicines. Journal of\nthe Research Program No. WU2023C020 of Research\nethnopharmacology199,161–167(2017).\nCenter for Industries of the Future, Westlake University;\n12. Joos,S.,Glassen,K.,Musselmann,B.,etal.Herbal\ntheWestlakeEducationFoundation.\nmedicineinprimaryhealthcareinGermany:thePa-\ntient’sperspective.Evidence-BasedComplementary\n9 Competinginterests andAlternativeMedicine2012(2012).\n13. Dom´ınguez,F.etal.MexicanTraditionalMedicine:\nYANG Zijie, ZHANG Yue, and XU Tian hold TraditionsofyesterdeyandPhytomedicinesforTo-\nthe relevant patents (2023117101437, 2023117101511, morrow.TherapeuticMedicinalPlants:FromLabto\n2023117101526) for the ShennongAlpha. The other au- theMarket,10–46(2015).\nthorsdeclarenocompetinginterests. 14. Dutra,R.C.,Campos,M.M.,Santos,A.R.&Cal-"
  },
  {
    "id": 1448,
    "text": "2023117101526) for the ShennongAlpha. The other au- theMarket,10–46(2015).\nthorsdeclarenocompetinginterests. 14. Dutra,R.C.,Campos,M.M.,Santos,A.R.&Cal-\nixto,J.B.MedicinalplantsinBrazil:Pharmacolog-\nicalstudies,drugdiscovery,challengesandperspec-\n10 References\ntives.Pharmacologicalresearch112,4–29(2016).\n1. Newman,D.J.&Cragg,G.M.Naturalproductsas 15. Mahomoodally,M.F.etal.Traditionalmedicinesin\nsources of new drugs over the nearly four decades Africa:anappraisaloftenpotentAfricanmedicinal\nfrom 01/1981 to 09/2019. Journal of natural prod- plants.Evidence-basedcomplementaryandalterna-\nucts83,770–803(2020). tivemedicine2013(2013).\n10"
  },
  {
    "id": 1449,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n16. Bultum,L.E.,Woyessa,A.M.&Lee,D.ETM-DB: westlake.edu.cn/en-zh/knowledge/nmm-\nintegratedEthiopiantraditionalherbalmedicineand 01yf.\nphytochemicalsdatabase.BMCcomplementaryand\n30. ShennongAlpha. ShennongAlpha Knowledge:\nalternativemedicine19,1–11(2019).\nNMM-0003, Ephedra sinica Stem-herbaceous Ac-\n17. Pan, S.-Y. et al. Historical perspective of tra- cessed: 2024-05-01. https://shennongalpha.\nditional indigenous medical practices: the cur- westlake.edu.cn/en-zh/knowledge/nmm-\nrent renaissance and conservation of herbal re- 0003.\nsources. Evidence-based complementary and alter-\n31. ShennongAlpha. ShennongAlpha Knowledge:\nnativemedicine2014(2014).\nNMM-000G,EphedrasinicaRoot Accessed:2024-\n18. Azaizeh, H., Saad, B., Cooper, E., Said, O., et 05-01. https://shennongalpha.westlake."
  },
  {
    "id": 1450,
    "text": "nativemedicine2014(2014).\nNMM-000G,EphedrasinicaRoot Accessed:2024-\n18. Azaizeh, H., Saad, B., Cooper, E., Said, O., et 05-01. https://shennongalpha.westlake.\nal. Traditional Arabic and Islamic medicine, a re- edu.cn/en-zh/knowledge/nmm-000g.\nemerging health aid. Evidence-Based Complemen-\n32. Chinese Pharmacopoeia Commission. Pharma-\ntaryandAlternativeMedicine7,419–424(2010).\ncopoeia of the People’s Republic of China: 2015\n19. National Administration of Traditional Chinese Edition: Volume I (China Medical Science Press,\nMedicine). Zhong Hua Ben Cao (Shanghai Scien- 2015).\ntificandTechnicalPublishers,1999).\n33. China Medical Information Platform. China Medi-\n20. Jang,C.-S.etal.Ch’angShan,aChineseantimalar- calInformationPlatformwww.dayi.org.cn.\nialherb.Science103,59–59(1946).\n34. Leigh,G.J.Principlesofchemicalnomenclature:a\n21. Tu, Y. Artemisinin—a gift from traditional Chi- guidetoIUPACrecommendations(RoyalSocietyof\nnese medicine to the world (Nobel lecture). Ange- Chemistry,2011)."
  },
  {
    "id": 1451,
    "text": "21. Tu, Y. Artemisinin—a gift from traditional Chi- guidetoIUPACrecommendations(RoyalSocietyof\nnese medicine to the world (Nobel lecture). Ange- Chemistry,2011).\nwandte Chemie International Edition 55, 10210–\n35. Serafini,M.etal.What’sinaName?DrugNomen-\n10226(2016).\nclatureandMedicinalChemistryTrendsusingINN\n22. Zhang, T., Zhang, P., Wang, S. & Han, T. Prelim-\nPublications. Journal of Medicinal Chemistry 64,\ninary clinical observations of 6 cases of leukemia\n4410–4429(2021).\ntreatedby“Ailinsolution”.MedicineandPharmacy\n36. Morgan,H.L.Thegenerationofauniquemachine\nofHeilongjiang,66–67(1973).\ndescription for chemical structures-a technique de-\n23. Chinese Pharmacopoeia Commission. Pharma-\nveloped at chemical abstracts service. Journal of\ncopoeia of the People’s Republic of China: 2020\nchemicaldocumentation5,107–113(1965).\nEdition: Volume I (China Medical Science Press,\n37. Kim,S.etal.PubChem2023update.NucleicAcids\n2020).\nResearch51,D1373–D1380(2023)."
  },
  {
    "id": 1452,
    "text": "chemicaldocumentation5,107–113(1965).\nEdition: Volume I (China Medical Science Press,\n37. Kim,S.etal.PubChem2023update.NucleicAcids\n2020).\nResearch51,D1373–D1380(2023).\n24. Indian Pharmacopoeia Commission. Indian Phar-\nmacopoeia: 2010 Edition: Volume III (The In- 38. YANG,Z.,WANG,Y.&ZHANG,L.AIbecomesa\ndian Pharmacopoeia Commission, Indian Pharma- masterbrainscientist.bioRxiv,2023–04(2023).\ncopoeiaLaboratory,GovernmentofIndia,Ministry 39. NetworkX Developers. NetworkX https : / /\nofHealth&FamilyWelfare,2010). github.com/networkx/networkx.2023.\n25. ShennongAlpha. ShennongAlpha Knowledge: 40. Jieba Developers. Jieba https://github.com/\nNMM-0016, Curcuma wenyujin Rhizome Freshly- fxsjy/jieba.2020.\nsliced Cleaned Accessed: 2024-05-01. https :\n41. Touvron, H. et al. Llama 2: Open founda-\n//shennongalpha.westlake.edu.cn/en-\ntion and fine-tuned chat models. arXiv preprint\nzh/knowledge/nmm-0016.\narXiv:2307.09288(2023).\n26. ShennongAlpha. ShennongAlpha Knowledge:"
  },
  {
    "id": 1453,
    "text": "//shennongalpha.westlake.edu.cn/en-\ntion and fine-tuned chat models. arXiv preprint\nzh/knowledge/nmm-0016.\narXiv:2307.09288(2023).\n26. ShennongAlpha. ShennongAlpha Knowledge:\n42. OpenAI. GPT-4 Technical Report 2023. arXiv:\nNMM-000B, Ephedra sinica Stem-herbaceous Seg-\n2303.08774[cs.CL].\nmented and Aquafried-honey Accessed: 2024-05-\n01. https://shennongalpha.westlake.edu. 43. Ji,Z.etal.SurveyofHallucinationinNaturalLan-\ncn/en-zh/knowledge/nmm-000b. guage Generation. ACM Comput. Surv. 55, 248:1–\n248:38. https://doi.org/10.1145/3571730\n27. ShennongAlpha. ShennongAlpha Knowledge:\n(2023).\nNMM-0006, Ephedra equisetina vel intermedia\nvel sinica Stem-herbaceous Accessed: 2024-05-01. 44. Peng, B. et al. Check your facts and try again:\nhttps://shennongalpha.westlake.edu.cn/ Improving large language models with external\nen-zh/knowledge/nmm-0006. knowledgeandautomatedfeedback.arXivpreprint\narXiv:2302.12813(2023).\n28. Catalogue of Life. Taraxacum Accessed: 2024-05-"
  },
  {
    "id": 1454,
    "text": "en-zh/knowledge/nmm-0006. knowledgeandautomatedfeedback.arXivpreprint\narXiv:2302.12813(2023).\n28. Catalogue of Life. Taraxacum Accessed: 2024-05-\n01. https://www.catalogueoflife.org/ 45. Nakano, R. et al. Webgpt: Browser-assisted\ndata/taxon/7SSF. question-answering with human feedback. arXiv\npreprintarXiv:2112.09332(2021).\n29. ShennongAlpha. ShennongAlpha Knowledge:\nNMM-01YF, Taraxacum unspecified Herb Ac- 46. LangChain Developers. LangChain https : / /\ncessed: 2024-05-01. https://shennongalpha. github.com/langchain-ai/langchain.2023.\n11"
  },
  {
    "id": 1455,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n47. Wei, J. et al. Chain-of-thought prompting elicits sociationforComputationalLinguistics:ACL2022\nreasoning in large language models. Advances in (2022),2579–2590.\nNeuralInformationProcessingSystems35,24824– 50. Hendy,A.etal.HowGoodAreGPTModelsatMa-\n24837(2022). chine Translation? A Comprehensive Evaluation.\nCoRR abs/2302.09210. https://doi.org/10.\n48. Vaswani, A. et al. Attention is All you Need in\n48550/arXiv.2302.09210(2023).\nNeurIPS2017(2017),5998–6008.\n51. Brown, T. et al. Language models are few-shot\n49. Li, Y., Yin, Y., Li, J. & Zhang, Y. Prompt-driven learners.Advancesinneuralinformationprocessing\nneural machine translation in Findings of the As- systems33,1877–1901(2020).\n12"
  },
  {
    "id": 1456,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\na b\nChallenges for utilizing and Our ShennongAlpha approaches\nsharing NMM knowledge\nShennongAlpha\nShennongAlpha Name\nAbsence of a standardized\na f n o d r a s c y c s u te ra m t a e t ly ic d n if o fe m re e n n t c ia la t t in u g re • • A A n S y a s lg te o m rit a h t m ic N fo o r m a e u n t c o l m at a u t r i e c a fo lly r N a M ss M ig s ning names Knowledge Base Knowledg L e a L B y a e a y r s e 5 e r\nand identifying NMMs and identities to NMMs\n•A web-based app for naming NMMs Systematic Layer 4\nNomenclature Search Engine Search Engine\nfor NMMs Layer\nShennongAlpha Web and Chat Layer 3\nLack of an efficient method •A knowledge base and web for curating bilingual Rules Large Language Model Artificial Intelligence\nto access NMM knowledge NMM knowledge and fostering collaboration Guide Layer"
  },
  {
    "id": 1457,
    "text": "to access NMM knowledge NMM knowledge and fostering collaboration Guide Layer\n•A N M w M e b k - n b o a w s l e e d d g a e p t p h ro fo u r g h in c te h l a li t g ti e n n g t acquisition of Algorithm Support Name Chat Translate Algorith L m ay a e n r d 2\nApplication Layer\nLayer 1\nShennongAlpha Translate Web Web and User\nDifficulty in transcending •A standardized and interpretable AI translation Interaction Layer\nlanguage barriers to share algorithm for automatically translating NMM\nNMM knowledge globally knowledge between Chinese and English User\n•A web-based app for translating NMM knowledge\nc d e\nFig.1: ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,\nandtranslationofNMMknowledge.\na. ThechallengesforutilizingandsharingNMMknowledgeandourShennongAlphaapproaches. b. Architectureof\ntheShennongAlpha. ShennongAlphaappliestheSystematicNomenclatureforNMMs(Fig.2)andintegratesShen-"
  },
  {
    "id": 1458,
    "text": "a. ThechallengesforutilizingandsharingNMMknowledgeandourShennongAlphaapproaches. b. Architectureof\ntheShennongAlpha. ShennongAlphaappliestheSystematicNomenclatureforNMMs(Fig.2)andintegratesShen-\nnongName (Fig. 3) with a hexa-domain modular system to form its structure. The hexa-domain system is outlined\nin the light teal block. ShennongAlpha is structured into five layers, from shallow to deep: Layer 1: Web and user\ninteraction layer. In this layer, users can access the system via our ShennongAlpha Web (Fig. 4). Layer 2: Algo-\nrithm and application layer. In this layer, we have specifically developed three applications customized for NMMs:\nShennongName,ShennongChat(Fig.5),andShennongTranslate(Fig.6). Userscanaccesstheseapplicationsonthe\ncorresponding pages of the ShennongAlpha Web. Layer 3: Artificial intelligence layer. In this layer, we have inte-\ngrated the ShennongAlpha Large Language Model system, allowing the ShennongAlpha to process and respond to"
  },
  {
    "id": 1459,
    "text": "grated the ShennongAlpha Large Language Model system, allowing the ShennongAlpha to process and respond to\ndatafromdifferentlayersintelligently. Layer4: Searchenginelayer. Inthislayer,wehaveintegratedtheShennon-\ngAlpha Search Engine customized for NMM-related data. Layer 5: Knowledge base layer. In this layer, we have\nintegratedtheShennongAlphaKnowledgeBasetocurateNMMknowledgeefficiently. Arrowsrepresenttheallowed\ndatainteractionsbetweendifferentlayers. c. Cross-platformanduser-friendlydesignoftheShennongAlpha. d. The\nEnglishhomepageoftheShennongAlphaWeb. e. TheChinesehomepageoftheShennongAlphaWeb.\n13"
  },
  {
    "id": 1460,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\na b\nTypes of Natural Medicinal Materials\nSpecies scientific name: Ephedra sinica\nComponents of Systematic Name Raw NMMs Species scientific name (Chinese): 草麻黄\nLa En En En Initial preparation at the production site (产地初加工)\nSpSe 物 pcei物 种 ecs种 i 基 eo I s 基 原 r iog源 rinigsin Me 药 di 用 ci I n I 部 al 位 part Speci 特 al 殊 d I e I 形 I sc 容 ription PPrrooccee 炮s 炮 sss 制 制 iinn IV g方 g 方 mm 法 法 eetthhoodds Agricultural NMMs S S G y y e s s n t t e e e r m m ic a a n t t i i a c c m n n e a a : m m e e : (Chinese): E 草 C p a 麻 h o e - 黄 m dr 草 a a - 质 h s u in 茎 a ic n a g Stem-herbaceous\nGeneric name (Chinese): 草麻黄\nNMM ID: NMM-0003\nRaw NMMs\nEx. Ephedra sinica (草麻黄) Natural medicinal processing (炮制)\nAgricultural NMMs Systematic name: Ephedra sinica Stem-herbaceous\nEx. Ephedra sinica Stem-herbaceous (草麻黄草质茎) Segmented and Aquafried-honey"
  },
  {
    "id": 1461,
    "text": "Ex. Ephedra sinica (草麻黄) Natural medicinal processing (炮制)\nAgricultural NMMs Systematic name: Ephedra sinica Stem-herbaceous\nEx. Ephedra sinica Stem-herbaceous (草麻黄草质茎) Segmented and Aquafried-honey\nSystematic name (Chinese): 蜜炙制段制草麻黄草质茎\nProcessed NMMs Processed NMMs Generic name: Mi-cao-ma-huang\nEx. Ephedra sinica Stem-herbaceous Segmented and Aquafried-honey (蜜炙制段制草麻黄草质茎) Generic name (Chinese): 蜜草麻黄\nNMM ID: NMM-000B\nc\nSystematic Name Generic Name NMM ID\nSpecies origin Medicinal part Special description Processing method\nEphe 草 d 麻 ra 黄 sinica Cao- 草 m 麻 a- 黄 huang NMM-0003\nEphedra intermedia Zhong-ma-huang Ephedrae Herba 中麻黄 中麻黄 麻黄\nEphedra equisetina Mu-zei-ma-huang 木贼麻黄 木贼麻黄\nFreshly sliced Cleaned Jing-pian-jiang-huang Wenyujin Rhizoma Concisum\n鲜切片 净制 净片姜黄 片姜黄\nCurcuma wenyujin Rhizome Sliced Wen-yu-jin-pian 温郁金 根茎 片制 温郁金片\nRoot tuber Sliced Wen-yu-jin-gen-pian 块根 片制 温郁金根片\nCurcuma kwangsiensis Rhizome Sliced Guang-xi-e-zhu-pian Curcumae Rhizoma 广西莪术 根茎 片制 广西莪术片 莪术"
  },
  {
    "id": 1462,
    "text": "Root tuber Sliced Wen-yu-jin-gen-pian 块根 片制 温郁金根片\nCurcuma kwangsiensis Rhizome Sliced Guang-xi-e-zhu-pian Curcumae Rhizoma 广西莪术 根茎 片制 广西莪术片 莪术\nRoot tuber Sliced Guang-xi-e-zhu-gen-pian 块根 片制 广西莪术根片\nCurcuma phaeocaulis Rhizome Sliced Dan-e-zhu-pian Curcumae Radix 莪术 根茎 片制 单莪术片 郁金\nRoot tuber Sliced Dan-e-zhu-gen-pian 块根 片制 单莪术根片\nCurcuma longa Rhizome Sliced Jiang-huang-pian Curcumae Longae Rhizoma 姜黄 根茎 片制 姜黄片 姜黄\nRoot tuber Sliced Jiang-huang-gen-pian 块根 片制 姜黄根片\nsMMN\nlarutlucirgA\nsMMN\ndessecorP\nConventional Name\nStem herbaceous 草质茎\nStem herbaceous 草质茎 NMM-0004\nStem herbaceous 草质茎 NMM-0005\nNMM-0016\nNMM-000X\nNMM-001E\nNMM-000Y\nNMM-001F\nNMM-000Z\nNMM-001G\nNMM-0018\nNMM-001H\nFig.2: SystematicNomenclatureforNaturalMedicinalMaterials.\nTheSystematicNomenclatureassignseachNMMauniqueSystematicName,GenericName,andNMMID.a. Com-\nponentsoftheSystematicName.Itconsistsoffourcomponents:I.Speciesorigin,includingspeciesnamesinLatin;II."
  },
  {
    "id": 1463,
    "text": "TheSystematicNomenclatureassignseachNMMauniqueSystematicName,GenericName,andNMMID.a. Com-\nponentsoftheSystematicName.Itconsistsoffourcomponents:I.Speciesorigin,includingspeciesnamesinLatin;II.\nMedicinalpart;III.Specialdescriptionforinitialpreparationsorspecificcharacteristics;andIV.Processingmethod.\nb. NMMTypes. RawNMMsareinitiallypreparedattheproductionsitestoproduceAgriculturalNMMs; Agricul-\nturalNMMsareoftenfurtherprocessedtoproduceProcessedNMMs. c. ExamplesoftraditionalChineseNMMsin\nSystematic Nomenclature. Conventional names often lead to confusion by collectively referring to multiple NMMs\nthatarenotidentical,duetomissingorincorrectinformationaboutspeciesorigin,medicinalpart,specialdescription,\nandprocessingmethod. Forexample,theillustrationshowsthreeAgriculturalNMMsfromtheEphedragenuswith\ntheherbaceousstemasthemedicinalpart, conventionallynamed“EphedraeHerba”(“麻黄”), leadingtoambiguity."
  },
  {
    "id": 1464,
    "text": "andprocessingmethod. Forexample,theillustrationshowsthreeAgriculturalNMMsfromtheEphedragenuswith\ntheherbaceousstemasthemedicinalpart, conventionallynamed“EphedraeHerba”(“麻黄”), leadingtoambiguity.\nSimilarly, nine Processed NMMs from the Curcuma genus, with different medicinal parts, initial preparations and\nprocessing methods, are collectively referred to by four names: “Wenyujin Rhizoma Concisum” (“片姜黄”), “Cur-\ncumae Rhizoma” (“莪术”), “Curcumae Radix” (“郁金”), and “Curcumae Longae Rhizoma” (“姜黄”). In contrast,\nourSystematicNomenclatureaccuratelyassignsdistinctSystematicNames,GenericNames,andNMMIDstothese\ntwelvedifferentAgriculturalandProcessedNMMs,eliminatingambiguity.Thedashedlinesconnecttheconventional\nnamestothedifferentNMMstheycollectivelyrepresent.\n14"
  },
  {
    "id": 1465,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n①\n③’Add additional information\n② ③\n④\n④’Search ShennongAlpha Knowledge Base in real-time\n⑤\n⑥\n⑦\n⑥’Automatically assign the NMM Systematic Name\nwith cautionary notations\n⑧\nFig.3: UsingShennongNametoautomaticallyconstructNMMSystematicNames.\nUsersselecttheNMMtypeandprovideinformationforthefournamecomponentsinarea 1. Byclickingonhyper-\nlinkslike 2,userscanviewentriesforthefournamecomponentsalreadycatalogedintheShennongAlphaKnowledge\nBase. Foreachnamecomponent,userscanaddadditionalinformationbyclickingonplusbuttonslike 3 (3’). When\nusersbeginenteringnamecomponentinformationintextboxeslike 4, ShennongNameperformsareal-timesearch\nin the Knowledge Base for relevant matching entries to enable auto-completion (4’). After users have entered the\nnecessarynaminginformationfortheNMM,theycanclickonthe“ConstructNMMSystematicName”button(5),"
  },
  {
    "id": 1466,
    "text": "necessarynaminginformationfortheNMM,theycanclickonthe“ConstructNMMSystematicName”button(5),\nallowingShennongNametoautomaticallyconstructtheSystematicNameusingthealgorithm. Iftheconstructionis\nsuccessful,thegeneratedinformationisdisplayedwithagreenbackground(6). Ifanyissuesariseduringconstruc-\ntion,therelevantinformationisdisplayedwithanorangebackground(6’). ForsuccessfullyconstructedSystematic\nNames,ShennongNamewillalsoautomaticallyperformasearchforitintheKnowledgeBase; ifamatchingNMM\nisfound, userswillbeinformedthattheNMMisalreadyrecordedintheKnowledgeBase, eliminatingtheneedfor\nredundant construction (7). After successfully constructing a new Systematic Name, if users wish to add it to the\nKnowledge Base, they can provide relevant details about the NMM in the textbox in area 8 and submit it. Once\nreviewedandapprovedbyShennongAlpha,theentrywillbeincorporatedintotheKnowledgeBase.\n15"
  },
  {
    "id": 1467,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nHome\nPage\n⑰ ⑱ ⑲ ⑳\n①\nSearch\nPage\n③\n④\nKnowledge Page\n②\n⑰ ⑱ ⑲ ⑳ ⑧ ⑩\n⑤\n⑬\n⑨ ⑪ ⑫\n⑥ ⑯\n⑭⑮\n⑦\nFig.4: BrowsingNMMknowledgeontheShennongAlphaWeb. (Legendcontinuedonnextpage.)\n16"
  },
  {
    "id": 1468,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nFig.4: (Legendcontinuedfromthepreviouspage.) UserscaninitiatetheirexplorationofNMMknowledgebyusing\nthe search bar located either on the homepage (1) or atop other pages (2). Post-search, users are directed to the\nsearch page where they can glance through the title (3) and summarized information (4) of each NMM entry to\nascertainitsrelevance. Byclickingonthetitleofanentry,usersarenavigatedtoadetailedknowledgepagededicated\ntothatspecificNMM.Theheaderofthisknowledgepage(5)displaystheSystematicNameoftheNMM,whilethe\nmain content area (6) is organized in a structured, section-by-section layout. The “Table of Contents” sidebar (7)\nenablesswiftnavigationbetweensections. Tofacilitatecross-languageaccessibilityforglobalusers,theWeboffers\nfour display modes (8): Bilingual (Chinese-English), Bilingual (English-Chinese), Chinese only, and English only."
  },
  {
    "id": 1469,
    "text": "four display modes (8): Bilingual (Chinese-English), Bilingual (English-Chinese), Chinese only, and English only.\nThe “Save” button (9) allows users to bookmark the knowledge page to their user dashboard (10). To encourage\nacademic references, the “Cite” button (11) offers citation formats in styles such as APA, MLA, GB/T 7714-2015,\nandBibTeX.The“Download”button(12)enablesuserstodownloadtheknowledgepage’scontentinJSONformat.\nFurthermore,withthe“Leaveyournameandknowledge!”button(13),userscanproposeneworrevisedNMM-related\nknowledge. Contributionscanalsobemadedirectlyviathe“EditContent”button(14),allowinguserstomodifythe\ncontentofeachsection. Toreviewpastmodifications,the“EditHistory”button(15)providesaccesstoallhistorical\nchanges. ApprovedusercontributionsarethenintegratedintotheShennongAlphaKnowledgeBase,andcontributors\nare recognized and acknowledged in the “Contributors” area (16), where their usernames and avatars are displayed."
  },
  {
    "id": 1470,
    "text": "are recognized and acknowledged in the “Contributors” area (16), where their usernames and avatars are displayed.\nUserscannavigatetotheShennongChat(17,Fig.5),ShennongTranslate(18,Fig.6),andShennongName(19,Fig.3)\napplicationsinShennongAlpha,aswellasthedetailedrulesoftheSystematicNomenclatureforNMMs(20),directly\nfromthehomepageorthroughtheheadernavigationbaronanypageoftheWeb.\n①\n③\n②\n④\n⑦\n⑧\n⑨\n⑩ ⑪\n⑫ ⑤ ⑥\nFig.5: Chat-basedNMMknowledgeacquisitionthroughShennongChat.\nUserscaninitiateanewchatbyclickingtheplusbutton(1). Thechathistoryarea(2)retainspreviouschatsessions,\nallowing users to navigate between historical chat sessions or delete unwanted chat sessions if needed. Within each\nchatsession,theguidancesection(3)highlightstheuniquefeaturesandcapabilitiesofShennongChat,ensuringusers\ncaneasilygetstarted.Toquicklyexperiencethechattingfeatures,userscanselectfromasetofsamplequestionsinthe\n4 area. Byinputtingaquestioninthechatbox(5)andhittingthesendbutton(6),theuser’squestion(7)isrelayed"
  },
  {
    "id": 1471,
    "text": "4 area. Byinputtingaquestioninthechatbox(5)andhittingthesendbutton(6),theuser’squestion(7)isrelayed\nto ShennongChat for a response. In formulating its response, ShennongChat integrates the ShennongAlpha Search\nEnginetoretrievestandardizedknowledgeabouttheNMMmentionedintheuser’squestionfromtheShennongAlpha\nKnowledge Base, providing a retrieval-augmented response (8). Users can expand the search dropdown (9) for\ndetailsaboutthesearchresults. Thecopybutton(10)enablesuserstoduplicatethetextofShennongChat’sresponse.\nFeedbackontheresponses(eitherpositiveornegative)canbeprovidedbytheuser(11),whichhelpsinimprovingthe\nqualityofShennongChat’sresponses. Dependingontheirpreference,userscantogglebetweenShennongChat’schat\nmodes: preciseorquick(12).\n17"
  },
  {
    "id": 1472,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n① ⑪\n⑧\n⑥ Hover\n⑦\n② ⑤\n④ ⑩ ⑨\n⑥’\n③\nFig.6: StandardizedandinterpretabletranslationofNMMtextthroughShennongTranslate.\nUsers can toggle between Chinese and English translation directions using 1. After inputting the NMM text to be\ntranslatedinthetextentryarea(2)andaddinganyuser-customizedglossary(3,optional),userscanclickthe“Trans-\nlate”button(4)tosubmitthetextfortranslation. Theuser-customizedglossariesaresaved,anduserscanadd/delete,\nactivate/deactivate these glossaries as needed, allowing ShennongTranslate to become increasingly tailored to their\ntranslation preferences. During the translation process, ShennongTranslate automatically uses the ShennongAlpha\nSearchEnginetoidentifyNMMtermsinthetextandthenretrievetheirPrimaryTermsandstandardizedtranslations"
  },
  {
    "id": 1473,
    "text": "SearchEnginetoidentifyNMMtermsinthetextandthenretrievetheirPrimaryTermsandstandardizedtranslations\nfromtheShennongAlphaKnowledgeBase.Inthetranslationresults(5).standardizedtranslationsofNMMterms(6)\nanduser-customizedterms(7)arehighlightedintealandblue,respectively. Hoveringoverastandardizedtranslation\nofanNMMterm(6)promptsShennongTranslatetodisplayatooltip(6’)containingitsintroductoryabstract;clicking\nonthetooltipdirectsuserstotheknowledgepageoftheNMM.Thebutton 8 allowsuserstotogglebetweenplaintext\nandMLMDsourcecodedisplays. Ineitherdisplaymode,userscancopythecontentbyclickingthe“Copy”button\n(9). Feedback on the translation (either positive or negative) can be provided by the user (10), helping to improve\nShennongTranslate’s performance. Depending on their preference, users can toggle between ShennongTranslate’s\ntranslatemodes: preciseorquick(11).\n18"
  },
  {
    "id": 1474,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nSupplementary information\n19\n\nShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nA Supplementaryfigures\nFig.S1: Wikipediaentry: Ephedra.\nWikipediastatesthatthespeciesoriginofEphedraisEphedrasinicaandclaimsthatitisknownas“MaHuang”in\ntraditionalChinesemedicine. However,thisinformationisinaccurate. AccordingtoChina’smostauthoritativephar-\nmaceutical reference, Chinese Pharmacopoeia (2020 Edition), Ephedra is a multi-species NMM. Its species origins\nincludeEphedrasinica,Ephedraintermedia,orEphedraequisetina.\n20"
  },
  {
    "id": 1475,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nFig.S2: GPT-4’sansweronthespeciesoriginofMaHuang.\nGPT-4incorrectlyassertsthatMaHuangisasingle-speciesNMMbasedsolelyonEphedrasinica.\nFig.S3: GoogleTranslate’stranslationoftextrelatedto“MaHuang”(麻黄).\nDue to the lack of a systematic nomenclature for NMMs, Google Translate simplifies the translation of “麻黄” to\n“Ephedra,”failingtoreflectthemulti-speciesintricaciesof“麻黄”. ThismistranslationcouldleadEnglish-speaking\nuserstoincorrectlyusetheterm“Ephedra”forfurtherretrievalofknowledge,potentiallydirectingthemtotheafore-\nmentioned Wikipedia entry containing inaccurate information, thereby hindering the globalization of NMM knowl-\nedge.\n21"
  },
  {
    "id": 1476,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nNatural Medicinal Material\nSystematic Name (NMMSN)\n天然药材系统名\nNMMSN\nNMMSN\nLa En En En\n物种基I原 II III 炮制IV方法\nSpecies origin Medicinal part Special description Processing method\nSpecies origins Processing methods\n物种基源 药用部位 特殊形容 炮制方法\nAgricultural NMMs (ANMMs, 农产天然药材)\nProcessed NMMs (PNMMs, 炮制天然药材)\nFig.S4: ParsingstructureoftheNMMSystematicName.\nThe parsing structure of a valid NMM Systematic Name (NMMSN) involves a lawful combination of four naming\ncomponents. Ataminimum, anNMMSNincludescomponentsIandII.ForAgriculturalNMMs, anNMMSNmay\ninclude components I, II, and III; for Processed NMMs, it may include components I, II, III, and IV. A solid line\nindicates that these components are required for that type of NMM; a dashed line indicates that component III is\noptional.\nSystematic Generic\nChinese name Chinese name\nSystematic Generic\nNMM ID\nName Name"
  },
  {
    "id": 1477,
    "text": "optional.\nSystematic Generic\nChinese name Chinese name\nSystematic Generic\nNMM ID\nName Name\nSpecies origin Coreference Primary\nTerm Graph\nMedicinal part\nSNNMM ShennongKB Relation NMM synonym\nSpecial description\nSpecies synonym\nProcessing method\nText Glossary\nKnowledge\nChinese Pharmacopeia (2020 Ed.)\nNMM translation\n(standard)\nChinese Pharmacopeia (2015 Ed.) NMM knowledge\n(structured, standard)\nFig.S5: NMMknowledgecuratedintheShennongAlphaKnowledgeBase\nThe ShennongAlpha Knowledge Base (ShennongKB) encompasses five major categories of information, including\ntheSystematicNomenclatureforNaturalMedicinalMaterials(SNNMM),text,knowledge,glossary,andrelation.\n22"
  },
  {
    "id": 1478,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nzh-en\nen-zh\nzh\nen\nFig.S6: LanguagedisplaymodesintheShennongAlphaWeb.\nTheShennongAlphaWebadoptsabilingualdesigninbothChineseandEnglish, utilizingMLMDtocurateparallel\nChinese and English texts within a single document. ShennongAlpha supports four language display modes: zh-\nen, en-zh, zh, and en. Notably, in the zh-en and en-zh modes, both the section titles and text of the knowledge are\ndisplayedbilingually,differingonlyintheirdisplayorder(inthezh-enmode,ChineseprecedesEnglish,whereasin\nen-zh,EnglishcomesbeforeChinese). Inthezhandenmodes,boththesectiontitlesandtextoftheknowledgeare\ndisplayedsolelyinChineseorEnglish,respectively.\n23"
  },
  {
    "id": 1479,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nShennongSearch\na. Coreference-based Graph Search\nCoreference Primary Term Graph Information Collections\n(Directed Acyclic Graph)\nprimary_term: nmm-0006\nstd_translation_en: Ephedra equisetina vel intermedia vel sinica\nEphedra equisetina vel intermedia vel sinica Stem-herbaceous Stem-herbaceous (NMM-0006, Ma-huang)\nstd_translation_zh: 木贼麻黄或中麻黄或草麻黄草质茎（\n木贼麻黄或中麻黄或草麻黄草质茎 NMM-0006，麻黄）\nnmm-0006 std_knowledge: Ephedra equisetina vel intermedia vel sinica\n(primary_term)\nEphedrae Herba 麻黄 Stem-herbaceous (NMM-0006, Ma-huang)\nis the dried herbaceous stem of Ephedra\nSynonym sinica, Ephedra intermedia or Ephedra\nEphedra Ma-huang\nMa Huang equisetina. The drug is collected in autumn,\nand dried in the sun...\nb. Vector Search\nKnowledge Texts as Vector similarity Query results\ntexts vector embeddings\nEmbed [ ··· ]\n[ ··· ] Most\nsimilar\n[ ··· ]\nQuery\ntext"
  },
  {
    "id": 1480,
    "text": "and dried in the sun...\nb. Vector Search\nKnowledge Texts as Vector similarity Query results\ntexts vector embeddings\nEmbed [ ··· ]\n[ ··· ] Most\nsimilar\n[ ··· ]\nQuery\ntext\nEmbed [ ··· ]\nc. Full-text Search\nInverted index\nTokens Document indexes\nQuery text Qing-hao Query results\nWhat is Qing-hao and Ma-huang?\nTokenize\nfor search Ma-huang\n[“Qing-hao”, “Ma-huang”]\nQing-hao-duan\nSearch\n··· ···\nFig.S7: AdvancedsearchmethodsintheShennongAlphaSearchEngine.\nTheShennongAlphaSearchEngine(ShennongSearch)hasbeendesignedtoofferthreedistinctadvancedsearchmeth-\nodstocatertothespecificsearchneedsassociatedwithNMMs: a. Coreference-basedGraphSearch(CGS),utilizing\naCoreferencePrimaryTermGraph(CPTG)todepicttherelationshipsamongNMMs’synonyms,therebyfacilitating\nthe retrieval of standardized knowledge and translations for a given NMM name; b. Vector search, which evaluates\nthesimilaritybetweenthevectorembeddingsofqueriesandthetextsarchivedinShennongKB,allowingforsemantic"
  },
  {
    "id": 1481,
    "text": "thesimilaritybetweenthevectorembeddingsofqueriesandthetextsarchivedinShennongKB,allowingforsemantic\nmatches; c. Full-textsearch, employingtokenizationandaninvertedindextopinpointandyieldthepertinentfuzzy\nresults.\n24"
  },
  {
    "id": 1482,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nMulti-turn\nchat\n1. Users input questions in ShennongChat frontend 6. ShennongChat frontend parses new\nresponse chat data and history, then\nA. Current session chat history (optional): updates the chat state accordingly\n...\nE. ShennongChat’s answer:\nB. New user question: The species origin of Ma Huang is Ephedra\nWhat is the species origin of Ma Huang? equisetina, Ephedra intermedia, or Ephedra\nsinica.\nShennongLLM 2. Assess whether a search is needed 5. Generate a\nShennongLLM retrieval-augmented\nanswer\nTrue\n3. Search for and acquire D. Structured question input:\nrelated knowledge Chat history:\nShennongSearch ShennongKB ...\nUser question:\nWhat is the species origin of Ma Huang?\nC. Related standardized knowledge: Related standardized knowledge:\nnmm_id: nmm-0006 nmm_id: nmm-0006"
  },
  {
    "id": 1483,
    "text": "ShennongSearch ShennongKB ...\nUser question:\nWhat is the species origin of Ma Huang?\nC. Related standardized knowledge: Related standardized knowledge:\nnmm_id: nmm-0006 nmm_id: nmm-0006\nsystematic_name: Ephedra equisetina vel systematic_name: Ephedra equisetina vel\nintermedia vel sinica Stem-herbaceous intermedia vel sinica Stem-herbaceous\nsystematic_chinese_name: 木贼麻黄或中麻黄或草麻黄 systematic_chinese_name: 木贼麻黄或中麻黄或草麻黄\n草质茎 草质茎\ngeneric_name: Ma-huang generic_name: Ma-huang\ngeneric_chinese_name: 麻黄 generic_chinese_name: 麻黄\nspecies_origins: Ephedra equisetina, or, Ephe- species_origins: Ephedra equisetina, or, Ephe-\ndra intermedia, or, Ephedra sinica dra intermedia, or, Ephedra sinica\n... ...\n4. Concatenate A+B+C\nFig.S8: AlgorithmbehindShennongChat.\nUpon receiving a new question from a user, ShennongChat initially employs the ShennongAlpha Large Language\nModel system (ShennongLLM) to assess if the existing chat history provides sufficient knowledge to formulate an"
  },
  {
    "id": 1484,
    "text": "Model system (ShennongLLM) to assess if the existing chat history provides sufficient knowledge to formulate an\nanswer. Inthepresentedcase,thechathistorylacksthenecessaryknowledge. Consequently,ShennongLLMdecides\nto dispatch ShennongSearch, which delves into ShennongKB to find relevant standardized knowledge. Specifically,\nknowledgerelatedtothespeciesoriginof“MaHuang”isretrieved. ShennongChatthenintegratestheuser’sprevious\nchat history, the new question, and the retrieved standardized knowledge to generate a retrieval-augmented answer.\nThisanswerissubsequentlydisplayedontheShennongChatuserinterface.\n25"
  },
  {
    "id": 1485,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\na\nb\nc\nFig.S9: ShennongChatofferspersonalizedquestioningandansweringstylesforusers.\nUserscanusetheirpreferredlanguage(suchasChinese)toquestionShennongChatandreceiveanswersinthesame\nlanguage(a). Userscanalsospecifytheirdesiredanswerstylebydescribingitinnaturallanguageintheirquestion,\nsuch as opting for a particular language (b) or a specific format (c). ShennongChat will tailor its answers to users’\nindicatedstyles.\n26"
  },
  {
    "id": 1486,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n1. Users input in ShennongTranslate frontend 6. ShennongTranslate frontend parses the\nformatted translation result and presents it\nA. Text for translation: as a user-friendly, interactive translation\n麻黄是一种天然药材。\nF. Formatted translation result:\n[[nmm-0006 | Ephedra equisetina vel\nB. Translation direction:\nintermedia vel sinica Stem-herbaceous\nzh → en\n(NMM-0006, Ma-huang)]] is a kind of\n[[Natural Medicinal Material]].\nC. Formatted user-customized dictionary:\n天然药材 → [[Natural Medicinal Material]]\n5. Generate a\nShennongLLM retrieval-augmented\n2. Tokenize intelligently translation\n[“麻黄”, “是”, “一种”, “天然药材”, “。”]\nE. Structured translation input:\n3. Detect and retrieve standardized translations Text for translation:\n麻黄是一种天然药材。\nShennongSearch ShennongKB Translation direction:\nzh → en\nFormatted dictionary:"
  },
  {
    "id": 1487,
    "text": "E. Structured translation input:\n3. Detect and retrieve standardized translations Text for translation:\n麻黄是一种天然药材。\nShennongSearch ShennongKB Translation direction:\nzh → en\nFormatted dictionary:\nD. Formatted standardized translation dictionary: 麻黄 → [[nmm-0006 | Ephedra equisetina vel\n麻黄 → [[nmm-0006 | Ephedra equisetina vel intermedia vel sinica Stem-herba-\nintermedia vel sinica Stem-herba- ceous (NMM-0006, Ma-huang)]]\nceous (NMM-0006, Ma-huang)]] 天然药材 → [[Natural Medicinal Material]]\n4. Concatenate A+B+C+D\nFig.S10: AlgorithmbehindShennongTranslate.\nShennongTranslate employs the translation algorithm we proposed, named “Neural Machine Translation based on\nCoreferencePrimaryTerm(NMT-CPT)”. Followingasequencefromsteps1-6,ShennongTranslategeneratesastan-\ndardizedtranslationwiththeformatshowninF.Theannotation“[[xxx|yyy]]”isaproprietaryMLMDsyntaxused\nintheNMT-CPT.Here,“xxx”standsforthePrimaryTermretrievedfromShennongKB,while“yyy”denotesitsstan-\ndardizedtranslationinthetargetlanguage.\n27"
  },
  {
    "id": 1488,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nB Supplementarytables\nTableS1: Problemsinthenamesof616NMMsintheChinesePharmacopoeia: 2020Edition: VolumeI.\nPleaserefertotheattachedExcelspreadsheetforspecifictablecontents.\nTableS2: ShennongAlphaKnowledgeBasedatacounts.\nData as of 2024-05-01. The up-to-date data counts are available on the ShennongAlpha Statistics page (https:\n//shennongalpha.westlake.edu.cn/statistics).\nType Count Description\nNMM 14,256 Unique NMMs under the Systematic Nomenclature, with distinct\nNMMIDs,SystematicNames,andGenericNames\nNMMknowledge 14,256 Structured,standardizedknowledgeofNMMs\nNMMstandardizedtranslation 14,256 Standardizedbilingual(Chinese-English)translationsofNMMs\nNMMtextinChP-2020 616 NMMmonographsandtheirtextintheChinesePharmacopoeia(2020\nedition)\nNMMtextinChP-2015 618 NMMmonographsandtheirtextintheChinesePharmacopoeia(2015\nedition)"
  },
  {
    "id": 1489,
    "text": "NMMtextinChP-2020 616 NMMmonographsandtheirtextintheChinesePharmacopoeia(2020\nedition)\nNMMtextinChP-2015 618 NMMmonographsandtheirtextintheChinesePharmacopoeia(2015\nedition)\nNMMsynonym 58,872 SynonymousnamesofNMMs\nSpeciesorigin 7,125 RelevantspeciesoriginsofthecuratedNMMs\nMedicinalpart 347 RelevantmedicinalpartsofthecuratedNMMs\nProcessingmethod 78 RelevantprocessingmethodsofthecuratedNMMs\n28"
  },
  {
    "id": 1490,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nC Supplementarytext: SystematicNomenclatureforNaturalMedicinalMaterials\n(SNNMM)\nRule1: Two names one ID rule. Each Natural Medicinal Material (NMM) is assigned an NMM Systematic Name\n(NMMSN),anNMMGenericName(NMMGN),andanNMMID.\nRule2: Uniquenessrule. TheNMMSN,NMMGN,andNMMIDgeneratedunderSNNMMareallunique.\nRule3: ANMM/PNMMdistinctnamerule. SNNMMclassifiesNMMsinto3types: RawNMMs(RNMMs), Agri-\nculturalNMMs(ANMMs), andProcessedNMMs(PNMMs)(Fig.2b). RNMMsaregenerallyregulatedas\nagriculturalproducts,whilePNMMsaregenerallyregulatedasdrugs.\nRule4: Legitimatecharacterrule. ToallowpeoplefromanycountrytoeasilyinputNMMSNorNMMGNusingtheir\nstandard keyboard, naming NMMSN and NMMGN can only use the following characters: a-zA-Z, and the\nhyphen“-”.\nRule5: Case-insensitiverule. InNMMSNorNMMGN,eventhoughbothuppercaseandlowercaselettersmightbe"
  },
  {
    "id": 1491,
    "text": "hyphen“-”.\nRule5: Case-insensitiverule. InNMMSNorNMMGN,eventhoughbothuppercaseandlowercaselettersmightbe\nused,it’sonlyforeaseofreading.DifferentNMMscannotbedistinguishedbydifferentcasesintheSNNMM.\nRule6: Uprighttyperule. NMMSNorNMMGNshouldbewritteninuprighttype.\nRule7: NMMSNrules.\nRule7.1: StandardChineseNMMSNcorrespondencerule.TofacilitatecommunicationinaChinesecontext,\nforeachNMMnamedwithNMMSN,SNNMMwillalsoprovideacorrespondingstandardChinese\nsystematicname,i.e.,NMMSN-zh.\nRule7.2: NMMSNcompositionRule. NMMSNconsistsofthefollowing4namecomponents(Fig.2a): I.\nSpeciesorigin;II.Medicinalpart;III.Specialdescription;IV.Processingmethod.\nRule7.3: NMMSNsyntaxparsingrule. Basedontheprinciplesofnaturallanguageprocessingandcompu-\ntationallinguistics,thesyntaxparsingstructureofNMMSNisasshowninFig.S4.\nRule7.4: MinimalNMMSNcompositionrule. TheNMMSNofANMMmustcontainatleastthefirsttwo\nnamecomponents:IandII;TheNMMSNofPNMMmustcontainatleastthreenamecomponents:\nI,II,andIV(Fig.S4)."
  },
  {
    "id": 1492,
    "text": "Rule7.4: MinimalNMMSNcompositionrule. TheNMMSNofANMMmustcontainatleastthefirsttwo\nnamecomponents:IandII;TheNMMSNofPNMMmustcontainatleastthreenamecomponents:\nI,II,andIV(Fig.S4).\nRule7.5: NMMSNorderrule. ThesequenceofnamecomponentsinanNMMSNmustadheretoaspecific\norder: I-II-III-IV.InthecaseofNMMSN-zh,whichfollowsChineseconventions,thecomponents\nareorderedasIV-III-I-II.\nRule7.6: Speciesoriginnamingrules.\nRule7.6.1: Completetaxonomicnamerule. PrioritizetheuseofthefullLatintaxonomicnameof\naspecieswhennaming. Whenaspecificspeciescanbeidentified,donotjustusethe\ngenusnameorspeciesepithetfornaming.\nEx. ✔ NMMSN: Solidago decurrens Herb [1]; NMMSN-zh: 一枝黄花全草(species\norigin: Solidagodecurrens)\n✘ SolidagoHerb\nEx. ✔ NMMSN: Panax ginseng Rhizome and Root [2]; NMMSN-zh: 人参根茎与\n根(speciesorigin: Panaxginseng)\n✘ GinsengRhizomeandRoot\nRule7.6.2: Standard taxonomic name rule. When naming NMMSN, prioritize using the current\nstandardtaxonomicname. Thestandardtaxonomicnameshouldprimarilyrefertothe"
  },
  {
    "id": 1493,
    "text": "✘ GinsengRhizomeandRoot\nRule7.6.2: Standard taxonomic name rule. When naming NMMSN, prioritize using the current\nstandardtaxonomicname. Thestandardtaxonomicnameshouldprimarilyrefertothe\nfollowing databases: “The Catalogue of Life” (https://www.catalogueoflife.\norg)and“CatalogueofLifeChina”(http://www.sp2000.org.cn).\nEx. ✔ Tetradiumruticarpum ✘ Euodiaruticarpa ✘ Euodiaruticarpavar.of-\nficinalis ✘ Euodiaruticarpavar. bodinieri\nIn the “Chinese Pharmacopoeia: 2020 Edition: Volume I” (hereinafter referred to as\n“ChP”)[3],theNMM“吴茱萸”isrecordedasamulti-speciesNMM,basedonEuo-\ndia ruticarpa or Euodia ruticarpa var. officinalis or Euodia ruticarpa var. bodinieri.\nHowever,allthreeofthesescientificnamesarenon-standard. Thecorrectnameforall\nofthemisTetradiumruticarpum. Therefore,“吴茱萸”isstillactuallyasingle-species\nNMM.ItscorrectNMMSNis:\nNMMSN:TetradiumruticarpumFruit[4];NMMSN-zh: 吴茱萸果实\nRule7.6.3: Multiplespeciesoriginsnamingrule. Whenthesamepartofmultiplespeciesisused"
  },
  {
    "id": 1494,
    "text": "NMM.ItscorrectNMMSNis:\nNMMSN:TetradiumruticarpumFruit[4];NMMSN-zh: 吴茱萸果实\nRule7.6.3: Multiplespeciesoriginsnamingrule. Whenthesamepartofmultiplespeciesisused\nasthemedicinalpartofanNMM,andthesespeciesareinterchangeable(i.e.,inan“or”\nrelationship),inordertoclarifythespecificspeciesoriginsoftheNMM,allspeciesori-\nginsshouldbefullylistedintheNMMSN.Taxonomicnamesofspeciesarelistedside\n29"
  },
  {
    "id": 1495,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nbyside, connectedbytheterm“vel”(Latinconjunctionmeaning“or”)(InNMMSN-\nzh, the word “或” is used), and arranged in alphabetical order by the Latin names.\nWhenthegenusnameappearsrepeatedlyamongthemultiplespecies,thegenusname\ncanbeomittedthesecondtimeitappears.\nEx. Multiple species originate from the same genus. In ChP, “麻黄” has 3 species ori-\ngins: EphedrasinicaorEphedraintermediaorEphedraequisetina. Thus,itshouldbe\nnamedas:\nEphedraequisetinavelEphedraintermediavelEphedrasinicaStem-herbaceous\nHowever,becausetheysharethesamegenusname,itshouldbefurtherabbreviatedas:\nNMMSN: Ephedra equisetina vel intermedia vel sinica Stem-herbaceous [5];\nNMMSN-zh: 木贼麻黄或中麻黄或草麻黄草质茎\nEx. Multiplespeciesoriginatefromdifferentgenera. Forexample:\nCremastraappendiculatavelPleionebulbocodioidesvelPleioneyunnanensisPseudob-\nulb"
  },
  {
    "id": 1496,
    "text": "NMMSN-zh: 木贼麻黄或中麻黄或草麻黄草质茎\nEx. Multiplespeciesoriginatefromdifferentgenera. Forexample:\nCremastraappendiculatavelPleionebulbocodioidesvelPleioneyunnanensisPseudob-\nulb\nAfterabbreviatingthegenusnameforthelasttwospecies,itbecomes:\nNMMSN:CremastraappendiculatavelPleionebulbocodioidesvelyunnanensisPseu-\ndobulb[6];NMMSN-zh: 杜鹃兰或独蒜兰或云南独蒜兰假鳞茎\nRule7.6.4: Refinementofspeciesoriginrule.Whenamorerefinedspeciesorigincanbespecified,\nweshouldprioritizeusingthismorerefinedspeciesoriginforreferringtotheNMM.\nEx. For“麻黄”withmorerefinedspeciesorigin,wecanadoptthefollowingnames:\nNMMSN:EphedrasinicaStem-herbaceous[7];NMMSN-zh: 草麻黄草质茎\nNMMSN:EphedraintermediaStem-herbaceous[8];NMMSN-zh: 中麻黄草质茎\nNMMSN:EphedraequisetinaStem-herbaceous[9];NMMSN-zh: 木贼麻黄草质茎\nRule7.6.5: Speciesinclusionrule. Foramulti-speciesNMM,ifthetaxonomiclevelsofitsspecies\noriginsareinahierarchicalrelationship,onlythespecieswiththehighesthierarchical\nlevelisusedfornaming."
  },
  {
    "id": 1497,
    "text": "Rule7.6.5: Speciesinclusionrule. Foramulti-speciesNMM,ifthetaxonomiclevelsofitsspecies\noriginsareinahierarchicalrelationship,onlythespecieswiththehighesthierarchical\nlevelisusedfornaming.\nEx. ChP records the species origins of “山楂” as Crataegus pinnatifida and Crataegus\npinnatifida var. major. However, since Crataegus pinnatifida var. major is a variety\nofCrataeguspinnatifida, thelatterincludestheformer, soweshouldnotincludethis\nvarietyasaspeciesoriginof“山楂”inthenaming.\n✔ NMMSN:CrataeguspinnatifidaFruit[10];NMMSN-zh: 山楂果实\n✘ CrataeguspinnatifidavelpinnatifidavarmajorFruit\nTheNMMwithmorerefinedspeciesorigincanbenamedseparatelytoclarify:\n✔ NMMSN:CrataeguspinnatifidavarmajorFruit[11]; NMMSN-zh: 山里红果\n实\nRule7.6.6: NamingruleforNMMswithuncertainspecificspeciesinformation. ForNMMswhere\nthe specific species information is uncertain, the naming can be done using only the\ngenus name, followed by “unspecified” (in NMMGN-zh, add “未定种”) to indicate\nthatthespeciesinformationhasyettobeclarified."
  },
  {
    "id": 1498,
    "text": "genus name, followed by “unspecified” (in NMMGN-zh, add “未定种”) to indicate\nthatthespeciesinformationhasyettobeclarified.\nEx. TherearethousandsofspeciesunderthegenusTaraxacum,allofwhichcouldpoten-\ntiallybeusedasanNMM:\nNMMSN:TaraxacumunspecifiedHerb[12];NMMSN-zh: 蒲公英属未定种全草\nRule7.6.7: Rule for omitting non-legitimate characters in taxonomic names. If the taxonomic\nnameofthespeciesoriginoftheNMMcontainsnon-legitimatecharacters,theseshould\nbeomitted.\nEx. ✔ NMMSN:ZiziphusjujubavarspinosaSeed[13];NMMSN-zh酸枣种子\n✘ Ziziphusjujubavar. spinosaSeed\nRule7.7: Medicinalpartnamingrules.\nRule7.7.1: Syntaxrule. ThemedicinalpartinNMMSNshouldbenamedusingsingularnounsor\nnounphrasesinEnglish.\nEx. ✔ NMMSN:PanaxginsengLeaf[14];NMMSN-zh: 人参叶\n✘ PanaxginsengLeaves\nRule7.7.2: Capitalization and hyphenation rule. For readability, the first letter of the medicinal\npartshouldbecapitalized. Ifthemedicinalpartconsistsofmultiplewords,theyshould\nbeconnectedwithahyphen“-”."
  },
  {
    "id": 1499,
    "text": "partshouldbecapitalized. Ifthemedicinalpartconsistsofmultiplewords,theyshould\nbeconnectedwithahyphen“-”.\nEx. NMMSN:SolidagodecurrensHerb[1];NMMSN-zh: 一枝黄花全草\nEx. NMMSN:EphedrasinicaStem-herbaceous[7];NMMSN-zh: 草麻黄草质茎\nRule7.7.3: Multiple medicinal parts naming rule. If an NMM uses multiple different medicinal\npartsfromthesamespecies,theycanbeconnectedwith“and”(forNMMSN-zh,using\n30"
  },
  {
    "id": 1500,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n“与”). If an NMM can use multiple interchangeable medicinal parts from the same\nspecies,theycanbeconnectedwith“or”(forNMMSN-zh,using“或”). Theorderof\nmedicinalpartsisbasedonthealphabeticalorder.\nEx. NMMSN:VincetoxicumpycnostelmaRhizomeandRoot[15]; NMMSN-zh: 徐长卿\n根茎与根\nRule7.7.4: Refinementofmedicinalpartrule. Whenamorespecificmedicinalpartcanbeiden-\ntified,weshouldprioritizeusingtherefinedmedicinalparttorefertotheNMM.This\nnamingrulehelpsustoclarifythemedicinalpartoftheNMMfurther.\nEx. “VincetoxicumpycnostelmaRhizomeandRoot”canbefurtherrefinedas:\nNMMSN:VincetoxicumpycnostelmaRhizome[16];NMMSN-zh: 徐长卿根茎\nNMMSN:VincetoxicumpycnostelmaRoot[17];NMMSN-zh: 徐长卿根\nRule7.8: Specialdescriptionsnamingrules.\nRule7.8.1: Syntax rule. Some NMMs require specific characteristics or need to undergo special"
  },
  {
    "id": 1501,
    "text": "NMMSN:VincetoxicumpycnostelmaRoot[17];NMMSN-zh: 徐长卿根\nRule7.8: Specialdescriptionsnamingrules.\nRule7.8.1: Syntax rule. Some NMMs require specific characteristics or need to undergo special\ninitialpreparationattheproductionsitebeforetheycanbeusedasmedicine. Forthese\nNMMs,adjectives,adjectivephrases,nouns,orappositivescanbeusedfornamingin\nelementIII.\nRule7.8.2: Capitalization and hyphenation rule. For readability, the first letter of the special de-\nscriptions should be capitalized. If the descriptions consist of multiple words, they\nshouldbeconnectedwithahyphen“-”.\nEx. NMMSN:ZingiberofficinaleRhizome[18];NMMSN-zh: 姜根茎\nNMMSN:ZingiberofficinaleRhizomeFresh[19];NMMSN-zh: 鲜姜根茎\nEx. NMMSN:CurcumawenyujinRhizome[20];NMMSN-zh: 温郁金根茎\nNMMSN: Curcuma wenyujin Rhizome Freshly-sliced [21]; NMMSN-zh: 鲜切片温\n郁金根茎\nRule7.8.3: RuleforgenuineregionalNMMs(道地药材).ForsomeNMMsthatspecificallyneed\ntobesourcedfromgenuineregions,geographicalnounscanbeusedasspecialdescrip-"
  },
  {
    "id": 1502,
    "text": "郁金根茎\nRule7.8.3: RuleforgenuineregionalNMMs(道地药材).ForsomeNMMsthatspecificallyneed\ntobesourcedfromgenuineregions,geographicalnounscanbeusedasspecialdescrip-\ntions. ForgenuineregionalChineseNMMs,thestandardEnglishnamesoftheprovin-\ncialcapitalsofthePeople’sRepublicofChinaaregenerallyused,whileinNMMSN-\nzh,theabbreviationoftheprovincialcapitalplus“产”(meaningproducedin)isused.\nEx. NMMSN:FritillariathunbergiiBulbZhejiang[22];NMMSN-zh: 浙产浙贝母鳞茎\nRule7.9: Processingmethodsnamingrules.\nRule7.9.1: Syntaxrule. PNMMmustincludeaprocessingmethod. PNMMisnamedbasedonits\ncorrespondingANMMbyaddinganEnglishadjective,adjectivephrase,appositive,or\nabbreviationrelatedtotheprocessingmethodinnameelementIV.Thecorresponding\nChinesewordfortheprocessingmethodshouldendwith“制”.\nEx. ANMM:NMMSN:ArtemisiaannualPart-aerial[23];NMMSN-zh: 黄花蒿地上部\nPNMM: NMMSN: Artemisia annual Part-aerial Segmented [24]; NMMSN-zh: 段制\n黄花蒿地上部\nRule7.9.2: Capitalization and hyphenation rule. For readability, the first letter of the processing"
  },
  {
    "id": 1503,
    "text": "PNMM: NMMSN: Artemisia annual Part-aerial Segmented [24]; NMMSN-zh: 段制\n黄花蒿地上部\nRule7.9.2: Capitalization and hyphenation rule. For readability, the first letter of the processing\nmethod should be capitalized. If the processing method consists of multiple words,\ntheyshouldbeconnectedusingahyphen“-”.\nRule7.9.3: Naming rule for NMM with multiple processing methods. For PNMMs that require\nmultipleprocessingmethods,theyshouldbeconnectedusing“and”,withthetermfor\nthelaterprocessingmethodplacedafter(InNMMSN-zh,processingmethods,because\nthey end in “制”, do not need logical connectors; the term for the later processing\nmethod is placed before). The sequence of processing methods is related to the pro-\ncessingorder,sotheorderofthewordscannotbechangedarbitrarily.\nEx. ✔ NMMSN: Ephedra sinica Stem-herbaceous Segmented and Aquafried-honey\n[25];NMMSN-zh: 蜜炙制段制草麻黄草质茎\n✘ EphedrasinicaStem-herbaceousAquafried-honeyandSegmented\nRule7.9.4: NamingruleforprocessingmethodsofChineseNMMs. Theprocessingmethodsfor"
  },
  {
    "id": 1504,
    "text": "[25];NMMSN-zh: 蜜炙制段制草麻黄草质茎\n✘ EphedrasinicaStem-herbaceousAquafried-honeyandSegmented\nRule7.9.4: NamingruleforprocessingmethodsofChineseNMMs. Theprocessingmethodsfor\nChinese NMMs are based on “0231 General Rules for Processing” in the “Chinese\nPharmacopoeia:2020Edition:VolumeIV”[26].Whennamingtheprocessingmethod,\nifaspecifictypeofprocessingcanbedetermined,itshouldbeused;ifnot,thegeneral\ncategorycanbeused.\nEx. NMMSN:CrataeguspinnatifidaFruitCleanedandStirfried-golden[27];NMMSN-zh:\n炒黄制净制山楂果实\nNMMSN:CrataeguspinnatifidaFruitCleanedandStirfried-charred[28];NMMSN-zh:\n炒焦制净制山楂果实\n31"
  },
  {
    "id": 1505,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nRule7.9.5: Classificationrulefornaturalmedicinalprocessing(炮制)methodsofChineseNMMs.\nTherearethreemaincategoriesofprocessingmethodsforChineseNMMs:processing\nbycleaning(净制),processingbycutting(切制),andprocessingbypreparing(备制).\nChinese NMMs that have undergone processing by cutting are assumed to have been\nprocessedbycleaning. Anythatneedsprocessingbypreparingmustfirstgothrough\nprocessingbycleaningorcutting.\nEx. ✔ NMMSN: Ephedra sinica Stem-herbaceous Segmented and Aquafried-honey;\nNMMSN-zh[25]: 蜜炙制段制草麻黄草质茎\n✘ EphedrasinicaStem-herbaceousAquafried-honey\nEx. ✔ NMMSN: Zingiber officinale Rhizome Cleaned and Stirfried-sand [29];\nNMMSN-zh: 砂炒制净制姜根茎\n✘ ZingiberofficinaleRhizomeStirfried-sand\nRule7.10: Naming rule for NMMs of non-species origin or with unclear species origin. If the NMM to be"
  },
  {
    "id": 1506,
    "text": "NMMSN-zh: 砂炒制净制姜根茎\n✘ ZingiberofficinaleRhizomeStirfried-sand\nRule7.10: Naming rule for NMMs of non-species origin or with unclear species origin. If the NMM to be\nnamedisnon-speciesoritsspeciesoriginishardtodetermine,itsNMMSNcanbenamedusingan\nEnglishcommonname.\nEx. NMMSN:Talc[30];NMMSN-zh: 滑石\nNMMSN:TalcPulverized[31];NMMSN-zh: 粉制滑石\nRule8: NMMGNRules.\nRule8.1: Concisenessrule. OwingtothedetailednamingrequirementsofNMMSN,whichincludespecify-\ningthespeciesorigin,medicinalpart,specialdescription,andprocessingmethod,NMMSNtends\ntobelengthy. Tofacilitateeverydayusageandclinicalprescription,eachNMMisalsoassigneda\ncorrespondingshorterNMMGN.\nRule8.2: Conventional naming rule for Chinese NMMGN-zh of Chinese NMMs. The NMMGN-zh of a\nChineseNMMprefersthecommonlyusedname. IftheNMMisfromtheChP,theChinesename\nlistedintheChPisgenerallytheNMMGN-zh. ForNMMsoriginatedfromarefinedsinglespecies,\ntopreventnamingconflicts, theprefix“单”/“独”(meaning“single”)isaddedtothebeginningof"
  },
  {
    "id": 1507,
    "text": "listedintheChPisgenerallytheNMMGN-zh. ForNMMsoriginatedfromarefinedsinglespecies,\ntopreventnamingconflicts, theprefix“单”/“独”(meaning“single”)isaddedtothebeginningof\nsomeNMMGN-zhtodifferentiatethem.\nRule8.3: PinyinnamingpriorityruleforNMMGNofChineseNMM.TocorrespondwithNMMGN-zh,we\nprefer the pinyin name of the NMMGN-zh when naming the NMMGN. The pinyin does not use\ntonemarksorspacesbutuseshyphenstoconnectthepinyinofdifferentcharacters,capitalizesthe\nfirstletter,anduses“v”insteadof“u¨”.\nEx. NMMSN:ErycibeobtusifoliaStem[32];NMMSN-zh:丁公藤茎;NMMGN:Dan-ding-gong-teng;\nNMMGN-zh: 单丁公藤(Todifferentiatewith“丁工藤”,addprefix“单”)\nNMMSN: Erycibe obtusifolia vel schmidtii Stem [33]; NMMSN-zh: 丁公藤或光叶丁公藤茎;\nNMMGN:Ding-gong-teng;NMMGN-zh: 丁公藤\nEx. NMMSN: Ligustrum lucidum Fruit [34]; NMMSN-zh: 女贞果实; NMMGN: Nv-zhen-zi;\nNMMGN-zh: 女贞子\nRule8.4: MinimumlengthruleforNMMGN-zh. NMMGN-zhmustbenamedusingtwoormorecharacters.\nEx. NMMSN:PrunusmumeFruit[35];NMMSN-zh: 梅果实\nNMMGN-zh: ✔ 乌梅 ✘ 梅"
  },
  {
    "id": 1508,
    "text": "NMMGN-zh: 女贞子\nRule8.4: MinimumlengthruleforNMMGN-zh. NMMGN-zhmustbenamedusingtwoormorecharacters.\nEx. NMMSN:PrunusmumeFruit[35];NMMSN-zh: 梅果实\nNMMGN-zh: ✔ 乌梅 ✘ 梅\nRule8.5: First-come-first-served rule. Due to the brevity of NMMGN, naming conflicts might arise. In\nsuchcases,weadheretothepriorityofthefirst-comeNMMGN,andsubsequentNMMGNsmust\nincorporateadditionalinformationfordifferentiation.\nEx. SupposewehavealreadynamedthefollowingNMM:\nNMMSN:EphedrasinicaStem-herbaceous[7];NMMSN-zh: 草麻黄草质茎;NMMGN:Cao-ma-\nhuang;NMMGN-zh: 草麻黄\nIn the aforementioned case, the NMMGN-zh of the NMM did not mention its medicinal part in-\nformation since it was prioritized, and thus omitted. But if we were to further name the root of\nEphedrasinica,itsNMMGN,andNMMGN-zhwouldneedappropriatedifferentiation:\nNMMSN: Ephedra sinica Root [36]; NMMSN-zh: 草麻黄根; NMMGN: Cao-ma-huang-gen;\nNMMGN-zh: 草麻黄根\nRule9: NMMIDrules. EachNMMisassignedauniqueNMMID.TheencodingruleforNMMIDis: NMM-XXXX"
  },
  {
    "id": 1509,
    "text": "NMMSN: Ephedra sinica Root [36]; NMMSN-zh: 草麻黄根; NMMGN: Cao-ma-huang-gen;\nNMMGN-zh: 草麻黄根\nRule9: NMMIDrules. EachNMMisassignedauniqueNMMID.TheencodingruleforNMMIDis: NMM-XXXX\n, where XXXX is a 4-digit number in base 36 (i.e., 0-9, A-Z), starting from 0001 and increasing to ZZZZ,\nencodingupto364−1 = 1,679,615kindsofNMMs. NMMIDiscase-insensitive,butforeaseofreading,\nitisusuallywritteninuppercase.\nEx. NMM-ID:NMM-0001[23]\nNMMSN:ArtemisiaannuaPart-aerial\n32"
  },
  {
    "id": 1510,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nNMMSN-zh: 黄花蒿地上部\nNMMGN:Qing-hao\nNMMGN-zh: 青蒿\nEx. NMM-ID:NMM-0002[24]\nNMMSN:ArtemisiaannuaPart-aerialSegmented\nNMMSN-zh: 段制黄花蒿地上部\nNMMGN:Qing-hao-duan\nNMMGN-zh: 青蒿段\nRule10: Standardreferencingrules.\nRule10.1: Firstcompleteappearancerule. ToensurethattheNMMnameinascientifictextaccuratelycor-\nresponds to the actual NMM it refers to, it is recommended to present the “NMMSN (NMM-ID,\nNMMGN)” format when an NMM first appears in an independent text (such as a paper or ency-\nclopedia). In subsequent appearances of the NMM within the text, only NMMSN, NMMGN, or\nNMMIDmaybeusedforreference.\nEx. ArtemisiaannuaPart-aerial(NMM-0001,Qing-hao)isacommonlyusedChinesenaturalmedicinal\nmaterial. ArtemisiaannuaPart-aerialhastheeffectoftreatingmalaria.\nEx. ArtemisiaannuaPart-aerial(NMM-0001,Qing-hao)isacommonlyusedChinesenaturalmedicinal"
  },
  {
    "id": 1511,
    "text": "material. ArtemisiaannuaPart-aerialhastheeffectoftreatingmalaria.\nEx. ArtemisiaannuaPart-aerial(NMM-0001,Qing-hao)isacommonlyusedChinesenaturalmedicinal\nmaterial. Qing-haohastheeffectoftreatingmalaria.\nEx. ArtemisiaannuaPart-aerial(NMM-0001,Qing-hao)isacommonlyusedChinesenaturalmedicinal\nmaterial. NMM-0001hastheeffectoftreatingmalaria.\nEx. 黄花蒿地上部（NMM-0001，青蒿）是一种常用的中药材。黄花蒿地上部可用于治疗疟\n疾。\nEx. 黄花蒿地上部（NMM-0001，青蒿）是一种常用的中药材。青蒿可用于治疗疟疾。\nEx. 黄花蒿地上部（NMM-0001，青蒿）是一种常用的中药材。NMM-0001可用于治疗疟疾。\nRule10.2: Appendixcompleteinformationrule. IfastudyinvolvesalargenumberofNMMs,presentingall\nNMMSN,NMMGN,andNMMIDinthemaintextcanmakeitexcessivelylengthy. Insuchcases,\nonlyoneoftheNMMSN,NMMGN,orNMMIDcanbeusedtorefertotheNMMinthemaintext;\nhowever,intheappendix,acompletelistintheformat“NMMID–NMMSN–NMMGN”should\nbeprovided.\n33"
  },
  {
    "id": 1512,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nD Supplementarytext: SystematicNomenclatureforNaturalMedicinalMaterials\n(SNNMM,天然药材系统命名法,Chineseversion)\n规则1：两名一ID规则。每个天然药材(Natural Medicinal Material, NMM) 均被被赋予一个天然药材系统\n名(NMMSystematicName,NMMSN)、一个天然药材通用名(NMMGenericName,NMMGN)和一个天\n然药材ID(NMMID)。\n规则2：唯一规则。在SNNMM下产生的NMMSN、NMMGN和NMMID均唯一。\n规则3：ANMM/PNMM不同名规则。SNNMM将NMMs分为3类，原始天然药材(Raw NMMs, RNMMs)、\n农产天然药材(Agricultural NMMs, ANMMs) 和炮制天然药材(Processed NMMs, PNMMs)\n(Fig.2b)。RNMMs一般按照农产品进行管理，PNMMs一般按照药品进行管理。\n规则4：合法字符规则。为便于世界上任何一个国家的人们都能够通过他们国家的标准键盘简单的输\n入NMMSN或NMMGN，NMMSN和NMMGN的命名只允许使用以下字符：大小写拉丁字母a-zA-Z、\n连词符“-”。\n规则5：大小写不敏感规则。在NMMSN或NMMGN中，尽管会有大小写字母同时使用的情况，但这仅仅是\n为了便于NMMSN和NMMGN的阅读的便利。SNNMM中，不得通过采用不同大小写的方式以区分\n不同的NMM。\n规则6：正体规则：NMMSN或NMMGN采用正体书写。\n规则7：NMMSN规则。\n规则7.1：NMMSN标准中文对应名规则。为便于中文语境下的交流，每个NMM在命名NMMSN的\n同时，SNNMM也会给出NMMSN的对应的标准中文对应名，即天然药材系统中文名\n（NMMSN-zh）。"
  },
  {
    "id": 1513,
    "text": "为了便于NMMSN和NMMGN的阅读的便利。SNNMM中，不得通过采用不同大小写的方式以区分\n不同的NMM。\n规则6：正体规则：NMMSN或NMMGN采用正体书写。\n规则7：NMMSN规则。\n规则7.1：NMMSN标准中文对应名规则。为便于中文语境下的交流，每个NMM在命名NMMSN的\n同时，SNNMM也会给出NMMSN的对应的标准中文对应名，即天然药材系统中文名\n（NMMSN-zh）。\n规则7.2：NMMSN构词规则。NMMSN由以下4种命名组件构成(Fig. 2a)：I. 物种基源(Species\norigin)、II. 药用部位(Medicinal part)、III. 特殊形容(Special description)、IV. 炮制方\n法(Processingmethod)。\n规则7.3：NMMSN语法分析规则。根据自然语言处理和计算语言学原理，NMMSN的语法分\n析(Parsing)结构如Fig.S4所示。\n规则7.4：NMMSN最 小 构 词 规 则 。ANMM的NMMSN至 少 包 含 以 下2种 命 名 组\n件：I和II；PNMM的NMMSN至少包含以下3种命名组件：I、II和IV(Fig.S4)。\n规则7.5：NMMSN语序规则。NMMSN命名组件的语序不可改变，语序为：I-II-III-IV；对\n于NMMSN-zh，命名组件为符合中文习惯，语序为：IV-III-I-II。\n规则7.6：物种基源命名规则。\n规则7.6.1：完整物种学名规则。命名时尽可能优先使用物种的完整的拉丁文物种学名。在\n可以明确具体物种时，不得仅采用物种的属名或种加词进行命名。\nEx. ✔ NMMSN:SolidagodecurrensHerb[1]; NMMSN-zh: 一枝黄花全草（物种\n基源：Solidagodecurrens）\n✘ SolidagoHerb\nEx. ✔ NMMSN:PanaxginsengRhizomeandRoot[2];NMMSN-zh: 人参根茎与根\n（物种基源：Panaxginseng）\n✘ GinsengRhizomeandRoot\n规则7.6.2：标准物种学名规则。NMMSN命名时优先使用现行的标准物种学名。标\n准物种学名优先参考以下数据库：“The Catalogue of Life”（https://www."
  },
  {
    "id": 1514,
    "text": "Ex. ✔ NMMSN:PanaxginsengRhizomeandRoot[2];NMMSN-zh: 人参根茎与根\n（物种基源：Panaxginseng）\n✘ GinsengRhizomeandRoot\n规则7.6.2：标准物种学名规则。NMMSN命名时优先使用现行的标准物种学名。标\n准物种学名优先参考以下数据库：“The Catalogue of Life”（https://www.\ncatalogueoflife.org）、“中国生物物种名录”（http://www.sp2000.org.\ncn）。\nEx. ✔ Tetradium ruticarpum ✘ Euodia ruticarpa ✘ Euodia ruticarpa var.\nofficinalis ✘ Euodiaruticarpavar. bodinieri\n在《中国药典·2020年版·一部》（下简称“ChP”）[3]中记载NMM“吴茱萸”为多\n物种基源NMM：基于Euodiaruticarpa或Euodiaruticarpavar. officinalis或Euodia\nruticarpavar. bodinieri。然而，这三个物种学名均非标准物种学名，其正名均\n为Tetradium ruticarpum。因此，“吴茱萸”实际仍然为单物种基源NMM。其正\n确NMMSN为：\nNMMSN:TetradiumruticarpumFruit[4];NMMSN-zh: 吴茱萸果实\n规则7.6.3：多物种基源命名规则。多种物种的相同部位用作一个NMM的药用部位，且\n这些物种互为可替代关系（即“或”关系）时，为了明确NMM的具体物种基\n源，NMMSN命名时要完整列出所有的物种基源。物种学名并列，中间采\n用“vel”（拉丁文连词，意为“或”）连接（NMMSN-zh中使用“或”连接），排\n序时以拉丁字母为顺序。当多物种中出现重复的属名时，属名第二次出现时可\n省略。\n34"
  },
  {
    "id": 1515,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nEx. 多物种基源来自相同属物种。ChP中，“麻黄”有3种物种基源：Ephedra\nsinica或Ephedraintermedia或Ephedraequisetina。因此应当命名为：\nEphedraequisetinavelEphedraintermediavelEphedrasinicaStem-herbaceous\n但由于属名一致，须进一步缩写为：\nNMMSN: Ephedra equisetina vel intermedia vel sinica Stem-herbaceous [5];\nNMMSN-zh: 木贼麻黄或中麻黄或草麻黄草质茎\nEx. 多物种基源来自不同属物种。如：\nCremastra appendiculata vel Pleione bulbocodioides vel Pleione yunnanensis Pseu-\ndobulb\n须进一步缩写后两种物种的属名：\nNMMSN:CremastraappendiculatavelPleionebulbocodioidesvelyunnanensisPseu-\ndobulb[6];NMMSN-zh: 杜鹃兰或独蒜兰或云南独蒜兰假鳞茎\n规则7.6.4：细化物种基源规则。当能够明确更细化的物种基源时，我们应当优先使用更细\n化的物种基源对NMM进行指代。\nEx. 我们可以对具有更细化的物种基源的“麻黄”采取以下命名：\nNMMSN:EphedrasinicaStem-herbaceous[7];NMMSN-zh: 草麻黄草质茎\nNMMSN:EphedraintermediaStem-herbaceous[8];NMMSN-zh中麻黄草质茎\nNMMSN:EphedraequisetinaStem-herbaceous[9];NMMSN-zh木贼麻黄草质茎\n规则7.6.5：物种包含规则。对于一个多物种基源NMM，若其物种基源的物种分类等级存\n在包含关系，则仅使用具有最高包含等级的物种进行命名。"
  },
  {
    "id": 1516,
    "text": "NMMSN:EphedraintermediaStem-herbaceous[8];NMMSN-zh中麻黄草质茎\nNMMSN:EphedraequisetinaStem-herbaceous[9];NMMSN-zh木贼麻黄草质茎\n规则7.6.5：物种包含规则。对于一个多物种基源NMM，若其物种基源的物种分类等级存\n在包含关系，则仅使用具有最高包含等级的物种进行命名。\nEx. ChP记载“山楂”的物种基源为Crataegus pinnatifida和Crataegus pinnatifida var.\nmajor。但由于Crataegus pinnatifida var. major是Crataegus pinnatifida的变种，\n后者对前者是包含关系，因此我们在命名时不应该包含此变种作为“山楂”的物\n种基源。\n✔ NMMSN:CrataeguspinnatifidaFruit[10];NMMSN-zh山楂果实\n✘ CrataeguspinnatifidavelpinnatifidavarmajorFruit\n具有更细化物种基源的NMM可以单列，以明确：\n✔ NMMSN:CrataeguspinnatifidavarmajorFruit[11];NMMSN-zh: 山里红果\n实\n规则7.6.6：具体物种信息不确定的NMM的命名规则。对于具体物种信息不确定的NMM，\n可以仅用属名进行命名，并在属名后加“unspecified”（NMMGN-zh中添加“未定\n种”）以提示物种信息尚未被明确。\nEx. 蒲公英属下有数千种物种，均可潜在用作NMM：\nNMMSN:TaraxacumunspecifiedHerb[12];NMMSN-zh: 蒲公英属未定种全草\n规则7.6.7：物种学名非合法字符省略规则。如果NMM的物种基源的物种学名含有非合法\n字符，则省略。\nEx. ✔ NMMSN:ZiziphusjujubavarspinosaSeed[13];NMMSN-zh酸枣种子\n✘ Ziziphusjujubavar. spinosaSeed\n规则7.7：药用部位命名规则。\n规则7.7.1：语法规则。NMMSN药用部位使用英文单数名词或名词短语进行命名。\nEx. ✔ NMMSN:PanaxginsengLeaf[14];NMMSN-zh: 人参叶"
  },
  {
    "id": 1517,
    "text": "Ex. ✔ NMMSN:ZiziphusjujubavarspinosaSeed[13];NMMSN-zh酸枣种子\n✘ Ziziphusjujubavar. spinosaSeed\n规则7.7：药用部位命名规则。\n规则7.7.1：语法规则。NMMSN药用部位使用英文单数名词或名词短语进行命名。\nEx. ✔ NMMSN:PanaxginsengLeaf[14];NMMSN-zh: 人参叶\n✘ PanaxginsengLeaves\n规则7.7.2：首字母大写和连词符规则。为便于阅读，药用部位的首字母应当大写；药用部\n位由多个词组成时，中间需要使用连词符“-”连接。\nEx. NMMSN:SolidagodecurrensHerb[1];NMMSN-zh: 一枝黄花全草\nEx. NMMSN:EphedrasinicaStem-herbaceous[7];NMMSN-zh: 草麻黄草质茎\n规则7.7.3：多药用部位命名规则。如果一个NMM同时使用同一物种的多种不同的药用部\n位入药，药用部位间可以使用“and”（“与”）连接。如果一个NMM可以使用同\n一物种的多种不同的药用部位入药，且互为可替代关系，则药用部位间可以使\n用“or”（“或”）连接。药用部位的排序根据拉丁字母顺序。\nEx. NMMSN:VincetoxicumpycnostelmaRhizomeandRoot[15];NMMSN-zh: 徐长卿\n根茎与根\n规则7.7.4：细化药用部位规则。当能够明确更细化的药用部位时，我们应当优先使用更细\n化的药用部位对NMM进行指代。该命名规则有助于我们进一步明确NMM的药\n用部位。\nEx. “VincetoxicumpycnostelmaRhizomeandRoot”可以进一步细化为：\nNMMSN:VincetoxicumpycnostelmaRhizome[16];NMMSN-zh: 徐长卿根茎\nNMMSN:VincetoxicumpycnostelmaRoot[17];NMMSN-zh: 徐长卿根\n规则7.8：特殊形容命名规则。\n35"
  },
  {
    "id": 1518,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n规则7.8.1：语法规则。一些NMM须具有某种特有的性状特征或经过某些特殊的产地初加\n工后方可入药，对于这些NMM，可以在命名组件III中构词部分使用英文形容\n词、形容词短语、名词或同位语进行命名。\n规则7.8.2：首字母大写和连词符规则。为便于阅读，特殊形容的首字母应当大写；特殊形\n容由多个词组成时，中间需要使用连词符“-”连接。\nEx. NMMSN:ZingiberofficinaleRhizome[18];NMMSN-zh: 姜根茎\nNMMSN:ZingiberofficinaleRhizomeFresh[19];NMMSN-zh: 鲜姜根茎\nEx. NMMSN:CurcumawenyujinRhizome[20];NMMSN-zh: 温郁金根茎\nNMMSN:CurcumawenyujinRhizomeFreshly-sliced[21]; NMMSN-zh: 鲜切片温\n郁金根茎\n规则7.8.3：道地药材规则。某些NMM需要特别明确道地产区时，可以使用地理名词作为\n特殊形容。中国产道地NMM的产地名通常使用中华人民共和国省会名称的标\n准英文名，NMMSN-zh中使用“省会标准缩写+产”作为其中文对应词。\nEx. NMMSN:FritillariathunbergiiBulbZhejiang[22];NMMSN-zh: 浙产浙贝母鳞茎\n规则7.9：炮制方法命名规则。\n规则7.9.1：语法规则。PNMM必须包含炮制方法。PNMM基于其对应的ANMM进行命\n名，通过在命名组件IV额外添加炮制方法所对应的英文形容词、形容词短\n语、同位语或炮制方法的英文缩写词（词组）进行命名。炮制方法的中文对应\n词必须以“制”结尾。\nEx. ANMM:NMMSN:ArtemisiaannualPart-aerial[23];NMMSN-zh: 黄花蒿地上部\nPNMM:NMMSN:ArtemisiaannualPart-aerialSegmented[24];NMMSN-zh: 段制\n黄花蒿地上部"
  },
  {
    "id": 1519,
    "text": "语、同位语或炮制方法的英文缩写词（词组）进行命名。炮制方法的中文对应\n词必须以“制”结尾。\nEx. ANMM:NMMSN:ArtemisiaannualPart-aerial[23];NMMSN-zh: 黄花蒿地上部\nPNMM:NMMSN:ArtemisiaannualPart-aerialSegmented[24];NMMSN-zh: 段制\n黄花蒿地上部\n规则7.9.2：首字母大写和连词符规则。为便于阅读，炮制方法的首字母应当大写；炮制方\n法由多个词组成时，中间需要使用连词符“-”连接。\n规则7.9.3：多重炮制的NMM命名规则。如果是多重炮制的PNMM，需要使用多个炮制方\n法，其炮制方法间使用“and”连接，居于更晚炮制过程的炮制方法的词序居后\n（NMMSN-zh的炮制方法由于有“制”作为词尾，因而无须使用逻辑连词；居于\n更晚炮制过程的炮制方法的词序居前）。炮制方法和炮制顺序相关，因此炮制\n方法的词序不可随意改变。\nEx. ✔ NMMSN: Ephedra sinica Stem-herbaceous Segmented and Aquafried-honey\n[25];NMMSN-zh: 蜜炙制段制草麻黄草质茎\n✘ EphedrasinicaStem-herbaceousAquafried-honeyandSegmented\n规则7.9.4：中国天然药材（中药材）炮制方法命名规则。中药材的炮制方法以《中国药\n典·2020年版·四部》“0231炮制通则”[26]为基础。炮制方法命名时，如果能明\n确炮制细类，则优先采用炮制细类进行命名；在不能明确炮制细类时，可以采\n用炮制大类进行命名。\nEx. NMMSN: Crataegus pinnatifida Fruit Cleaned and Stirfried-golden [27]; NMMSN-\nzh: 炒黄制净制山楂果实\nNMMSN: Crataegus pinnatifida Fruit Cleaned and Stirfried-charred [28]; NMMSN-\nzh: 炒焦制净制山楂果实\n规则7.9.5：中药材炮制分类规则。中药材炮制（natural medicinal processing）分3大类："
  },
  {
    "id": 1520,
    "text": "zh: 炒黄制净制山楂果实\nNMMSN: Crataegus pinnatifida Fruit Cleaned and Stirfried-charred [28]; NMMSN-\nzh: 炒焦制净制山楂果实\n规则7.9.5：中药材炮制分类规则。中药材炮制（natural medicinal processing）分3大类：\n净制(processing by cleaning)、切制(processing by cutting)、备制(processing by\npreparing)。经过切制的中药材默认已经经过净制。凡需要备制的，其必须\n首先经过净制或切制。\nEx. ✔ NMMSN: Ephedra sinica Stem-herbaceous Segmented and Aquafried-honey\n[25];NMMSN-zh: 蜜炙制段制草麻黄草质茎\n✘ EphedrasinicaStem-herbaceousAquafried-honey\nEx. ✔ NMMSN: Zingiber officinale Rhizome Cleaned and Stirfried-sand [29];\nNMMSN-zh: 砂炒制净制姜根茎\n✘ ZingiberofficinaleRhizomeStirfried-sand\n规则7.10：非物种类或物种基源难以确定的NMM的命名规则。若待命名NMM为非物种类或物种基源\n难以明确，其NMMSN可采用英语习称进行命名。\nEx. NMMSN:Talc[30];NMMSN-zh: 滑石\nNMMSN:TalcPulverized[31];NMMSN-zh: 粉制滑石\n规则8：NMMGN规则。\n规则8.1：简洁规则。NMMSN由于命名时需要明确NMM的物种基源、药用部位、特殊描述、炮\n制方法等信息，因此NMMSN通常较长。为了便于日常使用和临床处方时的便利，每\n个NMM也均有一个对应的较简短的NMMGN。\n36"
  },
  {
    "id": 1521,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n规则8.2：中药材NMMGN-zh的惯常规则。中药材的NMMGN-zh优先采用已经惯用的中药材名。\n如果中药材出自ChP，ChP收录的中药材的中文名通常即为NMMGN-zh。对于细化物\n种基源后的单物种基源中药材，为了防止命名冲突，个别中药材的NMMGN-zh开头添\n加“单”/“独”以区分。\n规则8.3：中药材NMMGN的拼音名优先规则。为了使得中药材的NMMGN和NMMGN-zh呼应，我们\n在命名中药材的NMMGN时，优先使用中药材NMMGN-zh的拼音名。拼音名不使用声调和\n空格，使用连词符“-”连接不同汉字的拼音，首字母大写，并使用“v”代替“u¨”。\nEx. NMMSN: Erycibe obtusifolia Stem [32]; NMMSN-zh: 丁公藤茎; NMMGN: Dan-ding-gong-\nteng;NMMGN-zh: 单丁公藤（为和“丁工藤”区分，添加“单”字。）\nNMMSN: Erycibe obtusifolia vel schmidtii Stem [33]; NMMSN-zh: 丁公藤或光叶丁公藤茎;\nNMMGN:Ding-gong-teng;NMMGN-zh: 丁公藤\nEx. NMMSN: Ligustrum lucidum Fruit [34]; NMMSN-zh: 女贞果实; NMMGN: Nv-zhen-zi;\nNMMGN-zh: 女贞子\n规则8.4：最短NMMGN-zh规则。NMMGN-zh必须使用2个及以上的汉字进行命名。\nEx. NMMSN:PrunusmumeFruit[35];NMMSN-zh: 梅果实\nNMMGN-zh: ✔ 乌梅 ✘ 梅\n规则8.5：先到先得规则。由于NMMGN较短，在其命名时，难免遇到命名冲突的情况。在这种情况\n下，我们需要遵循先到的NMMGN优先，对于后到的NMMGN，其须加一些额外信息以区\n分。\nEx. 假设我们已经对以下NMM进行命名："
  },
  {
    "id": 1522,
    "text": "Ex. NMMSN:PrunusmumeFruit[35];NMMSN-zh: 梅果实\nNMMGN-zh: ✔ 乌梅 ✘ 梅\n规则8.5：先到先得规则。由于NMMGN较短，在其命名时，难免遇到命名冲突的情况。在这种情况\n下，我们需要遵循先到的NMMGN优先，对于后到的NMMGN，其须加一些额外信息以区\n分。\nEx. 假设我们已经对以下NMM进行命名：\nNMMSN:EphedrasinicaStem-herbaceous[7];NMMSN-zh:草麻黄草质茎;NMMGN:Cao-ma-\nhuang;NMMGN-zh: 草麻黄\n在上述案例中该NMM的NMMGN-zh并未提及其药用部位信息，因为其是优先命名的，所\n以省略。但假设我们要进一步将Ephedrasinica的根用药，其NMMGN和NMMGN-zh就要适\n当做出区分：\nNMMSN: Ephedra sinica Root [36]; NMMSN-zh: 草麻黄根; NMMGN: Cao-ma-huang-gen;\nNMMGN-zh: 草麻黄根\n规则9：NMM ID规则。每种NMM被赋予唯一的NMM ID。NMM ID的编码规则为：NMM-XXXX，其中XXXX\n为4位36进制的数字（即0-9，A-Z），从0001开始递增，止于ZZZZ，至多可编码364 − 1 =\n1,679,615种NMM。NMMID大小写不敏感，但在书写时为了方便阅读通常使用全大写。\nEx. NMM-ID:NMM-0001[23]\nNMMSN:ArtemisiaannuaPart-aerial\nNMMSN-zh: 黄花蒿地上部\nNMMGN:Qing-hao\nNMMGN-zh: 青蒿\nEx. NMM-ID:NMM-0002[24]\nNMMSN:ArtemisiaannuaPart-aerialSegmented\nNMMSN-zh: 段制黄花蒿地上部\nNMMGN:Qing-hao-duan\nNMMGN-zh: 青蒿段\n规则10：标准指代规则。\n规则10.1：首次完整出现规则。为了保证科学文本中的NMM名称和其实际指代的NMM准确对应，推\n荐在每个独立文本（如一篇论文、百科等）中首次出现某NMM时，以“NMMSN (NMM-\nID, NMMGN)”格式给出NMM的两名一ID。在后续文本中NMM第二次出现时，允许只使"
  },
  {
    "id": 1523,
    "text": "NMMGN:Qing-hao-duan\nNMMGN-zh: 青蒿段\n规则10：标准指代规则。\n规则10.1：首次完整出现规则。为了保证科学文本中的NMM名称和其实际指代的NMM准确对应，推\n荐在每个独立文本（如一篇论文、百科等）中首次出现某NMM时，以“NMMSN (NMM-\nID, NMMGN)”格式给出NMM的两名一ID。在后续文本中NMM第二次出现时，允许只使\n用NMMSN、NMMGN或NMMID进行指代。\nEx. ArtemisiaannuaPart-aerial(NMM-0001,Qing-hao)isacommonlyusedChinesenaturalmedici-\nnalmaterial. ArtemisiaannuaPart-aerialhastheeffectoftreatingmalaria.\nEx. ArtemisiaannuaPart-aerial(NMM-0001,Qing-hao)isacommonlyusedChinesenaturalmedici-\nnalmaterial. Qing-haohastheeffectoftreatingmalaria.\nEx. ArtemisiaannuaPart-aerial(NMM-0001,Qing-hao)isacommonlyusedChinesenaturalmedici-\nnalmaterial. NMM-0001hastheeffectoftreatingmalaria.\nEx. 黄花蒿地上部（NMM-0001，青蒿）是一种常用的中药材。黄花蒿地上部可用于治疗疟\n疾。\nEx. 黄花蒿地上部（NMM-0001，青蒿）是一种常用的中药材。青蒿可用于治疗疟疾。\nEx. 黄花蒿地上部（NMM-0001，青蒿）是一种常用的中药材。NMM-0001可用于治疗疟疾。\n规则10.2：附录完整信息规则。如果某研究中涉及大量NMM，在正文文本中一一给出所有\n的NMMSN、NMMGN和NMMID可能导致正文文本过长。这种情况下，可以在正文中仅\n37"
  },
  {
    "id": 1524,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n使用NMMSN或NMMGN或NMMID中的一种来指代NMM；但在附录中，需要给出“NMM\nID–NMMSN–NMMGN”的完整列表。\n38"
  },
  {
    "id": 1525,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nE Supplementarymethod: SNNMMAlgorithm(SNNMMA)\nThe algorithm and code of SNNMMA has been open-sourced and is available on GitHub (https://github.\ncom/shennong-program/shennongname). TherelevantPythonpackagehasbeenpublishedandreleasedonPyPI\n(https://pypi.org/project/shennongname).\nThis supplementary method primarily elucidates the fundamental principles of the SNNMMA, emphasizing its data\ninputandoutputaspects. Fordetailedcodeimplementation,readersarereferredtotheaforementionedrepository.\nThe input to SNNMMA is a JSON Object. To elucidate, we illustrate with a common natural medicinal material,\nMi-ma-huang(蜜麻黄)(ListingS1).\n{\n1\n\"nmm_type\": \"processed\",\n2\n\"species_origins\": [[\"Ephedra sinica\", \"草麻黄\"], \"or\", [\"Ephedra intermedia\", \"中麻\n3\n(cid:44)→ 黄\"], \"or\", [\"Ephedra equisetina\", \"木贼麻黄\"]],\n\"medicinal_parts\": [[\"stem herbaceous\", \"草质茎\"]],\n4"
  },
  {
    "id": 1526,
    "text": "2\n\"species_origins\": [[\"Ephedra sinica\", \"草麻黄\"], \"or\", [\"Ephedra intermedia\", \"中麻\n3\n(cid:44)→ 黄\"], \"or\", [\"Ephedra equisetina\", \"木贼麻黄\"]],\n\"medicinal_parts\": [[\"stem herbaceous\", \"草质茎\"]],\n4\n\"special_descriptions\": [],\n5\n\"processing_methods\": [[\"segmented\", \"段制\"], \"and\", [\"aquafried honey\", \"蜜炙制\"]]\n6\n}\n7\nListingS1: ExampleofdatastructureforSNNMMAinput.\nWithintheaforementionedinputdatastructure,usersofSNNMMAarerequiredtoprovideinformationpertainingto\ntheNMMtypealongwiththeassociateddetailsforthefourkindsofnameelements. Itisnoteworthythatthesefour\ntypesofnameelementsarecollectivelystoredutilizingadatastructuredenominatedasNmmsnNeData.\nThebasicstructureisasfollows,exemplifiedbyspecies_origins(ListingS2):\n[\n1\n[\"Ephedra sinica\", \"草麻黄\"],\n2\n\"or\",\n3\n[\"Ephedra intermedia\", \"中麻黄\"],\n4\n\"or\",\n5\n[\"Ephedra equisetina\", \"木贼麻黄\"]\n6\n]\n7\nListingS2: ExampleofdatastructureforNmmsnNeDatainSNNMMA.\nThe list encompassed in NmmsnNeData permits the incorporation of multiple name element pairs, each with a data"
  },
  {
    "id": 1527,
    "text": "6\n]\n7\nListingS2: ExampleofdatastructureforNmmsnNeDatainSNNMMA.\nThe list encompassed in NmmsnNeData permits the incorporation of multiple name element pairs, each with a data\nsubstructure: [\"name element in English or Latin\", \"name element in Chinese\"],whichcanbeinter-\nconnectedbythelogicaloperatorstrings\"or\"or\"and\".\nSubsequently, whenNmmsnNeDataisconveyedtoSNNMMA,thealgorithmautonomouslyexecutesaseriesofpro-\ncesses for each name element type of NmmsnNeData. This includes string data verification, deduplication, sorting,\ncharacter transformation, and more. Ultimately, the algorithm calculates and derives the NMM Systematic Name\n(NMMSN)andNMMSystematicChineseName(NMMSN-zh),presentingtheresultsasanotherJSONObject(List-\ningS3).\n39"
  },
  {
    "id": 1528,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n{\n1\n\"success\": true,\n2\n\"error_msg\": \"Pipe: construct_nmmsn_spe_ori. Status: warning. Reason: Multiple\n3\n(cid:44)→ species origins detected.\",\n\"error_msg_en_zh\": {\n4\n\"en\": \"Multiple species origins detected.\",\n5\n\"zh\": \"检测到多个物种基源。\"\n6\n},\n7\n\"nmmsn\": {\n8\n\"nmmsn\": \"Ephedra equisetina vel intermedia vel sinica Stem-herbaceous Segmented\n9\n(cid:44)→ and Aquafried-honey\",\n\"nmmsn_zh\": {\n10\n\"zh\": \"蜜炙制段制木贼麻黄或中麻黄或草麻黄草质茎\",\n11\n\"pinyin\": \"m`ı zh`ı zh`ı du`an zh`ı m`u z´ei m´a hu´ang hu`o zh¯ong m´a hu´ang hu`o cˇao m´a\n12\n(cid:44)→ hu´ang cˇao zh`ı j¯ıng\"\n},\n13\n\"nmmsn_name_element\": {\n14\n\"nmm_type\": \"processed\",\n15\n\"species_origins\": [[\"Ephedra equisetina\", \"木贼麻黄\"], \"or\", [\"Ephedra\n16\n(cid:44)→ intermedia\", \"中麻黄\"], \"or\", [\"Ephedra sinica\", \"草麻黄\"]],\n\"medicinal_parts\": [[\"stem herbaceous\", \"草质茎\"]],\n17\n\"special_descriptions\": [],\n18"
  },
  {
    "id": 1529,
    "text": "16\n(cid:44)→ intermedia\", \"中麻黄\"], \"or\", [\"Ephedra sinica\", \"草麻黄\"]],\n\"medicinal_parts\": [[\"stem herbaceous\", \"草质茎\"]],\n17\n\"special_descriptions\": [],\n18\n\"processing_methods\": [[\"segmented\", \"段制\"], \"and\", [\"aquafried honey\", \"蜜\n19\n(cid:44)→\n炙制\"]]\n},\n20\n\"nmmsn_seq\": [[\"Ephedra equisetina vel intermedia vel sinica\", \"木贼麻黄或中麻黄或\n21\n(cid:44)→ 草麻黄\"], [\"Stem-herbaceous\", \"草质茎\"], [\"\", \"\"], [\"Segmented and\n(cid:44)→ Aquafried-honey\", \"蜜炙制段制\"]]\n}\n22\n}\n23\nListingS3: ExampleofdatastructureforSNNMMAoutput(NMMSNconstructionsuccessful).\nTheoutputoftheSNNMMAdisplaysthefollowingcharacteristics:\nOnceuserdataissuccessfullyprocessedbySNNMMAtoconstructanNMMSN,thevalueofsuccesswillbesetto\ntrue. Moreover,theresultantinformationpost-NMMSNconstructionbySNNMMAwillbestoredunderthenmmsn\nkey.\nThespecificmeaningsofeachhierarchicalkeyintheSNNMMAoutputaredescribedasfollows:\n• error_msg: In the SNNMM framework, there exist certain valid yet non-preferred rules. For instance, using"
  },
  {
    "id": 1530,
    "text": "key.\nThespecificmeaningsofeachhierarchicalkeyintheSNNMMAoutputaredescribedasfollows:\n• error_msg: In the SNNMM framework, there exist certain valid yet non-preferred rules. For instance, using\nmultiple species origins for systematic naming of NMM is not recommended. The SNNMMA can automatically\ndetect such anomalies during NMMSN construction. Consequently, there might be instances where the NMMSN\nis successfully constructed, yet the error_msg remains populated, recording any issues encountered during the\nprocess. Theseerrormessagesadheretoastandardizedformat: Pipe: xxx. Status: xxx. Reason: xxx.and\narestoredwithinerror_msg.\n• error_msg_en_zh: To enhance the user experience for both English and Chinese users, error messages in SNN-\nMMA have been localized. Information pertaining to the Reason in error_msg is processed and stored in both\nEnglish and Chinese within error_msg_en_zh.en and error_msg_en_zh.zh respectively. This ensures that"
  },
  {
    "id": 1531,
    "text": "English and Chinese within error_msg_en_zh.en and error_msg_en_zh.zh respectively. This ensures that\nevenuserswithprogrammingorlanguagebarrierscanclearlyunderstandanyissuesencounteredduringNMMSN\nconstructionbySNNMMA.\n• nmmsn: ThiskeyhousesallinformationdirectlyrelatedtoNMMSN.\n• nmmsn.nmmsn: RepresentsthesuccessfullyconstructedNMMSystematicName.\n• nmmsn.nmmsn_zh: nmmsn.nmmsn_zh.zh denotes the successfully constructed NMM Systematic Chinese Name,\nwhilenmmsn.nmmsn_zh.pinyinrepresentsthecorrespondingpinyin.\n40"
  },
  {
    "id": 1532,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n• nmmsn.nmmsn_name_element: The data structure of this key mirrors the structure of input data (Listing S1).\nHowever, the order of elements within the NmmsnNeData data structure might be adjusted or reordered based\non the SNNMM rules. For example, the order in species_origins might change from [[\"Ephedra\nsinica\", \"草麻黄\"], \"or\", [\"Ephedra intermedia\", \"中麻黄\"], \"or\", [\"Ephedra equisetina\",\n\"木贼麻黄\"]] to [[\"Ephedra equisetina\", \"木贼麻黄\"], \"or\", [\"Ephedra intermedia\", \"中麻\n黄\"], \"or\", [\"Ephedra sinica\", \"草麻黄\"]]duetothealphabeticalorderingbeinge -> i -> samong\nthethreespeciesorigins.\n• nmmsn.nmmsn_seq:GiventhatNMMSNcomprisesfournameelements,thiskeystorestheNMMSNcorresponding\ntoeachnameelement.ThissequencedNMMSN,withinShennongName,canbeutilizedtodistinctivelydisplayeach\nnameelementinauniquecolor,enhancinguser-friendliness."
  },
  {
    "id": 1533,
    "text": "toeachnameelement.ThissequencedNMMSN,withinShennongName,canbeutilizedtodistinctivelydisplayeach\nnameelementinauniquecolor,enhancinguser-friendliness.\nIfSNNMMAencountersissuesduringtheNMMSNconstructionprocessandfails,theoutputfromSNNMMAwillbe\nstrikinglysimilartothatshowninListingS3. However,thevalueofsuccesswillbesettofalse,andtheresulting\nJSONObjectwillnotincludethenmmsnkeyanditscorrespondingvalue(ListingS4).\n{\n1\n\"success\": false,\n2\n\"error_msg\": \"...\",\n3\n\"error_msg_en_zh\": {\n4\n\"en\": \"...\",\n5\n\"zh\": \"...\"\n6\n}\n7\n}\n8\nListingS4: ExampleofdatastructureforSNNMMAoutput(NMMSNconstructionfailed).\n41"
  },
  {
    "id": 1534,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nF Supplementarymethod: MultilingualMarkdown(MLMD)\nThe source code for the MLMD parser has been open-sourced and can be accessed on GitHub: https://github.\ncom/shennong-program/mlmd.\nTheassociatedTypeScriptpackagehasbeenreleasedonnpm: https://www.npmjs.com/package/mlmd\nThis Supplementary method primarily introduces the core design principles, essential syntax, and usage methods of\nMLMD.PleaserefertotherepositoryaboveforthedetailedsyntaxandparsingcodeofMLMD.\nF.1 Coredesignprinciples\nMultilingualMarkdown(MLMD)isanewlydesignedlightweightmarkuplanguageexplicitlytailoredformanaging\nmultilingualtext. ThesyntaxofMLMDintegratesthestrengthsofMarkdown[37]whilebeingspeciallydesignedfor\nmultilingualparallelcorpora.\nThe primary objective of MLMD is to allow users to write and manage multilingual parallel text in an easy-to-read"
  },
  {
    "id": 1535,
    "text": "multilingualparallelcorpora.\nThe primary objective of MLMD is to allow users to write and manage multilingual parallel text in an easy-to-read\nandeasy-to-writeplaintextformat.\nThankstotheMLMDsyntaxthatcanmanagemultilingualcontentinasingledocument,itcanbewidelyappliedin\nthefollowingscenarios:\n• Multilingualtextwriting,contentcreation\n• Unified/Structuredmanagement,storageofmultilingualtexts\n• Multilingualdisplayofmultilingualtexts\n• Cross-languagetextannotation,entityannotation\n• Cross-languagetranslation,proofreading\n• Cross-languagemachinetranslation\n• Textanalysis/miningofmultilingualtexts\n• Naturallanguageprocessingofmultilingualtexts\n• ...\nF.2 Writing,storage,andfileextension\nThecontentofMLMDisinplaintext,makingitpossibletocomposeusinganyplaintexteditor.\nWhen saving MLMD content as an independent file, the .mlmd extension should be used. For instance, if we save\nMLMD text in a file named abc.mlmd, then abc represents the filename and .mlmd is the standard extension for\nMLMD."
  },
  {
    "id": 1536,
    "text": "MLMD text in a file named abc.mlmd, then abc represents the filename and .mlmd is the standard extension for\nMLMD.\nSince MLMD text is plain text, an independent MLMD text can technically be embedded/stored as a single string\nwithinotherfileformatsordatabasessuchasJSON,SQL,MongoDB,etc.\nForexample,considerthefollowingMLMDtext(ListingS5):\n{{langs|zh|en}}\n1\n2\n你好，世界！\n3\nHello, world!\n4\nListingS5: MLMDexample: Hello,World!\nItcanbestoredasasinglestringinaJSONstructure(ListingS6):\n42"
  },
  {
    "id": 1537,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n{\n1\n\"mlmd_str\": \"{{langs|zh|en}}\\n\\n你好，世界！\\nHello, world!\"\n2\n}\n3\nListingS6: MLMDstoredasastringinsideaJSONstructure.\nF.3 Essentialsyntax\nF.3.1 MLMDlanguageheader\nSince MLMD can be used to store parallel corpora of any number of languages, it is required to explicitly de-\nfine the languages of the text stored in MLMD in the first line of every MLMD file, using the format {{langs|<\nlanguage_code_1>|<language_code_2>|...}}. FollowingthesyntaxofMLMD,MLMDdocumentscanman-\nage parallel corpora of any number of languages. Thus, non-repeating language codes can be used in the MLMD\nlanguageheader.\nIt is noteworthy that the first language code in the MLMD language header corresponds to the primary language of\nthatparticularMLMDdocument.\nForinstance,thelanguageheaderofabilingualMLMDdocumentinChineseandEnglishisasfollows(ListingS7):\n{{langs|zh|en}}\n1"
  },
  {
    "id": 1538,
    "text": "thatparticularMLMDdocument.\nForinstance,thelanguageheaderofabilingualMLMDdocumentinChineseandEnglishisasfollows(ListingS7):\n{{langs|zh|en}}\n1\nListingS7: MLMDlanguageheaderexample: Chinese-English.\nWhereas the language header of a trilingual MLMD document in Chinese, English, and Latin is as follows (List-\ningS8):\n{{langs|zh|en|la}}\n1\nListingS8: MLMDlanguageheaderexample: Chinese-English-Latin.\nF.3.2 Multilingualparallelparagraphs\nInMLMD,multilingualparallelcorporaaremanagedattheparagraphlevel. Eachmultilingualparagraphistreatedas\naBlockwithinMLMD.WithineachBlock,theparagraphsoftextindifferentlanguagesarearrangedintheorderof\nlanguagecodesandareseparatedbyalinebreak(whenstoredasstrings,thenewlinecharacter\\nisused). Different\nBlocks are separated by a blank line (when stored as strings, two newline characters \\n\\n are used). For examples\n(ListingS9andS10):\n{{langs|zh|en}}\n1\n2\n这是第1段中文。\n3\nThis is the 1st paragraph in English.\n4\n5\n这是第2段中文。\n6\nThis is the 2nd paragraph in English.\n7"
  },
  {
    "id": 1539,
    "text": "(ListingS9andS10):\n{{langs|zh|en}}\n1\n2\n这是第1段中文。\n3\nThis is the 1st paragraph in English.\n4\n5\n这是第2段中文。\n6\nThis is the 2nd paragraph in English.\n7\nListingS9: MLMDmultilingualparallelparagraphsexample: Chinese-English.\n43"
  },
  {
    "id": 1540,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n{{langs|zh|en|la}}\n1\n2\n这是第1段中文。\n3\nThis is the 1st paragraph in English.\n4\nHic est paragraphus Latinus primus.\n5\n6\n这是第2段中文。\n7\nThis is the 2nd paragraph in English.\n8\nHic est paragraphus Latinus secundus.\n9\nListingS10: MLMDmultilingualparallelparagraphsexample: Chinese-English-Latin.\nIncidentally,theconceptofaBlockisoneofthecoresofMLMD,aswhenwewishtostoredataofdifferentstructures\n(such as links, images, etc.) within MLMD, they are essentially treated as individual Blocks. Blocks’ management\napproach of Blocks allows MLMD to be conveniently parsed. The Blocks corresponding to multilingual parallel\nparagraphsareabbreviatedas“Multi”.\nF.3.3 Language-invariantparagraphs\nInmultilingualtexts,certaintextparagraphsremainthesameacrossalllanguages. Werefertosuchtextaslanguage-"
  },
  {
    "id": 1541,
    "text": "paragraphsareabbreviatedas“Multi”.\nF.3.3 Language-invariantparagraphs\nInmultilingualtexts,certaintextparagraphsremainthesameacrossalllanguages. Werefertosuchtextaslanguage-\ninvariantparagraphs. WhenaBlockinoursavedMLMDdocumentcontainsonlyoneparagraphoftext,itisautomat-\nicallyregardedasalanguage-invarianttext. ThistypeofBlockisabbreviatedas“Mono”. Forexample(ListingS11):\n{{langs|zh|en}}\n1\n2\n这是第1个Block (Multi)中的中文段落。\n3\nThis is an English paragraph in the 1st Block (Multi).\n4\n5\nThis is a language-invariant paragraph in English in the 2nd Block (Mono).\n6\n7\n这是第3个Block (Mono)中的一段中文的跨语言不变段落。\n8\n9\n这是第4个Block (Multi)中的中文段落。\n10\nThis is an English paragraph in the 4th Block (Multi).\n11\nListingS11: MLMDlanguage-invariantparagraphsexample.\nF.3.4 Emphasis\nIn MLMD, text can be emphasized by wrapping with ** for bold emphasis and * for italic emphasis. For example\n(ListingS12):\n{{langs|zh|en}}\n1\n2\n第1个Block (Multi)中的中文**加粗**段落。\n3\nA **bold** paragraph in English within the 1st Block (Multi).\n4\n5"
  },
  {
    "id": 1542,
    "text": "(ListingS12):\n{{langs|zh|en}}\n1\n2\n第1个Block (Multi)中的中文**加粗**段落。\n3\nA **bold** paragraph in English within the 1st Block (Multi).\n4\n5\nA **bold** and *italic* language-invariant paragraph in English in the 2nd Block (Mono).\n6\n7\n第3个Block (Mono)中的一段中文的**加粗**和*斜体*的跨语言不变段落。\n8\n9\n第4个Block (Multi)中的中文*斜体*段落。\n10\nAn *italic* paragraph in English within the 4th Block (Multi).\n11\nListingS12: MLMDemphasisexample.\n44"
  },
  {
    "id": 1543,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nF.3.5 Headings\nMLMD supports up to six levels of headings. Headings are denoted using the # symbol. By prefixing a paragraph\nwith1-6ofthe#symbols,youcandesignatetheheadinglevels1-6,respectively. Toseparatemultilingualheadings,\nusethe|symbol. Ifthereisno|,theheadingcontentistreatedasMono. Forexample(ListingS13):\n{{langs|zh|en}}\n1\n2\n# 一级标题 | Heading Level-1\n3\n4\n## 二级标题 | Heading Level-2\n5\n6\n### 三级标题 | Heading Level-3\n7\n8\n#### 四级标题 | Heading Level-4\n9\n10\n##### 五级标题 | Heading Level-5\n11\n12\n###### 六级标题 | Heading Level-6\n13\n14\n# 一级标题\n15\n16\n## Heading Level-2\n17\nListingS13: MLMDheadingsexample.\nF.3.6 Coreferenceannotation\nThecentrallinguisticprincipleofthissyntaxis“coreferenceconsistency”.Itrecognizesthatdifferenttermsorphrases\ncan refer to the same inherent concept or entity in multilingual or monolingual texts. This capacity to discern that,"
  },
  {
    "id": 1544,
    "text": "can refer to the same inherent concept or entity in multilingual or monolingual texts. This capacity to discern that,\ndespitevariationsinform,theessenceofthereferenceremainsunchangedunderpinsthisprinciple. InMLMD,terms\nwiththissharedreferentialqualityaredescribedashaving“coreferenceconsistency”.\nConsiderthisillustrativeexample(ListingS14):\n{{langs|zh|en}}\n1\n2\n神农被中国人认为是医药学的始祖。\n3\nShennong is considered the progenitor of medicine and pharmacy by the Chinese.\n4\n5\n炎帝被中国人认为是医药学的始祖。\n6\nYan Emperor is considered the progenitor of medicine and pharmacy by the Chinese.\n7\nListingS14: MLMDcoreferenceconsistencyexample(notannotated).\nTo those familiar with the historical background, irrespective of the terms “神农”, “炎帝”, “Shennong”, or “Yan\nEmperor” being used, they understand these allude to the same historical figure, specifically “神农” (Shennong).\nHowever,thesecouldappearasfourseparateconceptsforthosewithoutthisknowledge. Toconveythistextualcoref-"
  },
  {
    "id": 1545,
    "text": "However,thesecouldappearasfourseparateconceptsforthosewithoutthisknowledge. Toconveythistextualcoref-\nerenceconsistencywithinthecontent,MLMDintroducesadedicatedcoreferenceannotationsyntax. Thisfacilitates\neasier textual annotations of coreferential relationships and enhances machine translation and AI natural language\nprocessingcapabilitiesinunderstandingcoreferenceconsistency. Thespecificsyntaxisasfollows:\nThedoublebracketnotation[[...]]isutilizedforcoreferenceannotation. Nospacesmustexistbetween[[and]].\nTheprimarycoreferenceannotationsareoftwotypes:\n1. Whentheterm’sexpressionalignswithitsreference: [[term's reference]]\n45"
  },
  {
    "id": 1546,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n2. When there’s a divergence between the term’s expression and its reference: [[term's reference|term's\nexpression]]\nByemployingthisapproach,thesentencesmentionedabovecanbeannotatedas(ListingS15):\n{{langs|zh|en}}\n1\n2\n[[神农]]被中国人认为是医药学的始祖。\n3\n[[神农|Shennong]] is considered the progenitor of medicine and pharmacy by the Chinese.\n4\n5\n[[神农|炎帝]]被中国人认为是医药学的始祖。\n6\n[[神农|Yan Emperor]] is considered the progenitor of medicine and pharmacy by the\n7\n(cid:44)→ Chinese.\nListingS15: MLMDcoreferenceconsistencyexample(annotated).\nIn this framework, “神农” stands as the Primary Term denoting the concept, while “炎帝,” “Shennong,” and “Yan\nEmperor”areitscoreferentialterms. Hence,withtheabovecoreferenceannotations,eventhosewithoutthespecific\nhistoricalcontextcaninferthatthesesentencesallpointtothesameperson,specifically“神农”.\nF.3.7 Entityannotation"
  },
  {
    "id": 1547,
    "text": "historicalcontextcaninferthatthesesentencesallpointtothesameperson,specifically“神农”.\nF.3.7 Entityannotation\nIn natural language texts, certain terms often represent specific entities. We can achieve a unified annotation by\nutilizing the coreference annotation syntax to corefer a term to its corresponding entity. In practice, terms can be\ncoreferredtotheircorrespondingentityID.\nFor instance, both “青蒿” and “Qing-hao” correspond to the NMM ID “NMM-0001”. Hence, we can annotate the\nentityasfollows(ListingS16):\n{{langs|zh|en}}\n1\n2\n[[NMM-0001|青蒿]]是一种天然药材。\n3\n[[NMM-0001|Qing-hao]] is a kind of Natural Medicinal Material.\n4\nListingS16: MLMDentityannotationexample.\nF.3.8 Comments\nMLMDuses<!-- ... -->forcomments. Forexample(ListingS17):\n46"
  },
  {
    "id": 1548,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n{{langs|zh|en}}\n1\n2\n第1个Block (Multi)中的中文**加粗**段落。 <!-- 单行注释 -->\n3\nA **bold** paragraph in English within the 1st Block (Multi). <!-- Single-line comment\n4\n(cid:44)→ -->\n5\nA **bold** and *italic* language-invariant paragraph in English in the 2nd Block (Mono).\n6\n(cid:44)→ <!-- Single-line comment -->\n7\n<!-- 多行注释\n8\nMulti-line comment\n9\n-->\n10\n11\n第3个Block (Mono)中的一段中文的**加粗**和*斜体*的跨语言不变段落。 <!-- 单行注释 -->\n12\n13\n第4个Block (Multi)中的中文*斜体*段落。\n14\nAn *italic* paragraph in English within the 4th Block (Multi).\n15\nListingS17: MLMDcommentexample.\nF.3.9 Templates\nFor texts with specific functionalities, MLMD employs the template syntax {{...}} for annotation. No spaces are\nallowedbetweenthedoublecurlybrackets{{and}}.Forinstance,theMLMDlanguageheaderisatypeofspecialized\ntemplate."
  },
  {
    "id": 1549,
    "text": "allowedbetweenthedoublecurlybrackets{{and}}.Forinstance,theMLMDlanguageheaderisatypeofspecialized\ntemplate.\nMLMDcanbeextendedtocatertovariousuniquetemplatefunctionalrequirements. Detailedtemplatesyntaxcanbe\nfoundintheMLMDrepository. Here,weemphasizethecitationtemplatedirectlyrelatedtothispaper.\nF.3.10 Citations\nUsers can conveniently add citations to the MLMD text using the citation template: {{ref|@<ref_id>]}}. This\nmakesMLMDsuitableforrigorousacademictexts. Fordetailedinformationrelatedtotheref_id,onecanstoreit\nin BibTeX format using {{ref-citation|bibtex|<ref_citation_info>}}. The ref_id must match with the\nref_idintheBibTeXwithinref_citation_info. Forexample(ListingS18):\n{{langs|zh|en}}\n1\n2\nAI介导的五阶段科学革命，这一理论在《AI成为主脑科学家》这篇论文中被首次提\n3\n(cid:44)→\n出{{ref|@ai_masterbrain}}。\nThe theory of the \"Five Stages of AI-involved Scientific Revolution\" was first introduced\n4\n(cid:44)→ in the paper titled \"AI Becomes a Masterbrain Scientist\" {{ref|@ai_masterbrain}}.\n5\n{{ref-citation|bibtex|\n6"
  },
  {
    "id": 1550,
    "text": "4\n(cid:44)→ in the paper titled \"AI Becomes a Masterbrain Scientist\" {{ref|@ai_masterbrain}}.\n5\n{{ref-citation|bibtex|\n6\n@article{ai_masterbrain,\n7\ntitle={AI becomes a masterbrain scientist},\n8\nauthor={YANG, Zijie and WANG, Yukai and ZHANG, Lijing},\n9\njournal={bioRxiv},\n10\npages={2023--04},\n11\nyear={2023},\n12\npublisher={Cold Spring Harbor Laboratory}\n13\n}\n14\n}}\n15\nListingS18: MLMDcitationexample.\n47"
  },
  {
    "id": 1551,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nOn the ShennongAlpha website, all references are stored in the ShennongKB reference collection. Each reference\nis assigned a globally unique ID in the format sna-ref-xxx. Hence, in ShennongAlpha, one can employ a special\nreferenceannotationlike{{ref|[[sna-ref-1]]}}. Here,[[sna-ref-1]]servesasthe@<ref_id>. Sincesna-\nref-1isencapsulatedwithin[[]],itsignifiesthat[[sna-ref-1]]inherentlyisacoreferencerelatedtothecitation.\nTherefore, ShennongAlpha parses MLMD automatically fetches the reference information about sna-ref-1 from\nShennongKB. In this context, there’s no further need to supplement the specific reference details through {{ref-\ncitation|bibtex}}.\nF.4 MachineTranslationCompatibility\nMLMDnativelysupportsanovelmachinetranslationapproachintroducedinthispaper: NeuralMachineTranslation\nbasedonCoreferencePrimaryTerm(NMT-CPT)."
  },
  {
    "id": 1552,
    "text": "citation|bibtex}}.\nF.4 MachineTranslationCompatibility\nMLMDnativelysupportsanovelmachinetranslationapproachintroducedinthispaper: NeuralMachineTranslation\nbasedonCoreferencePrimaryTerm(NMT-CPT).\nUsingcoreferenceannotationtodenotetherelationshipsbetweenstandardPrimaryTermsandtheirtranslations,NMT-\nCPToffersusersastreamlinedandinteractivepresentationoftranslationresultsthroughtheShennongTranslateuser\ninterface. Thisapproachsignificantlyenhancestheinterpretabilityofmachinetranslations.\nF.5 HTMLParsing\nThroughaparsingflow: MLMD→AbstractSyntaxTree(AST)[38]→HTML,MLMDcanberenderedintoHTML\nwithrichtextformatting,enhancingtheuserexperiencewhilereadingitscontent,ShennongAlphanativelysupports\nHTMLrenderingofMLMD,offeringthebilingualcontentpresentationinfourmodes: “Chinese-English”,“English-\nChinese,”“Chinese”,and“English”(Fig.S6).\n48"
  },
  {
    "id": 1553,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nG Supplementarymethod: Coreference-basedGraphSearch(CGS)\nG.1 FoundationalCGS\nToelucidatetheprocessofCGS,weillustrateitwithastraightforwardcase:\nConsiderthesetofnodes:\nA,B,C,D,E,F\nThecoreferencerelationshipsaregivenasfollows:\nA→B\nB →C\nD →B\nE →F\nHere,C,F aremanuallydesignatedasPrimaryTerms.\nThe“CoreferencePrimaryTermGraph(CPTG)”canbeconstructedfromtheabovenodesanddirectededges. This\nCPTGmustsatisfythefollowingconditions:\n1. ItshouldbeaDirectedAcyclicGraph(DAG).\n2. Theout-degreeofanyPrimaryTerminthegraphshouldbezero.\n3. Theout-degreeofanynodeshouldbelessthanorequaltoone,indicatingthatnobranchingnodesarepresent.\nThegraphconstructedfromthegivenexampleisillustratedbelow:\nA\nB C\nD\nE F\nOnce a CPTG is constructed, preliminary graph computations can be conducted to optimize queries to identify the\nultimatePrimaryTermcorrespondingtoanytermnode."
  },
  {
    "id": 1554,
    "text": "A\nB C\nD\nE F\nOnce a CPTG is constructed, preliminary graph computations can be conducted to optimize queries to identify the\nultimatePrimaryTermcorrespondingtoanytermnode.\nTheresultsofthesecomputationsarestoredinanewlycreateddictionary.Notably,withinthisdictionary,relationships\nofdirectededgesfromaPrimaryTermtoitself(e.g.,C →C)arealsoincluded. Thisfeatureallowsthequeriedterm\ntoberecognizedasaPrimaryTerm.\nA→C\nB →C\nD →C\nE →F\nC →C\nF →F\nThus,bysearchingthisdictionary,itcanbeswiftlydeterminedwhetheracorrespondingPrimaryTermexistsforany\ngiventerm,includingPrimaryTerms,byverifyingitspresenceinthedictionary. Iffound,thevaluecorrespondingto\nthistermwillbeitsassociatedPrimaryTerm.\n49"
  },
  {
    "id": 1555,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nThepseudocodefortheFoundationalCGSAlgorithmisasfollows:\nAlgorithm1FoundationalCGSAlgorithm\n1: Input: SetofnodesN,CoreferencerelationshipsR\n2: Output: DictionaryDstoringtheultimatePrimaryTerms\n3: InitializethedictionaryD.\n4: ConstructtheCoreferencePrimaryTermGraph(CPTG)GfromthegivenN andR.\n5: foreachnodeninGdo\n6: ifnisnotaPrimaryTermthen\n7: FindtheultimatePrimaryTermpforn.\n8: ifpexiststhen\n9: Addthemappingn→ptoD.\n10: endif\n11: else\n12: Addtheself-referencingmappingn→ntoD.\n13: endif\n14: endfor\n15: Forsearching:\n16: Givenanytermt,queryDtoascertainwhetheracorrespondingPrimaryTermexists.\n17: iftexistsinDthen\n18: RetrievethecorrespondingPrimaryTermpfromD.\n19: endif\nG.2 WeightedCGS\nDuring the construction of the CPTG, directed edges are typically extracted automatically from databases. Conse-"
  },
  {
    "id": 1556,
    "text": "18: RetrievethecorrespondingPrimaryTermpfromD.\n19: endif\nG.2 WeightedCGS\nDuring the construction of the CPTG, directed edges are typically extracted automatically from databases. Conse-\nquently,weoftencannotsatisfythethirdrequirementofCPTG(i.e.,nobranchingnodes). Wecanassignaweightto\neachdirectededgebasedontheFoundationalCGSandconductaweightedgraphsearchinsuchinstances.\nLet’sillustratethiswithasimpleexample:\nConsiderthesetofnodes:\nA,B,C,D,E\nThecoreferencerelationshipsaregivenasfollows:\nA− w −A−B− = −→ 1 B\nB − w −B−C− = −→ 2 C\nD − w −A−B− = −→ 1 B\nB − w −A−B− = −→ 1 E\nHere, C and E are manually designated as Primary Terms. Each coreference relationship is assigned a weight to\nsignifytheimportanceofthatrelationship.\nACPTGcanbeconstructedfromtheabovenodesanddirectededges. ThisCPTGmeetsthefirsttworequirementsof\naCPTGinFoundationalCGSbutdoesnotsatisfythethirdrequirement.\nThegraphconstructedfromthegivenexamplewouldbeasfollows:\nA C\n1 2\nB\n1 1\nD E\n50"
  },
  {
    "id": 1557,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nInthisCPTG,weobserveananomalousbranchingnodeB,whichpointstobothC andEsimultaneously. Therefore,\nunderthesecircumstances,whenconductingpreliminarygraphcomputationstosearchfortheultimatePrimaryTerm\nofallnodes,weproceedasfollows: Foreachgivennode,weacquireallitsdownstreamdirectededges. Byevaluating\ntheweightsoftheseedges,weselectthedownstreamnodecorrespondingtotheedgewiththehighestweightasthe\nnextnode. WerepeatthisprocessuntilanyPrimaryTermisreached.\nForinstance,whensearchingfortheultimatePrimaryTermcorrespondingtonodeA,wenavigatefromAtoitssole\ndownstreamnodeB. However, B actuallyhastwodownstreamnodes, C andE. Atthisjuncture, sincetheweight\nofB → C (2)isgreaterthanthatofB → E (1), wefurtherselectC asB’sdownstreamnode. AsC isalreadya\nPrimaryTerm,weconcludeourgraphsearch,determiningthattheultimatePrimaryTermforAisC,withthegraph"
  },
  {
    "id": 1558,
    "text": "ofB → C (2)isgreaterthanthatofB → E (1), wefurtherselectC asB’sdownstreamnode. AsC isalreadya\nPrimaryTerm,weconcludeourgraphsearch,determiningthattheultimatePrimaryTermforAisC,withthegraph\nsearchpathbeingA→B →C,ratherthanA→B →E.\nThroughthismethod,wecanstillacquireadictionaryconsistentwiththeFoundationalCGS,asillustratedbelow:\nA→C\nB →C\nD →C\nC →C\nE →E\nThepseudocodefortheWeightedCGSAlgorithmisasfollows:\nAlgorithm2WeightedCGSAlgorithm\n1: Input: SetofnodesN,CoreferencerelationshipsR,WeightsW ofdirectededges\n2: Output: DictionaryDstoringtheultimatePrimaryTerms\n3: InitializethedictionaryD.\n4: ConstructtheCoreferencePrimaryTermGraph(CPTG)GfromthegivenN,R,andW.\n5: foreachnodeninGdo\n6: ifnisnotaPrimaryTermthen\n7: FindtheultimatePrimaryTermpfornusingweightedgraphsearch.\n8: ifpexiststhen\n9: Addthemappingn→ptoD.\n10: endif\n11: else\n12: Addtheself-referencingmappingn→ntoD.\n13: endif\n14: endfor\n15: Forsearching:\n16: Givenanytermt,queryDtoascertainwhetheracorrespondingPrimaryTermexists."
  },
  {
    "id": 1559,
    "text": "9: Addthemappingn→ptoD.\n10: endif\n11: else\n12: Addtheself-referencingmappingn→ntoD.\n13: endif\n14: endfor\n15: Forsearching:\n16: Givenanytermt,queryDtoascertainwhetheracorrespondingPrimaryTermexists.\n17: iftexistsinDthen\n18: RetrievethecorrespondingPrimaryTermpfromD.\n19: endif\n51"
  },
  {
    "id": 1560,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\nH Supplementaryreferences\n1. ShennongAlpha. ShennongAlpha Knowledge: NMM-0020, Solidago decurrens Herb Accessed: 2024-05-01.\nhttps://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-0020.\n2. ShennongAlpha. ShennongAlpha Knowledge: NMM-001L, Panax ginseng Rhizome and Root Accessed: 2024-\n05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-001l.\n3. ChinesePharmacopoeiaCommission.PharmacopoeiaofthePeople’sRepublicofChina:2020Edition:Volume\nI(ChinaMedicalSciencePress,2020).\n4. ShennongAlpha.ShennongAlphaKnowledge:NMM-012W,TetradiumruticarpumFruitAccessed:2024-05-01.\nhttps://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-012w.\n5. ShennongAlpha. ShennongAlpha Knowledge: NMM-0006, Ephedra equisetina vel intermedia vel sinica Stem-\nherbaceousAccessed:2024-05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-\n0006."
  },
  {
    "id": 1561,
    "text": "5. ShennongAlpha. ShennongAlpha Knowledge: NMM-0006, Ephedra equisetina vel intermedia vel sinica Stem-\nherbaceousAccessed:2024-05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-\n0006.\n6. ShennongAlpha. ShennongAlpha Knowledge: NMM-00AC, Cremastra appendiculata vel Pleione bulbocodi-\noidesvelyunnanensisPseudobulbAccessed:2024-05-01.https://shennongalpha.westlake.edu.cn/en-\nzh/knowledge/nmm-00ac.\n7. ShennongAlpha. ShennongAlpha Knowledge: NMM-0003, Ephedra sinica Stem-herbaceous Accessed: 2024-\n05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-0003.\n8. ShennongAlpha. ShennongAlpha Knowledge: NMM-0004, Ephedra intermedia Stem-herbaceous Accessed:\n2024-05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-0004.\n9. ShennongAlpha. ShennongAlpha Knowledge: NMM-0005, Ephedra equisetina Stem-herbaceous Accessed:\n2024-05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-0005."
  },
  {
    "id": 1562,
    "text": "9. ShennongAlpha. ShennongAlpha Knowledge: NMM-0005, Ephedra equisetina Stem-herbaceous Accessed:\n2024-05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-0005.\n10. ShennongAlpha. ShennongAlpha Knowledge:NMM-009X, Crataegus pinnatifidaFruit Accessed: 2024-05-01.\nhttps://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-009x.\n11. ShennongAlpha. ShennongAlpha Knowledge: NMM-009Y, Crataegus pinnatifida var major Fruit Accessed:\n2024-05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-009y.\n12. ShennongAlpha.ShennongAlphaKnowledge:NMM-01YF,TaraxacumunspecifiedHerbAccessed:2024-05-01.\nhttps://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-01yf.\n13. ShennongAlpha. ShennongAlpha Knowledge: NMM-005X, Ziziphus jujuba var spinosa Seed Accessed: 2024-\n05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-005x.\n14. ShennongAlpha.ShennongAlphaKnowledge:NMM-001S,PanaxginsengLeaf Accessed:2024-05-01.https:\n//shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-001s."
  },
  {
    "id": 1563,
    "text": "14. ShennongAlpha.ShennongAlphaKnowledge:NMM-001S,PanaxginsengLeaf Accessed:2024-05-01.https:\n//shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-001s.\n15. ShennongAlpha. ShennongAlpha Knowledge: NMM-01MZ, Vincetoxicum pycnostelma Rhizome and Root Ac-\ncessed:2024-05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-01mz.\n16. ShennongAlpha. ShennongAlpha Knowledge: NMM-01MX, Vincetoxicum pycnostelma Rhizome Accessed:\n2024-05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-01mx.\n17. ShennongAlpha.ShennongAlphaKnowledge:NMM-01MY,VincetoxicumpycnostelmaRootAccessed:2024-05-\n01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-01my.\n18. ShennongAlpha.ShennongAlphaKnowledge:NMM-003G,ZingiberofficinaleRhizomeAccessed:2024-05-01.\nhttps://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-003g.\n19. ShennongAlpha.ShennongAlphaKnowledge:NMM-003M,ZingiberofficinaleRhizomeFreshAccessed:2024-\n05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-003m."
  },
  {
    "id": 1564,
    "text": "19. ShennongAlpha.ShennongAlphaKnowledge:NMM-003M,ZingiberofficinaleRhizomeFreshAccessed:2024-\n05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-003m.\n20. ShennongAlpha.ShennongAlphaKnowledge:NMM-000T,CurcumawenyujinRhizomeAccessed:2024-05-01.\nhttps://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-000t.\n21. ShennongAlpha. ShennongAlpha Knowledge: NMM-0015, Curcuma wenyujin Rhizome Freshly-sliced Ac-\ncessed:2024-05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-0015.\n22. ShennongAlpha.ShennongAlphaKnowledge:NMM-024C,FritillariathunbergiiBulbZhejiangAccessed:2024-\n05-14.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-024c.\n23. ShennongAlpha.ShennongAlphaKnowledge:NMM-0001,ArtemisiaannuaPart-aerialAccessed:2024-05-01.\nhttps://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-0001.\n24. ShennongAlpha. ShennongAlpha Knowledge: NMM-0002, Artemisia annua Part-aerial Segmented Accessed:"
  },
  {
    "id": 1565,
    "text": "https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-0001.\n24. ShennongAlpha. ShennongAlpha Knowledge: NMM-0002, Artemisia annua Part-aerial Segmented Accessed:\n2024-05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-0002.\n25. ShennongAlpha. ShennongAlpha Knowledge: NMM-000B, Ephedra sinica Stem-herbaceous Segmented and\nAquafried-honeyAccessed:2024-05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/\nnmm-000b.\n52"
  },
  {
    "id": 1566,
    "text": "ShennongAlpha: anAI-drivensharingandcollaborationplatformforintelligentcuration,acquisition,andtranslation\nofnaturalmedicinalmaterialknowledge\n26. ChinesePharmacopoeiaCommission.PharmacopoeiaofthePeople’sRepublicofChina:2020Edition:Volume\nIV (ChinaMedicalSciencePress,2020).\n27. ShennongAlpha. ShennongAlpha Knowledge: NMM-00A1, Crataegus pinnatifida Fruit Cleaned and Stirfried-\ngolden Accessed: 2024-05-01. https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-\n00a1.\n28. ShennongAlpha. ShennongAlpha Knowledge: NMM-00A3, Crataegus pinnatifida Fruit Cleaned and Stirfried-\ncharred Accessed: 2024-05-01. https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-\n00a3.\n29. ShennongAlpha. ShennongAlpha Knowledge: NMM-003L, Zingiber officinale Rhizome Cleaned and Stirfried-\nsandAccessed:2024-05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-003l.\n30. ShennongAlpha. ShennongAlpha Knowledge: NMM-01XX, Talc Accessed: 2024-05-01. https : / /"
  },
  {
    "id": 1567,
    "text": "sandAccessed:2024-05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-003l.\n30. ShennongAlpha. ShennongAlpha Knowledge: NMM-01XX, Talc Accessed: 2024-05-01. https : / /\nshennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-01xx.\n31. ShennongAlpha. ShennongAlpha Knowledge: NMM-01XZ, Talc Pulverized Accessed: 2024-05-01. https://\nshennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-01xz.\n32. ShennongAlpha. ShennongAlpha Knowledge: NMM-0022, Erycibe obtusifolia Stem Accessed: 2024-05-01.\nhttps://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-0022.\n33. ShennongAlpha. ShennongAlpha Knowledge: NMM-0024, Erycibe obtusifolia vel schmidtii Stem Accessed:\n2024-05-01.https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-0024.\n34. ShennongAlpha. ShennongAlpha Knowledge: NMM-00D7, Ligustrum lucidum Fruit Accessed: 2024-05-01.\nhttps://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-00d7.\n35. ShennongAlpha.ShennongAlphaKnowledge:NMM-008W,PrunusmumeFruitAccessed:2024-05-01.https:"
  },
  {
    "id": 1568,
    "text": "https://shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-00d7.\n35. ShennongAlpha.ShennongAlphaKnowledge:NMM-008W,PrunusmumeFruitAccessed:2024-05-01.https:\n//shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-008w.\n36. ShennongAlpha.ShennongAlphaKnowledge:NMM-000G,EphedrasinicaRootAccessed:2024-05-01.https:\n//shennongalpha.westlake.edu.cn/en-zh/knowledge/nmm-000g.\n37. CommonMarkhttps://commonmark.org/.2023.\n38. mdastDevelopers.mdast:MarkdownAbstractSyntaxTreeGitHubrepository.2023.https://github.com/\nsyntax-tree/mdast.\n53"
  }
]